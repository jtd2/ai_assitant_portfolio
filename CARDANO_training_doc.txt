---
title: What is a blockchain?
metaTitle: What is a blockchain?
---

A blockchain is a type of database or ledger that is duplicated and distributed
to all participants within the blockchain network. It is made up of a set of
interconnected nodes that store data or items of value in blocks. These blocks
are validated, cryptograhpically secured, and linked to each other in
chronological order in a chain. The information stored in the validated blocks
of the blockchain cannot be altered later on. It is permanently inscribed in the
distributed ledger.

Blockchain technology, otherwise known as distributed ledger technology (DLT),
provides a decentralized and accessible data structure for various records. Such
records might include financial payment and transaction details, as well as
other types of information – from commerce to internet of things (IoT) records.

As a blockchain stores data in a decentralized manner, it is independent of
centralized, controlling entities, or middlemen. This provides enhanced
transparency of data storage and its management. An important feature of
blockchain is that it stores records immutably, which means that they cannot be
changed, forged, or deleted, as this will break the chain of records.

Blockchain can be compared to a book of permanent records, where every page acts
as an information holder:

![chain-of-blocks](chain-of-blocks.png)

Let’s take a closer look at existing data storage solutions to understand the
difference between these systems:

![data-storage](https://ucarecdn.com/af01444d-6881-4aab-a0a9-ec2ff6a0ea40/)

- **Centralized systems** — all data entries and activities are usually managed
  using one central server. This increases the risk of a single point of
  failure, and also means that the controlling entity (such as banks or
  government institutions, for example) act as decision-makers.
- **Distributed systems** — generally rely on multiple server nodes, each of
  which serves a subset of the total end clients.
- **Decentralized systems** — all data and records of transactions are stored
  not in one server, but in a system of interconnected, independent nodes and
  terminals. This ensures independence from centralized entities, transparency,
  and security.

Finally, blockchains not only provide an immutable and secure database but also
act as a functional environment to transact funds, create digital currencies,
and process complex deals using digital agreements
[(smart contracts)](/about-cardano/new-to-cardano/what-is-a-smart-contract).

---
title: What is a cryptocurrency?
metaTitle: What is a cryptocurrency?
---

A _cryptocurrency_ is a digital asset, which is stored on the ledger and is
designed to serve as a medium of exchange for goods or services. It is otherwise
called _crypto_.

Blockchain ledgers serve as the underlying technology for cryptocurrency
creation in a decentralized environment. Blockchain protocols use rigorous
cryptography techniques to enable the minting (creation) of cryptocurrency and
to secure and verify crypto ownership and fund movement records. The price of
cryptocurrency is not controlled by a government or centralized financial
institution. It is defined by its value, correlation to real-world figures, and
is driven by market supply and demand.

Addresses are used when sending cryptocurrency payments. They are unique
identifiers and are represented by a string of numbers and letters that are
obtained from the user's public keys.

### Ada is the native currency of Cardano

Each blockchain ledger has its underlying cryptocurrency or native currency. Ada
is the native, or principal currency on [Cardano](https://cardano.org/). This
means that ada is the main payment unit on Cardano; it is accepted as
fee-payment, to make deposits, and is also the only currency in which rewards
are distributed.

Lovelace is the smallest denomination of ada. One ada = 1,000,000 lovelaces. Ada
has six decimal places, which makes it easily divisible into smaller fractions.

### Native tokens

Cardano also supports the creation of native tokens ‒ digital assets that are
created for specific purposes. This means that users, developers, and businesses
can use the Cardano blockchain to create tokens that represent a footprint of
value (whether defined by the community, market state, or self-governed entity).
A token can be fungible (interchangeable) or non-fungible (unique), and act as a
payment unit, reward, trading asset, or information holder.

:::info

Related topics:

- [Cardano monetary policy](/about-cardano/explore-more/monetary-policy)
- [Learn about native tokens](/developer-resources/native-tokens).

:::

---
title: Proof of stake
metaTitle: Proof of stake
---

Proof of stake (PoS) is a type of
[consensus mechanism](/about-cardano/learn/consensus-explained) or protocol that
uses the amount of stake (or value) held in the system to determine consensus.
In essence, a consensus protocol is what controls the laws and parameters
governing the behavior of blockchains. Think of consensus as a ruleset that each
network participant adheres to. As blockchains are not controlled by any single,
central authority, a consensus protocol is used instead to allow distributed
network participants to agree on the _history_ of the network captured on the
blockchain – to reach consensus on what has happened, and continue from a single
source of truth.

Cardano is built on the ground-breaking PoS consensus protocol
[Ouroboros](https://iohk.io/en/blog/posts/2022/06/03/from-classic-to-chronos-the-implementations-of-ouroboros-explained/),
and the first blockchain consensus protocol to be developed through
peer-reviewed research. At the heart of the protocol are
[stake pools](/about-cardano/learn/stake-pools), reliable server nodes run by a
stake pool operator to which ada holders can delegate their stake. Stake pools
are used to ensure that everyone can participate in the protocol, regardless of
technical experience or availability to keep a node running. These stake pools
focus on maintenance and hold the combined stake of various stakeholders in a
single entity.

## How does proof of stake compare to proof of work protocols?

In contrast, proof of work (PoW) is a synchronous protocol that encourages
miners to compete to be the first who can solve any problems within the block. A
rewards system is used to incentivize this problem-solving. However, this
approach comes at a cost, with increased electricity usage and longer time spans
to solve problems within the chain. These factors can slow the network down
significantly and means it is costly to maintain.

## Features of proof of stake

One of the key features of PoS is that as a user's value increases, the
opportunity to maintain the ledger also increases. This means a higher chance to
produce new blocks that can be added to the blockchain and timestamped
accordingly. The creator of a new block is chosen based on a combination of
random selection and a determination of their stake, or wealth. A type of leader
election occurs within the chain. It is worth remembering that within a proof of
stake protocol, participants accumulate the transaction fees thereby adding to
their wealth as they go. This approach encourages steady and stable growth of
the blockchain and reduces the instances of stalled transactions that can
prevent chain growth.

## Primary advantages of proof of stake

Some of the primary advantages of PoS over PoW include:

- rigorous security protocols are incorporated into a PoS protocol
- reduced centralization - the risk of centralization is reduced by issuing
  penalties for selfish practices within the network
- energy efficiency - energy consumption is extremely efficient as a smaller
  amount of electricity, as well as hardware resources, are needed to produce
  and run the blockchain
- cost efficiencies - PoS currencies are far more cost-effective than those
  operating on PoW protocols.

---
title: Cardano tracking tools
metaTitle: Cardano tracking tools
---

Since Cardano is a public blockchain ledger, it is possible to easily track all
recent transactions, block details, and epoch data using different tools.

## Exploring transactions and blocks

**Cardano explorers**

There is a list of available [Cardano explorers](https://explorer.cardano.org). These explorers provide user-oriented tools that
fetch data from the main database and reflect it in a straightforward and
convenient web interface.

Explorers usually show epoch details, which include:

- a number of blocks produced during this epoch
- time the epoch started
- time of last produced block
- number of processed transactions
- total output in ada.

By choosing a specific block, you can explore it in more detail to see its ID,
size, epoch and block details, number of included transactions, and
confirmations.

You can also search for specific epochs, transactions, or blocks by pasting their
IDs in the search field.

## Exploring assets

Cardano supports multi-asset creation and management. To see a list of created
assets and tokens, you can use these tools:

- [AdaStat (tokens)](https://adastat.net/tokens)
- [Cardano Assets](https://cardanoassets.com/)
- [Cardanoscan (tokens)](https://cardanoscan.io/tokens)
- [Pool.pm (tokens)](https://pool.pm/tokens)
- [Cexplorer (tokens)](https://cexplorer.io/token)

## Exploring stake pools

To find a list of all registered stake pools, their tickers, pool names, and
IDs, you can use these tools:

- [Cardano PoolTool](https://pooltool.io/)
- [Cardanoscan (pools)](https://cardanoscan.io/pools)
- [Pool.pm (pools)](https://pool.pm/search)

:::tip

If you'd like to include additional references to ecosystem tracking tools, please submit a pull request.

:::

---
title: Types of wallets
metaTitle: Types of wallets
---

A blockchain wallet is a safe and secure place where users can keep their
digital assets.

:::warning

Please remember that it's only safe to store your digital assets in a wallet where you control the private keys (the password granting access to your assets). Storing digital assets on exchanges is highly discouraged.

:::

Blockchain wallets come in different forms, from desktop and mobile
applications to hardware (USB-like) devices and paper wallets. All wallets are
designed to enable the storage of digital assets and to allow the sending and
receiving of those. Technically, assets are not stored in the wallet ‒
their records live on the blockchain and can be accessed using unique private
keys, which grant the right to make transactions. Losing your private keys means
losing access to your digital assets. That is why it is crucial to keep your keys
safe and offline.

The difference between the wallet types lies in their security and supported
functionality. Paper wallets are the most secure, for example, but not as
convenient in use; some wallets support a limited number of assets, some
lack certain features (like delegation, for instance).

Let's take a closer look at the pros and cons of different wallet types:

- **Paper wallets** — these are paper documents that contain public and private
  key details. Such documents can be stored in a safe place and are highly
  secure as they can’t be compromised via online attacks. Paper wallets are best
  to use for long-term savings. On the other hand, the assets linked to paper
  wallets are not possible to operate online, as
  they will have to be moved to other wallets first.
- **Hardware wallets** — these are devices that store digital assets offline
  and can be connected to a computer to access them. Hardware wallets are
  secure and their benefit lies in the balance of offline security and
  convenience as they can be carried around and used when needed. The
  disadvantage of the hardware wallet is the risk of device loss or damage.
- **Online wallets** — these wallets can be commonly set up using a browser.
  They work through the internet and store assets in a certain
  application or software. This is convenient as users can send, receive, and
  use their digital assets as in any other bank account or payment system. However, online
  wallets are more open to security issues and can be compromised or attacked.
  _Search online for the most secure and suitable wallet; always keep your keys
  offline and use 2FA verification where possible (giving preference to security
  keys over SMS verification)._
- **Desktop wallets** — these wallets can be downloaded on a personal computer.
  They are secure and convenient in use. Before choosing the best-matching
  solution, it is important to ensure that your computer meets software
  requirements and that the wallet supports the asset you would like to
  work with. Desktop wallets are easy to use, but are not as flexible or
  portable.
- **Mobile wallets** — these are wallet applications that can be installed
  directly on a smartphone. Such wallets are simple to install and use and are a
  good additional option for digital asset storage.

## Where to store ada?

Different wallets support ada. Take a look at some possible options below to choose the one that matches your needs best.

**Daedalus**

Daedalus is a full node, desktop wallet. Daedalus downloads a full copy of the
Cardano blockchain and independently validates every transaction in its history.
This provides maximum security and completely trustless operation, without
centrally-hosted third-party servers. It supports all major desktop operating
systems and provides the following features:

- easy installation with one-click setup of bundled Cardano node
- locally stored wallets and encrypted private keys, not shared with third-party
  servers
- trustless operation with a locally running full Cardano node which independently
  validates full transaction history of the blockchain
- supports Cardano network by participating in the Ouroboros protocol
- wallet backup and restoration using mnemonics
- support for staking and delegation
- support for voting
- complete autonomy without reliance on third-party servers and services
- paper wallet generator for offline storage of funds

To install Daedalus, follow these steps:

1.  First, read
    [system requirements](https://iohk.zendesk.com/hc/en-us/articles/360010496553).
    Please note that Daedalus is a resource-intensive application. If you want
    to manage ada quickly and easily you can try Lace, Nami, Yoroi, or other wallets.
2.  Visit the official [Daedalus website.](https://daedaluswallet.io/)
3.  Follow
    [these installation instructions.](https://iohk.zendesk.com/hc/en-us/articles/360011602173-Quick-start-guide#:~:text=Go%20to%20https%3A%2F%2Fdaedaluswallet,Daedalus%20wallet%20on%20your%20Machine.)

**Hardware wallets**

Here is a list of hardware wallets to consider for storing and transacting with
ada:

1.  Trezor Model T
2.  [Ledger Nano S Plus](https://shop.ledger.com/pages/ledger-nano-s-plus)  
3.  [Ledger Nano X](https://shop.ledger.com/pages/ledger-nano-x)

See
[how to use hardware wallets with Daedalus.](https://iohk.zendesk.com/hc/en-us/articles/900004722083-How-to-use-Ledger-and-Trezor-HW-with-Daedalus)

**Other options**

- [Lace](https://www.lace.io/)
- [Yoroi](https://yoroi-wallet.com/)
- [Eternl](https://eternl.io/app/mainnet/welcome)
- [GeroWallet](https://gerowallet.io/)
- [Typhon](https://typhonwallet.io/#/)
- [Ellipal](https://www.ellipal.com/)
- [AdaLite](https://adalite.io/)
- [Infinito Wallet](https://www.infinitowallet.io/)
- [Atomic Wallet](https://atomicwallet.io/)
- [Guarda](https://guarda.com/)
- [Coin Wallet](https://coin.space/)
- [NuFi](https://nu.fi/) 
- [NOW Wallet](https://walletnow.app)

---
title: How to delegate?
metaTitle: How to delegate?
---

Ada that is held on Cardano represents its holder’s stake in the protocol. The
size of the stake is proportional to the amount of ada held. Users who hold a
stake in Cardano can get passive rewards for validating blocks.

Since not everyone has the time, knowledge, or resources to run a stake pool,
ada holders can delegate their stake to a preferred pool and let an operator
maintain their stake on their behalf. This allows everyone to participate in the
consensus and get rewards without having to continuously run the node online.
The higher the stake in a pool, the more rewards will be assigned to its
owners.

:::note

Note that you can spend your ada at any time, regardless of how you delegated it.

:::

Ada holders can delegate their stake using different ecosystem wallets that support this functionality. Here are
some useful articles to review:

- [How to stake your ada](https://www.essentialcardano.io/infographic/how-to-stake-your-ada)
- [How to choose a stake pool](https://iohk.zendesk.com/hc/en-us/articles/900002174303-How-to-choose-a-stake-pool)
- [How safe is it to delegate to a stake pool?](https://iohk.zendesk.com/hc/en-us/articles/900002046123-How-safe-is-it-to-delegate-to-a-stake-pool-)
- [How to delegate to a stake pool](https://iohk.zendesk.com/hc/en-us/articles/900005718683-How-to-Delegate-to-a-stake-pool)
  using Daedalus
- [Staking and delegating for beginners](https://forum.cardano.org/t/staking-and-delegating-for-beginners-a-step-by-step-guide/36681)
  (Daedalus)
- [How to delegate from the Yoroi wallet](https://forum.cardano.org/t/cardano-shelley-how-to-delegate-from-the-yoroi-wallet/38230)

:::tip

If you'd like to include additional references to delegation tutorials, please submit a pull request.

:::

---
title: Staking calculator
metaTitle: Staking calculator
---

import StakingCalculator from '../../../src/components/StakingCalculator/StakingCalculator'

Staking calculator is a tool used to predict approximate rewards a user may
receive by delegating ada to a stake pool. Note that the rewards predicted by
this calculator are only an **estimate**.

The actual amount of ada received in rewards may vary, and will depend on a
number of factors, including actual stake pool performance, which is the number
of blocks a stake pool is observed to produce in a given epoch versus the number
it was expected to produce. Changes to network parameters may also affect
rewards.

The annualized equivalent returns given by this calculator assume that stake is
delegated to the same stake pool for a 365-day period, and that stake pool
performance and other settings are consistent over that timeframe. IOG accepts
no responsibility for any discrepancy between estimated and actual rewards,
**this calculator is provided for guidance only.**

<StakingCalculator />

---
title: What is a smart contract?
metaTitle: What is a smart contract?
---

Smart contracts on Cardano are simple programs often referred to as validator
scripts in which users define custom logic. They are automatically executed by
each Cardano node validating the transaction when the transaction attempts to
move funds from the script's address. The address of each script is derived from
the hash-sum of the compiled script.

Once deployed, smart contracts become immutable, which means they cannot be
changed. They are distributable and tamper-proof, fast, and cost-effective, as
there is no intermediary, which saves both money and time.

## Tooling and languages

Cardano introduced smart contract support in 2021. As a multi-functional
environment, Cardano now supports the development and deployment of smart
contracts using different programming languages. Here are some examples:

- [Plutus](https://plutus.readthedocs.io/en/latest/) — a purpose-built smart
  contract development and execution platform. Plutus contracts consist of parts
  that run on the blockchain (on-chain code) and parts that run on a user’s
  machine (off-chain or client code). Plutus draws from modern language research
  to provide a safe, full-stack programming environment based on Haskell, the
  leading functional programming language.
- [Marlowe](https://docs.marlowe.iohk.io/docs/introduction) — a domain-specific
  language (DSL) for writing and executing financial contracts that allows
  building contracts visually as well as in more traditional code. Financial
  institutions can use it to develop and deploy custom instruments for their
  customers and clients, for example. The Marlowe language itself is now
  embedded in JavaScript, TypeScript, and Haskell offering a choice of editors
  depending on developers’ preferences and skills.
- [Aiken](https://aiken-lang.org/) - a language and toolchain favoring developer
  experience. Aiken is used for on-chain validator scripts only.
- [OpShin](https://github.com/OpShin) - a programming language based on Python
  for generic smart contracts for Cardano.
- [plu-ts](https://github.com/HarmonicLabs/plu-ts) - TypeScript-embedded smart
  contract programming language and a transaction creation library.

:::info

Discover more ecosystem
[builder tools here.](https://developers.cardano.org/tools)

Further reading and related topics:

- [Developer Portal - smart contracts overview](https://developers.cardano.org/docs/smart-contracts/)
- [A list of community-built developer tools on Cardano](https://www.essentialcardano.io/article/a-list-of-community-built-developer-tools-on-cardano)

:::

---
title: New to Cardano?
metaTitle: New to Cardano?
sidebar_position: 2
---

The understanding of basic concepts plays an integral role in getting acquainted
with blockchain and [Cardano](https://cardano.org/) in particular. In this
section, we start our journey with basic concepts such as the notion of
blockchain, understanding
proof of stake, and what is a
cryptocurrency. You will also find out about various types of wallets and tracking tools, and learn about the delegation options. 

---
title: Cardano nodes
metaTitle: Cardano nodes
---

The
[Cardano node](https://github.com/input-output-hk/cardano-node#overview-of-the-cardano-node-repository)
is the top-level component within the network. Network nodes connect to each
other within the networking layer, which is the driving force for delivering
information exchange requirements. This includes new block diffusion and
transaction information for establishing a better data flow. Cardano nodes
maintain connections with peers that have been chosen via a custom
peer-selection process. By running a Cardano node, you are participating in and
contributing to the network.

## How does it work?

[Stake pools](/about-cardano/learn/stake-pools) use the Cardano node to validate
how the pool interacts with the network and are responsible for transaction
processing and block production. They act as reliable server nodes that hold and
maintain the combined stake of various stakeholders in a single entity.

## Producing a block

The goal of blockchain technology is the production of an
independently-verifiable and cryptographically-linked chain of records (blocks).
A network of block producers works to collectively advance the blockchain. A
[consensus](/about-cardano/learn/consensus-explained) protocol provides
transparency and decides which candidate blocks should be used to extend the
chain.

Submitted valid transactions might be included in any new block. A block is
cryptographically signed by its producer and linked to the previous block in the
chain. This makes it impossible to delete transactions from a block, alter the
order of the blocks, remove a block from the chain (if it already has a number
of other blocks following it), or insert a new block into the chain without
alerting all the network participants. This ensures the integrity and
transparency of the blockchain expansion.

## Slots and epochs

The Cardano blockchain uses the
[Ouroboros Praos](https://eprint.iacr.org/2017/573.pdf) protocol to facilitate
consensus on the chain. Ouroboros Praos divides time into epochs. Each Cardano
epoch consists of a number of slots, where each slot lasts for one second. A
Cardano epoch currently includes 432,000 slots (5 days). In any slot, zero or
more block-producing nodes might be nominated to be the slot leader. On average,
one node is expected to be nominated every 20 seconds, for a total of 21,600
nominations per epoch. If randomly elected slot leaders produce blocks, one of
them will be added to the chain. Other candidate blocks will be discarded.

### Slot leader election

The Cardano network consists of a number of stake pools that control the
aggregated stake of their owners and other stakeholders, also known as
_delegators_. Slot leaders are randomly elected from among the stake pools. The
more stake a pool controls, the greater the chance it has of being elected as a
slot leader to produce a new block that is accepted into the blockchain. This is
the basic concept of
[proof of stake (PoS)](/about-cardano/new-to-cardano/proof-of-stake). To
maintain a level playing field, and prevent a situation where a small number of
very large pools control the majority of stake, Cardano has an incentive system
that discourages delegation to pools that already control a large portion of the
total stake.

### Transaction validation

When validating a transaction, a slot leader needs to ensure that the sender has
included enough funds to pay for that transaction _and_ must also ensure that
the transaction’s parameters are met. Assuming that the transaction meets all
these requirements, the slot leader will record it as a part of a new block,
which will then be connected to other blocks in the chain.

:::info

Related topics:

- [About the Cardano network](/about-cardano/explore-more/cardano-network)

:::

---
title: Stake pools
metaTitle: Stake pools
---

A stake pool is a reliable server node that focuses on ledger maintenance and
holds the combined resources - the 'stake' - of various stakeholders in a single
entity. Stake pools are responsible for processing transactions that will be
placed in the ledger, as well as producing new blocks. Stake pools are at the
core of
[Ouroboros](https://iohk.io/en/blog/posts/2022/06/03/from-classic-to-chronos-the-implementations-of-ouroboros-explained/),
Cardano's proof-of-stake protocol.

To be secure, Ouroboros requires a good number of stakeholders to be online and
maintain sufficiently good network connectivity at any given time. This is why
Ouroboros relies on stake pools, entities that are committed to running the
protocol 24/7, on behalf of the contributing stakeholders that hold ada. The
idea is that these resource holders can bring their resources (their stake)
together and form a _pool_, where typically one holder is the operator of the
pool and the rest are delegators. Typically, the stake pool operator (SPO)
installs and runs software compatible with the platform (the server node), while
delegators take a more passive role. They delegate their stake to the pool.

While Ouroboros is cheaper to run than a proof-of-work protocol, running
Ouroboros still incurs some costs. Therefore, SPOs are rewarded for running the
protocol in the form of incentives that come from the transaction fees and
inflation of the circulating ada supply.

:::info

Further reading:

- [About stake pools, operators, and owners](/stake-pool-operators/about-stake-pools)

:::

---
title: Delegation
metaTitle: Delegation
---

As Cardano is a proof of stake (PoS) system, owning ada not only allows you to
buy goods or services, but also confers upon you the right and obligation to
participate in the protocol and create blocks. Stake delegation is a mechanism
inherent in the [Ouroboros protocol](https://eprint.iacr.org/2016/889.pdf) that
allows the protocol to scale even in a setting where the set of stakeholders
might be highly fragmented.

Anyone who owns ada can participate in stake delegation while retaining their
spending power. Note that you can spend your ada normally at any time,
regardless of how you delegated it. This mechanism will enable stakeholders to
participate in the slot leader election process in each epoch.

Stake delegation gives rise to _stake pools_ that act similarly to mining pools
in the Bitcoin protocol. Stake pool operators (SPOs) _must_ be online to
generate blocks if they are selected as slot leaders.

There are three options ada holders can consider for delegating their stake:

1. Run their own stake pool
2. Agree with a third party to run a private stake pool for them
3. Delegate to other stake pools.

:::note

If you are not familiar with Cardano CLI or stake pool operations, see
[delegation tips here.](/about-cardano/new-to-cardano/how-to-delegate)

:::

## Stake delegation requirements

Delegating stake requires posting two certificates to the chain: a staking
address registration, and a delegation certificate. Posting certificates
requires funds, so a user setting up their first wallet will need a
bootstrapping mechanism. This mechanism relies on the possibility of base
addresses using a
[staking key](https://github.com/input-output-hk/cardano-rosetta/tree/master/examples#staking-key-registration-and-delegation)
before posting the registration certificate for that key. Note that the stake
address can be based either on a single key or on a script such as
multi-sig.

## Delegation scheme

With the concept of delegation, any stakeholder can allow a stake pool to
generate blocks for the Cardano network. Then the protocol will distribute the
rewards to all participants, including the fees for the SPO. A stakeholder
delegates to a particular pool ID, which is the hash of the operator's
[verification key](/about-cardano/learn/cardano-keys#vrf-keys).

To limit the delegate’s block generation power to a certain range of epochs and
slots, the stakeholder can limit the proxy signing key’s valid message space to
strings ending with a slot number in a specific range of values. This simple
scheme is secure due to the verifiability and prevention of misuse properties of
proxy signature schemes. This ensures that any stakeholder can verify that a
proxy signing key was actually issued by a specific stakeholder to a specific
delegate, and that the delegate can only use these keys to sign messages inside
the key’s valid message space, respectively.

The funds belonging to one staking key of a user’s wallet require posting a
single transaction, containing a delegation certificate. This will only incur
the usual transaction fees. In particular, a stakeholder needs to pay a deposit
for registering a stake address and not for the stake delegation itself. Once a
stake address is registered, the stakeholder will only pay fees to set the
delegation choice.

Note that the stakeholder's stake will count for the pool's stake during the
reward calculation.

## Stake delegation scenario

Imagine a user who is about to receive their first ada, through redemption, from
a trade on an exchange, or some other source. They will set up a new wallet, and
create an address to receive those funds. This address will be a base address,
using a staking key that is generated by the wallet, but not yet registered on
the chain.

After receiving the initial funds, the user can then participate in staking, by
posting a staking key registration certificate, and a delegation certificate for
their staking key. Once the key is registered, newly created addresses can be
pointer addresses to the staking key registration certificate.

:::info

Related topics:

- [Operating a stake pool](https://developers.cardano.org/docs/operate-a-stake-pool/)
- [Advice for stakeholders - delegators and stake pool operators](https://iohk.io/en/blog/posts/2020/11/13/the-general-perspective-on-staking-in-cardano/)

:::

---
title: Pledging and rewards
metaTitle: Pledging and rewards
---

Pledging is an important mechanism that encourages the growth of a healthy
ecosystem within the Cardano blockchain. When you register a stake pool you can
choose to pledge some, or all, of your ada to the pool, to make it more
attractive to people who want to delegate. Although pledging is not required
when setting up a stake pool, it can make the pool more attractive to delegators, as the higher the amount of ada that is pledged, the higher the
rewards that will be paid out. The a0 protocol parameter defines the influence
of the pledge on the pool reward.

## Distributing rewards 

During each epoch, rewards are distributed amongst all stakeholders who have delegated to a stake pool, either to their own stake pool, or another pool. These rewards are auto-generated by the protocol and are not managed by the stake pool operators (SPOs). Rewards come from two sources:

* All transaction fees: collated from the set of transactions included in a block that was minted during that epoch.
* Monetary expansion: involves distinguishing between the total supply of ada and the maximal supply of ada. The total supply consists of all ada currently in circulation, plus the ada in the treasury. The maximal supply is the maximal amount of ada that can ever exist. The difference between these two figures is called the *reserve*. During each epoch, a fixed but parameterizable percentage of the remaining reserve is taken from the reserve and used for epoch rewards and treasury, where the amount being sent to the treasury is a fixed percentage of the amount taken from the reserve.

Learn more about rewards sharing in the [rewards sharing and pledge on Cardano video](https://www.youtube.com/watch?v=EAzyN3H8MOA&t=7s). 

The following formula outlines how the reward mechanism works. The available rewards amount, transaction fees, plus monetary expansion, are denoted by R.
First, the share of all available rewards that a specific pool can receive is determined, as follows:

![pledge_formula](pledge_formula.png)

These elements are defined as follows:

- R - total available rewards for this epoch
- a<sub>0</sub> - pledge influence factor (can be between 0 and infinity)
- z<sub>0</sub> - relative pool saturation size, i.e. 0.5% based on a number of
  desired pools (k=200)
- σ - stake delegated to the pool (including stake pledged by the owners and
  stake delegated by others)
- σ’ = min(σ, z<sub>0</sub>) - as σ, but capped at z<sub>0</sub>
- s - stake pledged by the owners
- s’ = min(s, z<sub>0</sub>) - as s, but capped at z<sub>0</sub>

Note that z<sub>0</sub>, σ and s are all relative, so they are fractions of the
total supply, as they all lie between zero and one.

Two important considerations are:

1. Rewards increase with σ, but stop increasing once σ reaches z<sub>0</sub>, that is once the pool becomes saturated.
2. If a<sub>0</sub> (the pledge influence) is zero, this formula simply becomes R·σ’,
   so it is proportional to the pool's stake, up to the point of saturation. For larger values of a<sub>0</sub>, the pledge s becomes more important.
   
Remember that the pledge is declared during pool registration (alongside the cost and margin values),
and has to be honored by the pool owners who are delegating to the pool.
If they collectively delegate less than the declared pledge, pool rewards for that epoch will be zero. Note that the pool will be public if its operator margin is set to less than 100%.

The rewards that are produced by this formula are now adjusted by pool's
performance: we multiply by β/σ<sub>a</sub>, where β is the fraction of all blocks produced
by the pool during the epoch and σ<sub>a</sub> is the stake delegated to the pool relative to
the active stake (ie, total stake that is correctly delegated to a non-retired pool).

For an optimally performing pool (a pool producing all the blocks that it can produce),
this factor will be 1, on average. The actual value will fluctuate
due to the stochastic nature, or random process of the [Ouroboros Praos](https://eprint.iacr.org/2017/573.pdf) consensus
algorithm.

After pool rewards have been calculated and adjusted for pool performance, they
are distributed amongst the pool operator and the pool members, or people who
delegated part, or all of their stake, to the pool. This happens in several
steps:

- First, the declared costs are subtracted and given to the pool operator
- Next, the declared margin is subtracted and given to the pool operator
- Finally, the remainder is split fairly (proportional to delegated stake),
  amongst all stakeholders who delegated to the pool, including the pool owners.

The actual amount of ada
received in rewards may vary, and will depend on a number of factors including;
the actual stake pool performance, that is, the number of blocks a stake pool is
observed to produce in a given epoch versus the number it was *expected* to
produce. Changes to network parameters may also affect rewards.

---
title: Consensus explained
metaTitle: Consensus explained
---

Consensus is the process of reaching a majority opinion by everyone involved in
running the blockchain. An agreement must be made on which blocks to produce,
which chain to adopt and to determine the single state of the network. The
consensus protocol determines how individual nodes assess the current state of
the ledger system and reach a consensus.

Blockchains create consensus by allowing participants to bundle transactions
that others have submitted to the system in blocks, and add them to their chain
(sequence of blocks). Determining who is allowed to produce a block when, and
what to do in case of conflicts, (such as two participants adding different
blocks at the same point of the chain), is the purpose of the different
consensus protocols.

The protocol has three main responsibilities:

- perform a leader check and decide if a block should be produced
- handle chain selection
- verify produced blocks.

## About Ouroboros

Cardano runs on the Ouroboros consensus protocol, which was delivered with
several peer-reviewed papers presented at top-tier conferences and publications
in the area of cybersecurity and cryptography. Rather than relying on 'miners'
(as in proof-of-work protocols) to solve computationally complex equations to
create new blocks – and rewarding the first to do so – proof of stake selects
[stake pools to create new blocks](/about-cardano/learn/cardano-node#how-does-it-work)
based on the stake they control in the network.

## How Ouroboros works

Ouroboros divides time on Cardano into epochs where each epoch is divided into
slots. A slot is a short period of time in which a block can be created.
Grouping slots into epochs is central to adjusting the leader election process
to the dynamically changing stake distribution.

Central to Ouroboros’ design is that it must retain its security in the presence
of attacks. As such, the protocol has built-in tolerance to prevent attackers
from propagating alternative versions of the blockchain and assumes that an
adversary may send arbitrary messages to any participant at any time. The
protocol is guaranteed to be secure in the so-called synchronous setting (that
is, with strong guarantees on message delivery times) so long as more than 51%
of the stake is controlled by honest participants (ie, those following the
protocol).

A slot leader is elected for each slot, who is responsible for adding a block to
the chain and passing it to the next slot leader. To protect against adversarial
attempts to subvert the protocol, each new slot leader is required to consider
the last few blocks of the received chain as transient: only the chain that
precedes the prespecified number of transient blocks is considered settled. This
is also referred to as the settlement delay. Among other things, this means that
a stakeholder can go offline and still be synced to the blockchain, so long as
it’s not for more than the settlement delay.

Within the Ouroboros protocol, each network node stores a copy of the
transaction mempool – where transactions are added if they are consistent with
existing transactions – and the blockchain. The locally stored blockchain is
replaced when the node becomes aware of an alternative, longer valid chain.

Read the following section to learn more about the different types of Ouroboros.

---
title: Ouroboros overview
metaTitle: Ouroboros overview
---

In mythology, [Ouroboros (also, *uroboros*)](https://en.wikipedia.org/wiki/Ouroboros) is usually depicted as a snake (or sometimes a dragon) eating its own tail in a closed circle. The word *Ouroboros* itself derives from Ancient Greek, its literal meaning being 'tail eating' or 'tail devourer.' 

As a symbol, Ouroboros represents the infinity of time flowing back unto itself, in a never-ending cycle, as if caught in an eternal loop. Ouroboros first appeared in Egypt, in the 13th century BC. Later, alchemists adopted Ouroboros into their mystical symbolism.

Throughout the ages, Ouroboros has been interpreted and used in a variety of ways by a plethora of cultures. One of the most common interpretations is that the symbol represents the interconnectedness and infinity of the universe.

In 2017, [Charles Hoskinson](https://en.wikipedia.org/wiki/Charles_Hoskinson) adopted Ouroboros to name the proof-of-stake consensus protocol underlying Cardano. In this context, Ouroboros represents the possibility of infinite and ethical growth and scalability of the blockchain. Ouroboros' central message is the delivery of greater opportunities for the world, and its preservation through [much-reduced energy consumption](https://iohk.io/en/blog/posts/2021/08/17/why-they-re-calling-cardano-the-green-blockchain/).

## What is Ouroboros

[Ouroboros](https://eprint.iacr.org/2016/889.pdf) is the consensus protocol for Cardano, the first provably secure proof-of-stake protocol, and the first blockchain protocol based on peer-reviewed research. 

Combining unique technology and mathematically verified mechanisms (including behavioral psychology and economic philosophy principles), Ouroboros guarantees and supports the security and sustainability of any blockchain implementing it. The result is a protocol with proven security guarantees, able to facilitate the propagation of global, permissionless networks with minimal energy requirements. Cardano is the first such network.

Ouroboros selects participants - stake pools, in this case - to create new blocks based on the stake they control in the network, and facilitates the creation of distributed, permissionless networks capable of sustainably supporting new markets.

## Ouroboros implementations

Ouroboros comes in different versions:

-   [Ouroboros Classic](https://iohk.io/en/research/library/papers/ouroborosa-provably-secure-proof-of-stake-blockchain-protocol/)
-   [Ouroboros BFT](https://iohk.io/en/research/library/papers/ouroboros-bfta-simple-byzantine-fault-tolerant-consensus-protocol/)
-   [Ouroboros Praos](https://iohk.io/en/research/library/papers/ouroboros-praosan-adaptively-securesemi-synchronous-proof-of-stake-protocol/)
-   [Ouroboros Genesis](https://iohk.io/en/research/library/papers/ouroboros-genesiscomposable-proof-of-stake-blockchains-with-dynamic-availability/)
-   [Ouroboros Crypsinous](https://iohk.io/en/research/library/papers/ouroboros-crypsinousprivacy-preserving-proof-of-stake/)
-   [Ouroboros Chronos](https://iohk.io/en/research/library/papers/ouroboros-chronospermissionless-clock-synchronization-via-proof-of-stake/)
-   [Ouroboros Leios](https://iohk.io/en/research/library/papers/high-throughput-permissionless-blockchain-consensus-under-realistic-network-assumptions/).

### Ouroboros Classic

The first implementation of [Ouroboros](https://iohk.io/en/research/library/papers/ouroborosa-provably-secure-proof-of-stake-blockchain-protocol/) achieved three major milestones:

- the foundation for an energy-efficient protocol to rival proof of work
- the introduction of the mathematical framework to analyze proof of stake
- the implementation of a novel incentive mechanism to reward participants in a proof-of-stake setting.

But what really set Ouroboros apart from other blockchain protocols (specifically, proof-of-stake protocols), was its ability to generate unbiased randomness in the protocol’s leader selection algorithm, and the subsequent security assurances that provided. Randomness prevents the formation of patterns, which is critical for maintaining the protocol’s security. Ouroboros was the first blockchain protocol developed with this type of rigorous security analysis.

### Ouroboros BFT

[Ouroboros Byzantine Fault Tolerance (BFT)](https://iohk.io/en/research/library/papers/ouroboros-bfta-simple-byzantine-fault-tolerant-consensus-protocol/) was the protocol's second implementation, used during the Byron update (transition from the old Cardano codebase to the new). The second instance of the protocol prepared Cardano for the decentralization that came with the Shelley release.

Ouroboros BFT enabled synchronous communication between a network of federated servers – the blockchain – providing ledger consensus in a simpler and more deterministic manner.

### Ouroboros Praos

[Ouroboros Praos](https://iohk.io/en/research/library/papers/ouroboros-praosan-adaptively-securesemi-synchronous-proof-of-stake-protocol/) introduced substantial security and scalability improvements to the Ouroboros Classic implementation. Praos processes transaction blocks by dividing chains into slots, which are aggregated into epochs. But unlike Ouroboros Classic, Praos is analyzed in a semi-synchronous setting and is secure against adaptive attackers, using private-leader selection and forward-secure, key-evolving signatures to ensure that a strong adversary cannot predict the next slot leader and launch a focused attack (such as a DDoS attack).

### Ouroboros Genesis

The fourth iteration of Ouroboros - [Genesis](https://iohk.io/en/research/library/papers/ouroboros-genesiscomposable-proof-of-stake-blockchains-with-dynamic-availability/) - further improves upon Ouroboros Praos by adding a novel chain selection rule that enables parties to bootstrap from a genesis block without the need for trusted checkpoints or assumptions about past availability. The Genesis paper also provides proof of the protocol’s Universal Composability, which demonstrates that the protocol can be composed with other protocols in arbitrary configurations in a real-world setting, without losing its security properties.

### Ouroboros Crypsinous

[Ouroboros Crypsinous](https://iohk.io/en/research/library/papers/ouroboros-crypsinousprivacy-preserving-proof-of-stake/) equips Genesis with privacy-preserving properties. It is the first formally analyzed privacy-preserving proof-of-stake blockchain protocol, which achieves security against adaptive attacks while maintaining strong privacy guarantees by introducing a new coin evolution technique relying on Snarks and key-private forward-secure encryption. Crypsinous isn’t currently planned to be implemented on Cardano, but it can be used by other chains for increased privacy-preserving settings.

### Ouroboros Chronos

[Chronos](https://iohk.io/en/research/library/papers/ouroboros-chronospermissionless-clock-synchronization-via-proof-of-stake/) achieves two goals: first, it shows how blockchain protocols can synchronize clocks securely via a novel time synchronization mechanism and thereby become independent of external time services. Second, it provides a cryptographically secure source of time to other protocols. In short, Chronos makes the ledger more resistant to attacks that target time information.

From an application point of view, Chronos can dramatically boost the resilience of critical telecommunications, transport, and other IT infrastructures that require the synchronization of local time to a unified network clock that has no single point of failure.

### Ouroboros Leios

[Ouroboros Leios](https://iohk.io/en/research/library/papers/high-throughput-permissionless-blockchain-consensus-under-realistic-network-assumptions/), currently in research and development, is the protocol version designed to increase the network's throughput by optimizing the use of available resources and enabling faster transaction processing and confirmation.

Leios is specifically designed to solve scalability bottlenecks in existing systems by acting as a high-throughput overlay on top of base protocols. It tackles malicious actions like protocol bursts (sudden floods of valid messages to congest the network) and equivocations (double-signing attacks), enabling near-optimal transaction processing while preserving security under real-world network constraints.

Learn more about Leios by visiting its [website](https://leios.cardano-scaling.org/).

---
title: Cardano keys
metaTitle: Cardano keys
---

Cardano keys are asymmetric cryptography key pairs used for:

- signing and validating payments and staking certificates
- identifying and defining addresses on the Cardano blockchain.

This schematic illustrates the relationship between keys, addresses, and certificates:

![keys-certificates](keys-certificates.png)

## Types of keys

In Cardano, there are two main key types:

- node keys
- address keys.

### Node keys

Node keys represent the security of the blockchain and consist of the following keys:

- operator/operational key
- key evolving signature (KES) key pair
- verifiable random function (VRF) keys.

#### Operator/operational key 

These are operators’ offline key pairs that include a certificate counter for new certificates.

It is the responsibility of the operator to manage both the hot (online), and cold (offline) keys for the pool. Cold keys must be secure and should not reside on a device with internet connectivity. It is recommended to keep multiple backups of cold keys.

#### KES key pair

To create an operational certificate for a block-producing node, you need a KES key pair, which authenticates who you are.

A KES key can only evolve for a certain number of periods and becomes useless afterwards. This is useful, because even if an attacker compromises the key and gets access to the signing key, he can only use that to sign blocks from now on, but not blocks dating from earlier periods, making it impossible for the attacker to rewrite history. 

After the set number of periods has passed, the node operator must generate a new KES key pair, issue a new operational node certificate with that new key pair, and restart the node with the new certificate.

#### VRF keys

Ouroboros Praos adds an extra layer of security to block production via VRF keys. 

In other proof-of-stake blockchain protocols (Ouroboros Classic or BFT, for instance), we know who has the right to make the block in each slot, because the slot leader schedule is public. In this case, you only have to prove that you are who you say you are, and everyone can check the public slot leader schedule to verify it. 

However, Ouroboros Praos's slot leader schedule is kept private, which means that nobody knows in advance who is going to be the slot leader, but once someone is, they can prove to everyone else that they are using the VRF key. 

The VRF key is a signing verification key stored within the operational certificate. It proves that a node has the right to create a block in a given slot.

### Address keys

Address keys represent the functions of the addresses derived from the keys for identifying funds on the blockchain that consist of the following keys:

- Payment key: single address key pair usually used for generating UTXO addresses
- Staking key: stake/reward address key pair usually used for generating account/reward addresses.

:::info

Developer resources:

- [Creating keys and addresses](https://github.com/input-output-hk/cardano-node-wiki/wiki/3_keys_and_addresses)
- [Creating KES key pair](https://github.com/input-output-hk/cardano-node-wiki/wiki/7_KES_period)
- [Generating stake pool keys](https://iohk.zendesk.com/hc/en-us/articles/900001331786-Generate-stake-pool-keys)
- [Generating payment/staking keys and address](https://iohk.zendesk.com/hc/en-us/articles/900001219883-Generating-payment-staking-keys-and-address)

:::


---
title: Cardano addresses
metaTitle: Cardano addresses
---

The addresses are a blake2b-224 hash of the relevant verifying/public keys
concatenated with some metadata that can be stored on the
[Cardano blockchain](https://cardano.org/).

Shelley introduced four different types of addresses:

- base addresses
- pointer addresses
- enterprise addresses
- reward account addresses.

Besides those new addresses, Shelley continued to support Byron-era _bootstrap
addresses_ and _script addresses_, but only the new base and pointer addresses
carried stake rights. Therefore, addresses consist of some serialized data
specified in the ledger specification stored in the blockchain’s blocks, eg, an
Unspent Transaction Output (UTXO) address.

The serialized data (address) contains two parts:

- metadata: used for interpreting
- payload: the raw or encoded data.

## Base addresses

A base address directly specifies the staking key that should control the stake
for that address. The staking rights associated with funds held in this address
may be exercised by the owner of the staking key. Base addresses can be used in
transactions without registering the staking key in advance.

The stake rights can only be exercised by registering the stake key and
delegating to a stake pool. Once the stake key is
registered, the stake rights can be exercised for base addresses used in
transactions before or after the key registration.

## Pointer addresses

A pointer address indirectly specifies the staking key that should control the
stake for the address. It references a stake key by a stake key pointer, which
is a location on the blockchain of the stake key registration certificate for
that key. Pointer addresses can be used in transactions, even if their target is
not an active stake key registration. This covers the scenario where the key was
unregistered after (or indeed, before) the transaction, and also covers pointers
to targets that are plainly invalid. The reason for allowing such invalid
targets is so that nodes only need to track the _currently active_ stake keys.

The pointer can be considerably shorter than the hash used in base addresses.

There is a subtlety with pointer addresses. It might happen that a stake key
registration certificate referenced by a pointer address might be lost due to a
rollback. To prevent loss of funds in this scenario, the system considers
pointer addresses with an invalid pointer to be _valid_ for the purpose of using
funds stored therein as inputs for transactions (but ignores them for the
purpose of proof of stake participation). Optionally, a wallet might refuse to
create transactions to pointer addresses before the referenced certificate has
become immutable, to prevent funds from being excluded from the proof of stake,
in the case of rollbacks.

## Enterprise addresses

Enterprise addresses carry no stake rights, so using these addresses means that
you are opting out of participation in the proof-of-stake protocol.

Exchanges or other organizations that control large amounts of ada – but hold it
on behalf of other users – may wish to follow a policy of not exercising stake
rights. By using enterprise addresses, exchanges can demonstrate that they
follow this policy. Since enterprise addresses are not associated with any
staking key, they are automatically excluded from the mechanisms that influence
the slot leadership schedule. Note that using addresses with no stake rights
effectively decreases the total amount of stake, which plays into the hands of a
potential adversary.

Also note that enterprise addresses may still be used to receive, hold, and send  
tokens with a policyID (non-ada assets). 

## Reward account addresses

A reward address is a cryptographic hash of the public staking key of the
address. Reward account addresses are used to distribute rewards for
participating in the proof-of-stake protocol (either directly or via
delegation).

They have the following properties:

- account-style accounting is used, not UTXO-style
- funds cannot be received via transactions; instead, their balance is only
  increased when rewards are distributed
- a one-to-one correspondence exists between registered staking keys and reward
  account addresses.

This key is used whenever funds are withdrawn from the address. Furthermore,
stake associated with funds in the address contributes to the stake of this key.
Just as in the case of enterprise addresses, the staking object for a reward
address does not need to contain information.

---
title: Chain confirmation versus transaction confirmation
metaTitle: Chain confirmation versus transaction confirmation
---

When discussing [Cardano](https://cardano.org/), often-repeated questions are *what is Cardano’s transaction time?*, and *how many network confirmations does Cardano require before a transaction goes through?* The answers to these questions require a deeper look at the concepts of *chain confirmation* and *transaction confirmation*, and how these relate to the Ouroboros protocol.

## Chain confirmation

This is the point beyond which the chain is guaranteed by the protocol not to alter any further because of randomness, or random events.

Chain confirmation occurs at some point in the future, after a certain amount of future k blocks have been minted. The time between now and the point when chain confirmation for a particular transaction occurs is called the *stability window* (that is, the number of slots required for a block to become **stable**, where *stable* is defined as a block that cannot be rolled back). The formula to calculate this window is 3k/f (where k is the security parameter in genesis, and f is the active slot co-efficient parameter in genesis that determines the probability for amount of blocks created in an epoch.)

## Transaction confirmation

This is the point in time when a transaction is accepted into the chain and becomes immutable. The concepts of block depth and settlement window come into play here.

A transaction can be considered confirmed if the block that contains it is deep enough in the chain. *Deep enough* is a relative concept: the depth of a block indicates how many more blocks have been added to the chain since that particular block was appended to it. And because blocks have depth, so do the transactions contained in them.

If the depth of a particular block is greater than a pre-defined threshold, the transaction is considered to be confirmed, and the assets in that transaction can be used 'safely' (ie, the protocol guarantees that the transaction has become immutable, so the assets can be traded, exchanged, etc.)

The time period that elapses between the point when a transaction is confirmed, and when the transaction's assets can be used to exchange with other assets is called the *settlement window*.

## Likelihood of immutability

Another way of determining whether or not a transaction is confirmed is by considering the transaction's *likelihood of immutability*. The probability of a transaction being immutable depends on how many blocks have been added to the chain since that transaction was accepted into the chain. The more blocks added, the higher the probability that the transaction will become immutable.

A transaction becomes immutable as soon as its depth is greater than 3k/f slots. This is guaranteed by the [Ouroboros Praos protocol](https://eprint.iacr.org/2017/573.pdf). However, 3k/f slots normally exceed the requirements in most situations, so a more practical approach is to consider the probability for a transaction to become immutable. In this case, we consider that a transaction is confirmed if the probability for it to become immutable is *high enough*.

---
title: Extended UTXO model
metaTitle: Understanding the Extended UTXO model
---

:::info

The EUTXO handbook is now out! Deep dive into [Cardano's EUTXO accounting model here](https://ucarecdn.com/3da33f2f-73ac-4c9b-844b-f215dcce0628/EUTXOhandbook_for_EC.pdf).

:::

Cardano (like Bitcoin) is an Unspent Transaction Output (UTXO)-based blockchain, which utilizes a different accounting model for its ledger from other account-based blockchains like Ethereum. Cardano implements an innovative [Extended Unspent Transaction Output (EUTXO) model](https://iohk.io/en/blog/posts/2021/03/11/cardanos-extended-utxo-accounting-model/), which was introduced by the Alonzo upgrade to support multi-assets and smart contracts.

## Overview of the UTXO model

In the UTXO model, a transaction has inputs and outputs, where the inputs are outputs from previous transactions. A transaction can only have previously unspent transaction outputs as its inputs.

### Transaction output

A transaction output includes an *address* (that you can think of as a lock) and a *value*. In keeping with this analogy, the signature that belongs to the address is the key to unlock the output. Once unlocked, an output can be used as input. New transactions spend outputs of previous transactions, and produce new outputs that can be consumed by future transactions. Each UTXO can only be consumed once, and as a whole. Each output can be spent by exactly one input, and one input *only*.

### Transaction input

A transaction input is the output of a previous transaction. Transaction inputs include a pointer and a cryptographic signature that acts as the unlocking key. The pointer points back to a previous transaction output, and the key unlocks this output. When an output is unlocked by an input, the blockchain marks the unlocked output as 'spent'. New outputs created by a given transaction can then be pointed to by new inputs, and so the chain continues. These new outputs (which have not yet been unlocked, ie, spent) are the UTXOs. Unspent outputs are simply that, outputs that have not yet been spent.

In summary, transactions consume unspent outputs from previous transactions, and produce new outputs that can be used as inputs for future transactions.

![eutxo](https://ucarecdn.com/f4b3f730-2880-4098-897b-92346c73b8af/)

The users' wallets manage these UTXOs and initiate transactions involving the UTXOs owned by the user. Every node maintains a record of all currently unspent transaction outputs (UTXOs), called the UTXO set. In technical terms, this is the chainstate, which is stored in the data directory of every node. When a new block is added to the chain, the chainstate is updated accordingly. This new block contains the list of latest transactions (including, of course, a record of spent UTXOs, and new ones created since the chainstate was last updated). Every node maintains an exact copy of the chainstate.

## Cardano’s extended UTXO model 

The EUTXO model extends the UTXO model in two ways:

1. It generalizes the concept of ‘address’ by using the lock-and-key analogy. Instead of restricting locks to public keys and keys to signatures, addresses in the EUTXO model can contain arbitrary logic in the form of scripts. For example, when a node validates a transaction, the node determines whether or not the transaction is allowed to use a certain output as an input. The transaction will look up the script provided by the output's address and will execute the script if the transaction can use the output as an input.
2. The second difference between UTXO and EUTXO is that outputs can carry (almost) arbitrary data in addition to an address and value. This makes scripts much more powerful by allowing them to carry state information.

Furthermore, EUTXO extends the UTXO model by allowing output addresses to contain complex logic to decide which transactions can unlock them, and by adding custom data to all outputs. When validating an address, the script will access the data being carried by the output, the transaction being validated, and some additional pieces of data called redeemers, which the transaction provides for every input. By looking up all this information, the script has enough context to give a ‘yes’ or ‘no’ answer in what can be highly complex situations and use cases.

EUTXO enables arbitrary logic in the form of scripts. This arbitrary logic inspects the transaction and the data to decide whether the transaction is allowed to use an input or not.

The UTXO model with its graph structure is fundamentally different from the account-based model used by some existing smart-contract enabled blockchains. As a result, the design patterns that work for DApps on account-based blockchains do not translate directly to Cardano. New design patterns are needed because the underlying representation of the data is different.

EUTXO	 inherits the per-branches design of the UTXO (Bitcoin) model, where one branch is by definition a sequence of transactions that requires a sequence of validations. To split the logic across different branches and enforce more parallelism, it is essential to build DApps and other solutions using multiple UTXOs. This provides benefits in terms of scaling, just like developing Bitcoin services prerequisites splitting one wallet into sub wallets. 

## Advantages of EUTXO 

Cardano’s EUTXO model provides a secure and versatile environment to process multiple operations without system failures. This model offers better scalability and privacy, as well as more simplified transaction logic, as each UTXO can only be consumed once and as a whole, which makes transaction verification much simpler.

The EUTXO model offers unique advantages over other accounting models. The success or failure of transaction validation depends only on the transaction itself and its inputs, and not on anything else on the blockchain. As a consequence, the validity of a transaction can be checked off-chain, before the transaction is sent to the blockchain. A transaction can still fail if some other transaction concurrently consumes an input that the transaction is expecting, but if all inputs are still present, the transaction is guaranteed to succeed.

This contrasts with an account-based model (as used by Ethereum), where a transaction can fail in mid-script execution. This can never happen in EUTXO. 

Due to the ‘local’ nature of transaction validation, a high degree of parallelism is possible. A node could, in principle, validate transactions in parallel, if those transactions do not try to consume the same input. This is great both for efficiency and for reasoning, simplifying the analysis of possible outcomes, and proving that ‘nothing bad’ can happen. You can dive deeper into the EUTXO model blog post.

A powerful feature of the EUTXO model is that the fees required for a valid transaction can be predicted precisely prior to posting it. This is a unique feature not found in account-based models. Account-based blockchains, like Ethereum, are indeterministic, which means that they cannot guarantee the transaction’s effect on-chain. This uncertainty presents risks of monetary loss, unexpectedly high fees, and additional opportunities for adversarial behavior.

To summarize, EUTXO offers greater security, smart contract execution cost predictability (without unpleasant surprises) and more powerful parallelization. 

---
title: Transaction costs and determinism
metaTitle: Transaction costs and determinism
---

Cardano’s approach to price setting mainly relies on market demand over actual
supply. On Cardano, more than one type of demand competes for the common supply.
Thus, it is crucial to consider both _relative_ and _absolute_ pricing. One way
to do this is to inspect the effects of smart contract pricing, non-fungible
token (NFT) operations, and so on, with respect to some common value – in our
case, the consumption of Cardano’s processing power.

The Cardano ledger's design is focused on high assurance, security, and proven
formal verification. In alignment with this strategy, it is also important to
ensure that transaction processing is _deterministic_, meaning that a user can
predict the impact and outcome of a transaction before its actual execution.

The ability to guarantee the cost of transaction execution and how the
transaction behaves on the ledger _before_ it is submitted became even more
prominent with the introduction of smart contract support in 2021. This feature
is different from other blockchains, including Ethereum, where other network
activity can influence the gas cost. This ability is also guaranteed by the
deterministic nature of Cardano and Plutus scripts.

## Determinism

_Determinism_, in the context of transaction and script processing, is a synonym
for predictability. This means that a user can predict locally (off-chain) how
their transaction will impact the on-chain state of the ledger, without
encountering the following:

- unexpected script validation outcomes or failures
- unexpected fees
- unexpected ledger or script state updates.

## Validation

An important aspect of processing a transaction is validating the actions it is
taking. A transaction is _taking an action_ when it contains data in the
specific field to that action. For example, a transaction is _spending UTXO U_
when it contains a reference to U in its input field, and it is _minting_ a
token X when its mint field contains X.

Cardano uses scripts to validate actions. These scripts, which are pieces of
code, implement pure functions with _True_ or _False_ outputs. _Script
validation_ is the process of invoking the script interpreter to run a given
script on appropriate arguments.

Script validation can be performed for the following actions:

- Spending a UTXO locked by a script address: the script that is run is the one
  whose hash forms the address.
- Minting a token: the script that is run is the one whose hash forms the policy
  ID of the token being minted.
- Reward withdrawal: the script that is run is the one whose hash forms the
  staking address.
- Applying a certificate: the script that is run is the one whose hash forms the
  certificate author’s credential.

Besides letting the node know which script to run, all transaction actions
indicate how to assemble arguments passed to that script. Alonzo introduced a
new approach to transaction validation on Cardano due to the implementation of
Plutus scripts.

## Ledger features

The Alonzo upgrade changed the data on the ledger as follows:

1. Plutus scripts can lock UTXOs.
2. A component, added to the contents of the output parts of UTXOs, enables
   script state-like functionality. In addition to assets and an address, a UTXO
   locked by Plutus scripts also contains a datum. A datum is a piece of data
   that can be thought of as an interpretation of the script state.
3. There are a number of protocol parameters used to impose additional
   validation requirements on transactions. These include upper limits on
   computational resources that scripts can consume.

To support Plutus scripts, transactions have the following features:

1. For each of its actions, the transaction carries a user-specified argument,
   called a redeemer. Depending on the script, a redeemer can serve a different
   purpose. For example, it can act as the bid the user places in an auction, or
   the user’s guess in a guessing game, among many other functions.
2. The transaction specifies computational execution budgets for each script.
3. To ensure that a transaction can pay its execution fee, Alonzo introduced
   collateral.
4. Transactions contain an integrity hash, needed to ensure that it has not been
   compromised, outdated, etc.

With the Vasil upgrade, Plutus scripts were upgraded to V2 functionality with an
improved cost model and new built-ins. The use of reference inputs, inline
datums, and reference scripts allowed for efficient throughput and significantly
increased performance.
[See the complete Vasil feature overview here](/about-cardano/evolution/upgrades/vasil).

The node performs the checks that ensure the transaction is constructed
correctly. For example, it must not exceed the maximum execution resource
budget. It also invokes the Plutus script interpreter to run the scripts.

The non-deterministic gas model can charge users unpredictably large fees. In
Cardano scripts, this source of indeterminism is addressed by requiring that the
resource budget itself, as well as the fee required to cover this budget, are
included in the transaction. Script execution necessarily returns either _True_
or _False_, and will not loop indefinitely. The reason for this is that every
operation a script performs takes a non-zero amount of resources, which are
tracked by the interpreter. If the budget specified by the transaction is
exceeded, script execution terminates and returns _False_.

The following key points make the outcomes of script and transaction validation
**predictable**:

- the script interpreter will always terminate and return the same validation
  result when applied to the same arguments
- a transaction necessarily fixes all arguments that will be passed to the
  script interpreter during validation
- a transaction specifies all the actions it is taking that require script
  validation
- compulsory signatures on a transaction ensure that it cannot be altered by an
  adversary in a way that causes scripts to fail
- applying a transaction in the EUTXO ledger model is deterministic.

Script validation failure or success does affect how a transaction is processed.
However, the _True_ or _False_ outcome, as well as ledger changes associated
with either outcome, are predictable for a given transaction.

:::info

Further reading:

- [Cost model parameters for Plutus scripts](https://plutus-apps.readthedocs.io/en/latest/reference/cost-model-parameters.html)
- [No surprises transaction validation on Cardano: Part 1](https://iohk.io/en/blog/posts/2021/09/06/no-surprises-transaction-validation-on-cardano/)
- [No surprises transaction validation: Part 2 (about two phases of validation and collateral](https://iohk.io/en/blog/posts/2021/09/07/no-surprises-transaction-validation-part-2/)

:::

---
title: About the collateral mechanism
metaTitle: About the collateral mechanism
---

The collateral mechanism is an important feature that has been designed to
ensure successful smart contract execution.

Relying on the guarantees provided by the deterministic design of the ledger,
Cardano implements a two-phase validation scheme. The main reason for
two-phase validation is to limit the amount of uncompensated
validation work by nodes. Each phase serves a purpose in achieving this goal:

- The first phase checks whether the transaction is constructed correctly and
  can pay its processing fee
- The second phase runs the scripts included in the transaction.

If the transaction is _phase-1 valid_, phase-2 scripts run. If phase-1 fails, no
scripts run, and the transaction is immediately discarded.

Collateral is used to guarantee that nodes are compensated for their work in
case phase-2 validation fails. Thus, collateral is the monetary guarantee a user
gives to assure that the contract has been carefully designed and thoroughly
tested. The collateral amount is specified at the time of constructing the
transaction. Not directly, but by adding collateral inputs to the transaction.
The total balance in the UTXOs corresponding to these specially marked inputs is
the transaction’s collateral amount. If the user fulfills the conditions of the
guarantee, and a contract gets executed, the collateral is safe.

## The scenario

Without collateral, the user is not charged if a smart contract fails. However,
by the time the transaction fails, the network has already incurred some costs
to initiate and validate the transaction. This means that a malicious actor
could flood the network with invalid transactions, denying service to other
users at little cost.

## The solution

When a user initiates a transaction, they commit enough ada to cover its
execution cost. On Cardano, transactions that call and use non-native smart
contracts (known as phase-2 contracts) also need enough collateral to cover
costs related to potential transaction failures. This amount can be small, but
it is sufficient to make a denial of service (DOS) attack prohibitively
expensive.

Collateral fees are collected only if a transaction fails validation. If the
contract passes validation, the transaction fees are collected, but the
collateral is not.

## The reasoning

An honest user is _never_ at risk of losing their collateral.

The [Cardano](https://cardano.org/) blockchain is deterministic with respect to
transaction costs because these costs depend only on local values and local
state. That means a user can calculate the execution cost (in ada) of a
transaction before submitting it. This feature is different from other
blockchains, including Ethereum, where other network activity can influence the
gas cost. The required amount of collateral depends only on the execution cost.

The Cardano testnet
provides a safe environment with free test ada, so decentralized application
(DApp) developers can thoroughly test their smart contracts before deploying them to
mainnet. If transactions succeed on testnet, the developer can be
perfectly sure that all the scripts will indeed succeed.

If the on-chain conditions have changed since the transaction was constructed,
that transaction will be rejected entirely, and no fees will be charged. If a
signature is missing, for example, no collateral would be charged.

## Technical details

The term collateral refers to the total ada contained in the UTXOs referenced by
collateral inputs. A transaction uses collateral inputs to cover its fees if a
phase-2 script fails.

The Shelley formal specification introduced the concept of 'multi-signature'
scripts. Phase-1 scripts, such as these, are captured entirely by the ledger
rules. Execution costs can therefore be easily assessed before they are
processed by the implementation, and any fees can be calculated directly within
the ledger rule implementation, based on the size of the transaction that
includes the script, for example.

In contrast, phase-2 scripts can perform arbitrary (and, in principle,
Turing-complete) computations. Cardano requires transactions that use phase-2 scripts
to have a budget in terms of a number of abstract ExUnits. This budget gives a
quantitative bound on resource usage in terms of a number of specific metrics,
including memory usage or abstract execution steps. The budget is then used as
part of the transaction fee calculation. 

After the Vasil upgrade, DApp developers can also specify a
change address for the script collateral. This means that in case the script
fails phase-2 validation, only the right amount will be taken, and the remaining
funds will be sent to the provided change address. The
[how to use collateral outputs](https://github.com/perturbing/vasil-tests/blob/main/collateral-output-cip-40.md)
tutorial provides more details.

---
title: Networking protocol design overview
metaTitle: Networking protocol design overview
---

Transmission Control Protocols (TCP) and Internet Protocols (IP) form a protocol
suite universally deployed on the network. TCP/IP enables a reliable
bidirectional communication channel between systems on the internet.

The ordered delivery of _Cardano node communication protocols_ is guaranteed by
the TCP/IP protocol.

Operating systems limit the number of concurrent connections. By default, Linux,
for example, can open 1,024 connections per process, whereas macOS limits this
number to 256. To avoid excessive use of resources and enable reliable means for
connection establishment, Cardano uses a _multiplexer_.

## Connection management

The network layer handles a range of specific tasks besides the exchange of
block and transaction information required by the Ouroboros protocol.

Generally, connection management implementation includes the performance of the
following tasks:

- opening a socket and/or acquiring resources from the OS
- negotiating the protocol version with the handshake mini-protocol
- spawning the thread that runs the multiplexer (which can be instructed to
  start/stop various mini-protocols)
- discovering and classifying exceptions thrown by mini-protocols or the
  multiplexer itself
- shutting down the connection in case of an error
- handling a shutdown request from the peer
- shutting down the threads that run mini-protocols
- closing a socket.

## Multiplexing

The multiplexing layer acts as a central crossing between mini-protocols and the
network channel. It runs several
[mini-protocols](/about-cardano/explore-more/cardano-network/#utilizing-mini-protocols)
in parallel in a single channel ‒ TCP connection, for example.

Figure 1 reflects how data flows between two nodes, each running three
mini-protocols using a multiplexer (MUX) and a de-multiplexer (DEMUX).

![connection-manager](connection-manager.png)

Figure 1. Data flow between the nodes through multiplexing

Data transmitted between nodes passes through the MUX/DEMUX of the nodes. There
is a fixed pairing of mini-protocol instances, which means that each instance
only communicates with its dual instance (an initiator and a responder side).

The implementation of the mini-protocol also handles serialization and
de-serialization of its messages. Mini-protocols write chunks of bytes to the
MUX and read chunks of bytes from the DEMUX. The MUX reads the data from
mini-protocols, splits it into segments, adds a segment header, and transmits
the segments to the DEMUX of its peer. The DEMUX uses the segment’s headers to
reassemble byte streams for the mini-protocols on its side. The multiplexing
protocol (see the note below) itself is completely agnostic to the structure of
the multiplexed data.

:::note

This is not a generic, but specialized, use of multiplexing. Individual
mini-protocols have strict constraints on unacknowledged messages that can be in
flight. The design avoids the conditions in which the use of general TCP over
TCP multiplexing creates chaotic performance.

:::

## Data segments of the multiplexing protocol

Multiplexing data segments include the following details:

- **Transmission time** ‒ a timestamp based on the lower 32 bits of the sender’s
  monotonic clock with a resolution of one microsecond
- **Mini-protocol ID** ‒ the unique ID of the mini-protocol
- **Payload length** ‒ the size of the segment payload in bytes; the maximum
  payload length supported by the multiplexing wire format is 216 − 1. Note that
  an instance of the protocol can choose a smaller limit for the size of
  segments it transmits
- **Mode** ‒ the single bit M (the mode) is used to distinguish the dual
  instances of a mini-protocol. The mode is set to 0 in segments from the
  initiator (the side that initially has agency), and it is set to 1 in segments
  from the responder.

## Cardano node communication protocols

Cardano uses inter-process communication (IPC) protocols to allow for the
exchange of blocks and transactions between nodes, and to allow local
applications to interact with the blockchain via the node.

### Node-to-node IPC overview

The Node-to-node (NtN) protocol transfers transactions between full nodes. NtN
includes three mini-protocols (chain-sync, block-fetch, and tx-submission),
which are multiplexed over a single TCP channel using a network-mux package.

The following diagram represents the NtN operational flow:

![Node-to-Node](node-to-node-ipc.png)

NtN follows a pull-based strategy, where the initiator node queries for new
transactions and the responder node replies with the transactions if any exist.
This protocol perfectly suits a trustless setting where both sides need to be
protected against resource consumption attacks from the other side.

**NtN mini-protocols explained**

A brief explanation of the NtN mini-protocols:

- **chain-sync**: a protocol that allows a node to reconstruct a chain of an
  upstream node
- **block-fetch**: a protocol that allows a node to download block bodies from
  various peers
- **tx-submission**: a protocol that allows submission of transactions. The
  implementation of this protocol is based on a generic mini protocol framework,
  with one peculiarity: the roles of the initiator and the responder are
  reversed. The Server is the initiator that asks for new transactions, and the
  Client is the responder that replies with the transactions. This role reversal
  was designed thus for technical reasons.

To ensure optimal networking service, the team has also implemented an
additional protocol:

- **keep-alive**: a protocol that ensures continuous connection between nodes
  and minimizes performance faults.

### Node-to-client IPC overview

Node-to-client (NtC) is a connection between a full node and a client that
consumes data but does not take part in the Ouroboros protocol (a wallet, for
example.)

The purpose of the NtC IPC protocol is to allow local applications to interact
with the blockchain via the node. This includes applications such as wallet
backends or blockchain explorers. The NtC protocol enables these applications to
access the raw chain data and to query the current ledger state, and it also
provides the ability to submit new transactions to the system.

The NtC protocol uses the same design as the node-to-node (NtN) protocol, but
with a different set of mini-protocols, and using local pipes rather than TCP
connections. As such, it is a relatively low-level and narrow interface that
exposes only what the node can provide natively. For example, the node provides
access to all the raw chain data but does not provide a way to query data on the
chain. The job of providing data services and more convenient higher-level APIs
is delegated to dedicated clients, such as cardano-db-sync and the wallet
backend.

**NtC mini-protocols**

The NtC protocol consists of three mini-protocols:

- **chain-sync** - used for following the chain and getting blocks
- **local-tx-submission** - used for submitting transactions
- **local-state-query** - used for querying the ledger state.

The NtC version of chain-sync uses full blocks, rather than just block headers.
This is why no separate block-fetch protocol is needed. The local-tx-submission
protocol is like the NtN tx-submission protocol but simpler, and it returns the
details of transaction validation failures. The local-state-query protocol
provides query access to the current ledger state, which contains a lot of
interesting data that is not directly reflected on the chain itself.

**How NtC works**

In NtC, the node runs the producer side of the chain-sync protocol only, and the
client runs the consumer side only.

This table shows which mini-protocols are enabled for NtC communication:

![Node-to-Client](node-to-client-ipc.png)

---
title: Peer-to-peer (P2P) networking
metaTitle: Peer-to-peer (P2P) networking
---

Cardano nodes and the interactions between them are combined together within a
networking layer, which distributes information about transactions and block
creation among all active nodes. In this way, the system validates and adds
blocks to the chain and also verifies transactions. A distributed network of
nodes must keep communication delays to a minimum, and be resilient enough to
cope with failures or capacity constraints.

In the Byron federated system, nodes were connected by a static configuration
that was defined in a topology file. Since the introduction of Shelley, the
system has been functioning in a hybrid mode. Moving from the federated state of
network connectivity to the hybrid one, the teams delivered a manually
constructed P2P network of SPO relay nodes. This means that SPO block-producing
nodes can connect to both trusted relay nodes and other SPO-run relay nodes.
Hybrid connectivity is not automated, however, it enables the exchange of block
and transaction information without relying on trusted relays only.

A trusted relay is the one that the SPO, wallet, or exchange accessing the
network ‘trusts’. Though this role has mostly been performed by IOG, other
entities, such as the Cardano Foundation, a wallet, or exchanges can also run
trusted relays. Block-producing nodes can connect to any relays they deem
trustable.

1. Federated: introduced in the Byron development phase in 2017, trusted core
   and relay nodes maintained the network and connected users, wallets, and
   exchanges.

![federated](https://ucarecdn.com/dcd6bf70-5c73-4fbc-8683-1f199e8f554b/)

2. Hybrid: since the Shelley development phase in 2020, block-producing nodes
   send and receive communications through trusted relays and/or a manual
   community-developed and managed tool called the
   [topology updater](https://github.com/cardano-community/guild-operators/blob/alpha/docs/Scripts/topologyupdater.md).

![hybrid](https://ucarecdn.com/3e7d4b86-040c-416a-9eda-076e8ffe41c9/)

3. Dynamic P2P: provides automation and resilience to optimize network
   performance. SPO relays can automatically connect to each other through
   self-discovery and optimization.

![dynamic-p2p](https://ucarecdn.com/48167b72-9192-42e0-a367-5f3e40612388/)

To ensure efficient communication between nodes, it is desirable to enable
automated connections of SPO relays to each other, so that less static
configuration is needed. Dynamic P2P has been gradually enabled since the node
[v.1.35.6](https://github.com/input-output-hk/cardano-node/releases/tag/1.35.6)
release. For more details, see
[this blog post](https://iohk.io/en/blog/posts/2023/03/16/dynamic-p2p-is-coming-to-cardano/).

With the deployment of Dynamic P2P, networking keeps evolving with
additions such as Ouroboros Genesis and peer sharing:

4. Ouroboros Genesis: in development. Anyone running their own node or Daedalus
   wallet will connect to a fully decentralized and self-organized network.
   Genesis enables secure node bootstrapping and the removal of trusted relays. Read this blog post to learn more about [the transition to Genesis](https://iohk.io/en/blog/posts/2024/03/14/approaching-full-p2p-node-operations/).

![genesis](https://ucarecdn.com/9ce084f4-28d6-4557-919e-d60a18ded168/)

5. Peer sharing: in development. Peer sharing will facilitate the discovery of
   potential peers that are not registered on the chain within the overall
   Cardano node network. This phase will also allow anyone to contribute to
   running the network, rather than just using resources from SPOs.

![peer-sharing](https://ucarecdn.com/9359a2da-a6ed-4edd-89b4-508ae96145e7/)

## Dynamic P2P capabilities

The network stack undergoes a number of improvements to achieve the desired
network resilience. These improvements do not require a protocol change, but
rather enable automated peer selection and communication between peers and
nodes.

The P2P networking is enabled due to the use of the following components:

- **Outbound governor** (formerly known as P2P governor) ‒ manages the automatic
  initiation of outbound connections between peers and classifies them as cold,
  warm, or hot (known, ready, or in use). The governor establishes a
  connectivity map of the local network. One part of the governor is the churn
  governor, which optimizes the network graph by dropping the 20% lowest
  performing peers.
- **Server** ‒ accepts connections. There is a soft and a hard limit on the
  number of connections that influence the speed of connection acceptance.
- **Inbound protocol governor** ‒ runs and governs inbound network
  ‘mini-protocols’ on an accepted connection.
- **Connection manager** ‒ tracks the state of each network connection: each one
  might be used as a warm or hot outbound connection or used by the inbound
  side. Warm connections only keep connectivity while hot ones take an active
  part in the consensus protocol.

Next, we take a closer look at how the outbound governor works to ensure
automated connectivity between peer nodes on the network.

### Outbound governor functionality

The Cardano network consists of multiple peer nodes. Some peer nodes are more
active than others, some have established connections, and some should be
promoted to ensure the best system performance. Each block-producing node
maintains a set of peers mapped into three categories:

- cold peers ‒ existing peers without an established network connection
- warm peers ‒ peers with an established bearer connection, which is only used
  for network measurements without implementing any of the node-to-node
  mini-protocols
- hot peers ‒ peers that have a connection, which is being used by all three
  node-to-node mini-protocols.

Newly discovered peers are initially added to the cold peer set. The outbound
governor is then responsible for peer connection management: it defines which
peers are beneficial for connection purposes, and which should be promoted or
demoted between cold, warm, or hot sets.

The primary goal of the outbound governor is to maintain the target number of
cold, warm, and hot peers. This builds and maintains a connectivity map of the
local part of the network, and simplifies the process of connection definitions
by handling them automatically.

To establish connectivity between nodes, the outbound governor engages in the
following activities:

- promotion of cold peers to warm peers
- demotion of warm peers to cold peers
- promotion of warm peers to hot peers
- demotion of hot peers to warm peers.

The outbound governor needs to establish and maintain:

- a target number of cold peers (for example, 100)
- a target number of hot peers (for example, between 2–20)
- a target number of warm peers (for example, between 10–50)
- a set of warm peers that are sufficiently diverse in terms of hop distance and
  geographic locations
- a target churn frequency for hot or warm changes
- a target churn frequency for warm or cold changes
- a target churn frequency for cold or unknown changes.

![peer-discovery](https://ucarecdn.com/b6bae79e-5960-4d4e-9f6c-993a100b01c3/)

Using 2–20 hot peers is cost-efficient, as peers exchange only their block
headers. The block body, in turn, is typically requested only once, and it tends
to follow the shortest path through the connectivity graph.

The purpose of warm peers is to provide access to those peers that can be
quickly promoted to hot ones (in case any of the hot peers fail), and also
provide candidates for the churn of hot peers.

The policy for selecting which warm peers to promote to hot relies on the
upstream measurements. The purpose of a degree of churn between cold and warm
peers is, in part, to discover the network distance between more peers and to
allow potentially better warm peers to take over from existing hot peers. This
enables further optimization and adjustments. Maintaining diversity in hop
distances contributes to better block distribution times across the globally
distributed network.

Overall, this approach follows a common pattern for probabilistic search or
optimization that uses a balance of local optimization with some elements of
higher-order disruption to avoid becoming trapped in a poor local optimum.

Peers maintain a limited set of information, which is based on their previous
direct interactions. Cold peers, for instance, may not maintain any data, as
they have not established interactions yet. Such data can be compared to a
‘reputation’ property, however, these details are purely local and are not
shared among other nodes. Local peer reputation information is also updated when
peer connections fail. The network does not maintain negative peer information
for extended periods of time: to bound resources and because of the simplicity
of
[Sybil attacks](https://iohk.io/en/blog/posts/2018/10/29/preventing-sybil-attacks/).

The implementation classifies exceptions that cause connection failures into
three classes:

- internal node exceptions (for example, local disk corruption)
- network failures (for example, dropped TCP connections)
- adversarial behavior (for example, a protocol violation detected by the
  typed-protocols layer or by the consensus layer).

In the case of adversarial behavior, the peer can be immediately demoted from
the hot, warm, or cold sets.

---
title: Cardano network
metaTitle: Cardano network
sidebar_position: 4
collapsible: true
collapsed: true
---

The Cardano network is a technical infrastructure combining Cardano nodes and
their interactions in one unified system. It consists of a collection
of nodes that communicate with each other to maintain the distributed ledger.
These nodes are the actors on Cardano that validate blocks, add blocks to the
chain, and distribute transactions.

The networking layer is the driving force for delivering information exchange
requirements, which includes new blocks diffusion and transaction information
for establishing a better data flow. Cardano nodes maintain connections with
peers that have been chosen via a custom peer selection process.

:::info

Read the following specifications for more details:

- [The Shelley networking protocol](https://ouroboros-network.cardano.intersectmbo.org/pdfs/network-spec/network-spec.pdf)
- [An introduction to the design of data diffusion and networking for Cardano Shelley.](https://ouroboros-network.cardano.intersectmbo.org/pdfs/network-design/network-design.pdf)

:::

## Data flow between and within nodes

To understand how nodes communicate with each other, let’s suppose that node _A_
is connected to node _B_. Then, the Ouroboros protocol schedules a node _N_ to
generate a new block in a given time slot. Depending on the location of nodes
_A_, _B_, and _N_ in the network topology, and whether a new block arrives first
at _A_ or _B_, node _A_ can be either upstream or downstream of node _B_.

A set of mini-protocols is used to enable communication between different nodes.
Each mini-protocol implements a basic information exchange requirement, such as
informing peers of the latest block, sharing blocks as needed, or sharing new
transactions around the Cardano network. For connection purposes, mini-protocols
are determined by the version of the network protocol. For example, there are
two protocol suites: node-to-node and node-to-client. The node-to-client
protocol suite is used by wallets and chain consumers. Protocol suites use
different sets of mini-protocols and the version is negotiated when a new
connection is established using a specific protocol (protocols are described in
the following sections).

Clients can also choose which node-to-client mini-protocols to use, but it is
important to note that the node needs to be able to reply to all of them to
support different use cases. For example, to communicate, node A runs its
client-side instance of the _chain-sync mini-protocol_ that talks with a server
instance of the _chain-sync mini-protocol_ at node _B_. Such a situation is
similar to the functionality of other mini-protocols.

The scheme below illustrates how data flows within a node. Circles represent
protocol threads and internal threads that are responsible for running the
client and server processes within the respective mini-protocols.

![node_data_flow](node_data_flow.jpg)

Two types of data flow exist:

1. Mini-protocols communicate with mini-protocols of other nodes by sending and
   receiving messages across a public network (internet); this flow is not
   covered within the scheme above.
2. Within a node, mini-protocols communicate with each other by reading from,
   and writing to, a shared mutable variable, which is represented by boxes in
   the scheme. Arrows indicate whether a thread has _read_ or _write_ access to
   the shared state.

## Addressing network complexities and constraints

To design an efficient and robust networking architecture, a number of potential
issues regarding complexity and constraints have been evaluated.

**Congestion control** is one such feature and is used to deal with system
overload. Congestion control is vital to ensure that the system is robust enough
while operating high workloads. Within the networking design, it is common that
the number of transactions that occur can be higher than the number that can be
actually processed for inclusion in the blockchain. Therefore, it is important
to ensure that the increasing rate of transaction submission into a block does
not decrease the performance of blockchain.

The actual node has a limit to the amount of data it can process. In particular,
a node might have to share its processing power with other processes that run on
the same machine or operating system instance. This means that a node can slow
down and result in the system not being able to process all the available data
from the network.

To address these issues, the congestion control feature has been designed to
operate appropriately in such a situation and recover from transient conditions.
In any case, a node must not exceed its memory limits so there must be defined
memory limits, breaches of which are treated as protocol violations. These
factors mean that the system will be able to meet performance goals.

**Real-time constraints** and **coordinated universal time** are other aspects
that have been considered while designing the networking architecture. In
Cardano, Ouroboros consensus protocols model the passage of physical time as an
infinite sequence of time slots, assigning slot leaders to create a new block in
those time slots. Choosing a slot time, however, might cause certain
complexities in terms of the slot length, as it should be long enough for a new
block to have a good chance to reach the next slot leader in time. Therefore, a
chosen value for the slot length was initially set to 20 seconds in the Byron
era. With [Ouroboros Praos](https://eprint.iacr.org/2017/573.pdf)
implemented in the Shelley era, a slot length of 1 second is chosen but, on
average, only 0.05 of slots will produce a block (and thus on average, there
will be 20-second intervals between blocks). It is assumed that the clock skews
between the local clocks of the nodes are small with respect to the slot length.
Possible clock inaccuracies should still be taken into consideration, especially
when dealing with time-stamped incoming blocks. It is important to differentiate
whether there is a time difference or whether the node considers an adversarial
behavior of another node.

## Utilizing mini-protocols

Mini-protocols are used to communicate between multiple nodes while implementing
information exchange requirements. A mini-protocol is a defined modular building
block of the network protocol. Structuring the network protocol around mini-protocols helps to manage the overall complexity of the design while ensuring
useful flexibility.

Mini-protocols describe both the _initiator_ and the _responder_ within the
communication stream. The initiator is the dual element of the responder and
vice versa. A node typically runs many instances of mini-protocols, which
includes many instances of the same mini-protocol. Each mini-protocol instance
of the node then communicates with the dual instance of the exact peer. All mini-protocols that communicate with the same peer share a single communication
channel (pipe or socket). A multiplexer or de-multiplexer is used to multiplex
respective protocols over that channel.

The set of mini-protocols that is used for connection between two participants
of the system depends on the role of these participants, for instance, whether
the node acts as a full node or a blockchain consumer (for example, a wallet).

It is also worth noting that the implementation of mini-protocols uses a generic
framework for **state machines**. This framework uses _correct-by-construction_
techniques to guarantee the implementation of several properties of the
protocol. In particular, this technique assures that no deadlocks occur and
communication is canceled in the following scenarios:

- when one side is expected to transmit the next message, and the other side is
  awaiting the message, and both sides agree that the protocol has been
  terminated
- when either side receives a message that is not expected according to the
  protocol.

All mini-protocols based on this framework include the following information in
their description:

- an informal description of the protocol
- state machine states
- exchanged messages
- a transition graph of the state machine global view
- the client’s implementation of the protocol
- the server implementation of the protocol.

## Example mini-protocols

This section outlines some examples of mini-protocols.

### Ping pong protocol

This is a simple protocol for testing that a client can use to check that the
server is responsive. The ping pong protocol is very simple because the messages
do not carry any data and the ping pong client, as well as the ping pong server,
do not access the internal state of the node.

### Request response protocol

The request response protocol is polymorphic in the request and response data
that is being transmitted. This means that there are different possible
applications of this protocol and the application of the protocol determines the
types of requests sent and responses received.

### Chain synchronization protocol

The chain synchronization protocol is used by a blockchain consumer to replicate
the producer’s blockchain locally. A node communicates with several upstream and
downstream nodes and runs an independent client instance and an independent
server instance for each node with which it communicates.

The chain synchronization protocol is polymorphic. The node-to-client protocol
uses an instance of the chain synchronization protocol that transfers full
blocks, while the node-to-node instance only transfers block headers. In the
node-to-node scenario, the block fetch protocol is used to transfer full blocks.

### Block fetch protocol

The block-fetching protocol enables a node to download a range of blocks.

### Local transaction submission mini-protocol

The local transaction submission mini-protocol is used by local clients, for
example, wallets or CLI tools, to submit transactions to a local node. The
protocol is not used to forward transactions from one core node to another. The
protocol follows a simple request-response pattern:

1. the client sends a request with a single transaction
2. the server either accepts the transaction (returning a confirmation), or
   rejects it (returning the reason).

### Node-to-node transaction submission protocol

The node-to-node transaction submission protocol is used to transfer
transactions between full nodes. The protocol follows a pull-based strategy
where the initiator asks for new transactions and the responder replies with
transactions. It is suitable for a trustless setting where both sides need to
guard against resource consumption attacks from the other side. The
implementation of the node-to-node transaction mini-protocol is based on a
generic mini-protocol framework (the same as for all other mini-protocols). For
technical reasons, the roles of the initiator and the responder are reversed in
this case compared to the way other mini-protocols are implemented in the
framework. In other words, the server is the initiator who requests new
transactions, and the client is the responder who replies with transactions.

### Handshake mini-protocol

The handshake mini-protocol is used to negotiate the protocol version and the
protocol parameters that are used by the client and the server. It is used first
when a new connection is initialized and consists of a single request from the
client and a single reply from the server. The handshake mini-protocol is a
generic protocol that can negotiate any kind of protocol parameters. It assumes
that protocol parameters can be encoded to, and decoded from Concise Binary
Object Representation (CBOR) terms. A node that runs the handshake protocol must
instantiate it with the set of supported protocol versions and callback
functions for handling the protocol parameters. These callback functions are
specific to the supported protocol versions.

---
title: Cardano architecture
metaTitle: Cardano architecture
---

This section describes the high-level architecture of Cardano, providing details
on the core components and their interactions.

The following diagram outlines the interaction between the system components of
Cardano:

![cardano_components](https://ucarecdn.com/3756645a-a4a2-4d2f-846a-e454bf7cba60/)

## System components

The _current_ implementation of Cardano is highly modular. It includes the
following components (different deployment use cases will use different
combinations of components):

- [Node](https://github.com/input-output-hk/cardano-node)
- [Command line interface (CLI)](https://github.com/IntersectMBO/cardano-cli)
- [Cardano wallet](https://github.com/input-output-hk/cardano-wallet)
- [Cardano db-sync](https://github.com/input-output-hk/cardano-db-sync)
- [GraphQL](https://github.com/input-output-hk/cardano-graphql) API server
  (Apollo)
- [SMASH server](https://github.com/IntersectMBO/cardano-db-sync/tree/master/cardano-smash-server).

## Nodes and remote nodes

A blockchain system consists of a set of nodes distributed across a network that
communicate with each other to achieve
[consensus](/about-cardano/learn/consensus-explained) about the system’s state.

Nodes are responsible for:

- Executing the
  [Ouroboros](https://github.com/IntersectMBO/ouroboros-network?tab=readme-ov-file#ouroboros-network)
  protocol
- Validating and relaying blocks
- Producing blocks (some nodes)
- Providing information about the state of the blockchain to other local
  clients.

### Node process

The `cardano-node` is the top level Cardano component that consists of the other
subsystems, of which the most significant are
[consensus](https://github.com/IntersectMBO/ouroboros-consensus?tab=readme-ov-file#ouroboros-consensus),
[ledger](https://github.com/IntersectMBO/cardano-ledger?tab=readme-ov-file#cardano-ledger),
and
[networking](https://github.com/IntersectMBO/ouroboros-network?tab=readme-ov-file#ouroboros-network)
with ancillary configuration, CLI, logging, and monitoring.

### Node-to-node IPC protocol

The purpose of the node-to-node Inter-Process Communication (IPC) protocol is to
allow for the exchange of blocks and transactions between nodes as part of the
Ouroboros consensus algorithm.

The node-to-node protocol is a composite protocol, consisting of three
mini-protocols:

- **chain-sync**: used for following the chain and getting block headers
- **block-fetch**: used for getting block bodies
- **tx-submission**: used for forwarding transactions.

These mini-protocols are multiplexed on a single long-running Transmission
Control Protocol (TCP) connection between nodes. They can be run in _both_
directions on the same TCP connection to allow for peer-to-peer (P2P) settings.

The overall protocol – and each mini-protocol – is designed for a trustless
setting where both sides need to guard against Denial-of-Service (DoS) attacks.
For example, each mini-protocol uses consumer-driven control flow, so a node
only requests more work when it is ready, rather than having work _pushed_ upon
it.

The protocol design is modular and evolvable: version negotiation is used to
agree on the set of mini-protocols to use, which allows additional or updated
mini-protocols to be added over time without causing compatibility issues.

### Node-to-client IPC

The purpose of the node-to-client IPC protocol is to allow local applications to
interact with the blockchain via the node. This includes applications such as
wallet backends or blockchain explorers. The node-to-client protocol enables
these applications to access the raw chain data and to query the current ledger
state. It also provides the ability to submit new transactions to the system.

The node-to-client protocol uses the same design as the node-to-node protocol,
but with a different set of mini-protocols, and local pipes rather than TCP
connections. As such, it is a relatively low-level narrow interface that exposes
only what the node can provide natively. For example, the node provides access
to all the raw chain data but does not provide a way to query data on the chain.
The job of providing data services and more convenient higher level APIs is
delegated to dedicated clients, such as cardano-db-sync and the wallet backend.

The node-to-client protocol consists of three mini-protocols:

- **chain-sync**: used for following the chain and getting blocks
- **local-tx-submission**: used for submitting transactions
- **local-state-query**: used for querying the ledger state.

The node-to-client version of chain sync uses _full_ blocks, rather than just
block headers. This is why no separate block-fetch protocol is needed. The
local-tx-submission protocol is like the node-to-node tx-submission protocol but
simpler, and it returns the details of transaction validation failures. The
local state query protocol provides query access to the current ledger state,
which contains a lot of interesting data that is _not_ directly reflected on the
chain itself.

:::info

Read more about the
[networking protocol design here](/about-cardano/explore-more/cardano-network/networking-protocol).

:::

## Command line interface (CLI)

The node’s CLI tool is the 'swiss army knife' of the system. It can do almost
everything, but it is quite low level and not very convenient because it’s
text-based and lacks a graphical user interface (GUI).

The CLI tool can:

- query the node for information
- submit transactions
- build and sign transactions
- manage cryptographic keys.

## Daedalus wallet

Daedalus is a full node wallet that helps users to manage their ada, and can
send and receive payments on the Cardano blockchain. Daedalus consists of a
wallet frontend and a backend. The frontend is the graphical application that
users see and interact with. The backend is a service process that monitors the
state of the user’s wallet and does all the 'heavy lifting', such as coin
selection, transaction construction, and submission. The backend interacts with
a local node via the node-to-client IPC protocol, and interacts with the
frontend via a HTTP API. The backend also provides a CLI that enables
interaction with the wallet. The wallet backend can also be used on its own –
without Daedalus – via its API. This is a convenient way for software developers
to integrate Cardano with other applications and systems.

## Cardano DB Sync

The Cardano node stores only the blockchain itself and the associated
information needed to validate the blockchain. This design principle is about
minimizing code complexity (and reducing computational cost and resource use) to
keep the node's local interfaces as minimal as possible and to use external
clients to provide a variety of convenient interfaces and extra functionality.
In particular, the node does not provide a convenient query interface for
historical information on the blockchain. This data service is provided by a
separate component using a Structured Query Language (SQL) database.

Learn more about
[Cardano DB Sync here](https://github.com/IntersectMBO/cardano-db-sync?tab=readme-ov-file#cardano-db-sync).

---
title: Cardano fee structure
metaTitle: Cardano fee structure
---

[Cardano](https://cardano.org/) uses a transaction fee system that covers the processing and long-term
storage cost of transactions.

The Cardano environment is unique in the way it handles fees, as fees do not go
directly to the block producer. Instead, they are pooled and then distributed to
all pools that created blocks during an epoch.

Currently, there are no fees for the memory cost of tracking the accumulated
chain state, in particular, UTXO.

## Preventing economic attacks

The Shelley hard fork event meant that the Cardano blockchain moved from
federated to a fully decentralized environment, which might increase the
incentive for malicious actors to perpetrate economic attacks.

An economic attack might occur where the costs incurred by the operators of a
system are not covered by fees on the users of a given system. These situations
allow users to impose costs on operators without paying the full cost
themselves, which could potentially lead to a severe drop in operator
participation, and might ultimately lead to the collapse of the system itself.

To prevent this situation from arising, it is crucially important to address
both the existing unaccounted operator costs and the new costs.

Cardano's fee structure is quite simple.

Fees are constructed around two constants (a and b). The formula for calculating
minimal fees for a transaction (tx) is a \* size(tx) + b, where:

- a/b are protocol parameters
- size(tx) is the transaction size in bytes.

## Protocol parameters (a and b)

Protocol parameters are values that can be altered by Cardano's update system to
react and adapt to changes in transaction volume, hardware prices, and ada
valuation. Changing these parameters constitutes a hard fork, since it
influences which transactions are accepted by the system.

**Protocol parameter a**

Parameter a reflects the dependence of the transaction cost on the size of the
transaction. The larger the transaction, the more resources are needed to store
and process it.

**Protocol parameter b**

The value of b is a payable fee, regardless of the size of the transaction. This
parameter was primarily introduced to prevent Distributed-Denial-of-Service
(DDoS) attacks. b makes such attacks prohibitively expensive, and eliminates the
possibility of an attacker generating millions of small transactions to flood
and crash the system.

**Protocol parameter x**

x represents the size of the transaction in bytes.

---
title: Cardano monetary policy
metaTitle: Cardano monetary policy
---

Cardano achieves true decentralization with the Voltaire development phase, which includes decentralized voting and
treasury systems to empower the community to influence Cardano's evolution, and
provide a funding mechanism to transform Cardano into a self-funded,
self-sustainable environment.

Decentralization does come with its own set of challenges, as factors such
as security, performance, stability, sustainability, fairness, and crucially,
economic viability come into play.

Maintaining, developing, and improving a blockchain project requires ongoing
monetary support. With Voltaire, the treasury performs this
function, and funding for new features, improvements, etc. is allocated
through a decentralized voting system.

## Stake pools

The principles of decentralization and fairness drive Cardano's development.
Cardano offers equal opportunities to anyone who wishes to become part
of the network by running a stake pool. But the very freedom to participate
opens up the possibility of unfair advantage posed by larger stake pool
operators. This presents the challenge of maintaining the integrity of the chain
while incentivizing people to create pools and support the chain.

## Incentives mechanism

Cardano's monetary expansion relies on a long-term commitment by stake pool
operators to provide ongoing support for the chain. This commitment requires a
solid and stable incentivization mechanism for the operators, so this mechanism
must ensure that the incentive system does not significantly change in time in a
way that might adversely affect the operators’ income.

Cardano's incentive system for stake pool operators is designed to balance k
fully saturated pools (where k is the number of desired pools), so this
equilibrium means that rewards will be optimal for everybody when all stake is
delegated uniformly to the k most attractive pools.

## Monetary policy

Cardano's monetary policy addresses two issues:

- the necessity to offer rewards for people who participate in the network
- funding the treasury.

**Rewards**

The expansion and future improvement of the Cardano blockchain will be greatly
influenced by its community, who need to be incentivized through rewards to
participate in Cardano’s development.

Staking rewards for delegators and stake pool operators come from two sources:

- Transaction fees - fees from every transaction from all blocks produced during
  every epoch go into a virtual 'pot'. A fixed percentage (ρ) of the remaining
  ada reserves is added to that pot.
- Monetary expansion - a certain percentage (τ) of the pot is sent to the
  treasury, and the rest is used as epoch rewards.

This system is designed to ensure that the portion of rewards taken from the
reserves is high at the beginning when transaction numbers are still relatively
low. This incentivizes early adopters to move quickly to benefit from high
initial rewards. Over time, and as the number of transactions increases,
additional fees will compensate for smaller reserves.

This mechanism also ensures that available rewards are predictable and do not
vary dramatically. Instead, rewards change gradually. The fixed percentage taken
from remaining reserves every epoch guarantees a smooth exponential decline.

**Funding the treasury**

The treasury's goal is the provision of funds to develop Cardano activities
through a voting process. This necessitates a process whereby funds are
regularly sent to the treasury to ensure that funds are always available.

## Policy rationale for the ρ and τ values

A lot of thought was put into determining what the values for ρ (fixed
percentage) and τ (funds going into the treasury) should be.

**Calculating ρ**

While searching for the right ρ value, the team faced a quandary: A higher value would mean higher rewards for everybody initially, and the treasury would fill faster. But higher values of ρ would also mean that the reserves would deplete faster. Paying high rewards and incentivizing early adopters is a crucial consideration, but so is to offer a long-term perspective for all stakeholders. Therefore, the solution to this quandary requires a tradeoff between these two issues.

Adopting an exponential decay approach to prevent Cardano’s reserve from running out makes sense in this situation.

Calculating the 'reserve half life' (that is, the time that it takes for half of the reserve to be used up) visualizes the impact of choosing a specific value of ρ over another. This was the subject of much discussion, and eventually, the value assigned was 0.3%. The reason why is that mathematical projections showed that a ρ (the fixed percentage of ada going into the virtual pot every epoch) value of 0.3% would mean a reserve half-life of four to five years. In simple terms, just half of the remaining reserve would be used every four to five years.

**Choosing τ**

Determining the right value for τ (the percentage of unclaimed rewards
automatically going to the reserves every epoch) was equally challenging.
Following discussions, deliberations, and projections, the τ value was set at a fixed rate of 20%. 

This rate applies to both the per-epoch monetary expansion and the fees that the blockchain charges for each transaction. The treasury amount is deducted before any other rewards are allocated. The remaining per-epoch monetary expansion and fees may be allocated to stake pools based on the provided formula. 

It is important to note that the rewards given to stake pools may be reduced under certain conditions, such as when pools are not saturated, not fully pledged, or fail to produce their allocated blocks. Any unclaimed rewards remain in the reserves for future allocation. This explains the slightly varying difference in the percentage of treasury allocations.

---
title: Time handling on Cardano
metaTitle: Time handling on Cardano
---

Time is necessarily relative to every participant in the blockchain system and is critically important to support and maintain the safety properties of the Ouroboros consensus protocol. Minted blocks are expected to be propagated to all nodes in the system in a timely manner, so time requires the construction of a globally acceptable representation for consensus to be reached.

## Time-handling with Ouroboros

Locally, a node computes the passing of time using a 'wall-clock' system. The code for this clock is complicated because the slot length can vary at a hard fork boundary, so the time must be computed carefully taking this into account.

The code performs four steps to get the currentSlot:

1. Waits for some delay corresponding to either the time left till the next slot, or an arbitrary 60s delay if the current slot is unknown, which happens when syncing
2. Gets current system time and translates it to a slot number according to slot length for the current era
3. If the new slot is greater than the previous one, it 'ticks' a new current slot
4. If the above is not true, it either waits a bit longer or reports an error if current time jumped too far back.

The local currentSlot is compared to the slot reported by the tip of the node's ledger. If the latter is older, it is ignored because this means that the node is synchronizing its state with the chain.

Since the slot length can change at a hard fork, consensus can only convert slots to time up to a fixed point in the future – the 'stability window' – in which no hard forks can happen. In practice, the stability window is essential as it provides a measure of time needed to guarantee transaction finality and chain state immutability. During the stability window, the network must produce at least *k* blocks, where *k* is the number of blocks after which the chain becomes immutable. The stability window can take up to 3*k*/f, which is 36 hours with current parameters, or roughly a day.

## Current challenges

There is a fundamental physical limitation to the speed at which information can travel: the speed of light. This implies that synchronizing time over a network of nodes **takes time.**

Network Time Protocol ([NTP](https://www.newyorker.com/tech/annals-of-technology/the-thorny-problem-of-keeping-the-internets-time)) exists to provide a synchronization mechanism, which addresses time limitations and measurement differences. On the other hand, NTP does not guarantee a monotonic increase: time can sometimes jump back and forth a few seconds or even hours. Existing systems providing accurate, precise, and reliable clocks at a global scale are *centralized*, like the global clock provided by [Spanner](https://research.google/pubs/pub39966/), for example.

Currently, on Cardano:

1. Network parameters are set in such a way that the granularity of observable time intervals (eg, block time) on the chain is 20s, which is equal to the slot length (1s) divided by the block coefficient f (the expected block frequency, 0.05). These parameters are unlikely to change in the short-term future.
2. These 20 seconds have been determined as the optimum budget to guarantee the protocol's safety, given the constraints to replicate new transactions and blocks across the network (300ms TCP delay across the globe, with thousands of nodes). While block throughput might be increased in the future, it's unlikely that it will reduce the granularity of observable on-chain time.
3. Transaction finality can be achieved in approximately one day, and *cannot* happen in *less* than a day, according to Ouroboros consensus design. Note that although a high level of confidence is already reached in a matter of minutes or hours, the probability of a block being ultimately discarded decreases exponentially with its depth and the number of nodes that have to adopt this block.

Finally, in the longer term, the current Ouroboros protocol is planned to be replaced with **[Ouroboros Chronos](https://iohk.io/en/blog/posts/2021/10/27/ouroboros-chronos-provides-the-first-high-resilience-cryptographic-time-source-based-on-blockchain/)**. Ouroboros Chronos addresses the timekeeping challenges by providing the first high-resilience cryptographic time source based on blockchain technology.

## The importance of determinism in a blockchain environment

In the current context, determinism means that a given transaction has a 'fixed effect' on the ledger state. But it is important to distinguish between the concepts of *historical* and *prospective* determinism.

Blockchains are based on the principle of replicating a fixed sequence of transactions (grouped in blocks) to reach a consensus about the state of the world. All blockchains have *historical* determinism, meaning that transactions in the chain have a fixed effect, else the results of validating the chain would be non-deterministic, which would break consensus.

But few blockchains have *prospective* determinism, meaning that a transaction that has not yet been added to the chain does have a fixed effect (or won't apply). Cardano does feature prospective determinism.  

On blockchains that do not have prospective determinism users cannot know how much gas fees they need to pay for transactions which then results in such users overpaying for transactions. The lack of prospective determinism is also why a risk exists that a transaction on such blockchains can fail while also consuming lots of gas.

### The power of prospective determinism

Prospective determinism is a very powerful feature of Cardano, for multiple reasons:

* Users know, ahead of time, what a transaction will do, so there are no surprises. This is particularly relevant for scripts because users know exactly:
  * how the scripts will behave, and
  * how much execution budget is needed.
* Proposed transactions can safely be processed in parallel. This parallelism is one of the reasons for Hydra's speed.
* Because users know in advance whether a transaction will fail or not, script failures can be punished harshly (because they will never happen to non-malicious users).
* Broadly, it makes interacting with, and evolving the blockchain easier and more predictable.

Prospective determinism of transactions requires that every part of transaction validation, including script execution, is completely deterministic. This is ultimately why Cardano cannot have non-deterministic operations in scripts.

One of the ways to get prospective determinism is by having the effect of a transaction be entirely determined by the transaction itself and the outputs it references. In the context of Cardano, this is called *locality*. Locality is also of great benefit for users since it means that anyone can know what a transaction does just by looking at the transaction.

## How do Plutus scripts handle time?

Plutus scripts get access to the transaction validity range, defined by its creator. When defining the validity range, a creator can decide that a transaction is valid from slot X to slot Y, or leave one or both of the bounds undefined. This imposes constraints into which slot a transaction can be included, which is very useful on-chain to define various 'contracts'.

The script can assume that the actual time of validation is in this range. Otherwise, the transaction will fail in [phase 1](https://iohk.io/en/blog/posts/2021/09/07/no-surprises-transaction-validation-part-2/) before script execution. This ensures determinism since the script will _always_ see the same piece of information (the validity range) regardless of when the script is validated, so the behavior will be the same.

The limits of the validity interval are expressed in real time (POSIXTime), rather than slots, and the conversion from slots to real time is done by consensus. Using real time rather than slots is important because slot lengths can change at a hard fork, so assumptions based on counting slots are generally unreliable. The fact that scripts see the validity range allows the scripts to make assertions like 'the current time is before/after some given time', but it does not enable a script to make any other kind of assertion ('the second in which this transaction is validated is even', for example.)

In Alonzo's original design, the validity range covered all known uses of time, while also fitting neatly with the existing time-to-live (TTL) field. In theory, it is possible to apply the same principles to other kinds of assertions, for example – let the script rely on assertions checked in phase 1. However, this would not be easy as it implies deep structural changes to various parts of the Cardano network. And because phase 1 checks need to be valid for every node across the network, this precludes any kind of assertion that depends on some local condition, like 'Current Time'.

## Use cases for time

Time has different applications in the Cardano ecosystem.

### Hydra

The Hydra protocol is dependent on time to compute and enforce the contestation deadline, which is a critical part of the protocol's safety mechanism. The Hydra Head state machine tracks the passage of time using UTCTime but the tick comes from the chain, based on the slot number observed from blocks produced by the chain. The reason for using UTCTime addresses the limitations inherent in the slot-to-time conversion the validity window imposes. One cannot convert a slot too far in the future, which means that it is simpler to use UTCTime off-chain and only do conversions when submitting/receiving transactions to or from the chain.

This implies that the tick's granularity is roughly 20s, as this is the expected frequency at which blocks are produced. Using this measure of time, Hydra is available to react to the protocol-relevant crossing of the contestation deadline.

What's important is that local time in the Hydra Head (and nodes) is tied to the on-chain time handled by Ouroboros. As Hydra is an isomorphic protocol, it is desirable to process 'timed transactions' also on layer 2 (see [issue #196](https://github.com/input-output-hk/hydra-poc/issues/196)). However, Hydra does not produce blocks, so the consensus itself does not need a notion of time (yet).

This requires an understanding of the precision and process of discretizing time on a layer 2 ledger. Although the complexities of handling time on-chain also apply to layer 2, layer 2 can provide better solutions since such networks are much smaller, have a shorter lifespan, and do not need to scale globally.

If you're interested in getting involved in discussions and sharing the types of applications you have and the resolution time they'd require, join this [Hydra Discord channel](https://discord.com/channels/826816523368005654/890903732462710836/890951034099335178).

### Marlowe

Marlowe is a domain-specific language (DSL) for writing financial and transactional contracts, nearly all of which involve time. A wide variety of standard financial contracts have been written in Marlowe, including most of the [ACTUS](https://www.actusfrf.org/) standard contracts (eg, loans, swaps, options, and derivatives). Furthermore, Marlowe supports a variety of non-financial contract types such as auctions, token swaps, and even games. Cardano's existing mechanisms for working with time dovetail nicely with Marlowe's semantics and provide Marlowe transactions with locality and determinism inherited from Plutus.

In Marlowe, contract's time typically appears in the deadlines and timeouts that constrain how the execution of the contract evolves, and this works perfectly with Cardano's validity intervals. The timeout logic is needed in a loan contract, for example, to handle the situation where a loan payment is missed: then different logic needs to be executed in order to impose a penalty, adjust the schedule of future payments, etc. Contracts may also directly use the time endpoints of the validity interval in numerical computations, perhaps to adjust future payment amounts based on when an early payment was received. The fact that time appears as an interval has little practical impact on Marlowe because the interval can be as short as the time taken from submitting a transaction to its appearing in a block on the Cardano network – just a few minutes.

## Solutions

Cardano could potentially provide more accurate time-related data during transaction validation, such as the timestamp from the block producer at which the block was minted, converted from its slot, or even the actual timestamp in UTC with milliseconds precision. However, this would _break prospective determinism_ as it does on protocols which do not include this feature: a user could no longer tell if a transaction can definitely apply to the ledger or not, because that would depend on the exact timestamp that the block producer used when creating the block.

Another option is adding various assertion kinds to transaction bodies beyond the validity interval. This is possible, but difficult as already outlined before, hence there needs to be a use case for these assertion kinds to be useful.

Finally, layer 2 networks such as Hydra, can provide increased accuracy through shorter 'slot length' and shorter validity range, alongside decreased latency in transactions finality. Note that the current Hydra Head implementation does not _yet_ provide that level of flexibility.

## Conclusion

Time _is_ a complex topic, especially in a decentralized blockchain setting. There is a clear notion of on-chain time on Cardano with specific constraints and available improvement options in the longer term.

On-chain time behaves in a somewhat different way from the time in traditional software. This divergence is defined in a consistent way to address system constraints while ensuring security and usability for end-users and stake pool operators. Moreover, Cardano's measure of time appears to be good enough to cover multiple use cases, even when compared to traditional finance uses.

---
title: Understanding concurrency
metaTitle: Understanding concurrency
---

Concurrency refers to the ability of different actors to perform work simultaneously without interfering with each other. In Cardano's Extended Unspent Transaction Output (EUTXO) model, transactions can be processed in parallel, enabling multiple operations without causing system failures. This approach enhances system throughput while maintaining the performance of individual operations.

The EUTXO model, employed by Cardano, allows for secure and predictable execution of smart contracts. By splitting logic across different branches and utilizing multiple UTXOs, Cardano achieves greater parallelism, leading to improved scalability.

The level of concurrency sets the limit for simultaneous operations. Developers should adopt strategies to minimize contention and translate concurrency into parallelism, which may involve a learning curve but yields significant benefits.

:::info

Learn more:

- [Concurrency: Cardano smart contracts and the EUTXO model](https://www.essentialcardano.io/article/concurrency-and-all-that-cardano-smart-contracts-and-the-eutxo-model)
- [How to design a scalable Plutus application](https://plutus-apps.readthedocs.io/en/stable/plutus/howtos/writing-a-scalable-app.html)
- [About the order book pattern](https://plutus-apps.readthedocs.io/en/stable/plutus/explanations/order-book-pattern.html).

:::


---
title: Cardano protocol parameters reference guide
metaTitle: Cardano protocol parameters reference guide
---

Protocol parameters on Cardano are the various settings that define the blockchain's behavior. Some protocol parameters are stable and non-updatable while others can be adjusted to adapt to changing conditions over time.

Examples of important Cardano protocol parameters include the maximum block size, the transaction fee structure, the block production schedule, and the minimum ada balance required for staking (read more in the following sections). Parameter choices can have a significant impact on the network's performance, security, and usability, and their alteration must be carefully considered.

The Voltaire governance system for Cardano aims to provide a flexible and adaptable framework that can evolve over time. This includes monitoring and changing the various protocol parameters. The collaboration of founding entities (Input Output Global, the Cardano Foundation, and EMURGO) with the community allows for a decentralized approach to governance and decision making. Cardano can then respond to changing conditions and optimize its performance and functionality to meet the needs of its users.

Additionally, the use of protocol parameters allows for agility and innovation within the network, as new ideas and proposals can be tested and implemented without requiring a hard fork or other disruptive changes to the network's architecture. This enables Cardano to remain at the forefront of blockchain technology and provide a robust and sustainable infrastructure for decentralized applications (DApps) and services.

## Types of protocol parameters on Cardano

Cardano protocol parameters are classified as follows:

-   **Network parameters** – affect Cardano’s connectivity properties and performance. Examples include the maximum number of connections allowed per node, peer discovery algorithm, the network propagation speed, block body and header size, transaction size, etc.
-   **Economic parameters** – affect the economic aspects of blockchain operations. Examples include parameters such as the transaction fee structure, the minimum ada balance required for staking or transacting, stake pool fees, treasury settings, etc.
-   **Technical parameters** – affect technical protocol properties like the consensus algorithm or cost models, for example. These include the stake pool pledge influence, a target number of stake pools, collateral percentage, and others.
-   **Governance parameters** – as Voltaire is rolled out, a new class of parameters will be introduced to define the governance process.
    
Protocol parameters are either updatable or non-updatable:

-   **Updatable** parameters can be adjusted through governance processes. These parameters can be used to change the operation of the block-producing protocol, vary transaction fees, define the influence of pledge, etc.
-   **Non-updatable** parameters affect the fundamentals of the Cardano protocol and are stable, which means that they cannot be changed except via a hard fork. Non-updatable parameters include those defining the genesis block or basic security properties, for example. Some non-updatable parameters may be embedded in the source code or implemented as software. Each major protocol version defines its own set of updatable and non-updatable parameters.
 
Updatable parameters are designed to be flexible and adaptable, allowing the network to evolve over time in response to changing conditions and community needs. Non-updatable parameters, on the other hand, are typically set at the launch of the network or when introducing major changes to the protocol and are intended to provide a stable and secure foundation for the network's operations.

## A list of updatable protocol parameters


| Parameter                                  | Category                         | Description                                                                                                                                                                                                      |
| ------------------------------------------ | -------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| maxBlockBodySize                           | Network                          | Maximum size of a block body. Limits blockchain storage size and communication costs.                                                                                                                          |
| maxTxSize                                  | Network                          | Maximum size of a transaction. While several transactions may be included in a block, the maximum transaction size must be strictly less than the maximum block size.                            
| txFeePerByte aka minFeeA                  | Economic                         | Additional transaction fee per byte of data (in lovelace).                                                                                                                                                       
| lovelacePerUTxOWord aka utxoCostPerWord   | Economic                         | The deposit charged per word of UTXO storage. This parameter sets the cost for storing UTXOs and protects against low-cost denial of service attacks.                                                            
| stakePoolDeposit aka poolDeposit          | Economic                         | Pool deposit (in lovelace).                                                                                                                                                                                      
| treasuryCut aka tau                       | Economic                         | Treasury rate (0.2 = 20%). The proportion of total rewards allocated to treasury each epoch before the remaining rewards are distributed to pools.                                                               
| minPoolCost                                | Economic                         | Minimum pool cost per epoch (in lovelace), which enables the pledge effect.                                                                                                                                     
| txFeeFixed aka minFeeB                    | Economic                         | Base transaction fee (in lovelace).                                                                                                                                                                              
| stakeAddressDeposit aka keyDeposit        | Economic                         | Deposit charged for stake keys (in Lovelace), which ensures that unused keys are returned thus freeing resources.                                                                                                
| monetaryExpansion aka rho                 | Economic                         | Monetary expansion rate per epoch, which governs the rewards that are returned from reserves to the ecosystem (treasury, stake pools, and delegators).                                                           
| executionUnitPrices aka executionPrices   | Economic                         | Execution prices are specified in fractions of lovelace per Plutus CPU execution step or memory unit. These have been set to be consistent with the cost for a full transaction that contains no Plutus scripts. |
| poolRetireMaxEpoch aka eMax               | Technical                        | Maximum number of epochs within which a pool can be announced to retire starting from the next epoch.                                                                                                            
| collateralPercentage                       | Technical                        | Percentage of fee that is used as collateral for a failed transaction.                                                                                                                                           
| stakePoolTargetNum (k parameter) aka nOpt | Technical                        | The target number of stake pools, also known as k, impacts the saturation threshold and encourages the growth of stake pools.                                                                                   
| maxCollateralInputs                        | Technical                        | Maximum number of collateral inputs in a transaction.                                                                                                                                                            |
| maxValueSize                               | Technical                        | The limit on the serialized size of the value in each output.                                                                                                                                                    |
| costModels                                 | Technical                        | Plutus cost models.                                                                                                                                                                                              |
| maxBlockExecutionUnits/exUnitsMem          | Network/Technical              | Maximum number of Plutus execution steps that can be used in a single block.                                                                                                                                     |
| maxBlockHeaderSize                         | Network/ Technical              | Maximum size of the block header, which should be significantly less than the maximum block size.                                                                                                                
| maxTxExecutionUnits/ exUnitsMem           | Network/ Technical              | Maximum number of Plutus memory units that can be used in a single transaction.                                                                                                                                  |
| maxTxExecutionUnits/ exUnitsSteps         | Network/Technical              | Maximum number of Plutus execution steps that can be used in a single transaction.                                                                                                                               
| protocolVersion                            | Technical/ Requires a hard fork | Protocol version. Major versions are “hard forks” such as Byron (1), Shelley (2), Allegra (3), etc.                                                                                                              |
| poolPledgeInfluence aka a0                | Economic/ Technical             | The ‘influence factor’ that governs how much impact the pledge has on rewards.                                                                                                                                   |

You can find a [list of non-updatable protocol parameters here](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0009/README.md#non-updatable-parameters).

[CIP-09](https://cips.cardano.org/cip/CIP-0009), [CIP-28,](https://cips.cardano.org/cip/CIP-0028) and [CIP-55](https://cips.cardano.org/cip/CIP-0055) provide an overview of protocol parameters in the Shelley, Alonzo, and Babbage eras.

---
title: Relevant research papers and specifications
metaTitle: Relevant research papers and specifications
---

Cardano is a third-generation blockchain platform that aims to provide the
community with more advanced features than any protocol yet developed. As a
proof-of-stake blockchain, it is built with the rigor of high-assurance formal
development methods and aims to achieve the scalability, interoperability, and
sustainability needed for real-world applications. Cardano is the first platform
to evolve out of scientific philosophy based on discovery, peer review, and
cryptographic research.

Building a strong foundation in the blockchain industry, Cardano has an ethos of
openness and transparency. All the
[research and technical specifications](https://iohk.io/en/research/library/)
that underpin Cardano are publicly published and available to the community.

Cardano’s development roadmap is divided into 5 eras that focus on the
foundation, decentralization, smart-contract and multi-asset ledger deployment,
scalability, and governance of a pioneering blockchain environment. Relevant
research papers and specifications according to the Cardano development roadmap
are outlined below.

## Byron

A period dedicated to building a foundational federated network that enabled the
purchase and sale of ada. The network ran the proof-of-stake Ouroboros consensus
protocol.

**Relevant research papers:**

[Ouroboros: A Provably Secure Proof-of-Stake Blockchain Protocol](https://eprint.iacr.org/2016/889.pdf)

[Ouroboros-BFT: A Simple Byzantine Fault Tolerant Consensus Protocol](https://eprint.iacr.org/2018/1049.pdf)

**Relevant specifications:**

[A Formal Specification of the Cardano Ledger](https://github.com/input-output-hk/cardano-ledger/releases/latest/download/byron-ledger.pdf "A Formal Specification of the Cardano Ledger")

[Specification of the Blockchain Layer](https://github.com/input-output-hk/cardano-ledger/releases/latest/download/byron-blockchain.pdf "Specification of the Blockchain Layer")

[Formal Specification for a Cardano Wallet](https://iohk.io/en/research/library/papers/formal-specification-for-a-cardano-wallet/)

[Small Step Semantics for Cardano](https://github.com/input-output-hk/cardano-ledger/releases/latest/download/small-step-semantics.pdf)

## Shelley

A period of growth and development for the network focused on ensuring greater
decentralization, which will lead to enhanced security and a more robust
environment once the majority of nodes are run by network participants.

**Relevant research papers:**

[Ouroboros Praos: An adaptively-secure, semi-synchronous proof-of-stake blockchain](https://eprint.iacr.org/2017/573.pdf)

[Ouroboros Genesis: Composable Proof-of-Stake Blockchains with Dynamic Availability](https://eprint.iacr.org/2018/378.pdf)

[Stake-Bleeding Attacks on Proof-of-Stake Blockchains](https://eprint.iacr.org/2018/248.pdf)

[Reward Sharing Schemes for Stake Pools](https://arxiv.org/ftp/arxiv/papers/1807/1807.11218.pdf)

[Account Management in Proof of Stake Ledgers](https://eprint.iacr.org/2020/525.pdf)

[Flexible Formality: Practical Experience with Agile Formal Methods](https://iohk.io/en/research/library/papers/flexible-formalitypractical-experience-with-agile-formal-methods/)

[Coalition-Safe Equilibria with Virtual Payoffs](https://arxiv.org/pdf/2001.00047.pdf)

[Efficient Random Beacons with Adaptive Security for Ungrindable Blockchains](https://iohk.io/en/research/library/papers/efficient-random-beacons-with-adaptive-security-for-ungrindable-blockchains/)

[SecureCyclon: Dependable Peer Sampling](https://iohk.io/en/research/library/papers/securecyclon-dependable-peer-sampling/)

[Practical Settlement Bounds for Longest-Chain Consensus](https://iohk.io/en/research/library/papers/practical-settlement-bounds-for-longest-chain-consensus/)

[Blockchain Participation Games](https://iohk.io/en/research/library/papers/blockchain-participation-games/)

[Consensus Redux: Distributed Ledgers in the Face of Adversarial Supremacy](https://iohk.io/en/research/library/papers/consensus-redux-distributed-ledgers-in-the-face-of-adversarial-supremacy/)

**Relevant specifications:**

[Engineering Design Specification for Delegation and Incentives in Cardano–Shelley](https://github.com/input-output-hk/cardano-ledger/releases/latest/download/shelley-delegation.pdf "Design Specification for Delegation and Incentives in Cardano")

[A Specification of the Non-Integral Calculations in the Shelley Ledger](https://github.com/input-output-hk/cardano-ledger/releases/latest/download/non-integer-calculations.pdf)

## Goguen

The Goguen era will introduce smart-contract functionality to build
decentralized applications while supporting multi-functional assets, fungible,
and non-fungible token standards.

**Relevant research papers:**

[The Extended UTXO Model](https://iohk.io/en/research/library/papers/the-extended-utxo-model/)

[UTXOma: UTXO with Multi-Asset Support](https://iohk.io/en/research/library/papers/utxoma-utxo-with-multi-asset-support/)

[Native Custom Tokens in the Extended UTXO Model](https://iohk.io/en/research/library/papers/native-custom-tokens-in-the-extended-utxo-model/)

[Functional Blockchain Contracts](https://iohk.io/en/research/library/papers/functional-blockchain-contracts/)

[Scripting Smart Contracts for Distributed Ledger Technology](https://eprint.iacr.org/2016/1156.pdf)

[Marlowe: financial contracts on blockchain](https://iohk.io/en/research/library/papers/marlowefinancial-contracts-on-blockchain/)

[Marlowe: implementing and analysing financial contracts on blockchain](https://iohk.io/en/research/library/papers/marloweimplementing-and-analysing-financial-contracts-on-blockchain/)

[Unraveling recursion: compiling an IR with recursion to System F](https://iohk.io/en/research/library/papers/unraveling-recursioncompiling-an-ir-with-recursion-to-system-f/)

[System F in Agda, for fun and profit](https://iohk.io/en/research/library/papers/system-f-in-agdafor-fun-and-profit/)

[Translation Certification for Smart Contracts](https://iohk.io/en/research/library/papers/translation-certification-for-smart-contracts/)

[Message-passing in the Extended UTxO Ledger Model](https://iohk.io/en/research/library/papers/message-passing-in-the-extended-utxo-ledger-model/)

[Structured Contracts in the EUTxO Ledger Model](https://iohk.io/en/research/library/papers/structured-contracts-in-the-eutxo-ledger-model/)

**Relevant specifications:**

[A Formal Specification of the Cardano Ledger with a Native
Multi-Asset Implementation](https://github.com/input-output-hk/cardano-ledger/releases/latest/download/mary-ledger.pdf "A Formal Specification of the Cardano Ledger with a Native Multi-Asset Implementation")

[A Formal Specification of the Cardano Ledger integrating Plutus Core](https://github.com/input-output-hk/cardano-ledger/releases/latest/download/alonzo-ledger.pdf "A Formal Specification of the Cardano Ledger integrating Plutus Core")

## Basho

An era of optimization, improving the scalability and interoperability of the
network. Enhancing the network performance, Basho will introduce _sidechains_,
new blockchains, interoperable with the main Cardano chain, with immense
potential to extend the network’s capabilities.

**Relevant research papers:**

[Proof-of-Stake Sidechains](https://eprint.iacr.org/2018/1239.pdf)

[Hydra: Fast Isomorphic State Channels](https://eprint.iacr.org/2020/299.pdf)

[Interhead Hydra: Two Heads are Better than One](https://iohk.io/en/research/library/papers/interhead-hydratwo-heads-are-better-than-one/)

[Mithril: Stake-based Threshold Multisignatures](https://iohk.io/en/research/library/papers/mithrilstake-based-threshold-multisignatures/)

[Babel Fees via Limited Liabilities](https://iohk.io/en/research/library/papers/babel-fees-via-limited-liabilities/)

[Djed: A Formally Verified Crypto-Backed Pegged Algorithmic Stablecoin](https://iohk.io/en/research/library/papers/djeda-formally-verified-crypto-backed-pegged-algorithmic-stablecoin/)

[State Machines across Isomorphic Layer 2 Ledgers](https://iohk.io/en/research/library/papers/state-machines-across-isomorphic-layer-2-ledgers/)

[Tiered Mechanisms for Blockchain Transaction Fees](https://iohk.io/en/research/library/papers/tiered-mechanisms-for-blockchain-transaction-fees/)

[Approximate Lower Bound Arguments](https://iohk.io/en/research/library/papers/approximate-lower-bound-arguments/)

**Relevant specifications:**

[Formal Specification of the Cardano Ledger for the Babbage era](https://github.com/input-output-hk/cardano-ledger/releases/latest/download/babbage-ledger.pdf "Formal Specification of the Cardano Ledger for the Babbage era")

## Voltaire

A development era enabling the Cardano network to become a self-sustaining
system. Voltaire will introduce a voting and treasury system that will enable
network participants to use their stake and voting rights to influence the
future development of the blockchain.

**Relevant research papers:**

[A Treasury System for Cryptocurrencies: Enabling Better Collaborative Intelligence](https://eprint.iacr.org/2018/435.pdf)

[Updatable Blockchains](https://eprint.iacr.org/2020/887.pdf)

[SoK: Blockchain Governance](https://iohk.io/en/research/library/papers/sokblockchain-governance/)

[Fait Accompli Committee Selection: Improving the Size-Security Tradeoff of Stake-Based Committees](https://iohk.io/en/research/library/papers/fait-accompli-committee-selection-improving-the-size-security-tradeoff-of-stake-based-committees/)

**Relevant specifications:**

[CIP-1694: An On-Chain Decentralized Governance Mechanism for Voltaire](https://github.com/JaredCorduan/CIPs/blob/voltaire-v1/CIP-1694/README.md)

Formal specification is a work in progress

[Formal specification of the Cardano blockchain ledger, mechanized in Agda](https://iohk.io/en/research/library/papers/formal-specification-of-the-cardano-blockchain-ledger-mechanized-in-agda/)

---
title: Byron to Shelley
metaTitle: Moving from Byron Ouroboros Classic to Shelley Ouroboros Praos
---

Cardano's Byron mainnet ran on the [Ouroboros Classic](https://iohk.io/en/research/library/papers/ouroboros-a-provably-secure-proof-of-stake-blockchain-protocol/) consensus protocol. Cardano's Shelley mainnet transitioned to a decentralized network running on the [Ouroboros Praos](https://iohk.io/en/research/library/papers/ouroboros-praos-an-adaptively-secure-semi-synchronous-proof-of-stake-protocol/) consensus protocol, which enabled more extended capabilities while also supporting the staking process with monetary rewards for ada holders and stake pool operators.

To enable orderly transitions in Cardano without any diversions in the system, it was necessary to update the code to support the new protocol’s conditions. Doing so in a single update might have caused a range of complexities, so Cardano decided to take a two-stage approach, using the Ouroboros _Byzantine Fault Tolerance_ (BFT) protocol as an intermediary.

The shift from Ouroboros Classic to BFT (which happened on February 20, 2020) is the only traditional hard fork within the Cardano blockchain. This forking event restarted the Byron mainnet to run the BFT protocol and enable a smoother transition to Ouroboros Praos without any further chain interruptions. The BFT protocol was carefully designed so that blockchain history would remain unchanged, and the blockchain would appear as a single entity.

---
title: Allegra
metaTitle: Allegra - token locking
---

Allegra was the Shelley protocol upgrade that introduced token locking support to enable various kinds of smart contract use cases.

Allegra represented a relatively small technical change to the consensus protocol, with a slight impact on the actual ledger. However, it was significant since it prepared the platform for smart contracts and the creation of assets (in addition to ada) that run on Cardano. It also provided an important piece of Voltaire (governance) functionality, supporting a voting mechanism. 

Token locking is a way of recording that a specific token is being used for some purpose. Locking, in this case, means ‘reserving’ a certain number of tokens for a specified period of time so they cannot be disposed of to gain a benefit (such as voting, or running a smart contract).

:::info  
  
Read more about token locking [in this blog post.](https://iohk.io/en/blog/posts/2020/12/02/goguen-brings-token-locking-to-cardano/)
  
:::

## Token locking use cases

Support for token locking is crucial to enable complex deal settlement and funds accounting.

It can be used in the following scenarios:

- **Contractual agreement** – when someone enters into a contractual agreement, to sell a property, for instance, it is important to promise that this property will not be sold to anyone else – only to the person who actually pays the money. In this case, the token can represent the property and the ‘promise’ – the actual token locking. If the property is sold to a different third party, then the contract becomes void.
- **Vote registry** – within the Voltaire voting mechanism, token locking will enable users to lock a certain amount of their tokens to represent their voting rights. Ada holders who participate in the voting process will be required to ‘lock’ their tokens. This will represent their voting rights, according to the stake that they hold, and eliminate the risks associated with scenarios such as double-counting votes, allocating more votes than possible, contradictory votes, or vote duplication.
- **Multi-asset tokens** – Cardano provides support for multi-asset tokens, where the ledger supports the creation and use of multiple custom token types, besides ada. Token locking allows ada tokens to be ‘locked’, for example, to create another custom asset of equivalent value.

---
title: Mary
metaTitle: Mary - multi-asset support
---

_Mary_ is the Shelley protocol upgrade implemented in March 2021. It introduced
native token and multi-asset support on Cardano. Mary allowed users to create
uniquely defined (custom) tokens and carry out transactions with them directly
on the Cardano blockchain.

With the Mary upgrade, the ledger’s accounting infrastructure processes not only
ada transactions but also transactions that simultaneously carry several asset
types. Native support grants distinct advantages for developers as there is no
need to create smart contracts to handle custom token creation or transactions.
Instead, the accounting ledger tracks the ownership and transfer of assets,
removing extra complexity and potential for manual errors, while ensuring
significant cost efficiency.

Developers, businesses, and applications can create general purpose (fungible)
or specialized (non-fungible) tokens to achieve commercial or business
objectives. These might include the creation of custom payment tokens or rewards
for decentralized applications; stablecoins pegged to other currencies; or
unique assets that represent intellectual property. All these assets can then be
traded, exchanged, or used as payment for products or services.

:::info

Further reading:

- [Learn about native tokens](/developer-resources/native-tokens)

:::

---
title: Alonzo
metaTitle: Alonzo - smart contract support
---

*Alonzo* is the protocol upgrade implemented in September 2021, as part of the Goguen development phase. It built on top of transaction metadata, token locking, and native asset functionality to enable smart contract development.

This upgrade introduced a versatile platform opening up opportunities for businesses and developers, by allowing the creation of smart contracts and decentralized applications (DApps) for decentralized finance (DeFi).

Such capability was enabled by adding the necessary tools and the infrastructure using the Plutus platform. Applying a rigorous approach based on formal methods and verification, Alonzo extended the basic multi-signature scripting language (multisig) used in Cardano Shelley. Multisig was upgraded to the Plutus Core language for more powerful and secure scripting options. For this, Alonzo implemented the [extended unspent transaction output (EUTXO) accounting model](https://iohk.io/en/blog/posts/2021/03/12/cardanos-extended-utxo-accounting-model-part-2/).

:::info

Further reading:
-   [Smart contracts - here we come](https://iohk.io/en/blog/posts/2021/04/08/smart-contracts-%E2%80%93-here-we-come/)
-   [Plutus: what you need to know](https://iohk.io/en/blog/posts/2021/04/13/plutus-what-you-need-to-know/)

:::

---
title: Vasil
metaTitle: Vasil upgrade
---

Vasil is the protocol upgrade implemented in June 2022. Named after the late Bulgarian mathematician and prominent Cardano community member Vasil Dabov, the Vasil upgrade introduced five key mechanisms to improve the blockchain's performance: [CIP-31](https://cips.cardano.org/cip/CIP-0031) (reference inputs), [CIP-32](https://cips.cardano.org/cip/CIP-0032) (inline datums), [CIP-33](https://cips.cardano.org/cip/CIP-0033) (reference scripts), [CIP-40](https://cips.cardano.org/cip/CIP-0040) (collateral outputs), and diffusion pipelining.

Here's a more detailed feature overview.

## Diffusion pipelining

Diffusion pipelining is a feature that improves block propagation times and further leads to higher throughput. In essence, it streamlines the process of sharing information about newly created blocks among network participants. The goal is to ensure that blocks can be shared (propagated) in the network within five seconds after their creation. For this, diffusion pipelining propagates blocks before their full validation thus overlapping the time spent on diffusion with the time needed on validation.

Pipelining also ensures that the block header referencing the hash of a previous block is propagated correctly. The body of the block is retained within the metadata included in the next block, which is essential for DDoS attack resistance even without full block confirmation.

Diffusion pipelining provides more space for block size increase and Plutus script improvements, leading to a more scalable setting overall. 

## Plutus Core changes

Plutus Core is a scripting language used in the Cardano ledger. It consists of basic core  language constructs and also includes built-in types (integers, strings, etc) and built-in functions (integer addition, etc) that provide functionality that would be difficult or expensive to implement in the Plutus Core code. Built-in functions mostly operate on built-in types. Built-in types come with a size metric that is used by costing functions. For example, the size metric for integers returns the bit-size of the integer.

The performance of Plutus Core scripts relates to how expensive it is to run a script in the ledger. The cost model describes CPU and memory fees for each language primitive and can be used off-chain to predict fees for running such scripts.

Model performance is calculated by `costing _evaluation_` in abstract resource units (exunits) of CPU and memory. Individual steps of evaluation are costed, and built-in functions must also come with a `_costing function_` that provides costing information for them. The costing function for a built-in function is a mathematical function that takes the sizes of the arguments (as computed by their size metrics) and returns an estimate of the budget that will be used to perform that function. 

For example, the costing function for addition says that the CPU and memory costs are both proportional to the maximum of the sizes of the input integers (with some appropriate constants). Determining costing functions is done empirically by running the function in question against a large number of inputs and choosing a costing function that fits the data well.

### Scripts in the Cardano ledger

The Cardano ledger recognizes various types of scripts that are identified by ‘language’ version. This language tag allows the ledger to distinguish between different script types. When a new behavior or functionality is introduced, so is a new language.  Each new version of Plutus will be its own language, and all Plutus Core language versions are supported forever in the ledger. This provides the ability to validate the history of the chain indefinitely.

Part of the specification of a language in the ledger explains how language scripts run, what arguments they are given, and how those arguments are structured. Languages also have an associated subset of Cardano protocol parameters that control some aspects of the script evaluation. For Plutus, this is the cost model that is associated with each new language version.

### Plutus evaluator speed improvements

Due to performance improvements in the Plutus evaluator, both Plutus V1 and Plutus V2 scripts have lower cost model parameters than before, resulting in 20-30% improvements in script resource usage. 

### Updated cost model parameters

The updated cost model parameters include the following changes:

1.  Extend the set of built-in functions by adding the new built-in “serialiseData.”

2.  The built-in function “verifySignature” was renamed “verifyEd25519Signature” to make it more clear what its function is.
    
3.  Recalibrate the cost model for the version of the evaluator in the node to align with the CPU parameters changes.
    
### New Plutus Core built-in

The built-in types and type operators remain unchanged from the Alonzo upgrade. All the new built-in functions are backward compatible. Adding them does not break any older script validators. The Vasil release continues to support the Alonzo built-in functions and adds the following new function:

**serialiseData**

A new Plutus built-in is added for serializing `BuiltinData` to `BuiltinByteString`. The serialiseData function takes a data object and converts it into a [CBOR](https://cbor.io/) object.

Plutus already provides a built-in for hashing data structure, for example, `sha2_256 :: BuiltinByteString -> BuiltinByteString`, it does not provide generic ways of serializing some data types to `BuiltinByteString`.

The overall memory and CPU costs are reduced by having a new built-in to serialize any Plutus ‘BuiltinData’ to ‘BuiltinByteString’ such that validators can leverage more optimized implementations and bytestring builders via built-ins than what is available on-chain.

For more explanations, how-to guides, and tutorials, see [Plutus Docs.](https://plutus.readthedocs.io/en/latest/index.html)

### Plutus script addresses

*A Plutus V2 script does not have the same hash value as a Plutus V1 script.*

Since scripts must match their on-chain hashes exactly, it is important that the scripts that an application uses do not accidentally change. For example, changing the source code or updating dependencies or tooling may lead to small changes in the script. As a result, the hash will change. In cases where the hashes must match exactly, even changes which do not alter the functionality of the script can be problematic.

In light of this consideration, some DApp developers might expect that when doing a migration from Plutus V1 scripts to Plutus V2 scripts, the same source code, when recompiled, will generate the same hash value of that script address. However, it is impossible for a compiled V2 script to have the same script hash and address as a compiled V1 script. 

Using the exact same script with different language versions will result in different hashes. The exact same script (as in UPLC.Program) can be used as a Plutus V1 script or a Plutus V2 script, and since the language version is part of the hash, the two hashes will be different. 

**A Plutus V1 script will not necessarily have the same hash value when recompiled with a later version of the Plutus compiler**

Suppose you write your Haskell source code (Plutus Tx), compile it into Plutus Core code (PLC), generate its hash value, then use it in a transaction. If you don’t save your compiled code, and then decide to use the same script in the future, you would have to recompile it. This could result in a different hash value of the script address even without upgrading from Plutus V1 to Plutus V2 scripts. This is because the hash is computed based on the output of the compiled code. 

Given Plutus compiler version changes, changes in the dependencies, and multiple other improvements, it is expected that the hash value of the script address will change after the source code is recompiled. 

**When to export and save the output of a compiled script**

Once you expect that you will not modify the on-chain part of your application and you don’t want the hash value of your script address to change, the best way to keep it the same is to save the output of your final compiled Plutus Core code (PLC) to a file.

For details on how to export scripts, please see: [How to export scripts, datums, and redeemers](https://plutus.readthedocs.io/en/latest/howtos/exporting-a-script.html) in the Plutus Core user documentation.

## Reference inputs (CIP-31)

Transaction outputs carry datums, which enable access to information on the blockchain. However, these datums are constrained in a number of ways. For example, to access information in the datum, you’d have to spend the output that the datum is attached to. This requires the re-creation of a spent output. Any user who wishes to look at the data cannot spend the old output (which is gone), but must spend the new output (which they will not know about until the next block). In practice, this limits some applications to one ‘operation’ per block, thus decreasing the desired performance.

CIP-31 introduces a new mechanism for accessing information in datums – a reference input. Reference inputs allow looking at an output without spending it. This will facilitate access to information stored on the blockchain without the need for spending and re-creating unspent transaction outputs (UTXOs).

The key use case of CIP-31 is to support reference scripts (CIP-33). Other use cases include:

1.  Inspecting the state (datum, or locked value) of an on-chain application without having to consume the output. For example, checking the current state of a stablecoin state machine.
2.  The ability to reference on-chain data providers that store data in outputs by other scripts.

See the [how to use reference inputs](https://github.com/perturbing/vasil-tests/blob/main/reference-inputs-cip-31.md) tutorial for more details. 
    
## Inline datums (CIP-32)

Datums carrying transaction information are commonly implemented by attaching hashes of datums to outputs. This is quite inconvenient for users. Datums tend to represent the result of computation done by the party who creates the output, and as such, there is almost no chance that the spending party will know the datum without communicating with the creating party. This means that either the datum must be communicated between parties off-chain, or on-chain by including it in the witness map of the transaction that creates the output (‘extra datums’). Such a case requires the spending party to watch the whole chain to find the datum, which is also inconvenient.

CIP-32 suggests a solution that allows datums themselves to be attached to outputs instead of datum hashes. This will allow much simpler communication of datum values between users.

**Use cases** include:

-   Creating a single UTXO with data to be used in multiple subsequent transactions, but only paying the cost for submitting it once.
-   Storing little information on-chain. For example, Oracles can benefit from this by simply adding some off-chain data to the main chain.

See the [how to use inline datums](https://github.com/perturbing/vasil-tests/blob/main/inline-datums-cip-32.md) tutorial for more details.
    
## Reference scripts (CIP-33)

When you spend an output locked with a Plutus script, you must include the script in the spending transaction. Hence, the size of the scripts contributes to transaction size, which directly influences Cardano’s throughput.

Large script sizes pose problems for users because:

1.  Larger transactions result in higher fees.
2.  Transactions have size limits. Large scripts can hit the limits. Even if one script fits, multiple scripts in one transaction might not fit. This makes it difficult to execute complex transactions that rely on several scripts.
    
CIP-33 introduces the ability to reference a script without including it in each transaction. This hugely reduces the contribution of scripts to the transaction size.

See the [how to reference scripts](https://github.com/perturbing/vasil-tests/blob/main/referencing-scripts-cip-33.md) tutorial for more details.

## Transaction redeemers

Two important elements in Plutus are *datums* and *redeemers*. The datum is a piece of information that can be associated with a UTXO and is used to carry script state information. It is frequently used in combination with a redeemer, which is like an instruction or command to the contract.

With the Vasil hard fork, developers can see redeemers for all inputs rather than just the one being passed to the currently executing script. 

## Collateral change address

Script collateral is the monetary guarantee a user gives to assure that the transaction that uses a contract has been carefully constructed and thoroughly tested before submission to the validators. It is used to guarantee that nodes are compensated for their work in case phase-2 validation fails. The collateral amount is specified at the time of constructing the transaction and is reserved to allow for the on-chain script execution. 

With the Vasil hard fork, DApp developers can specify a change address for the script collateral. This means that in case the script fails phase-2 validation, only the right amount will be taken, and the remaining funds will be sent to the provided change address.

See the [how to use collateral outputs](https://github.com/perturbing/vasil-tests/blob/main/collateral-output-cip-40.md) tutorial for more details.

## Single VRF implementation

On Cardano, the Verifiable Random Function (VRF) determines which SPO creates the next block. Before Vasil, there were two VFR functions executed on every network hop to validate a block. With the Vasil hard fork, one of these functions is dropped, resulting in faster block validation and overall network syncing times. 

---
title: Valentine (SECP) 
metaTitle: Valentine (SECP) upgrade
---

The Valentine (or SECP) upgrade is Cardano’s intra-era hard fork that followed the Vasil upgrade. Valentine was a small and focused semantic change to the ledger, which brought new built-in functions to Plutus to support SECP elliptic curves (ECDSA and Schnorr). Although an intra-era hard fork requires a hard fork combinator event, it does not change the ledger era, which means that this was an upgrade to the Babbage ledger era.

## About ECC

ECC is a popular primitive for developing cryptographic protocols and secure applications using custom encryption and decryption algorithms validated by digital signatures. ECC provides the same level of security as other mechanisms while using shorter keys and signatures.

There are different elliptic curves one can use, with secp256k1 as one of the options. Each of these curves differs in its parameters. The secp256k1 curve provides two common signature schemes – ECDSA and Schnorr.

Cardano uses the Edwards-curve Digital Signature Algorithm (EdDSA) with elliptic curve Curve25519 as its base curve (Ed25519). Ed25519 is designed to be resistant to certain types of cryptographic attacks, making it a secure choice.

Ed25519 is part of the family of [safeCurves](https://safecurves.cr.yp.to/), which secp256k1 is not part of. The variance in algorithms means that Plutus DApp developers who want to work with other blockchains and need to validate ECDSA and Schnorr signatures would have to spend time, effort, and funds to implement such algorithms over the Standards for Efficient Cryptography (SECP) elliptic curves in Plutus. This extra implementation considerably increases potential security risks.

Since only Cardano’s primary signature algorithm Ed25519 was provided as a Plutus built-in function, ECDSA and Schnorr operations would be more expensive and time-consuming unless also provided as built-in functions.

**What did the SECP upgrade bring?**

Cardano’s Valentine upgrade added new built-in functions to Plutus to support ECDSA and Schnorr signatures along with Cardano’s native signature.

These built-in functions are now native to Cardano, and since they are implemented and audited by experts, they provide the highest level of security. This standardization allows any Plutus DApp developer to widen the choice of multi-signature or threshold signature design to use.

[CIP-49](https://github.com/mlabs-haskell/CIPs/blob/c5bdd66fe49c19c341499f86cebaa2eef9e90b74/CIP-0049/README.md) provides a more in-depth oversight of the motivation and specification for the new implementation of built-in functions.

After the new cryptographic primitives implementation, Plutus can easily verify transactions from other blockchains using ECDSA and Schnorr standards. For example, Plutus can natively verify signatures generated in EVM sidechains, which improves the developer experience in terms of process simplicity, cost, and advanced security.

![cardano-secp](https://ucarecdn.com/7c41014d-2a03-493e-83f1-054c6e3ac78f/)

## Example scripts

Below is a link to examples of scripts and script data files containing the inputs for working with SECP256k1 elliptic curves.

The use of these scripts is similar to a [token minting process](https://github.com/input-output-hk/cardano-node-wiki/blob/main/docs/reference/plutus/plutus-minting-script-example.md), where you build a transaction to mint a token using `--mint-script-file` with a provided Plutus script and `--mint-redeemer-file` for provided input script data.

See the tutorial on [how to use SECP256K1 primitives](https://github.com/input-output-hk/Vasil-testnet/blob/main/secp-primitives-cip.md).

---
title: Chang
metaTitle: Chang - on-chain participatory governance
---

The Chang upgrade ushered in the Conway ledger era, a deeply transformative advancement for Cardano's governance, where any ada holder can submit or participate in the voting process for governance actions. The upgrade happened in two stages, Chang #1 and Chang #2, now known as the Plomin hard fork.

Conway, named after the eminent mathematician [John Horton Conway](https://en.wikipedia.org/wiki/John_Horton_Conway), builds upon previous phases, emphasizing decentralized decision-making and enhanced smart contract functionalities.

 In November 2024, following the sudden passing of Matthew Plomin, the hard fork working group recommended to rename the second upgrade after him. As a founding partner of the USDM Stablecoin, Matt was instrumental in the construction and regulatory compliance of groundbreaking financial technologies. The renaming has been widely affirmed by the community in a following governance info action. 

## What does Chang enable?

:::info

Learn more about Cardano governance and related terminology in [CIP-1694](https://github.com/cardano-foundation/CIPs/tree/master/CIP-1694).
:::

Chang enables on-chain participatory governance through the implementation of mechanisms outlined in CIP-1694. This upgrade ensures transparency, inclusivity, and resilience of governance in the Cardano ecosystem. The Conway era is based on liquid democracy, which enables individual empowerment through democratic consent by leveraging a voting process with the option of direct and representative voting. Ada holders can vote individually on every governance matter, or delegate their voting power to [delegate representatives (DReps)](https://iohk.io/en/blog/posts/2022/04/11/introducing-the-concept-of-delegate-representatives-dreps/) they trust in decision making. DReps consolidate the voting influence of numerous ada holders, ensuring that each participant can contribute to the future of Cardano’s development.

With the introduction of [PlutusV3](https://iohk.io/en/blog/posts/2024/02/12/unlocking-more-opportunities-with-plutus-v3/), Plutus scripts are available for use as part of the governance system, enabling sophisticated voting possibilities like supporting DAOs, for example. Chang also brings advanced Plutus cryptographic primitives, other new primitives, and performance enhancements to the platform. These additions provide developers with a richer smart contract creation toolkit, enhancing both developer and user experiences, and unlocking new possibilities for decentralized applications (DApps) on Cardano.

### Chang’s key features

**Decentralized governance mechanisms:** through Chang, Cardano institutes on-chain decentralized governance mechanisms including a constitutional committee (CC), DReps, and a new governance role for stake pool operators (SPOs), fostering community-driven decision-making processes.

**On-chain voting:** Chang facilitates on-chain voting for governance actions, allowing stakeholders to propose and vote on protocol changes and other governance-related matters.

**PlutusV3 ledger language**: [PlutusV3](https://iohk.io/en/blog/posts/2024/02/12/unlocking-more-opportunities-with-plutus-v3/) introduces advanced Plutus primitives and cryptographic capabilities, enhancing the platform's smart contract functionalities. Cryptographic primitives are central to every blockchain, including hash functions, digital signatures, and zero-knowledge proof (ZKP) systems. These elements are designed for computational efficiency, ensuring robust security for their applications. PlutusV3 enhancements include:

- **BLS12-381 primitives.** This collection of new built-in functions is fundamental for cryptographic operations that allow for efficient verification of zero-knowledge proofs.
- **New hash primitives**. Blake2b-224 and [Keccak-256](https://cips.cardano.org/cip/CIP-101) are new cryptographic hash functions used for validating transaction signatures (improving compatibility with Ethereum), and secure data verification respectively.
- **Sums of products (SOPs)**. PlutusV3 introduces SOPs, a new approach to encoding data types in Plutus Core. This method improves script efficiency and code generation for Plutus Core compilers, leading to faster program execution and streamlined smart contract development.
- **Bitwise primitives**. The Chang upgrade brings two new bitwise primitives – `integerToByteString` and `byteStringToInteger`. Additional primitives will be added to Plutus V1, V2, and V3 with future upgrades. These primitives offer robust capabilities for low-level bit manipulations. This enhances performance optimization, cryptographic support, integer-string conversions, and efficient algorithms and data structures within smart contracts.

## Additional functionalities 

Conway also introduces several new functionalities:

- Reference scripts are enabled for Plutus V1 scripts. This enables legacy scripts to take advantage of reference scripts, reducing DApp costs and increasing on-chain capacity.
- The `minFeeRefScriptsCoinsPerByte` protocol parameter refines the Plutus cost model and improves fee calculations by allowing separate tuning of the costs for reference scripts.
- Adjustments to SPO deposits: SPO deposits are tracked individually, distinguishing between stake address registration deposits and stake pool registration deposits. This distinction has been retroactively applied all the way back to Shelley, facilitated by the static nature of deposits since their introduction.
- General ledger rules improvements.

## The Chang upgrade in stages

Chang was implemented in two stages. The first stage established foundational governance features, and the second will emphasize community-driven decision-making and governance.

**First stage/bootstrapping**

The first stage was completed in August, 2024. Its purpose is to enable initial governance features and lay the groundwork for Conway-era functionality. During this time, DReps can register, and ada holders can delegate their voting power. PlutusV3 was introduced, which included new governance features.

An [interim constitution](https://docs.intersectmbo.org/cardano/cardano-governance/cardano-constitution/interim-cardano-constitution) was established with safeguards supported by a technical guardrails script. An interim constitutional committee oversees the constitutionality of governance actions during the interim period, prior to ratification of a full constitution. The bootstrap period is used to form the group of DReps. Limited governance actions are supported, including parameter changes, hard forks, and 'Info' actions.

During the interim period, only SPOs and the interim constitutional committee can vote on governance actions, including possible protocol parameter changes. As an exception, DReps can participate in 'Info' actions at this stage.

The goal of the first stage is to ensure security and continuity during the governance bootstrapping phase, allowing DReps to register and campaign for delegation, while developing and ratifying a final constitution.

**The second stage/on-chain decision making**  

The second stage, which emphasized decentralized governance, was initiated by the community after the bootstrap period. The governance action to approve this Plomin hard fork was submitted on chain at the end of epoch 529, December 20, 2025 at 21:44 UTC. [Intersect](https://www.intersectmbo.org/) oversaw, established, and helped facilitate the community in self-government initiatives for the Cardano ecosystem. All governance entities, including DReps, SPOs, and the interim constitutional committee, can vote on governance actions during this stage. The conclusion of this stage allowed for treasury withdrawals.

**The Cardano constitution**

The Cardano Constitutional Convention was held simultaneously in Buenos Aires - Argentina, and Nairobi - Kenya from December 4 to 6, 2024. After 63 workshops across 51 countries, participants came together to refine and strengthen the constitution. The draft constitution was agreed to by 95% of elected delegates. 

The delegate-approved Cardano constitution is the foundational document that translates the community’s values and decisions into a clear set of principles, rules, and responsibilities. It defines the governance framework for the Cardano ecosystem, outlining stakeholders' roles and obligations, including DReps, SPOs, and others. This constitution guides Cardano toward becoming a sustainable platform for Web3 products and services.

---
title: Plomin
metaTitle: Plomin - the second governance upgrade in the Conway era
---

# Plomin hard fork

Plomin marks the second governance upgrade in the Conway era, completing Cardano's on-chain governance features under [CIP-1694](https://github.com/cardano-foundation/CIPs/tree/master/CIP-1694) and marking the transition to community-driven decision-making. See also the companion explainer for [Chang](/about-cardano/evolution/upgrades/chang).

## What is Plomin

Plomin is the January 29, 2025 hard fork (epoch 537, protocol version 10.0) that introduces the **second batch** of CIP-1694 governance capabilities: it enables the full set of governance actions and activates the **delegated representative (DRep)** role for ada holders.

## Why it matters

With Plomin, Cardano completed the Conway governance era and moved to **decentralized on-chain governance**. Ada holders can now participate directly or via DReps in decisions such as protocol updates, treasury withdrawals, and future hard forks. The upgrade was enacted following approval by stake pool operators (SPOs) and the interim constitutional committee (ICC), making it the first community-driven hard fork in Cardano history.

## Background

Cardano introduced on-chain governance in two stages within the Conway era: **Chang #1** and **Chang #2**. In November 2024, the community affirmed renaming the second stage to **Plomin** in honor of Matthew Plomin.

## Governance scope after Plomin

Plomin enables a **tripartite** governance system in which representative bodies can consider and ratify major decisions. In the immediate post-fork period, Cardano’s elected delegates focus on three foundational items: **the constitution, the budget, and the roadmap** – each to be decided through on-chain processes now made possible by Plomin. 

## Feature overview

* Full set of CIP-1694 governance actions
* Activation of the DRep role for delegated voting
* Community-driven hard fork, approved by SPOs and the ICC
* Completion of the Conway governance era under Voltaire.

---
title: Design rationale
metaTitle: Cardano design rationale
---

[Cardano](https://cardano.org/) is an open source
[proof-of-stake](/about-cardano/new-to-cardano/proof-of-stake) blockchain
project that began in 2015 to address existing blockchain challenges in the
design and development of cryptocurrencies. It aims to provide a more balanced
and sustainable ecosystem that better accounts for the needs of its users as
well as other systems seeking integration.

The first generation of blockchains (like Bitcoin) offered decentralized ledgers
for secure cryptocurrency transfer. However, such blockchains did not provide a
functional environment for complex deal settlement and decentralized application
(DApp) development. As blockchain technology matured, the second generation
(like Ethereum) provided more enhanced solutions for writing and executing smart
contracts, application development, and the creation of different token types.
On the other hand, the second generation of blockchains often faces issues in
terms of scalability.

Cardano is conceived as the third-generation blockchain as it combines the
properties of the prior generations and evolves to meet all the arising needs of
users. When comparing blockchain properties, many aspects should be considered.
Thus, the best solution must ensure the highest security, scalability
(transaction throughput, data scale, network bandwidth), and functionality
(besides transaction processing, the blockchain should provide all means for
business deal settlement). Moreover, it is important to ensure that blockchain
technology is constantly developing in terms of sustainability and is
interoperable with other blockchains and financial institutions.

To address these needs, Cardano has been built focusing on such core concepts
as:

- **Scalability** – ensures that the Cardano network is capable of processing an
  increasing number of transactions as user demand grows. Scalability also
  provides higher bandwidth capabilities to allow transactions to carry a
  significant amount of supportive data that can be easily managed within the
  network. For these needs, Cardano is implementing various techniques (like
  data compression for instance), and introduces such scaling solutions as
  [Hydra](https://hydra.family/head-protocol/) and
  [Mithril](https://mithril.network/doc/), for example. Read more about the
  [research underpinning Cardano's scalability here](https://www.essentialcardano.io/article/an-analysis-of-the-research-underpinning-cardanos-scalability).
- **Interoperability** – ensures the most multi-functional environment for
  financial, business, or commercial operations by enabling users to interact
  with different blockchain systems. Cardano aims to support cross-chain
  transfers, multiple token types, and commonly used smart contract languages.
  Read more about the concept of
  [partner chains](https://iohk.io/en/blog/posts/2023/11/03/partner-chains-are-coming-to-cardano/).
- **Sustainability** – designing a proof-of-stake blockchain means it is vital
  to ensure that the system is self-sustainable. To drive growth and maturity in
  a truly decentralized manner, Cardano is built to allow the community to
  maintain its continuous development by participating, proposing, and
  implementing system improvements. This is now being implemented through
  [CIP-1694](https://cips.cardano.org/cip/CIP-1694) on-chain governance
  mechanisms.

## Cardano advantages

- **Academic research** – formal methods, such as mathematical specifications,
  property-based tests, and proofs, are the best way to deliver high assurance
  software systems and give confidence to users for the management of digital
  funds. Cardano has been built using formal methods to achieve strong
  guarantees on the functional correctness of core components of the system. All
  of the research and technical specifications that underpin Cardano are
  publicly available, and all Cardano development activity is published online.
- **System design** – Cardano is written in Haskell, a secure functional
  programming language that encourages building a system using pure functions,
  which leads to a design where components are conveniently testable in
  isolation. Advanced features of Haskell enable employing a whole range of
  powerful methods for ensuring code correctness, such as basing the
  implementation on formal and executable specifications, extensive
  property-based testing, and running tests in simulation.
- **Security** –
  [Ouroboros](https://iohk.io/en/blog/posts/2020/06/23/the-ouroboros-path-to-decentralization/)
  (the Cardano proof-of-stake protocol) establishes rigorous security
  guarantees; it was delivered with several peer-reviewed papers presented in
  top-tier conferences and publications in the area of cybersecurity and
  cryptography.
- **Energy efficiency** – Cardano is a proof-of-stake blockchain. In contrast to
  proof-of-work blockchains,
  [Cardano requires much less energy](https://iohk.io/en/blog/posts/2021/08/17/why-they-re-calling-cardano-the-green-blockchain/)
  and computational power. The Bitcoin network is secured through computers
  doing ever-more-energy-intensive computations – proof of work – which is
  unsustainable in the long term. Cambridge University has an online tool that
  shows the [computers powering Bitcoin](https://cbeci.org/) already consume
  more electricity than some countries, like
  [Switzerland](https://www.bfe.admin.ch/bfe/en/home/supply/statistics-and-geodata/energy-statistics/overall-energy-statistics.html)
  for example.
- **Seamless upgrades** – traditionally, blockchains upgrade using hard forks.
  When conducting a hard fork, the current protocol would stop operating, new
  rules and changes would be implemented, and the chain would restart – with its
  previous history being erased. Cardano handles hard forks differently. Instead
  of implementing radical changes, the Cardano
  [hard fork combinator technology](https://iohk.io/en/blog/posts/2020/05/07/combinator-makes-easy-work-of-shelley-hard-fork/)
  ensures a smooth transition to a new protocol while saving the history of the
  previous blocks and not causing any disruptions for end users.
- **Decentralization** – Cardano is maintained by over 3,000 distributed stake
  pools that are operated by the community. All blocks and transactions are
  validated by network participants without any reliance on a centralized
  authority.
- **Functional environment for business use cases** – Cardano is establishing a
  foundation for global, decentralized finance to develop a range of DApps that
  can run using functional and domain-specific smart contracts, providing
  multi-asset tokens for any needs.

---
title: Development phases and eras 
metaTitle: Development phases and eras on Cardano
---

:::info

This overview is based on [CIP-59](https://cips.cardano.org/cip/CIP-0059).

::: 

Cardano’s development follows a well-defined and clearly communicated [roadmap](https://roadmap.cardano.org/en/). Firmly based on academic research and rigorous testing, this process has resulted in a chain with zero outages.

Cardano has gone through multiple development *phases and eras enabled by hard fork combinator events*. The following concepts are explained below:

- **Development phase** – a high-level collection of features described on the [Cardano roadmap](https://roadmap.cardano.org/en/). 
- **Ledger era** – a collection of ledger features introduced with a hard fork. Starting with Alonzo, eras are planned to be named after mathematicians and computer scientists in *a, b,c* order. 
- **Intra-era hard fork** – a small and focused semantic change to the ledger requiring a hard fork.
- **Consensus mechanism** – a collection of consensus features introduced with a hard fork. Historically, these have featured the name ‘Ouroboros’.
- **Ledger protocol** – a collection of ledger features between the consensus layer and the ledger layer, roughly characterized by block header validation.

## Development phases

Cardano’s development phases include Byron, Shelley, Goguen, Basho, and Voltaire – all named after poets except for Goguen, a computer scientist.

 -   **Byron**. Byron set the foundation for Cardano development allowing users to buy and sell ada on a proof-of-stake blockchain network. Initially, the Cardano ledger was established as a federated network, where block production and transaction validation were maintained by [the founding entities](https://www.essentialcardano.io/article/founding-and-iog-organization). Byron saw the delivery of [Daedalus](https://daedaluswallet.io/) and Yoroi wallets, and also provided users with a Block Explorer ‒ a tool specifically designed for browsing the blockchain.
 -   **Shelley**. The Shelley development theme introduced a decentralized ledger creating a completely new economic system, which drives the network’s growth and gradual optimization. Shelley evolved from Byron’s federated network maintenance, with more and more blocks being produced by the distributed stake pool operator community. This theme focused on many critical steps that ensure enhanced user experience in terms of stake pool operation, delegation preferences, and incentives.
 -   **Goguen**. Goguen development focused on the establishment of a global, financial, and multi-functional system for decentralized application (DApp) building, smart contract support, and custom token issuance. Goguen is a key building block to establish a versatile platform to build solutions around such application domains as supply chain, track and trace, finance, medical records, identity voting, property registration, P2P payments, and many others.
 -   **Basho**. Basho focuses on Cardano’s optimization in terms of improving the scalability and interoperability of the network. Whereas other development stages focus on decentralization and new functionality, Basho is about improving the underlying performance of the Cardano network to better support growth and adoption for applications with high transaction volume.
 - **Voltaire**. Decentralized governance and decision making lie at the heart of Voltaire granting the Cardano community the ability to vote on network development updates, technical improvements, and project funding. For the Cardano network to become more decentralized, it requires not only the distributed infrastructure introduced during Shelley but also the capacity to be maintained and improved over time in a decentralized way.

## Ledger eras

There are several eras within the evolution of Cardano. Each era refers to the rules of the ledger. For example, what transaction types and what data is stored in the ledger, or the validity and meaning of the transactions.

### Byron and Shelley eras

The evolution of the Cardano mainnet began with the Byron ledger rules. The mainnet underwent a hard fork in late July 2020 to switch from the Byron rules to the Shelley ledger rules. It was a full reimplementation of Cardano, which enabled two fundamental changes: the support for multiple sets of ledger rules, and the management of the hard fork process of switching from one set of rules to the next. In other words, the new implementation could support both the Byron rules and the Shelley rules, which meant that, when deployed to the mainnet in early 2020, the implementation was fully compatible with the Byron rules. This allowed for a smooth transition from the old to the new implementation. Once all Cardano users had upgraded their nodes to the new implementation, it became possible to invoke the hard fork combinator event and switch to the Shelley rules.

### Allegra, Mary, and Alonzo eras

Allegra, Mary, and Alonzo eras are all part of the *Goguen development phase.*

Starting with Goguen, the ledger team introduced the notion of *era* into the ledger code. Shelley ledger rules then became ‘the Shelley era’.

Because Goguen features were implemented in steps, each set of functionality was introduced with a different hard fork, hence there were several ledger eras:

- Allegra: introduced token locking support
- Mary: brought native tokens and multi-asset functionality to Cardano
- Alonzo: introduced smart contract support.

The names Allegra and Mary were chosen for their connection to the poet Percy Shelley and were only intended to be used as [variable names](https://github.com/input-output-hk/cardano-ledger/blob/1cbf1fc2bb005a8206e5b5a7cdf44d35baaca455/eras/shelley-ma/impl/src/Cardano/Ledger/Allegra.hs#L40) for a very specific abstraction used in the ledger code.

Goguen, the smart contract development phase, was the only phase named not after a poet. So the name of the ledger era that introduced smart contracts was named after Alonzo Church – the person who invented the lambda calculus (Plutus Core uses a variant of [system F](https://en.wikipedia.org/wiki/System_F)). 

Going forward, the teams decided to use names in *a,b,c* order, after individuals who contributed to mathematics and computer science. One lack of consistency to notice is that eras can use both first and last names. This is driven by conciseness.

### Babbage era

The Babbage ledger era introduced features such as inline datums, reference scripts, and reference inputs as part of the _Basho development phase_. The release is also known as Vasil, named to honor the late Bulgarian mathematician and Cardano ambassador Vasil Dabov.

The Vasil upgrade significantly enhanced Cardano’s performance and developer experience. It introduced reference inputs ([CIP-31](https://cips.cardano.org/cip/CIP-31)), which allow transactions to read UTXO data without consuming it; inline datums ([CIP-32](https://cips.cardano.org/cip/CIP-32)), which embed datums directly inside outputs; and reference scripts ([CIP-33](https://cips.cardano.org/cip/CIP-33)), enabling developers to reuse existing scripts rather than resubmitting them. It also added collateral outputs ([CIP-40](https://cips.cardano.org/cip/CIP-40)) for improved fallback handling and diffusion pipelining to enhance block propagation and network throughput.

These features collectively improved the efficiency, expressivity, and scalability of the Cardano network, laying a stronger foundation for advanced smart contracts and DApp development.

The Valentine (SECP) upgrade, executed on February 14, 2023, was an intra-era hard fork in the Babbage ledger era that followed the Vasil upgrade. It introduced new Plutus built-in functions for SECP elliptic curves – enabling native verification of ECDSA and Schnorr signatures on Cardano. This focused change improved interoperability and developer experience without changing the ledger era.
Key facts:
- Intra-era hard fork that kept the ledger in Babbage and followed Vasil
- Added Plutus built-ins for ECDSA and Schnorr; implemented and audited by experts to provide a high level of security
- Helps DApp developers verify signatures from other ecosystems (including EVM sidechains) more simply and cost-effectively
- Took place on February 14, 2023 (epoch 394), protocol version 8.0.

### Conway era

The Conway ledger era is part of Cardano’s _Voltaire development phase_, focused on transforming the network into a self-governing system. It encompasses two hard forks: _Chang_, which introduced the initial governance framework, and _Plomin_, which completed the governance feature set (including DReps, treasury withdrawals, and full governance actions) under [CIP-1694](https://cips.cardano.org/cip/CIP-1694).

## Future plans

With on-chain governance now operational, Cardano’s roadmap shifts toward refining governance, focusing on improving usability, governance tooling, voter participation, and ecosystem integration.

Two additional eras are currently planned: Dijkstra, which will focus on Plutus v4, consensus and ledger integration, and Conway refactors for transition, and Euler, whose full scope is to be determined. More details about these upcoming eras will be shared closer to their respective upgrades.

## Intra-era hard forks

A new era *must* be introduced with a hard fork, but the ledger can also change semantics during a controlled hard fork with another mechanism – an intra-era hard fork. This is an implementation detail that involves bumping the major protocol version but not creating a new ledger era. The Alonzo era, for example, experienced an intra-era hard fork when going from major protocol version 5 to 6.

You can see a table with Cardano’s phases, eras, and intra-era hard forks visualized below:

![phases-eras-table](https://ucarecdn.com/17dfdeb2-0f39-4b21-989d-18c64d5494ee/)

Image source: [CIP-59 Cardano features annex](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0059/feature-table.md)

## Conclusion

It is important to understand that not all of the semantic changes to the Cardano network involve the ledger. Changes to the consensus protocol or the networking layer may also require a hard fork. There is also an abstraction between the consensus and ledger layers, named the *protocol*. The distinction between the ledger protocols and the ledger eras corresponds roughly to how block headers are validated (protocol) versus how block bodies are validated (era). The Shelley era used the ‘transitional Praos’ protocol (TPraos), which consisted of Praos combined with a transition system to move away from Ouroboros BFT. The Babbage era replaced TPraos with Praos.

Note that the protocol version is unrelated to the node-to-node and node-to-client protocol versions. The consensus layer maintains a versioning scheme for the node queries that does not necessarily align with the protocol version described in this overview.

The protocol version is also included in each block header indicating the maximum protocol version that the block producer is capable of supporting.

---
title: About hard forks
metaTitle: About hard forks
---

The term _hard fork_ describes a radical change in the blockchain: a change from
one protocol to another, for example. In most blockchains, a hard fork
indicates block changes or a change to their interpretation. 

Traditionally, when conducting a hard fork, the current protocol would stop operating,
new rules and changes would be implemented, and the chain would restart. It is important to
note that a hard-forked chain _will be different_ from the previous version and
that the history of the pre-forked blockchain will no longer be available.

The Cardano blockchain hard forked from a Byron federated model to
a Shelley decentralized one in 2020. However, this hard fork was unique. Instead of
implementing radical changes, Cardano ensured a smooth transition to a new protocol
while saving the history of the previous blocks. This means that the chain did
not change *radically*, instead, it contained Byron blocks, and after a transition
period, added Shelley blocks. There was no fundamental restart point that erased
the history of previous activities.

## What is a hard fork combinator?

A combinator is a technical term used to indicate the combination of certain
processes or things. In the case of Cardano, a hard fork combinator combines
protocols, thereby enabling the [era-to-era transition](https://iohk.io/en/blog/posts/2020/04/29/from-byron-to-shelley-part-one-the-testnets/) without system
interruption or restart. It ensured that Byron and Shelley ledgers appeared as *one*
ledger. Shifting from [Ouroboros BFT](https://eprint.iacr.org/2018/1049.pdf) to [Ouroboros Praos](https://iohk.io/en/research/library/papers/ouroboros-praosan-adaptively-securesemi-synchronous-proof-of-stake-protocol/) did not require all nodes to
update simultaneously. Instead, nodes could update gradually, in fact, some could
run Byron blocks, while others could run Shelley blocks.

The hard fork combinator is designed to enable the combination of several
protocols, without having to make significant adjustments. 

Read more about Cardano's upgrades in the following section. 

---
title: Governance overview
metaTitle: Governance overview
---
Governance in Cardano has evolved significantly since the introduction of Cardano Improvement Proposal #1694 ([CIP-1694](https://cips.cardano.org/cip/CIP-1694)), which laid the foundation for a more decentralized and community-driven decision-making process. Named after the birth year of the philosopher Voltaire, this proposal marks the transition to the [Voltaire development phase](https://roadmap.cardano.org/en/voltaire/), where governance is self-sustaining and controlled by the Cardano community.

CIP-1694 introduces a tripartite governance structure consisting of delegated representatives (DReps), the constitutional committee, and [stake pool operators (SPOs)](/stake-pool-operators/about-stake-pools). These bodies work together to review and approve governance actions, which can be submitted by anyone, ensuring a transparent and democratic process. 

Cardano’s self-sustaining governance requires the following roles to function properly:

- **Ada owners:**  
    the ones who have ‘skin in the game’
- **DReps:**  
    the ones delegated voting power by ada owners
- **SPOs:**  
    the block producers who maintain the network and vote on certain governance actions
- **Constitutional committee:**  
    the elected representatives who ensure governance actions comply with the Cardano constitution.

As with stake pools, any ada owner may register to be a DRep and represent themselves and/or others. Also, as with stake pools, ada owners may, instead, delegate their voting rights to any DRep. Voting rights are based on the total ada that is delegated, as a whole number of lovelaces (one ada = 1,000,000 lovelaces).
## Driven by the Cardano constitution 
The Cardano Blockchain Ecosystem Constitution (‘[Cardano constitution](https://constitution.gov.tools/en/constitution)') is designed to be the founding document that brings together the community’s values and vision and puts them into text as a set of rules and principles to abide by to maintain the sustainability of the Cardano network.
## Why bother?
The pioneer entities of Cardano – EMURGO, Input | Output, and the Cardano Foundation – have relinquished the genesis keys of the blockchain, handing a more mature governance system to the community. Now, the community must take up the challenge and take Cardano to the next level, which means all ada owners must get involved.  

These off-chain elements are where Intersect fits in. Becoming an Intersect member offers many opportunities to be involved in framing and maturing Cardano’s governance, much of which happens off-chain.   

In addition to Intersect membership, other opportunities include participating in the [Cardano forum](https://forum.cardano.org/) and social networks.

Join Cardano’s community in guiding its future and encouraging its mass adoption to ensure the success of the most decentralized blockchain. See the sections below for more details.
## How to participate
To get involved with decentralized decision-making,  start with choosing and installing a governance-compatible wallet. Intersect MBO maintains a [current list](https://docs.gov.tools/cardano-govtool/using-govtool/getting-started/compatible-wallets). Then, check out the [governance tools documentation](https://docs.gov.tools) to see what you can do and how.  

You can use governance tools to delegate your voting power to a DRep, become a DRep yourself, review and vote on governance actions, submit your own governance action, and more.

Useful resources:
- [How to become a DRep](https://docs.intersectmbo.org/cardano/cardano-governance/governance-roles/delegated-representatives-dreps)
- [DRep Pioneer program](https://dreppioneeringprogram.docs.intersectmbo.org/)
- [How to delegate your voting power](https://docs.gov.tools/cardano-govtool/using-govtool/delegating)
- [How to vote on governance actions.](https://docs.gov.tools/cardano-govtool/using-govtool/governance-actions/vote-on-governance-actions)

## Governance in more detail
Any ada owner can submit a governance action on the Cardano blockchain. Every governance action must be ratified by at least two of the three governance bodies by on-chain votes. The type of action and the state of the governance system determine which bodies must ratify it.  

The constitutional committee votes on the basis of one member, one vote. SPOs and DReps vote on the basis of one lovelace, one vote.   

| Governance action | Name | CC vote | DRep vote | SPO vote |
|------------------ | -------------------- | -------- | ------- | ------- |  
|1 | Motion of no-confidence | - | ✓ | ✓ |  
| 2a | Update committee/threshold (normal state) | - | ✓ | ✓ |
| 2b | Update committee/threshold (state of no-confidence) | - | ✓ | ✓ |
| 3 | New constitution or guardrails script | ✓ | ✓ | - |
| 4 | Hard fork initiation | ✓ | ✓ | ✓ |
| 5a | Protocol parameter changes, network group | ✓ | ✓ | - |
| 5b | Protocol parameter changes, economic group | ✓ | ✓ | - |
| 5c | Protocol parameter changes, technical group | ✓ | ✓ | - |
| 5d | Protocol parameter changes, governance group | ✓ | ✓ | - |
| 6 | Treasury withdrawal | ✓ | ✓ | - |
| 7 | Info | ✓ | ✓ | ✓ |


There is a separate protocol parameter that sets the minimum yes vote for each DRep and SPO vote for each governance action. Some parameters are relevant to security properties of the system. Any proposal attempting to change such a parameter requires an additional vote of the SPO.  For more details, see CIP-1694.
## The Intersect member-based organization
Intersect MBO is a not-for-profit organization incorporated in Wyoming, USA. Created to support the new governance system, Intersect helps ensure the continuity and future development of Cardano and facilitates governance processes. It fulfills this role with a management board and a number of advisory committees that coordinate the ongoing maintenance and development of the Cardano blockchain and ecosystem. Committees are elected by Intersect members in elections that Intersect facilitates twice a year. Any interested community member or organization can become an Intersect member. Members vote in committee elections on the basis of one member, one vote, regardless of ada holdings.  

There are three membership tiers:

- **Associate level**: free to join, invited to community events and kept informed of community news
- **Individual level**: a small fee to join (currently US$10.00), entitled to vote in Intersect elections, eligible to serve on Intersect committees, and can apply for grants
- **Enterprise level**: a larger fee to join (currently US$1,000), all the benefits of an individual, with company profile posted on the Intersect website and the annual report.

The benefits listed above are a summary; for details and how to join, see the [Intersect website](https://www.intersectmbo.org/membership).

---
title: Contribution guidelines
metaTitle: Contribution guidelines
metaDescription: Contribution guidelines
sidebar_position: 6
---

import ReactPlayer from 'react-player'

The Cardano Docs site follows the principles of open-source collaboration,
inclusiveness, and community-driven success. Contributing to Cardano Docs helps
create a rich, accurate, and up-to-date resource for the entire Cardano
community. By sharing your knowledge and expertise, you ensure the documentation
remains a valuable and reliable source of information for all.

Note that Cardano Docs focuses on providing information about Cardano's
functionalities and features, while the
[Developer Portal](https://developers.cardano.org/) offers developer-related
tutorials and guides. These two resources exist in tandem, ensuring a seamless
user experience for both general users and developers.

By using this website and contributing to Cardano Docs, you agree to be bound by
and comply with the CC BY 4.0 license.

## What to contribute?

You can contribute in several ways, including:

- **Edits**: spot typos or inaccuracies? Fix them!
- **Feature overviews**: suggest additional overviews or functionality
  explainers relevant to Cardano
- **References**: add more references to community-driven projects or resources
- **Graphics**: contribute with graphics, infographics, etc. Please refer to the
  [Cardano branding assets](https://cardano.org/brand-assets/) for this.

## How to contribute?

### Creating a pull request

1. **Find the page**: navigate to the page you want to edit
2. **Edit the page**: click the ‘Edit this page’ button
3. **Make changes**: make changes using
   [Markdown (MD) formatting](https://docusaurus.io/docs/next/markdown-features)
   and adhere to the style guidelines (please see below)
4. **Create a pull request**: submit your changes through a pull request.

<div style={{ width: '100%', maxWidth: '700px', paddingBottom: '1.5rem' }}>
  <ReactPlayer width={'100%'} controls url="/pull-request-render.mp4" />
</div>

If you prefer working with GitHub locally, follow these steps:

1. **Fork the repository**: go to the
   [Cardano Docs GitHub repository](https://github.com/input-output-hk/cardano-documentation)
   and fork it to your account
2. **Clone the repository**: clone the forked repository to your local machine
3. **Create a new branch**: create a new branch for your changes
4. **Make your edits**: edit the documentation files using MD formatting
5. **Commit your changes**: commit your changes with a clear and descriptive
   message
6. **Push to GitHub**: push your changes to your forked repository
7. **Submit a pull request**: open a pull request from your new branch to the
   main repository.

See more about
[Docusaurus set up here](https://github.com/input-output-hk/cardano-documentation?tab=readme-ov-file#website).

### Raising an issue

By raising an issue, you can identify areas for improvement, report problems, or
suggest new content.

To create an issue:

1. **Go to the GitHub repository**: navigate to the Cardano Docs
   [GitHub repository](https://github.com/input-output-hk/cardano-documentation)
2. **Create a new issue**: click ‘Issues’ and then ‘New issue’
3. **Describe the issue**: provide a clear and detailed description of the issue
   or suggestion
4. **Submit the issue**: click ‘Submit new issue.’

Your contributions are vital to maintaining the quality and integrity of Cardano
Docs. Whether you’re correcting a typo, adding new content, or suggesting
improvements, every contribution helps. Thank you for being an active Cardano
community member and helping make Cardano Docs an invaluable resource for
everyone.

## Style guide

### General notes

Ensure simplicity, accuracy, and accessibility. Aim for a broad understanding,
even for select audiences. Be concise – avoid unnecessary words.

### Style principles

**Language**

- Use American English: -ize rather than -ise endings, ‘color’ instead of
  ‘colour’
- Avoid slang words
- Use gender-inclusive pronouns – they/their/them
- Active voice, avoid passive.

**Abbreviations and acronyms**

- No full stops in abbreviations: eg, ie, PhD, etc, v (note, not vs), plc, Inc
- Write acronyms as said (eg, radar, laser, captcha), use initial caps for
  entities, avoid capitalization
- Minimize the use of acronyms and initialisms. Spell out on first use, eg
  peer-to-peer (P2P)
- Refer to the [Cambridge dictionary](https://dictionary.cambridge.org/).

**Legal and investment**

- Avoid investment or legal advice.

**Punctuation**

- Use the Oxford/serial comma
- Use single quote marks: ‘cascading disruption’
- Use en dash – (option-hyphen on a Mac keyboard; Alt-hyphen on PC)
- Use a full stop at the end of the last bullet point.

**Capitalization**

- Lowercase for job titles, degree names, and university departments
- Use lowercase after a colon, eg ‘Feature: this feature is about…’
- Book, film, and game titles should be in italics, and academic papers should
  be in ‘single quotes’ – avoid subtitles.

**Numbers and time**

- Spell out one to nine: one, two, three, four, five, once I caught a fish
  alive, except for percentages: 1%, 2%, 3%
- Use numerals for anything higher: 10, 11, 12, 13, 14, 15
- Spell out million, billion when in prose: there are nine million bicycles in
  Beijing
- Use abbreviations for money: £5m, $10bn, 12tn yen
- When talking generally, use MMMM DD, YYYY format, eg February 1, 2018
- For technical purposes, use ISO 8601 standard for dates and time: YYYY-MM-DD
  eg 2018-02-01.

**Bullet lists**

- Start with a capital letter and use full stops in complete sentences. You can
  format the bullet ‘title’ in bold if there is one:
  - Feature name 1. This full sentence describes the feature in more detail.
  - Feature name 2. This full sentence describes the second feature in more
    detail.
- Omit full stops in short itemized lists; use it after the last bullet point.
  For example, the discussed feature includes such items as:
  - Item 1
  - Item 2
  - Item 3.
- In shorter statements, full stops can also be omitted. For example:
  - Maintain uniformity across all documentation
  - Encourage user interaction with clear instructions
  - Incorporate visuals judiciously to enhance understanding.

**Links**

- Write clear and meaningful links; don’t use ‘here’ as a link. Always embed
  links; don’t just paste one as is.

**Headlines and titles**

- Headlines are ‘Sentence case only’. The limited use of capitals is a way of
  avoiding confusion for proper names. Use initial caps for navigation section
  names and topic headings.
- No full stops at the end of headlines, pull quotes, captions, and other
  display matter, or when referencing figures: just ‘See Figure 1 for details…’.

### Markdown features

Cardano Docs uses Markdown for formatting. Below are the main Markdown features:

**Headers**

- Use `#` for H1, `##` for H2, and so on.

```markdown
# H1

## H2

### H3
```

**Emphasis**

- Use `*` or `_` for italics, `**` or `__` for bold.

```markdown
_italic_ or _italic_

**bold** or **bold**
```

**Lists**

- Use `-` or `*` for unordered lists and `1.` for ordered lists.

```markdown
- Item 1

- Item 2

* Item 3.

1. First item

2. Second item.
```

**Links**

- Use `[text](url)` for links.

```markdown
[Cardano Docs](https://docs.cardano.org/)
```

**Images**

- Use `![alt text](url)` for images.

**Code**

- Use backticks for inline code and triple backticks for code blocks.

**Blockquotes**

- Use `>` for blockquotes.

```markdown
> This is a blockquote.
```

**Tables**

- Use `|` to create tables.

```markdown
| Header 1 | Header 2 |

|----------|----------|

| Cell 1 | Cell 2 |
```

For more details, see
[MD features here](https://docusaurus.io/docs/next/markdown-features).

---
title: 'Introduction'
metaTitle: 'Introduction'
metaDescription: 'Introduction'
sidebar_position: 1
---

Welcome to the central hub for Cardano documentation. Here, you'll find content
that describes and supports the features on both the Cardano mainnet and testnet
environments.

This includes basic explainers for newcomers to Cardano, explanations of the
core features, details about Cardano's design and evolution, insights into how
the Cardano network operates, and platform architecture. You can also access
developer resources that explain the core concepts and provide links to
developer documentation for more technical tutorials.

If you are interested in building tools on Cardano, integrating with Cardano,
and connecting with the wider developer community, please visit the
[Cardano Developer Portal](https://developers.cardano.org/home/).

## Cardano explained

Cardano is a decentralized third-generation proof-of-stake blockchain platform
and home to the ada cryptocurrency. It is the first blockchain platform to
evolve out of a scientific philosophy and a research-first driven approach.

The Cardano platform has been designed from the ground up and verified by an
industry-leading combination of top engineers and academic experts in the fields
of blockchain and cryptography. It has a strong focus on sustainability,
scalability, and transparency. It is a fully open source project that aims to
deliver an inclusive, fair, and resilient infrastructure for financial and
social applications on a global scale. One of its primary goals is to bring
reliable, secure financial services to those people who do not currently have
access.

Cardano has been designed with security as one of its founding principles. It is
written in Haskell, a functional programming language. In a functional language
like Haskell, building your system using pure functions is encouraged, which
leads to a design where components are conveniently testable in isolation.
Furthermore, advanced features of Haskell enable employing a whole range of
powerful methods for ensuring the correctness of the code, such as basing the
implementation on formal and executable specifications, extensive property-based
testing, and running tests in simulation.

Cardano's smart contract platform seeks to deliver more advanced features than
any protocol previously developed and will serve as a stable and secure platform
for the development of enterprise-level DApps. Cardano's democratic governance
system, being implemented based on
[CIP-1694](https://cips.cardano.org/cip/CIP-1694) on-chain governance
mechanisms, will enable the project to evolve over time and sustainably fund
itself through a visionary treasury system.

You can read more about Cardano on the
[official Cardano website](https://cardano.org/) and watch a summary of
Cardano's mission in this
[explainer video](https://www.youtube.com/watch?v=l_Nv0-PVrnM/).

---
title: Testnets faucet
metaTitle: Testnets faucet
---

import Faucet from '../../../src/components/Faucet'
import FaucetAddresses from '../../../src/components/FaucetAddresses'

Since Cardano testnets are independent networks, separate from the Cardano
mainnet, they require their own tokens.

The faucet is a web-based service that provides test ada to users of the Cardano
testnets. While these tokens have no 'real world' value, they enable users to
experiment with Cardano testnet features, without having to spend real ada on
the mainnet.

### To request tokens using the faucet:

1. Select the desired action type:
   - Enter the account address where you want to top up funds
   - Enter the pool ID in bech32 format (`pool18ttg8k...a1c`) where you want to
     delegate funds
2. If you have been issued with an API key, please enter this to access any
   additional funds you may have been allocated
3. Click **Receive test ada**
4. Funds will be in the testnet account you specified within a few minutes.

### Delegation

If you're an SPO and wish to test stake pool operations on preview or
pre-production testnets, you can request some test ada to be delegated to your
pool. For this, choose the environment you're working in, and select 'receive
pool delegation' from the action menu.

<Faucet />

When you have finished using your test tokens, please return them to the faucet
so that other members of the community can use them. You can return your test
tokens to the relevant address:

<FaucetAddresses
  preProduction={
    'addr_test1vzpwq95z3xyum8vqndgdd9mdnmafh3djcxnc6jemlgdmswcve6tkw'
  }
  preview={'addr_test1vqeux7xwusdju9dvsj8h7mca9aup2k439kfmwy773xxc2hcu7zy99'}
/>

---
title: Daedalus wallet for the Cardano testnets
metaTitle: Daedalus wallet for the Cardano testnets
---

import WalletDownloaders from '../../../src/components/WalletDownloaders'

Here you can download any of the special testnet-only versions of the Daedalus
wallet. Please note that this wallet is for testing purposes only. It may not be
fully-featured and may contain bugs.

### Daedalus system requirements

**Operating systems**

- Windows 8.1, Windows 10 (Only 64-bit Windows is supported)
- macOS 10.13, macOS 10.14, macOS 10.15, macOS 11.1
- Linux OS tested against:
  - Ubuntu 18.04 LTS, Ubuntu 20.04 LTS
  - Fedora 28
  - Aimed at all Linux distributions

**Recommended hardware requirements**

- 64-bit Dual core processors
- 16 GB of RAM
- 25 GB of free drive space
- Broadband Internet connection

<WalletDownloaders env="preview" />
<WalletDownloaders env="preprod" />

---
title: Testnet environments
metaTitle: Testnet environments
---

Cardano testnets sit at the vanguard of network development, providing sandboxed
environments for continuing innovation, harnessing the power of the Cardano
community to iterate and improve.

Stake pool operators, exchanges, smart contract developers, and projects can
engage with different early-stage and pre-production networks to actively test
core Cardano functionality prior to deploying on mainnet.

Discover the various testnet environments available on Cardano to select the one
best suited for your testing needs.

## Early-stage testing networks

### Preview

Preview is the network environment for testing release candidates and expanded
test scenarios. Preview is meant for DApps, stake pool operators (SPOs), and
exchanges who wish to test mature release candidates.

- [**Preview configurations**](https://book.world.dev.cardano.org/env-preview.html)

## Late-stage testing networks

### Pre-production

Pre-production is the most mature network for testing purposes, which resembles
a production (mainnet) environment. It is meant for exchanges, SPOs,
pre-deployment DApps, and wallets that wish to test release functionality before
deploying on mainnet.

- [**Pre-production configurations**](https://book.world.dev.cardano.org/env-preprod.html)

### Production network (mainnet)

Production is the live network, also referred to as mainnet. It features
official functionality releases. Exchanges, SPOs, DApps, wallets, and end users
can use the mainnet for development, transaction processing, and other needs.

- [**Production configurations**](https://book.world.dev.cardano.org/env-mainnet.html)

---
title: Getting started with Cardano testnets
metaTitle: Getting started with the Cardano testnets
---

To get started and join Cardano testnets, you should install and configure the
Cardano node and the command line interface (CLI), configure your testing
environment, and generate payment keys and addresses. Note, that you will also
need [to get some test ada](/cardano-testnets/tools/faucet) to test your
transactions.

## System requirements

The Cardano node can run on the following platforms:

- Linux Ubuntu 19.10 (Eoan Ermine) or later
- Linux Mint 19.3 (Tricia) or later
- Linux Debian 10.3 (Buster) or later
- MacOSX 10.14.0 (Mojave) or later
- Windows 10.

## Installing the node

Download the latest version of the node:

- [Cardano node releases](https://github.com/input-output-hk/cardano-node/releases)

Check your executables. The latest versions of the Cardano node and CLI can be
found here:

- [Cardano node executables](https://github.com/input-output-hk/cardano-node#executables)

There are a number of ways to install and run a Cardano node on testnet. The
choice of the best-matching method depends on the operating system, level of
technical expertise, and personal preferences.

For more information on the various options, see:

- [Cardano course: building the node](https://cardano-course.gitbook.io/cardano-course/handbook/module-1-build-and-run-the-node/building-the-node).

## Configurations

To start working with the node, it is essential to ensure that the node
configuration, genesis, and topology files are correctly set up. In addition,
the node will need the configuration files specific to each era that the Cardano
network has experienced thus far.

These configurations tell the node how to handle the updates that come with each
era (ie, Mary, Alonzo, Babbage, etc). Each new era (implemented using the
hard fork combinator technology)
introduces protocol changes and new ledger rules. While old configurations are
still valid, the new configurations and features offer new rules and
improvements. In the Babbage era, for example, Plutus V2 scripts work better
than Plutus V1 scripts. Plutus V1 scripts, however, are still supported.

For more details, see:

- [Cardano course: running the node and connecting to a network](https://cardano-course.gitbook.io/cardano-course/handbook/module-1-build-and-run-the-node/running-the-node-and-connecting-to-a-network).

## Working with the Cardano testnets

Note that mainnet and testnet commands are very similar except for the flag
usage. For example, when interacting with cardano-cli on mainnet, you should add
the `--mainnet` flag. However, to use the same commands on testnets, you should
use the `--testnet-magic INTEGER` flag instead.

`INTEGER` indicates the number of the testnet:

- Preview integer is `2`
- Pre-production integer is `1`

### Creating keys and addresses

To send and receive transactions, you should first create the payment key pair:

```
cardano-cli address key-gen \
--verification-key-file payment.vkey \
--signing-key-file payment.skey
```

This creates two files: `payment.vkey` (the public verification key) and
`payment.skey` (the private signing key).

Both verification keys (payment.vkey and stake.vkey) are used to build the
address and the resulting payment address is associated with these keys.

To build the address, run:

```
cardano-cli address build \
--payment-verification-key-file payment.vkey \
--stake-verification-key-file stake.vkey \
--out-file payment.addr \
--testnet-magic INTEGER
```

To query the balance of the address, run:

```
cardano-cli query utxo \
--address $(cat payment.addr) \
--testnet-magic INTEGER
```

For more commands, see:

- [Generating wallet keys](https://developers.cardano.org/docs/operate-a-stake-pool/generating-wallet-keys)
- [Creating keys and addresses](https://cardano-course.gitbook.io/cardano-course/handbook/building-and-running-the-node/create-keys-and-addresses).

:::note

Note to use the `--testnet-magic INTEGER` flag instead of `--mainnet`.

:::

### Funding the address using a faucet

To fund your testnet address, go to the testnets faucet and request some test
ada:

- [The testnet faucet](/cardano-testnets/tools/faucet)

If you're working in a _pre-production environment_ and have an API key, you can
run the following command to fund your address:

`curl -X POST -s 'https://faucet.preprod.world.dev.cardano.org/send-money/YOURADDRESS?api_key=YOURAPIKEY'`

### Creating, signing, and submitting transactions

You’re now ready to create, sign, and submit transactions on testnets. See the
tutorials:

- [Building and signing transactions](https://developers.cardano.org/docs/get-started/create-simple-transaction)
- [Creating simple transactions](https://cardano-course.gitbook.io/cardano-course/handbook/building-and-running-the-node/create-a-simple-transaction).

---
title: Creating a local testnet 
metaTitle: Creating a local testnet 
---

A local testnet enables developers and projects building on Cardano to test new features or functionality prior to deployment on global testnet environments. While preview and pre-production environments benefit the wider developer community, a local testnet allows the execution of a hard fork in your own setup and subsequent testing of DApp behavior.

There are several possible solutions to create a local testnet. These include the use of Plutip (a tool for private network creation) or Nix (a tool for package management and system configuration).

## Creating a local testnet using Plutip

[Plutip](https://github.com/mlabs-haskell/plutip), a tool developed on the Cardano blockchain, facilitates the creation of a private network to run Plutus contracts on.

Plutip can be used in two different ways:

-   One option is to use the Plutip tool with a setup that provides an executable for starting a private network and setting up some funded wallets. See [this tutorial](https://github.com/mlabs-haskell/plutip/tree/master/local-cluster).
-   Another option is to use a separate branch of Plutip, which provides a modified setup for hard forks and instructions on how to execute them. See [this tutorial](https://github.com/mlabs-haskell/plutip/blob/vasil-local-cluster-cabal-build/vasil-hardfork.md).
-   The third option is to run Plutip with cardano-transaction-lib, which provides a declarative interface to local clusters for using in test suites. See  [this tutorial](https://github.com/Plutonomicon/cardano-transaction-lib/blob/develop/doc/plutip-testing.md).
    
*Tutorials referenced above are created by the MLabs team.*

## Creating a local testnet using Nix

Another possible option is to spin up a local testnet using Nix. For more information, learn about the [packaging principles](https://github.com/input-output-hk/cardano-world/blob/master/docs/explain/packaging-principles.md), and see how to [create a local testnet using Nix](https://github.com/input-output-hk/cardano-world/blob/master/docs/explain/create-testnet.md).

---
title: Resources
metaTitle: Resources
---

## Useful materials

- [Vasil testnet tutorials](https://github.com/input-output-hk/Vasil-testnet)
- [Alonzo testnet tutorials](https://github.com/input-output-hk/Alonzo-testnet/tree/main/Alonzo-tutorials)
- [Alonzo testnet exercises](https://github.com/input-output-hk/Alonzo-testnet/tree/main/Alonzo-exercises/alonzo-purple)
- [Alonzo explainers](https://github.com/input-output-hk/Alonzo-testnet/tree/main/explainers)
- [Cardano testnet support page](https://iohk.zendesk.com/hc/en-us/categories/900000102203-Shelley-Testnet)
- [Telegram Cardano testnet and stake pool workgroup](https://t.me/CardanoStakePoolWorkgroup)
- [Cardano Stack Exchange](https://cardano.stackexchange.com/)
- [Cardano Discord channel](https://discord.com/channels/826816523368005654/826829738156621895)

## Blog posts

IOG’s researchers and engineers often post on
[IO blog page](https://iohk.io/en/blog/posts/page-1/). Here are a couple of
posts which you may find helpful:

- _[Stake pools in Cardano](https://iohk.io/en/blog/posts/2018/10/23/stake-pools-in-cardano/)_
- _[How Pledging will keep Cardano healthy](https://iohk.io/en/blog/posts/2020/05/12/how-pledging-encourages-a-healthy-decentralized-cardano-ecosystem/)_
- _[Smart contracts here we come](https://iohk.io/en/blog/posts/2021/04/08/smart-contracts-%E2%80%93-here-we-come/)_
- _[What you need to know about Alonzo](https://iohk.io/en/blog/posts/2021/04/13/plutus-what-you-need-to-know/)_
- _[Boosting Cardano's throughput with script referencing](https://iohk.io/en/blog/posts/2022/04/13/boosting-cardano-s-throughput-with-script-referencing/)_
- _[Increasing the transaction throughput of Cardano](https://iohk.io/en/blog/posts/2022/03/21/increasing-the-transaction-throughput-of-cardano/)_
- _[How we're scaling Cardano in 2022](https://iohk.io/en/blog/posts/2022/01/14/how-we-re-scaling-cardano-in-2022/)_
- _[Cardano’s approaching Vasil upgrade: what to expect](https://iohk.io/en/blog/posts/2022/07/04/cardano-s-approaching-vasil-upgrade-what-to-expect/)_

## Community content

- [Guild operator guide](https://cardano-community.github.io/guild-operators)
- [Big pey's stake pool video tutorials](https://www.youtube.com/playlist?list=PLyThQPJpttTJ4r9wUdlWi1DMty4nAT85d)
- [Creating a private testnet using Docker](https://github.com/ItFlyingStart/shelley-private-testnet)
- [Useful setup scripts](https://github.com/gitmachtl/scripts/tree/master/cardano)
- [Video guide: how to create a Cardano stake pool](https://www.youtube.com/playlist?list=PLyThQPJpttTJ4r9wUdlWi1DMty4nAT85d)
- [Building a Cardano stake pool using CNTools](https://www.youtube.com/watch?v=UN4rSRr7LDk&feature=youtu.be)
- [Vasil hard fork changes explained](https://www.youtube.com/watch?v=Tsp9F23dBiM)
  by dcSpark
- [CoinCashew guide: how to build a Cardano stake pool](https://www.coincashew.com/coins/overview-ada/guide-how-to-build-a-haskell-stakepool-node)
- [StakePoolOperator scripts](https://github.com/gitmachtl/scripts)
- [Plutus Pioneers program notes](https://plutus-pioneer-program.readthedocs.io/en/latest/plutus_pioneer_program.html)
- [Developer resources on Essential Cardano](https://www.essentialcardano.io/developer)

:::tip

If you have produced material and would like to contribute your content for
inclusion on this page, please raise a pull request.

:::

---
title: Support and feedback
metaTitle: Support and feedback
---

## Support

To get technical support and help with your issues or questions, you can join a dedicated Telegram channel or go to IOG's [support page](https://iohk.zendesk.com/hc/en-us/categories/900000102203-Shelley-Testnet).

TELEGRAM CHANNEL: CARDANO DEVELOPERS

To reach the developers of Cardano directly on Telegram, join the  [@CardanoStakePoolWorkgroup channel](https://t.me/CardanoStakePoolWorkgroup).

To contact IOG Technical Support, submit a request using the [Submit a request form](https://iohk.zendesk.com/hc/en-us/requests/new). You can also click the 'Support' button at the bottom right of your screen. The request form is better if you have a lot to write.

## Feedback

User feedback is an invaluable and critical aspect of maintaining the Cardano testnet. 

We welcome feedback on any issues you have encountered:

-   Via [Discord channels](https://discord.com/channels/826816523368005654/826816523964383263) for general questions or discussions.
-   Via the [Cardano node issue tracker](https://github.com/input-output-hk/cardano-node/issues) for any bugs or feature requests in the node. 
-   Via the [Plutus issue tracker](https://github.com/input-output-hk/plutus/issues) for any bugs or feature requests with Plutus.

You can also join  ['IO DEV announcements' Telegram channel](https://t.me/IOdevannouncements) to receive key development updates.

---
title: Minting transactions
metaTitle: Minting transactions
---

The ability to create and manage custom tokens is one of the distinguishing features of the Cardano blockchain. Minting refers to the process of 'producing new tokens, either as part of an initial coin offering (ICO) or ongoing token issuance'. Minting transactions enable users to generate or burn tokens according to predefined rules. This tutorial explores the details of minting transactions on Cardano.

Consider a scenario where Bob wants to give Alice a token, but only Bob has the minting authority for that token. To achieve this, Bob needs to create a script that defines the minting and burning rules. There are two approaches to accomplish this: using Plutus policy scripts or simple multi-signature scripts. In this case, the tutorial suggests using a simple multi-signature script since the rule is straightforward: only Bob can mint the token.

Before you begin, ensure that you have all the necessary components ready. First, you need to generate new keys for the script itself. The process for obtaining script keys is as follows:

```
$ cardano-cli address key-gen \
    --verification-key-file transactions-tutorial-policy/policy.vkey \
    --signing-key-file transactions-tutorial-policy/policy.skey
```

Before creating the script, you need to get the key hash of the new keys:

```
$ cardano-cli address key-hash \
    --payment-verification-key-file transactions-tutorial-policy/policy.vkey
8ebd5f9c84fc25ae4506c7d0b687b2f7e82fe3f891036833e7f25c9b
```

You can now create the script by writing the following code into `transactions-tutorial-policy/policy.script`:

```
{
    “keyHash”: “8ebd5f9c84fc25ae4506c7d0b687b2f7e82fe3f891036833e7f25c9b”,
    “type”: “sig”
}
```

You can get the `PolicyId` with the following command:

```
$ cardano-cli transaction policyid \
    --script-file transactions-tutorial-policy/policy.script 
C38b0924e32e677f7787f0a0247b177588ec135db927688d8a63310a
```

It is also helpful to store it in an environment variable:

```
export POLICY_ID=C38b0924e32e677f7787f0a0247b177588ec135db927688d8a63310a
```

Now, you need to create a token name, which has to be in hexadecimal (54657374546F6B656E):

```
export TOKEN_NAME=54657374546F6B656E
```

Build the transaction:

```
cardano-cli transaction build \
    --tx-in f947f84f1156995afd695247a8dc8a508fd40d371ce0afb801029769a0104874#0 \
    --mint "1 $(echo $POLICY_ID).$(echo $TOKEN_NAME)" \
    --testnet-magic 1 \
    --change-address $(cat bob.addr) \
    --mint-script-file ../cardano/transactions-tutorial-policy/policy.script \
    --out-file minting.tx \
    --tx-out "$(cat alice.addr)+1047330+1 $(echo $POLICY_ID).$(echo $TOKEN_NAME)"
```

---
title: Staking transactions
metaTitle: Staking transactions
---

Staking is a fundamental feature of Cardano, allowing you to earn rewards by delegating ada to stake pools. This tutorial shows how to delegate your ada to a stake pool, withdraw rewards, or stop delegating your ada if you decide to switch to another stake pool.

To delegate ada, first, you need to generate a stake key pair. This involves creating a `stake.vkey` and `stake.skey`, in addition to your regular payment key pair (`payment.vkey` and `payment.skey`), and obtaining a new address called `stake.addr`, similar to your `payment.addr`.

Assuming you already have your `payment.vkey` and `payment.skey`, which you can easily generate using the `cardano-cli address key-gen` command, proceed by creating the `stake.vkey` and `stake.skey`:

```
cardano-cli stake-address key-gen \
    --verification-key-file stake.vkey \
    --signing-key-file stake.skey
```

Now you have your payment key pair and stake key pair. The next step is to build the `payment.addr` and `stake.addr` files:

```
# payment.addr
cardano-cli address build \
    --payment-verification-key-file payment.vkey \
    --stake-verification-key-file stake.vkey \
    --out-file payment.addr
```

Note that the example includes the `--stake-verification-key-file` argument. This command is used to create an address that includes its staking part. Addresses without a staking part are primarily used for sending and receiving ada, as well as interacting with DApps. Some examples may omit this argument, but it's crucial for delegation and participating in the staking process.

```
# stake.addr
cardano-cli stake-address build \
    --stake-verification-key-file stake.vkey \
    --out-file stake.addr
```

This command uses only the stake verification file.

Before proceeding with delegation, it's essential to create two new certificates: a registration certificate and a delegation certificate. These files are required to register the stake key on the blockchain and indicate your intention to delegate ada.

This command generates a certificate, which indicates the intention to register the stake key:

```
# registration.cert
cardano-cli stake-address registration-certificate \
    --stake-verification-key-file stake.vkey \
    --out-file reg.cert
```

Create a delegation certificate:

```
# delegate.cert
cardano-cli stake-address delegation-certificate \
    --stake-verification-key-file stake.vkey \
    --stake-pool-id pool18pn6p9ef58u4ga3wagp44qhzm8f6zncl57g6qgh0pk3yytwz54h \
    --out-file delegation.cert
```

At this stage of the tutorial, you should have already selected a stake pool. The example features ADACT (PoolId: pool18pn6p9ef58u4ga3wagp44qhzm8f6zncl57g6qgh0pk3yytwz54h). You can list all pools using the command `cardano-cli query stake-pools` or visit https://preview.cardanoscan.io/pools to select the pool that best suits your needs. Please note that the example uses the `preview` testnet. If you're using another testnet or mainnet, make sure that the pool ID is on the correct network.

Now that you have the `reg.cert` and `delegation.cert` files, you can proceed to the next step. You need to send a transaction containing the certificates. There are two options: sending them individually or sending them together. The example shows how to send them both at the same time.

Start by querying the funds of `payment.addr`:

```
cardano-cli query utxo --address $(cat payment.addr)
                           TxHash                                 TxIx        Amount
--------------------------------------------------------------------------------------
142c46bb93b9c80140a6302e4a8a360e6f46f55aaf001c825ca790bb23572754     0        10000000000 lovelace + TxOutDatumNone
```

If there are no funds in the address, you can request them from the [faucet](https://docs.cardano.org/cardano-testnet/tools/faucet/). Make sure you select the right testnet.

It's time to build, sign, and submit your transaction.

The command below introduces two new arguments of transaction creation: `--witness-override` and `certificate-file`. The first one specifies that the transaction will require two signatures. Therefore, transaction fees will be higher. `certificate-file` adds your new certificates to the transaction:

```
cardano-cli transaction build \
    --witness-override 2 \
    --tx-in 142c46bb93b9c80140a6302e4a8a360e6f46f55aaf001c825ca790bb23572754#0 \
    --certificate-file reg.cert \
    --certificate-file delegation.cert \
    --change-address $(cat payment.addr) \
    --out-file delegateTx.raw
```

In the signing phase, you need to add two signatures: `payment.skey` and `stake.skey`:

```
cardano-cli transaction sign \
    --tx-body-file delegateTx.raw \
    --signing-key-file payment.skey \
    --signing-key-file stake.skey \
    --out-file delegateTx.signed
	
cardano-cli transaction submit \
    --tx-file delegateTx.signed
Transaction successfully submitted.
```

Once the transaction is submitted and processed, you have successfully delegated your ada. Now, let's take a look at your delegation and rewards:

```
cardano-cli query stake-address-info --address $(cat stake.addr)
[
    {
        "address": "stake_test1uq954t492tmusk2dy9z505g3cz3sfpnh0swsqjmzk47rasqyn8uqp",
        "delegation": "pool18pn6p9ef58u4ga3wagp44qhzm8f6zncl57g6qgh0pk3yytwz54h",
        "rewardAccountBalance": 0
    }
]
```

After a few days, you will receive rewards, and the next part of the tutorial demonstrates how to withdraw them.

---
title: Withdrawing transactions
metaTitle: Withdrawing transactions 
---

This part of the tutorial will demonstrate how easy it is to claim rewards using the `cardano-cli`. First, you need to ensure that you already have rewards available in your stake address. Check it with the following command:

```
$ cardano-cli query stake-address-info --address $(cat stake.addr)
[
    {
        "address": "stake_test1uq954t492tmusk2dy9z505g3cz3sfpnh0swsqjmzk47rasqyn8uqp",
        "delegation": "pool18pn6p9ef58u4ga3wagp44qhzm8f6zncl57g6qgh0pk3yytwz54h",
        "rewardAccountBalance": 29385845
    }
]
```

In this case, you have ~29 ada. You also need some of your own ada to pay the fees, so you need to query your payment address:

```
$ cardano-cli query utxo --address $(cat payment.addr)
                           TxHash                                 TxIx        Amount
--------------------------------------------------------------------------------------
afb33e353a9880b7cbd9e5eb2cbffa024d1b3b938ee2c739e53dd187094e8f0d     0        10000000 lovelace + TxOutDatumInline ReferenceTxInsScriptsInlineDatumsInBabbageEra (HashableScriptData "\216y\159\CAN*\255" (ScriptDataConstructor 0 [ScriptDataNumber 42]))
afb33e353a9880b7cbd9e5eb2cbffa024d1b3b938ee2c739e53dd187094e8f0d     1        9987657206 lovelace + TxOutDatumNone
```

Use the second UTXO with TxIx=1. You're ready to create your withdrawal transaction:

```
cardano-cli transaction build \
    --tx-in afb33e353a9880b7cbd9e5eb2cbffa024d1b3b938ee2c739e53dd187094e8f0d#1 \
    --withdrawal $(cat stake.addr)+29385845 \
    --change-address $(cat payment.addr) \
    --out-file withdrawal-tx.raw
```

Note that this transaction will take funds from the stake address, so it will need to be signed with the `stake.skey` file:

```
cardano-cli transaction sign \
    --tx-file withdrawal-tx.raw \
    --signing-key-file payment.skey \
    --signing-key-file stake.skey \
    --out-file withdrawal-tx.signed

cardano-cli transaction submit --tx-file withdrawal-tx.signed
```

Finally, verify that you have received your rewards in your payment address:

```
$ cardano-cli query utxo --address $(cat payment.addr)
                           TxHash                                 TxIx        Amount
--------------------------------------------------------------------------------------
2b1bfc342c1f5531df4cfa220eac79574142c7263d97885d2ad8588ca1a7e22b     0        10016871698 lovelace + TxOutDatumNone
afb33e353a9880b7cbd9e5eb2cbffa024d1b3b938ee2c739e53dd187094e8f0d     0        10000000 lovelace + TxOutDatumInline ReferenceTxInsScriptsInlineDatumsInBabbageEra (HashableScriptData "\216y\159\CAN*\255" (ScriptDataConstructor 0 [ScriptDataNumber 42]))
```

---
title: Redelegation
metaTitle: Redelegation
---

This tutorial will show how to redelegate your stake to another stake pool. There can be multiple reasons to redelegate your stake, but the most common ones are:

- Original stake pool is no longer available
- You've discovered a more convenient option
- You have your own stake pool and you want to delegate your stake to it.

To do that, you need to create a new certificate that you are sending in a transaction, as in the first part of the tutorial:

```
cardano-cli stake-address delegation-certificate \
    --stake-verification-key-file stake.vkey \
    --stake-pool-id pool1vzqtn3mtfvvuy8ghksy34gs9g97tszj5f8mr3sn7asy5vk577ec \
    --out-file new-delegation.cert
```

Now the delegation certificate contains the `--stake-pool-id` of the new stake pool. It means that you're ready to build your transaction, selecting the UTXO from which you will pay the fee:

```
$ cardano-cli query utxo --address $(cat payment.addr)
                           TxHash                                 TxIx        Amount
--------------------------------------------------------------------------------------
2b1bfc342c1f5531df4cfa220eac79574142c7263d97885d2ad8588ca1a7e22b     0        10016871698 lovelace + TxOutDatumNone
afb33e353a9880b7cbd9e5eb2cbffa024d1b3b938ee2c739e53dd187094e8f0d     0        10000000 lovelace + TxOutDatumInline ReferenceTxInsScriptsInlineDatumsInBabbageEra (HashableScriptData "\216y\159\CAN*\255" (ScriptDataConstructor 0 [ScriptDataNumber 42]))
```

Build:

```
cardano-cli transaction build \
    --witness-override 2 \
    --tx-in 2b1bfc342c1f5531df4cfa220eac79574142c7263d97885d2ad8588ca1a7e22b#0 \
    --change-address $(cat payment.addr) \
    --certificate new-delegation.cert \
    --out-file new-delegation.tx

Sign:

cardano-cli transaction sign \
    --tx-file new-delegation.tx \
    --signing-key-file payment.skey \
    --signing-key-file stake.skey \
    --out-file new-delegation.signed

cardano-cli transaction submit --tx-file new-delegation.signed 
```

If you check your stake address details, you'll see that you have delegated your stake to a new stake pool and you have zero rewards for now:

```
$ cardano-cli query stake-address-info --address $(cat stake.addr)
[
    {
        "address": "stake_test1uq954t492tmusk2dy9z505g3cz3sfpnh0swsqjmzk47rasqyn8uqp",
        "delegation": "pool1vzqtn3mtfvvuy8ghksy34gs9g97tszj5f8mr3sn7asy5vk577ec",
        "rewardAccountBalance": 0
    }
]
```

---
title: Multiple purposes
metaTitle: Multiple purposes
---

Cardano allows more complex transactions. So far you've seen various types of transactions: simple script, minting, staking, delegating, etc. But Cardano provides the flexibility to create more complex transactions and mix all the types of transactions needed. For example, adding metadata to minting transactions, or adding a staking component to a simple script transaction. This tutorial shows how to send a transaction with multiple purposes.


In this tutorial, you will learn how to:

- Withdraw funds from a script
- Mint a token
- Withdraw delegated funds 
- Set metadata for the transaction

As mentioned in the minting tutorial, one way to mint a token is by creating a script derived from a `keyHash`. For that, you first need to generate new keys and a key hash using the following commands:

```
cardano-cli address key-gen \
    --verification-key-file script.vkey \
    --signing-key-file script.skey

cardano-cli address key-hash --payment-verification-key-file script.vkey
54a318b79a805f4f4cf0562a27302e93ed5b2e657cccdf4f6a8330ef
```
Having the key hash, you can create the script as follows:

```
cat payment.script
{
    "keyHash": "54a318b79a805f4f4cf0562a27302e93ed5b2e657cccdf4f6a8330ef",
    "type": "sig"
}
```

You also need to create the address and the policy ID of the script:

```
cardano-cli address build --payment-script-file payment.script 
addr_test1wzgd97kc864w62ulela5ykw8m64z2nu4wjejacyfp6u7qwqyz9v9m
```

```
cardano-cli transaction policyid --script-file payment.script
90d2fad83eaaed2b9fcffb4259c7deaa254f9574b32ee0890eb9e038
```

## First transaction

First of all, you need to fund the script address using `payment.addr` funds:

```
cardano-cli query utxo --address $(cat payment.addr)
                           TxHash                                 TxIx        Amount
--------------------------------------------------------------------------------------
12bc01bca615bb4cbe5f36b06d86092ad3c8a10e6a37e09f94a5f006f3aaf230     0        10040283782 lovelace + TxOutDatumNone
```

In this transaction, the example sets a `tx-out-inline-datum` file that only contains the number 42. This is essential since you wouldn't be able to get the funds from the script if it does not have a datum:

```
cardano-cli transaction build --tx-in 12bc01bca615bb4cbe5f36b06d86092ad3c8a10e6a37e09f94a5f006f3aaf230#0 --tx-out $(cat payment.script.addr)+10000000 --change-address $(cat payment.addr) --tx-out-inline-datum-file datum.json --out-file 1st.tx

cat datum.json
{
  "constructor": 0,
    "fields": [{"int": 42}]
}
```

## Second transaction

In this transaction, you can query the UTXO details:

```
cardano-cli query utxo --address $(cat  payment.script.addr)
                           TxHash                                 TxIx        Amount
--------------------------------------------------------------------------------------
04dfd8feaf87f82c8ec771954179879b22b5f693ed86d78a749111eed2de7879     0        10000000 lovelace + TxOutDatumInline ReferenceTxInsScriptsInlineDatumsInBabbageEra (HashableScriptData "\216y\159\CAN*\255" (ScriptDataConstructor 0 [ScriptDataNumber 42]))
```

```
cardano-cli query utxo --address $(cat  payment.addr)
                           TxHash                                 TxIx        Amount
--------------------------------------------------------------------------------------
04dfd8feaf87f82c8ec771954179879b22b5f693ed86d78a749111eed2de7879     1        10030116301 lovelace + TxOutDatumNone
```

You can also verify that you have available rewards:

```
cardano-cli query stake-address-info --address $(cat stake.addr)
[
    {
        "address": "stake_test1uq954t492tmusk2dy9z505g3cz3sfpnh0swsqjmzk47rasqyn8uqp",
        "delegation": "pool1vzqtn3mtfvvuy8ghksy34gs9g97tszj5f8mr3sn7asy5vk577ec",
        "rewardAccountBalance": 15192167
    }
]
```

Now you can build the transaction with multiple purposes:

```
cardano-cli transaction build \
    --tx-in 04dfd8feaf87f82c8ec771954179879b22b5f693ed86d78a749111eed2de7879#1 \
    --tx-in 04dfd8feaf87f82c8ec771954179879b22b5f693ed86d78a749111eed2de7879#0 \
    --tx-in-script-file payment.script \
    --change-address $(cat payment.addr) \
    --mint "1 $(cat payment.script.policyid).746F6B656E" \
    --mint-script-file payment.script \
    --withdrawal $(cat stake.addr)+15192167 \
    --metadata-json-file metadata.json \
    --out-file 2nd.tx
```
You can see multiple things happening in this transaction:

 - There are two UTXOs to be spent
   - One is from a script, and the other is from a payment
 - You are minting a token and this only can be done by defining the `mint-script-file` - the rules to mint that token
 - You are withdrawing funds from the delegation
 - Finally, you are setting a metadata file

Finally, you can verify the result of signing and submitting the transaction:

The payment address now has:
- The new token minted
- Funds that were in the script address
- Rewards from delegation

```
cardano-cli query utxo --address $(cat payment.addr)
                           TxHash                                 TxIx        Amount
--------------------------------------------------------------------------------------
c260ebce5868a56ce559f5277e6ca2d47b0eecb5b851b957fcb40a2194f67b29     0        10055118943 lovelace + 1 90d2fad83eaaed2b9fcffb4259c7deaa254f9574b32ee0890eb9e038.746f6b656e + TxOutDatumNone
```

The script address does not have any funds.

```
cardano-cli query utxo --address $(cat payment.script.addr)
                           TxHash                                 TxIx        Amount
--------------------------------------------------------------------------------------
```
We no longer have rewards from delegating:

```
cardano-cli query stake-address-info --address $(cat stake.addr)
[
    {
        "address": "stake_test1uq954t492tmusk2dy9z505g3cz3sfpnh0swsqjmzk47rasqyn8uqp",
        "delegation": "pool1vzqtn3mtfvvuy8ghksy34gs9g97tszj5f8mr3sn7asy5vk577ec",
        "rewardAccountBalance": 0
    }
]
```

And the best part, it was only done in one transaction.

---
title: Transaction tutorials
metaTitle: Transaction tutorials
sidebar_position: 3
collapsible: true
collapsed: true
---

A transaction is an event created, signed, and sent by a user to modify the
ledger's state. It is commonly used to transfer ada or other tokens from one
user to another. Additionally, it can serve various purposes, such as token
creation, delegation registration to a stake pool, or interaction with smart
contracts, among others. The process to modify the ledger through a transaction
is as follows:

- **Creating the transaction**. A transaction includes a set of data that
  specifies how you intend to modify the ledger. The fundamental components of a
  transaction include UTXOs, from which the funds are sourced, and destination
  addresses to which you want to send funds along with the desired amount of
  tokens. There are various tools available to assist in creating transactions,
  such as wallets and `cardano-cli`.

- **Signing the transaction**. A user who owns the funds to be spent must
  provide authorization for the transaction through a signature. If the funds
  are held in a smart contract address, the authorization is carried out by
  executing the smart contract itself. Various tools are available to assist in
  signing transactions, including wallets and `cardano-cli`.

- **Submitting the transaction**. For a transaction to be reflected in the
  ledger, the user must submit it. These transactions are received by stake
  pools, validated, and then added to the ledger within a block.

Each transaction includes an identifier known as `TxId`, and after completing
these three steps, you can view transaction's content in the Cardano Explorer,
accessible at https://explorer.cardano.org, for example.

## Transaction tutorials index

In this section, you will find tutorials on how to create:

- [Minting transactions](https://docs.cardano.org/developer-resources/transaction-tutorials/minting-transaction)
- [Staking transactions](https://docs.cardano.org/developer-resources/transaction-tutorials/stake-transaction)
- [Withdrawing transactions](https://docs.cardano.org/developer-resources/transaction-tutorials/withdraw-transaction)
- [Transactions with funds redelegation](https://docs.cardano.org/developer-resources/transaction-tutorials/redelegate-transaction)
- [Transactions for multiple purposes](https://docs.cardano.org/developer-resources/transaction-tutorials/multiple-purposes)

**For additional references, see**:

- [Building and signing transactions](https://github.com/input-output-hk/cardano-node-wiki/blob/main/docs/reference/building-and-signing-tx.md)
- [Diagnosing transaction problems and troubleshooting](https://github.com/input-output-hk/cardano-node-wiki/blob/main/docs/reference/diagnosing-transactions.md)
- [Transaction metadata](https://github.com/input-output-hk/cardano-node-wiki/blob/main/docs/reference/tx-metadata.md)
- [Simple transactions](https://developers.cardano.org/docs/get-started/create-simple-transaction/)
- [Native assets](https://developers.cardano.org/docs/native-tokens/)
- [Cardano governance update proposals](https://github.com/input-output-hk/cardano-node-wiki/blob/main/docs/reference/cardano-governance.md)
- [Creating Cardano testnets](https://github.com/input-output-hk/cardano-node-wiki/blob/main/docs/reference/create-cardano.md)

**Plutus scripts**

- [About Plutus scripts](https://github.com/input-output-hk/cardano-node-wiki/blob/main/docs/reference/plutus/babbage-script-example.md)
  (Plutus V2 examples)
- [Plutus spending script](https://github.com/input-output-hk/cardano-node-wiki/blob/main/docs/reference/plutus/plutus-spending-script-example.md)
- [Babbage certifying script example](https://github.com/input-output-hk/cardano-node-wiki/blob/main/docs/reference/plutus/babbage-certifying-script-example.md)
- [Babbage withdrawing script example](https://github.com/input-output-hk/cardano-node-wiki/blob/main/docs/reference/plutus/babbage-withdrawing-script-example.md)

> Note: For more information about Plutus, tutorials, and how-to guides go to
> [Plinth user guide](https://plutus.readthedocs.io/en/latest/).

**Vasil tutorials**

The tutorials below provide explainers of the CIPs introduced with Vasil
functionality. Examples include the usage of Plutus v2 scripts:

- [How to use reference inputs](https://github.com/perturbing/vasil-tests/blob/main/reference-inputs-cip-31.md)
- [How to use inline datums](https://github.com/perturbing/vasil-tests/blob/main/inline-datums-cip-32.md)
- [How to reference scripts](https://github.com/perturbing/vasil-tests/blob/main/referencing-scripts-cip-33.md)
- [How to use collateral outputs](https://github.com/perturbing/vasil-tests/blob/main/collateral-output-cip-40.md)

---
title: Plutus
metaTitle: Learn about Plutus
sidebar_position: 1
---

:::info

To start developing with Plutus, see:

- [Plutus Core documentation](https://plutus.cardano.intersectmbo.org/docs/)
- [Developer portal Plinth overview](https://developers.cardano.org/docs/smart-contracts/smart-contract-languages/plinth)
- 'Plutus resources' below for more details.

:::

Plutus is the native smart contract language for Cardano. It is a
Turing-complete language written in Haskell, and Plutus smart contracts are
effectively Haskell programs. By using Plutus, you can be confident in the
correct execution of your smart contracts. It draws from modern language
research to provide a safe, full-stack programming environment based on Haskell,
the leading purely-functional programming language.

## Plutus smart contracts

Plutus smart contracts consist of parts that run on the blockchain (on-chain
code) and parts that run on a user’s machine (off-chain or client code).
Off-chain code can be written using the Plutus Application Framework (PAF), and
this code is then compiled by the GHC (Glasgow Haskell Compiler), whereas
on-chain code is compiled by the Plutus compiler into Plutus Core.

## Accounting model used by Plutus

Cardano uses the
[extended UTXO accounting model (EUTXO)](https://ucarecdn.com/3da33f2f-73ac-4c9b-844b-f215dcce0628/EUTXOhandbook_for_EC.pdf)
which extends the unspent (U) transaction (TX) output (O) accounting model
(UTXO) (used by Bitcoin). In the UTXO model, a _transaction_ has _inputs_
and _outputs_, where the **inputs** are unspent outputs from previous
transactions. As soon as an output is used as input in a transaction, it becomes
spent and can never be used again. The **output** is specified by an _address_
(a public key or public key hash) and a _value_ (consisting of an ada amount and
optional, additional native token amounts).

EUTXO extends the UTXO model by allowing output addresses to contain complex
logic to decide which transactions can unlock them, and by adding _custom data_
to _all_ outputs. This model offers unique advantages over other accounting
models. The success or failure of transaction validation depends only on the
transaction itself and its inputs and _not_ on anything else on the blockchain.
Consequently, the validity of a transaction can be checked _off-chain_ before
the transaction is sent to the blockchain. A transaction can still fail if some
other transaction concurrently consumes an input that the transaction is
expecting. However, if all inputs are still present, the transaction is
_guaranteed_ to succeed.

## Plutus Core

Plutus Core is the scripting language used by Cardano to implement the EUTXO
model. It is a functional language similar to Haskell, and a large
subset of Haskell can be used to write Plutus Core scripts. As a smart contract
author, you don’t write any Plutus Core; rather, all Plutus Core scripts are
generated by a Haskell compiler plugin.

These scripts will be executed by nodes during transaction validation ‘live’ on
the chain. They will either lock EUTXOs in the form of validator scripts or as
minting policies, which control the minting and burning of native tokens.

In practice, when developing smart contracts, you will write validator scripts
in Haskell, which will then be automatically compiled into Plutus Core using a
GHC plug-in called Plutus Tx.

## Plutus and native tokens

Each native token comes with its own minting policy, which determines the
conditions under which tokens can be minted and burnt. Users can write minting
policies in Haskell and compile them to Plutus Core. During the minting or
burning process, the Plutus Core policy script is executed in the context of the
minting or burning transaction, and the script will have to approve or forbid
the action. This feature further accelerates the growth of non-fungible tokens
(NFTs) on Cardano by enabling the creation of much more complex minting policies
and allowing the creation of NFTs in a trustless manner.

## Advantages of Plutus

Plutus provides considerable security advantages. It delivers an easier, more
robust way to show that your smart contracts are correct and will not encounter
the problems found in previous smart contract language design.

Plutus enables a novel integrated approach to smart contract and distributed
application development that is more convenient and safer than previous
alternatives. Both the on-chain and off-chain code are based on the same
language. You use a uniform code base, which the Plutus toolchain then
automatically separates into on-chain and off-chain code and packages for
deployment.

Additionally, in contrast to Ethereum, Plutus supports user-defined tokens (both
fungible and non-fungible) natively which does not require an accompanying smart
contract.

## Plutus developer resources

### Plutus Core

Plutus Core is the scripting language embedded in the Cardano ledger. It forms
the basis of the Plutus Platform – an application development platform for
developing decentralized applications (DApps) on the Cardano blockchain.

**Developer resources**:

- [Plutus Core GitHub repository](https://github.com/IntersectMBO/plutus):
  includes the implementation, specification, and mechanized metatheory of
  Plutus Core as well as Plutus Tx – the compiler from Haskell to Plutus Core.

**Associated links**:

- [Talks](https://github.com/IntersectMBO/plutus#talks)
- [Specifications and design](https://github.com/IntersectMBO/plutus#specifications-and-design)
- [Academic papers](https://github.com/IntersectMBO/plutus#academic-papers)

### Official Haskell language server implementation

Haskell language server implementation is an implementation of a server for the
Language Server Protocol (LSP). A language server talks to a client, who can ask
the server to perform various operations, such as reporting errors or providing
code completions.

**Developer resources**:

- [Haskell language server documentation](https://haskell-language-server.readthedocs.io/en/latest/index.html)

### Haskell course

Plutus Core is written in Haskell. To use Plutus,
it is essential to know the basics of Haskell. To help with the onboarding
journey, IOG's education team created the Haskell course for beginners. You can
find the repository with explainers, links to lectures, and more below.

- [Haskell course](https://github.com/input-output-hk/haskell-course#haskell-course).

## Plutus user documentation

This section includes references to Plutus user documentation such as
explanations, tutorials, how-to guides, etc.

**Documentation**:

- [Plutus Core and Plutus Tx user guide](https://plutus.readthedocs.io/en/latest/):
  introduces the Plutus Core programming language and programming with Plinth. Documentation includes explanations, tutorials, how-to instructions,
  troubleshooting, and reference information:

- [Video presentation on the Plutus Platform](https://www.youtube.com/watch?v=usMPt8KpBeI&feature=youtu.be):
  in this session from the 2020 Cardano Summit, Michael Peyton-Jones walks us
  through working with Plutus, while Jann Müller then takes us through the
  Plutus Application Platform, where assets can be built and launched. He also
  demonstrates how tokens can be transferred using a smart contract.

## Plutus Pioneer program

- [Plutus Pioneer program](https://github.com/input-output-hk/plutus-pioneer-program#readme):
  includes lectures, examples, and exercises
- [Plutus Pioneer program notes and documentation](https://plutus-pioneer-program.readthedocs.io/en/latest/)
- ['plutus-pioneer-program' questions on Cardano Stack Exchange](https://cardano.stackexchange.com/questions/tagged/plutus-pioneer-program)
- [Plutus Pioneer Discord](https://discord.com/channels/826816523368005654/826836848520200233)

**Plutus Pioneer community notes**:

- [How to prepare for the Plutus Pioneer program](https://www.essentialcardano.io/article/how-to-prepare-for-the-plutus-pioneer-program)
- [Essential resources and documentation for the Plutus Pioneer program](https://www.essentialcardano.io/article/essential-resources-and-documentation-for-the-plutus-pioneer-program)
- [Plutus Pioneer program part 1: understanding the EUTXO model and coding the first smart contract](https://www.essentialcardano.io/article/plutus-pioneer-program-part-1-understanding-the-eutxo-model-and-coding-the-first-smart-contract)
- [Plutus Pioneer program - part 2: how to 'deploy' a smart contract on Cardano](https://www.essentialcardano.io/article/plutus-pioneer-program-part-2-how-to-deploy-a-smart-contract-in-cardano)
- [Plutus Pioneer program - part 3: how to mint and burn tokens and NFTs on Cardano](https://www.essentialcardano.io/article/plutus-pioneer-program-part-3-how-to-mint-and-burn-tokens-and-nfts-in-cardano)
- [Plutus Pioneer program - part 4: state machine with Plutus (1/2)](https://www.essentialcardano.io/article/plutus-pioneer-program-part-4-state-machine-with-plutus-12)
- [Plutus Pioneer program - part 5: state machine with Plutus (2/2)](https://www.essentialcardano.io/article/plutus-pioneer-program-part-5-state-machine-with-plutus-22)

## Plutus community resources

This section provides links to Plutus community resources.

If you are a community member engaged with Plutus development, please raise a
pull request to add more relevant links and information.

- [Cardano forum for Plutus discussions](https://forum.cardano.org/c/developers/cardano-plutus/148)
- [Plutus community docs](https://www.essentialcardano.io/article/plutus-community-docs-a-new-documentation-site-for-devs-by-devs)
  article
- [Plutus community docs](https://plutus-community.readthedocs.io/en/latest/)

**Discussions & support:**

- [IOG Technical Discord](https://discord.com/channels/826816523368005654/897221118111400007)
- [Cardano Stack Exchange](https://cardano.stackexchange.com/)
- [Cardano Forum](https://forum.cardano.org/)

## Plutus tooling

There are different tools that developers can use to evaluate and deploy smart
contracts on Cardano:

- [An extended list of community-built developer tools on Cardano](https://www.essentialcardano.io/article/a-list-of-community-built-developer-tools-on-cardano):
  includes NFT creation tools, smart contract platforms, software libraries,
  APIs, interfaces, SDKs, digital product platforms, data services, blockchain
  integration, automation tools, decentralized protocols, enterprise apps, stake
  pool tooling, and more.

- **Emurgo**
  - [Cardano serialization library](https://github.com/Emurgo/cardano-serialization-lib#readme)

- **Plutonomicon**

  - [Plutonomicon - independent community Plutus documentation](https://github.com/Plutonomicon/plutonomicon/blob/main/README.md)
  - [Plutonomicon - GitHub repositories](https://github.com/Plutonomicon)
  - [Plutonomicon - cardano-transaction-lib](https://github.com/Plutonomicon/cardano-transaction-lib#readme)

- **OpShin**

  - [opshin - Pythonic smart contract tooling](https://github.com/OpShin)
  - [Smart contract benchmark](https://github.com/OpShin/plutus-bench)
  - [Generic smart contract optimization CLI](https://github.com/OpShin/plutonomy-cli)

- **Harmonic**
  - [HarmonicPool (Harmonic) - GitHub repositories](https://github.com/HarmonicPool)
- **TxPipe**
  - [Aiken](https://github.com/txpipe/aiken)
  - [Oura](https://github.com/txpipe/oura)
  - [Scrolls](https://github.com/txpipe/scrolls)
  - [Pallas](https://github.com/txpipe/pallas#readme)
  - [Demeter Run](https://demeter.run/) - a cloud environment with all the tools
    for building DApps on Cardano
- **dcSpark**

  - [Carp](https://github.com/dcSpark/carp/tree/main/docs)
  - [Cardano Multiplatform lib](https://github.com/dcSpark/cardano-multiplatform-lib)

- **MLabs**

  - [Plutlib](https://github.com/mlabs-haskell/plutip)

- **dQuadrant**

  - [Kuber](https://github.com/dQuadrant/kuber#readme)

- **CardanoSolutions**

  - [Kupo](https://github.com/CardanoSolutions/kupo#readme)
  - [Ogmios](https://github.com/CardanoSolutions/ogmios#readme)

- **Other tools**
  - [Blockfrost](https://github.com/blockfrost)
  - [Helios](https://github.com/Hyperion-bt/Helios#readme)
  - [Koios](https://koios.rest)
  - [Lucid](https://github.com/spacebudz/lucid#readme)

---
title: Marlowe
metaTitle: Learn about Marlowe
sidebar_position: 2
---

:::info

To get started with Marlowe, find the various resources on the
[documentation website](https://docs.marlowe.iohk.io/docs/introduction):

- [Tutorials](https://docs.marlowe.iohk.io/tutorials)
- [Concepts](https://docs.marlowe.iohk.io/tutorials/concepts/overview)
- [Guides](https://docs.marlowe.iohk.io/tutorials/guides/overview)
- [Playbooks](https://docs.marlowe.iohk.io/tutorials/playbooks/overview)
- [Videos](https://docs.marlowe.iohk.io/tutorials/videos)
- [Contract gallery](https://docs.marlowe.iohk.io/docs/examples/examples-contract-gallery)
- [Developer tools](https://docs.marlowe.iohk.io/docs/developer-tools/overview).

:::

Marlowe is a set of open source tools designed to simplify the creation,
testing, and deployment of secure smart contracts on the Cardano blockchain. It
caters to developers, regardless of their expertise in software development, by
offering intuitive solutions to create, utilize, and monetize smart contracts
with ease.

Marlowe contracts are unique due to their foundation in peer-reviewed research.
Formal proofs, extensive testing, and analysis tools provide strong assurances
for the safety of Marlowe contracts.

Supported programming languages in Marlowe include Marlowe itself, Haskell,
JavaScript, and TypeScript. Marlowe also includes a Blockly editor for visual
programming.

## Marlowe features

The deployment of Marlowe onto the mainnet includes the following features:

- **Marlowe language**: a domain-specific language (DSL) that includes a
  web-based platform to build and run smart contracts.
- **Marlowe Playground**: a simulator that allows testing smart contracts before
  deployment to ensure the code behaves as intended.
- **Marlowe Runtime**: the application backend for managing Marlowe contracts on
  the Cardano blockchain.
- **Marlowe Runtime APIs**: easy-to-use, higher-level APIs that enable
  developers to build and deploy enterprise and Web3 DApp solutions.
- **Marlowe CLI**: provides capabilities to work with Marlowe's Plutus
  validators and run Marlowe contracts manually.
- **Marlowe starter kit**: provides tutorials for developers to learn and run
  simple Marlowe contracts on the Cardano blockchain.
- **Demeter Run integration**: allows using the starter kit without installing
  any software.
- **Updated documentation website**: significantly expanded, updated, and
  integrated into the updated Marlowe website.

---
title: Aiken
metaTitle: Learn about Aiken
sidebar_position: 3
---

:::info

To get started with Aiken, see:

- [Aiken website](https://aiken-lang.org)
- [GitHub repository](https://github.com/aiken-lang/aiken)

:::

Aiken is a modern programming language and toolchain for developing smart contracts
on the Cardano blockchain. It draws inspiration from various modern languages,
like Gleam, Rust, and Elm, renowned for their friendly error messages and an
overall excellent developer experience.

The language is exclusively used for creating on-chain validator scripts. Users
will need to write their off-chain code for generating transactions in another
language, such as Rust, Haskell, JavaScript, Python, etc.

As a language, Aiken is purely functional with static typing and type inference.
This means most of the time, the compiler is smart enough to determine the type
of something without requiring user annotation. It also allows the creation of
custom types resembling records and enums. Aiken does not include higher-kinded
types or type classes because it aims for simplicity.

On-chain scripts are typically small in size and scope compared to other kinds
of applications being developed today. Consequently, they do not necessitate as
many features as general-purpose languages that must tackle far more complex
issues.

Aiken is easier than Plutus to get started with for those who are less familiar
with functional languages like Haskell. Similar to Plutus, Aiken scripts are
compiled down to the untyped Plutus Core (UPLC).

---
title: Hydra
metaTitle: Hydra
sidebar_position: 1
---

Hydra is a layer 2 scaling solution for Cardano rooted in peer-reviewed research that increases transaction throughput and ensures cost efficiency while maintaining rigorous security.

Hydra Head is the first protocol of the Hydra family of protocols, which
embodies the foundation for more advanced deployment scenarios relying on
isomorphic, multi-party state channels. Each Hydra Head works as an off-chain
mini ledger shared between a small group of participants. Developers can use
Hydra Heads to add specialized, complex protocols on top of Cardano.

### Developer documentation

- [Hydra Head documentation](https://hydra.family/head-protocol/).

### Further reading

- [Hydra – Cardano’s solution for ultimate layer 2 scalability](https://iohk.io/en/blog/posts/2021/09/17/hydra-cardano-s-solution-for-ultimate-scalability/)
  blog post
- [Implementing Hydra Heads: the first step towards the full Hydra vision](https://iohk.io/en/blog/posts/2022/02/03/implementing-hydra-heads-the-first-step-towards-the-full-hydra-vision/)
  blog post
- [Hydra: Fast Isomorphic State Channels](https://iohk.io/en/research/library/papers/hydra-fast-isomorphic-state-channels/)
  research paper
- [Interhead Hydra: Two Heads are Better than One](https://iohk.io/en/research/library/papers/interhead-hydra-two-heads-are-better-than-one/)
  research paper.

---
title: Mithril
metaTitle: Mithril
sidebar_position: 2
---

Mithril is a stake-based multi-signature scheme that leverages the existing
Cardano network to provide certified snapshots of all or part of the blockchain
state. These snapshots can be useful in multiple use cases such as secure
voting, data exchange, and synchronization between applications, sidechains,
light wallets, etc.

The first application of Mithril allows faster bootstrapping of Cardano
nodes. As each node has a full copy of the blockchain and needs to verify each
block from the history, it takes a significant amount of time to start a new
node from scratch. With Mithril, it is possible to start a node from a
predefined state, with the same security guarantees that the chain itself
provides, but much faster.

## Developer documentation

- [Mithril documentation](https://mithril.network/doc/).

### Further reading

- [Mithril nears mainnet release](https://iohk.io/en/blog/posts/2023/07/20/mithril-nears-mainnet-release/) blog post
- [Mithril proof of concept is now open-source](https://iohk.io/en/blog/posts/2022/08/29/mithril-proof-of-concept-is-now-open-source/)
  blog post
- [Mithril: a stronger and lighter blockchain for better efficiency](https://iohk.io/en/blog/posts/2021/10/29/mithril-a-stronger-and-lighter-blockchain-for-better-efficiency/)
  blog post
- [Mithril: Stake-based Threshold Multisignatures](https://iohk.io/en/research/library/papers/mithril-stake-based-threshold-multisignatures/)
  research paper.

---
title: Release notes
metaTitle: Release notes
sidebar_position: 1
---

Here is the latest release information for the Cardano components. Component
dependencies and compatibility information can be found in the
[compatibility matrix](/developer-resources/release-notes/comp-matrix). Release
notes for specific components are available on each component's release page.

Cardano node configuration details (for mainnet and testnets) are available on
the
[Cardano environments page](https://book.world.dev.cardano.org/environments.html).

## Latest Cardano component releases

| Name                                                                               | Version                                                                                      | Release date      |
|------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|-------------------|
| [Cardano node](https://github.com/IntersectMBO/cardano-node)                       | [10.5.1](https://github.com/IntersectMBO/cardano-node/releases/tag/10.5.1)                   | 22 July, 2025   |
| [Cardano DB Sync](https://github.com/IntersectMBO/cardano-db-sync)                 | [13.6.0.5](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/13.6.0.5)            | 18 March, 2025  |
| [Cardano Rosetta](https://github.com/cardano-foundation/cardano-rosetta)           | [2.5.0](https://github.com/cardano-foundation/cardano-rosetta/releases/tag/2.5.0)            | 9 April, 2025  |
| [Cardano Rosetta Java](https://github.com/cardano-foundation/cardano-rosetta-java) | [1.4.0](https://github.com/cardano-foundation/cardano-rosetta-java/releases/tag/1.4.0)       | 28 October, 2025  |
| [Cardano GraphQL](https://github.com/cardano-foundation/cardano-graphql)           | [8.4.0](https://github.com/cardano-foundation/cardano-graphql/releases/tag/8.4.0)            | 17 October, 2025  |
| [Cardano addresses](https://github.com/IntersectMBO/cardano-addresses)             | [4.0.1](https://github.com/IntersectMBO/cardano-addresses/releases/tag/4.0.1)                | 30 October, 2025   |
| [Cardano wallet](https://github.com/cardano-foundation/cardano-wallet)             | [v2025-03-31](https://github.com/cardano-foundation/cardano-wallet/releases/tag/v2025-03-31) | 31 March, 2025 |
| [Bech32](https://github.com/IntersectMBO/bech32)                                   | [1.1.7](https://github.com/IntersectMBO/bech32/releases/tag/v1.1.7)                          | 20 May, 2024      |

---
title: Compatibility matrix
metaTitle: Compatibility matrix
sidebar_position: 2
---

This сomponent compatibility with the Cardano node matrix shows the runtime dependencies of Cardano components on a specific version of
[cardano-node](https://github.com/IntersectMBO/cardano-node/releases). (It
does not reflect any stacks.) The compatibility matrix should be read in
conjunction with the Cardano component dependencies definition.

If you only require using the node, such as interacting directly with the node
or managing stake pool operations, we recommend the latest node version.
If you need compatibility with all components, we recommend using
version 10.1.4.

| [Cardano node](https://github.com/IntersectMBO/cardano-node/releases)                          | [Cardano DB Sync](https://github.com/IntersectMBO/cardano-db-sync/releases)                                                                                                                                                         | [Cardano Rosetta server](https://github.com/cardano-foundation/cardano-rosetta/releases)                                                                            | [Cardano Rosetta Java](https://github.com/cardano-foundation/cardano-rosetta-java)     | [Cardano GraphQL](https://github.com/cardano-foundation/cardano-graphql/releases)                                                                                   | [Cardano wallet](https://github.com/cardano-foundation/cardano-wallet/releases)                                                                                                         |
|------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **[10.5.1](https://github.com/IntersectMBO/cardano-node/releases/tag/10.5.1)** (update)        | 
| **[10.4.1](https://github.com/IntersectMBO/cardano-node/releases/tag/10.4.1)** (major release) | [13.6.0.5](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/13.6.0.5)                                                                                                                                                   |                                                                                                                                                                     | [1.3.0](https://github.com/cardano-foundation/cardano-rosetta-java/releases/tag/1.3.0) 
| **[10.3.1](https://github.com/IntersectMBO/cardano-node/releases/tag/10.3.1)** (major release) |                                                                                                                                                                                                                                     |                                                                                                                                                                     | [1.2.8](https://github.com/cardano-foundation/cardano-rosetta-java/releases/tag/1.2.8) |                                                                                                                                                                     | 
| **[10.2.1](https://github.com/IntersectMBO/cardano-node/releases/tag/10.2.1)** (major release) |                                                                                                                                                                                                                                     |                                                                                                                                                                     | [1.2.7](https://github.com/cardano-foundation/cardano-rosetta-java/releases/tag/1.2.7) |                                                                                                                                                                     | [v2025-03-31](https://github.com/cardano-foundation/cardano-wallet/releases/tag/v2025-03-31)
| **[10.1.4](https://github.com/IntersectMBO/cardano-node/releases/tag/10.1.4)** (update)        | [13.6.0.4](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/13.6.0.4)                                                                                                                                                   | [2.4.3](https://github.com/cardano-foundation/cardano-rosetta/releases/tag/2.4.3)                                                                                   | [1.2.6](https://github.com/cardano-foundation/cardano-rosetta-java/releases/tag/1.2.6) | [8.3.3](https://github.com/cardano-foundation/cardano-graphql/releases/tag/8.3.3)                                                                                   | [v2025-03-04](https://github.com/cardano-foundation/cardano-wallet/releases/tag/v2025-03-04)                                                                                            |                                                                                                                                                                                |
| **[10.1.3](https://github.com/IntersectMBO/cardano-node/releases/tag/10.1.3)** (update)        | [13.6.0.4](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/13.6.0.4)                                                                                                                                                   | [2.4.2](https://github.com/cardano-foundation/cardano-rosetta/releases/tag/2.4.2)                                                                                   | [1.1.3](https://github.com/cardano-foundation/cardano-rosetta-java/releases/tag/1.1.3) | [8.3.2](https://github.com/cardano-foundation/cardano-graphql/releases/tag/8.3.2)                                                                                   |                                                                                                                                                                                         |
| **[10.1.2](https://github.com/IntersectMBO/cardano-node/releases/tag/10.1.2)** (update)        | [13.6.0.2](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/13.6.0.2)                                                                                                                                                   | [2.4.1](https://github.com/cardano-foundation/cardano-rosetta/releases/tag/2.4.1)                                                                                   | [1.1.1](https://github.com/cardano-foundation/cardano-rosetta-java/releases/tag/1.1.1) | [8.3.0](https://github.com/cardano-foundation/cardano-graphql/releases/tag/8.3.0)                                                                                   | [v2024-11-18](https://github.com/cardano-foundation/cardano-wallet/releases/tag/v2024-11-18)                                                                                            |                                                                                                                                                                                                                                     | [2.4.0](https://github.com/cardano-foundation/cardano-rosetta/releases/tag/2.4.0)                                                                                   | [1.1.0](https://github.com/cardano-foundation/cardano-rosetta-java/releases/tag/1.1.0) | [8.3.0](https://github.com/cardano-foundation/cardano-graphql/releases/tag/8.3.0)                                                                                   | [v2024-11-18](https://github.com/cardano-foundation/cardano-wallet/releases/tag/v2024-11-18)                                                                                            |
| **[10.1.1](https://github.com/IntersectMBO/cardano-node/releases/tag/10.1.1)** (major release) |                                                                                                                                                                                                                                     |                                                                                                                                                                     |                                                                                        |                                                                                                                                                                     |                                                                                                                                                                                         |
| **[9.2.1](https://github.com/IntersectMBO/cardano-node/releases/tag/9.2.1)** (minor fix)       |                                                                                                                                                                                                                                     |                                                                                                                                                                     |                                                                                        |                                                                                                                                                                     | [v2024-09-29](https://github.com/cardano-foundation/cardano-wallet/releases/tag/v2024-09-29)                                                                                            |
| **[9.2.0](https://github.com/IntersectMBO/cardano-node/releases/tag/9.2.0)** (update)          |                                                                                                                                                                                                                                     |                                                                                                                                                                     | [1.0.0](https://github.com/cardano-foundation/cardano-rosetta-java/releases/tag/1.0.0) |                                                                                                                                                                     |                                                                                                                                                                                         |
| **[9.1.1](https://github.com/IntersectMBO/cardano-node/releases/tag/9.1.1)** (fix)             | [13.5.0.2](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/13.5.0.2)                                                                                                                                                   | [2.3.3](https://github.com/cardano-foundation/cardano-rosetta/releases/tag/2.3.3)                                                                                   |                                                                                        | [8.2.2](https://github.com/cardano-foundation/cardano-graphql/releases/tag/8.2.2)                                                                                   | [v2024-09-03](https://github.com/cardano-foundation/cardano-wallet/releases/tag/v2024-09-03)                                                                                            |
| **[9.1.0](https://github.com/IntersectMBO/cardano-node/releases/tag/9.1.0)** (update)          | [13.5.0.0](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/13.5.0.0)                                                                                                                                                   | [2.3.2](https://github.com/cardano-foundation/cardano-rosetta/releases/tag/2.3.2)                                                                                   |                                                                                        | [8.2.1](https://github.com/cardano-foundation/cardano-graphql/releases/tag/8.2.1)                                                                                   | [v2024-08-11](https://github.com/cardano-foundation/cardano-wallet/releases/tag/v2024-08-11)                                                                                            |
| **[9.0.0](https://github.com/IntersectMBO/cardano-node/releases/tag/9.0.0)** (major release)   |                                                                                                                                                                                                                                     |                                                                                                                                                                     |                                                                                        |                                                                                                                                                                     |                                                                                                                                                                                         |
| **[8.12.2](https://github.com/IntersectMBO/cardano-node/releases/tag/8.12.2)** (fix)           |                                                                                                                                                                                                                                     |                                                                                                                                                                     |                                                                                        |                                                                                                                                                                     |                                                                                                                                                                                         |
| **[8.9.4](https://github.com/IntersectMBO/cardano-node/releases/tag/8.9.4)** (fix)             |                                                                                                                                                                                                                                     | [2.2.5](https://github.com/cardano-foundation/cardano-rosetta/releases/tag/2.2.5)                                                                                   |                                                                                        |                                                                                                                                                                     |                                                                                                                                                                                         |
| **[8.9.3](https://github.com/IntersectMBO/cardano-node/releases/tag/8.9.3)** (enhancements)    |                                                                                                                                                                                                                                     | [2.2.4](https://github.com/cardano-foundation/cardano-rosetta/releases/tag/2.2.4)                                                                                   |                                                                                        | [8.1.0](https://github.com/cardano-foundation/cardano-graphql/releases/tag/8.1.0)                                                                                   |                                                                                                                                                                                         |
| **[8.9.2](https://github.com/IntersectMBO/cardano-node/releases/tag/8.9.2)** (fix)             | [13.2.0.2](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/13.2.0.2)                                                                                                                                                   | [2.2.2](https://github.com/cardano-foundation/cardano-rosetta/releases/tag/2.2.2)                                                                                   |                                                                                        |                                                                                                                                                                     | [2024-05-05](https://github.com/cardano-foundation/cardano-wallet/releases/tag/v2024-05-05)                                                                                             |
| **[8.9.1](https://github.com/IntersectMBO/cardano-node/releases/tag/8.9.1)** (minor update)    | [13.2.0.2](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/13.2.0.2)                                                                                                                                                   |                                                                                                                                                                     |                                                                                        |                                                                                                                                                                     | [2024-03-27](https://github.com/cardano-foundation/cardano-wallet/releases/tag/v2024-03-27)                                                                                             |
| **[8.9.0](https://github.com/IntersectMBO/cardano-node/releases/tag/8.9.0)** (bug fix)         |                                                                                                                                                                                                                                     |                                                                                                                                                                     |                                                                                        | [8.0.3](https://github.com/cardano-foundation/cardano-graphql/releases/tag/8.0.3)                                                                                   |                                                                                                                                                                                         |
| **[8.7.3](https://github.com/IntersectMBO/cardano-node/releases/tag/8.7.3)** (minor bug fix)   | [13.2.0.1](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/13.2.0.1)                                                                                                                                                   | [2.2.1](https://github.com/cardano-foundation/cardano-rosetta/releases/tag/2.2.1)                                                                                   |                                                                                        | [8.0.1](https://github.com/cardano-foundation/cardano-graphql/releases/tag/8.0.1)/[8.0.2](https://github.com/cardano-foundation/cardano-graphql/releases/tag/8.0.2) | [2024-03-01](https://github.com/cardano-foundation/cardano-wallet/releases/tag/v2024-03-01)                                                                                             |
| **[8.7.2](https://github.com/IntersectMBO/cardano-node/releases/tag/8.7.2)** (major update)    |                                                                                                                                                                                                                                     |                                                                                                                                                                     |                                                                                        |                                                                                                                                                                     |                                                                                                                                                                                         |
| **[8.1.2](https://github.com/IntersectMBO/cardano-node/releases/tag/8.1.2)** (minor update)    | [13.1.1.3](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/13.1.1.3)                                                                                                                                                   | [2.2.0](https://github.com/cardano-foundation/cardano-rosetta/releases/tag/2.2.0)                                                                                   |                                                                                        |                                                                                                                                                                     | [2024-12-18](https://github.com/cardano-foundation/cardano-wallet/releases/tag/v2023-12-18)                                                                                             |
| **[8.1.1](https://github.com/IntersectMBO/cardano-node/releases/tag/8.1.1)** (bug fix)         | [13.1.1.3](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/13.1.1.3)                                                                                                                                                   |                                                                                                                                                                     |                                                                                        |                                                                                                                                                                     | [2023-07-18](https://github.com/cardano-foundation/cardano-wallet/releases/tag/v2023-07-18)                                                                                             |
| **[8.0.0](https://github.com/IntersectMBO/cardano-node/releases/tag/8.0.0)** (major)           | [13.1.1.3](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/13.1.1.3)                                                                                                                                                   |                                                                                                                                                                     |                                                                                        |                                                                                                                                                                     |                                                                                                                                                                                         |
| **[1.35.7](https://github.com/IntersectMBO/cardano-node/releases/tag/1.35.7)** (bug fix)       | [13.0.5](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/13.0.5)/[13.1.0.0](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/13.1.0.0)                                                                     | [2.0.0](https://github.com/cardano-foundation/cardano-rosetta/releases/tag/2.0.0)/[2.1.0](https://github.com/cardano-foundation/cardano-rosetta/releases/tag/2.1.0) |                                                                                        | [7.0.2](https://github.com/cardano-foundation/cardano-graphql/releases/tag/7.0.2)/[8.0.0](https://github.com/cardano-foundation/cardano-graphql/releases/tag/8.0.0) | [2022-12-14](https://github.com/cardano-foundation/cardano-wallet/releases/tag/v2022-12-14)                                                                                             |
| **[1.35.6](https://github.com/IntersectMBO/cardano-node/releases/tag/1.35.6)** (new features)  | [13.0.5](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/13.0.5)/[13.1.0.0](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/13.1.0.0)                                                                     | [2.0.0](https://github.com/cardano-foundation/cardano-rosetta/releases/tag/2.0.0)/[2.1.0](https://github.com/cardano-foundation/cardano-rosetta/releases/tag/2.1.0) |                                                                                        | [7.0.2](https://github.com/cardano-foundation/cardano-graphql/releases/tag/7.0.2)/[8.0.0](https://github.com/cardano-foundation/cardano-graphql/releases/tag/8.0.0) | [2022-12-14](https://github.com/cardano-foundation/cardano-wallet/releases/tag/v2022-12-14)                                                                                             |
| **[1.35.5](https://github.com/IntersectMBO/cardano-node/releases/tag/1.35.5)** (bug fix)       | [13.0.5](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/13.0.5)/[13.1.0.0](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/13.1.0.0)                                                                     | [2.0.0](https://github.com/cardano-foundation/cardano-rosetta/releases/tag/2.0.0)/[2.1.0](https://github.com/cardano-foundation/cardano-rosetta/releases/tag/2.1.0) |                                                                                        | [7.0.2](https://github.com/cardano-foundation/cardano-graphql/releases/tag/7.0.2)/[8.0.0](https://github.com/cardano-foundation/cardano-graphql/releases/tag/8.0.0) | [2022-12-14](https://github.com/cardano-foundation/cardano-wallet/releases/tag/v2022-12-14)                                                                                             |
| **[1.35.4](https://github.com/IntersectMBO/cardano-node/releases/tag/1.35.4)** (major)         | [13.0.5](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/13.0.5)/[13.1.0.0](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/13.1.0.0)                                                                     | [2.0.0](https://github.com/cardano-foundation/cardano-rosetta/releases/tag/2.0.0)/[2.1.0](https://github.com/cardano-foundation/cardano-rosetta/releases/tag/2.1.0) |                                                                                        | [7.0.2](https://github.com/cardano-foundation/cardano-graphql/releases/tag/7.0.2)/[8.0.0](https://github.com/cardano-foundation/cardano-graphql/releases/tag/8.0.0) | [2022-12-14](https://github.com/cardano-foundation/cardano-wallet/releases/tag/v2022-12-14)                                                                                             |
| **[1.35.3](https://github.com/IntersectMBO/cardano-node/releases/tag/1.35.3)** (bug fix)       | [13.0.5](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/13.0.5)                                                                                                                                                       | [2.0.0](https://github.com/cardano-foundation/cardano-rosetta/releases/tag/2.0.0)                                                                                   |                                                                                        | [7.0.2](https://github.com/cardano-foundation/cardano-graphql/releases/tag/7.0.2)                                                                                   | [2022-10-06](https://github.com/cardano-foundation/cardano-wallet/releases/tag/v2022-10-06)                                                                                             |
| **[1.34.1](https://github.com/IntersectMBO/cardano-node/releases/tag/1.34.1)**                 | [12.0.2](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/12.0.2)                                                                                                                                                       | [1.7.0](https://github.com/cardano-foundation/cardano-rosetta/releases/tag/1.7.0)                                                                                   |                                                                                        | [6.2.0](https://github.com/cardano-foundation/cardano-graphql/releases/tag/6.2.0)                                                                                   | [2022-05-27](https://github.com/cardano-foundation/cardano-wallet/releases/tag/v2022-05-27)                                                                                             |
| **[1.34.0](https://github.com/IntersectMBO/cardano-node/releases/tag/1.34.0)**                 | [12.0.2](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/12.0.2)                                                                                                                                                       | [1.7.0](https://github.com/cardano-foundation/cardano-rosetta/releases/tag/1.7.0)                                                                                   |                                                                                        | [6.2.0](https://github.com/cardano-foundation/cardano-graphql/releases/tag/6.2.0)                                                                                   |                                                                                                                                                                                         |
| **[1.33.0](https://github.com/IntersectMBO/cardano-node/releases/tag/1.33.0)**                 | [12.0.2](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/12.0.2)                                                                                                                                                       | [1.7.0](https://github.com/cardano-foundation/cardano-rosetta/releases/tag/1.7.0)                                                                                   |                                                                                        | [6.2.0](https://github.com/cardano-foundation/cardano-graphql/releases/tag/6.2.0)                                                                                   | [2022-01-18](https://github.com/cardano-foundation/cardano-wallet/releases/tag/v2022-01-18)                                                                                             |
| **[1.32.1](https://github.com/IntersectMBO/cardano-node/releases/tag/1.32.1)**                 |                                                                                                                                                                                                                                     |                                                                                                                                                                     |                                                                                        |                                                                                                                                                                     | [2021-12-15](https://github.com/cardano-foundation/cardano-wallet/releases/tag/v2021-12-15)                                                                                             |
| **[1.31.0](https://github.com/IntersectMBO/cardano-node/releases/tag/1.31.0)**                 | [12.0.2](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/12.0.2)                                                                                                                                                       | [1.6.1](https://github.com/cardano-foundation/cardano-rosetta/releases/tag/1.6.1)                                                                                   |                                                                                        | [6.1.0](https://github.com/cardano-foundation/cardano-graphql/releases/tag/6.1.0)                                                                                   |                                                                                                                                                                                         |
| **[1.30.1](https://github.com/IntersectMBO/cardano-node/releases/tag/1.30.1)**                 | [11.0.3](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/11.0.3)/[11.0.4](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/11.0.4)                                                                         | [1.5.0](https://github.com/cardano-foundation/cardano-rosetta/releases/tag/1.5.0)                                                                                   |                                                                                        | [6.0.0](https://github.com/cardano-foundation/cardano-graphql/releases/tag/6.0.0)                                                                                   | [2021-11-11 ](https://github.com/cardano-foundation/cardano-wallet/releases/tag/v2021-11-11)                                                                                            |
| **[1.29.0](https://github.com/IntersectMBO/cardano-node/releases/tag/1.29.0)**                 | [11.0.0](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/11.0.0)                                                                                                                                                       | [1.4.0](https://github.com/cardano-foundation/cardano-rosetta/releases/tag/1.4.0)                                                                                   |                                                                                        | [5.1.0](https://github.com/cardano-foundation/cardano-graphql/releases/tag/5.1.0)                                                                                   | [2021-09-09](https://github.com/cardano-foundation/cardano-wallet/releases/tag/v2021-09-09)                                                                                             |
| **[1.27.0](https://github.com/IntersectMBO/cardano-node/releases/tag/1.27.0)**                 | [10.0.1](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/10.0.1)                                                                                                                                                       | [1.3.0](https://github.com/cardano-foundation/cardano-rosetta/releases/tag/1.3.0)                                                                                   |                                                                                        | [5.0.0](https://github.com/cardano-foundation/cardano-graphql/releases/tag/5.0.0)                                                                                   | [2021-06-11](https://github.com/cardano-foundation/cardano-wallet/releases/tag/v2021-06-11)                                                                                             |
| **[1.26.2](https://github.com/IntersectMBO/cardano-node/releases/tag/1.26.2)**                 | [9.0.0](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/9.0.0)                                                                                                                                                         |                                                                                                                                                                     |                                                                                        |                                                                                                                                                                     |                                                                                                                                                                                         |
| **[1.25.1](https://github.com/IntersectMBO/cardano-node/releases/tag/1.25.1)**                 | [8.0.0](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/8.0.0)/[9.0.0](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/9.0.0)                                                                             | [1.1.0](https://github.com/cardano-foundation/cardano-rosetta/releases/tag/1.1.0)/[1.2.1](https://github.com/cardano-foundation/cardano-rosetta/releases/tag/1.2.1) |                                                                                        | [3.2.0](https://github.com/cardano-foundation/cardano-graphql/releases/tag/3.2.0)                                                                                   | [2021-02-12](https://github.com/cardano-foundation/cardano-wallet/releases/tag/v2021-02-12)/[2021-02-15](https://github.com/cardano-foundation/cardano-wallet/releases/tag/v2021-02-15) |
| **[1.24.2](https://github.com/IntersectMBO/cardano-node/releases/tag/1.24.2)**                 | [7.1.0](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/7.1.0)                                                                                                                                                         | [1.0.0](https://github.com/cardano-foundation/cardano-rosetta/releases/tag/1.0.0)                                                                                   |                                                                                        | [3.1.1](https://github.com/cardano-foundation/cardano-graphql/releases/tag/3.1.1)                                                                                   | [2020-12-21](https://github.com/cardano-foundation/cardano-wallet/releases/tag/v2020-12-21)/[2021-01-12](https://github.com/cardano-foundation/cardano-wallet/releases/tag/v2021-01-12) |
| **[1.21.1](https://github.com/IntersectMBO/cardano-node/releases/tag/1.21.1)**                 | [5.0.2](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/5.0.2)/[5.0.3](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/5.0.3)/[6.0.0](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/6.0.0) |                                                                                                                                                                     |                                                                                        | [2.2.1](https://github.com/cardano-foundation/cardano-graphql/releases/tag/2.2.1)                                                                                   | [2020-10-13](https://github.com/cardano-foundation/cardano-wallet/releases/tag/v2020-10-13)                                                                                             |
| **[1.20.0](https://github.com/IntersectMBO/cardano-node/releases/tag/1.20.0)**                 | [5.0.1](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/5.0.1)/[5.0.3](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/5.0.3)                                                                             |                                                                                                                                                                     |                                                                                        | [2.2.0](https://github.com/cardano-foundation/cardano-graphql/releases/tag/2.2.0)                                                                                   | [2020-9-22](https://github.com/cardano-foundation/cardano-wallet/releases/tag/v2020-09-22)/[2020-09-30](https://github.com/cardano-foundation/cardano-wallet/releases/tag/v2020-09-30)  |
| **[1.19.1](https://github.com/IntersectMBO/cardano-node/releases/tag/1.19.1)**                 | [5.0.0](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/5.0.0)/[5.0.1](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/5.0.1)/[5.0.3](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/5.0.3) |                                                                                                                                                                     |                                                                                        | [2.1.0](https://github.com/cardano-foundation/cardano-graphql/releases/tag/2.1.0)                                                                                   | [2020-9-11](https://github.com/cardano-foundation/cardano-wallet/releases/tag/v2020-09-11)                                                                                              |
| **[1.19.0](https://github.com/IntersectMBO/cardano-node/releases/tag/1.19.0)**                 |                                                                                                                                                                                                                                     |                                                                                                                                                                     |                                                                                        |                                                                                                                                                                     |                                                                                                                                                                                         |
| **[1.18.0](https://github.com/IntersectMBO/cardano-node/releases/tag/1.18.0)**                 | [4.0.0](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/4.0.0)                                                                                                                                                         |                                                                                                                                                                     |                                                                                        |                                                                                                                                                                     |                                                                                                                                                                                         |
| **[1.18.0](https://github.com/IntersectMBO/cardano-node/releases/tag/1.18.0)**                 | [3.1.0](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/3.1.0)                                                                                                                                                         |                                                                                                                                                                     |                                                                                        | [2.0.0](https://github.com/cardano-foundation/cardano-graphql/releases/tag/2.0.0)                                                                                   | [2020-8-3](https://github.com/cardano-foundation/cardano-wallet/releases/tag/v2020-08-03)                                                                                               |


The table below illustrates the key architectural components used in various Cardano blockchain operations. It details essential and optional tools for tasks such as stake pool operations, transaction management, and data handling.

| Stack | Core components | Description | Optional components|
|--|--|--|--|
| Stake pool operations  | `cardano-node` `cardano-cli` | Essential for block creation and participating in the consensus process. `cardano-cli` provides command-line interface for managing and interacting with the node. | Monitoring tools like `Prometheus` and `Grafana` for performance metrics.|
| Address management  | `cardano-addresses` `cardano-cli` | `cardano-addresses` is used for creating and managing cryptographic addresses. It's often bundled with `cardano-node` for ease of use in address operations. | |
| Handling wallets and transactions  | `cardano-rosetta` `cardano-wallet` | `cardano-wallet` is used for direct wallet management and transactions. `cardano-rosetta` offers a standardized interface for interacting with the blockchain. | |
| On-chain data and metadata | `cardano-db-sync` `cardano-graphql` | `cardano-db-sync` populates a PostgreSQL database from node data, making it queryable. `cardano-graphql` allows querying this data via GraphQL.  | `cardano-rosetta` can be used for blockchain agnostic data interaction. |

---
title: The Cardano node video course
metaTitle: The Cardano node video course
---

The IOG Academy provides this course on YouTube.

<p align="right">
  <small>Instructor: Carlos Lopez de Lara, product owner at [Input Output](https://iohk.io/).</small>
</p>

| Video | Summary |
| -------- | --------- |
| [Cardano node course](https://www.youtube.com/playlist?list=PLNEK_Ejlx3x2ut-Pq-hi0NFVsgKB3EddR) |  Introduction to the course. |
| [Building the node with Cabal](https://www.youtube.com/watch?v=csqvbw3F_BU&list=PLNEK_Ejlx3x2ut-Pq-hi0NFVsgKB3EddR&index=1)   | Step-by-step demonstration of the whole installation, with prerequisites. |
| [Building the node with Nix](https://www.youtube.com/watch?v=iREukg3-JSM&list=PLNEK_Ejlx3x2ut-Pq-hi0NFVsgKB3EddR&index=2&pp=iAQB) | Another full demonstration, this time using Nix. |
| [Running the node](https://www.youtube.com/watch?v=YlUljmlCPYs&list=PLNEK_Ejlx3x2ut-Pq-hi0NFVsgKB3EddR&index=3&pp=iAQB) | Running the node and connecting to a network.  |
| [Generating keys and addresses](https://www.youtube.com/watch?v=8ZYuiRxPAZc&list=PLNEK_Ejlx3x2ut-Pq-hi0NFVsgKB3EddR&index=4&pp=iAQB) | A detailed explanation of generating the different types of keys and addresses used in Cardano. |
| [Simple transactions with the build-raw command](https://www.youtube.com/watch?v=rbst_uiGpI4&list=PLNEK_Ejlx3x2ut-Pq-hi0NFVsgKB3EddR&index=5&pp=iAQB) | Carlos describes the two commands used to build a transaction, and demonstrates the hard way; using the `build-raw` command. |
| [Simple transaction with the build command](https://www.youtube.com/watch?v=AVz_zsDd6wE&list=PLNEK_Ejlx3x2ut-Pq-hi0NFVsgKB3EddR&index=6&pp=iAQB) | A demonstration of the power and convenience of using the `build` command. |
| [Register stake address and delegate to a stake pool](https://www.youtube.com/watch?v=m0BmjjNt19w&list=PLNEK_Ejlx3x2ut-Pq-hi0NFVsgKB3EddR&index=7&pp=iAQB) | Carlos explains certificates and demonstrates the process of delegating ada to the stake pool using the Cardano command line interface. |
| [Protocol parameters 1/4](https://www.youtube.com/watch?v=Czwc4U3B5k8&list=PLNEK_Ejlx3x2ut-Pq-hi0NFVsgKB3EddR&index=8&pp=iAQB) | Parameters, one of four. These videos explain what the protocol parameters are, and how they influence the behavior of Cardano. |
| [Protocol parameters 2/4](https://www.youtube.com/watch?v=Czwc4U3B5k8&list=PLNEK_Ejlx3x2ut-Pq-hi0NFVsgKB3EddR&index=9&pp=iAQB) | Parameters, two of four. |
| [Protocol parameters 3/4](https://www.youtube.com/watch?v=Czwc4U3B5k8&list=PLNEK_Ejlx3x2ut-Pq-hi0NFVsgKB3EddR&index=10&pp=iAQB) | Parameters, three of four. |
| [Protocol parameters 4/4](https://www.youtube.com/watch?v=Czwc4U3B5k8&list=PLNEK_Ejlx3x2ut-Pq-hi0NFVsgKB3EddR&index=11&pp=iAQB) | Parameters, four of four. |
| [Peer-to-peer (P2P) networking](https://www.youtube.com/watch?v=7YyCJ3fDN0I&list=PLNEK_Ejlx3x2ut-Pq-hi0NFVsgKB3EddR&index=12&pp=iAQB) | P2P networking; a detailed explanation of Dynamic P2P networking and its advantages. |
| [P2P networking; topology and configuration](https://www.youtube.com/watch?v=hOFVL6gjFrw&list=PLNEK_Ejlx3x2ut-Pq-hi0NFVsgKB3EddR&index=13&pp=iAQB) | P2P networking; how to configure the topology and configuration files for the different types of nodes. |
| [Creating a stake pool. The setup](https://www.youtube.com/watch?v=6AQaPdmxnfo&list=PLNEK_Ejlx3x2ut-Pq-hi0NFVsgKB3EddR&index=14&pp=iAQB) | Creating a stake pool; recommended hardware layout and software configuration. |
| [Creating a stake pool. Generating stake pool keys](https://www.youtube.com/watch?v=xydi4_pqOdo&list=PLNEK_Ejlx3x2ut-Pq-hi0NFVsgKB3EddR&index=15&pp=iAQB) | Creating a stake pool; a detailed demonstration and explanation of generating the required keys and other artifacts. |
| [Creating a stake pool. Registering the stake pool](https://www.youtube.com/watch?v=9UK_EFG-nkA&list=PLNEK_Ejlx3x2ut-Pq-hi0NFVsgKB3EddR&index=16&pp=iAQB) | Creating a stake pool; using the artifacts created earlier to complete the registration. |
| [Creating a stake pool. RTS, topology, and systemd](https://www.youtube.com/watch?v=1CaMmsgFPnw&list=PLNEK_Ejlx3x2ut-Pq-hi0NFVsgKB3EddR&index=17&pp=iAQB) | Creating a stake pool; the Haskell runtime options, and how to extend and override them. Details of the topology files and the startup scripts. Using systemd to restart your node as required. |
| [Stake snapshots](https://www.youtube.com/watch?v=t5BOFmy6IYg&list=PLNEK_Ejlx3x2ut-Pq-hi0NFVsgKB3EddR&index=18&pp=iAQB) | How to request test funds from the faucet. Details of stake distribution snapshots. |
| [Stake pool operations. Useful commands 1/2](https://www.youtube.com/watch?v=NCdsk75-7NA&list=PLNEK_Ejlx3x2ut-Pq-hi0NFVsgKB3EddR&index=19&pp=iAQB) | Stake pool operations; useful commands for managing your stake pool; video one of two. |
| [Stake pool operations. Useful commands 2/2](https://www.youtube.com/watch?v=NCdsk75-7NA&list=PLNEK_Ejlx3x2ut-Pq-hi0NFVsgKB3EddR&index=20&pp=iAQB) | Stake pool operations; useful commands two of two. |
| [Monitoring with Prometheus](https://www.youtube.com/watch?v=iqAmwhk7djE&list=PLNEK_Ejlx3x2ut-Pq-hi0NFVsgKB3EddR&index=21&pp=iAQB) | Stake pool operations; how to install and configure Prometheus to monitor your Cardano node. |
| [Cardano governance](https://www.youtube.com/watch?v=biUHK1UDkAY&list=PLNEK_Ejlx3x2ut-Pq-hi0NFVsgKB3EddR&index=22&pp=iAQB) | Carlos discusses the history and mechanism of implementing Cardano updates.
| [Creating a local cluster](https://www.youtube.com/watch?v=2oAonlDUcNY&list=PLNEK_Ejlx3x2ut-Pq-hi0NFVsgKB3EddR&index=23&pp=iAQB) | Creating a local cluster; using the `create-cardano` command. |
| [Local cluster from Byron to Shelley](https://www.youtube.com/watch?v=8Sp2ljOrXXw&list=PLNEK_Ejlx3x2ut-Pq-hi0NFVsgKB3EddR&index=24&pp=iAQB) | Creating a local cluster; moving from genesis to Byron to Shelley. |
| [Creating a stake pool on the local cluster](https://www.youtube.com/watch?v=0svni-Dy4zM&list=PLNEK_Ejlx3x2ut-Pq-hi0NFVsgKB3EddR&index=25&pp=iAQB) | Creating a local cluster; providing funds, keys, and configuration files for a new stake pool. |
| [Bringing the d parameter down](https://www.youtube.com/watch?v=LNUzmhdGPbY&list=PLNEK_Ejlx3x2ut-Pq-hi0NFVsgKB3EddR&index=26&pp=iAQB) | Creating a local cluster; submitting a proposal to bring the decentralization parameter (d) down from 1 to 0.80. |
| [From Shelley to Alonzo](https://www.youtube.com/watch?v=mFyutul_5mU&list=PLNEK_Ejlx3x2ut-Pq-hi0NFVsgKB3EddR&index=27&pp=iAQB) | Creating a local cluster; moving our local cluster from the Shelley era to Alonzo. |
## Cardano node tests

Node tests are CLI-based system and end-to-end tests for the Cardano node. Node
tests are open source for Cardano node users running the CLI, SPOs, third-party
tool developers, and others to test Cardano functionality from the end-user
perspective.

You can access
[**Cardano node tests documentation here**](https://tests.cardano.intersectmbo.org/).

---
title: Native tokens
metaTitle: Learn about native tokens
---

:::info

To start working with native tokens, see:

- [Developer portal native token tutorials](https://developers.cardano.org/docs/native-tokens/)
- [Ledger explanations about native tokens](https://cardano-ledger.readthedocs.io/en/latest/explanations/index.html).

:::

_Native tokens_ is a feature that enables the transacting of multi-assets on
[Cardano](https://cardano.org/). Users can transact with ada, and an unlimited
number of user-defined (custom) tokens natively.

Native support offers distinct advantages for developers: there is no need to
create smart contracts to handle custom tokens, for example, which removes a
layer of added complexity and potential for manual errors since the ledger
handles all token-related functionality.

The native tokens feature extends the accounting infrastructure defined in the
ledger model (originally designed for processing ada-only transactions) to
accommodate transactions using a range of assets. These assets include ada and a
variety of user-defined custom token types.

Read more about
[native tokens and how they compare to ada and ERC20](https://cardano-ledger.readthedocs.io/en/latest/explanations/features.html)
and watch this
[native tokens explainer video](https://www.youtube.com/watch?v=PVqsCXh-V5Y).

## Single asset ledgers

Cryptocurrency ledgers that track exactly one type of asset are called
single-asset ledgers.

## Multi-asset (MA) support

A blockchain, ledger, or cryptocurrency is said to have multi-asset (MA) support
when the network or ledger supports tracking, transfer, and ownership of
different types of assets on its ledger. In the Cardano environment, this
functionality is provided by the native tokens feature.

This feature accommodates transactions that simultaneously use a range of
assets. These assets include ada and a variety of user-defined custom token
types.

## Native versus non-native MA support

Some cryptocurrency ledgers have built-in support to track ownership and
transfer of more than one type of asset. This type of MA support is called
native. Cardano's MA functionality is native.

If a cryptocurrency platform has sufficiently powerful smart contract
functionality, it is possible to track assets for which there is no ledger
accounting support. This is done with a layer-2 solution built using smart
contracts. This type of MA support is non-native.

## Assets and tokens

**Assets**

An _asset_ is an object that represents value on the blockchain. These objects
can be a variety of things, such as a digital asset like ada, a role, a
credential, or a quantity of goods.

The term _asset_ can refer to either:

- the identifier of a class of objects, such as 'ada' or 'couttscoins'; or
- a particular quantity of a specific object, such as '100 lovelace', '24
  couttscoins', 'this house' or 'these 10 tonnes of coffee'.

An asset is uniquely identified by an _asset ID_, which is a pair of both the
_policy ID_ and _asset name_. It is important to note that although ada can
_act_ as an asset, it is not represented using an explicit policy ID.

Tokens that have the same asset ID have the property of being fungible with each
other, and are _not_ fungible with tokens that have a different asset ID. An
asset ID is a unique identifier for a collection of fungible tokens.

- _PolicyID_ - the unique identifier that is associated with a minting policy.
  Let’s take a look at two policy ID examples: `NFLPlayerCardsPolicyID` and
  `RushConcertPolicyID`. The ID is computed by applying a hash function to the
  policy itself, and is thus a sequence of letters and numbers. For example:

```
NFLPlayerCardsPolicyID = e0d123e5f316bef7
```

- _Asset name_ - an (immutable) property of an asset that is used to distinguish
  different assets within the same policy. Unlike the _policyID_, the asset name
  does not refer to any code or set of rules, and can be common words, such as
  `‘tickets’` or `‘VIPTickets’`, for example. However, the policy under which an
  asset is scoped can specify some constraints on valid asset names.

Different policies can use the same asset names for different tokens. For
example, the token bundle:

```
FAKERushConcertPolicyID {  (Tickets, 500),
                           (VIPTickets, 50)}
```

contains the `Tickets` and `VIPTickets` asset names, but these are not fungible
with the `RushConcertPolicyID` tickets that have been defined in another token
bundle, since they are scoped under different policies.

**Tokens**

A _token_ is a short term for 'asset token', which is the on-chain
representation of an asset and its basic accounting unit. A token can represent
one ada, one house, or the value of ten tonnes of coffee, for example.

## Currencies

_Currency_ is a medium of exchange for goods and services that commonly refers
to a payment unit. Cardano supports currencies such as ada and native tokens,
which act similarly in the network.

However, ada is the _principal currency_ that, at this time, is accepted as
fee-payment, to make deposits, and is also the only currency in which rewards
are distributed. This property of ada (and no other type of asset) is due to the
construction of the underlying consensus protocol.

Native tokens represent some value and act as an accounting unit, which can be
used for payments, transactions, and can be sent to an exchange address.
_Native_ means that these tokens are supported by the Cardano accounting ledger
without the need for additional smart contracts, as the ledger features built-in
support to track ownership and transfer of more than one type of asset.

While both ada and native tokens hold value and act as a payment and transaction
unit, only ada is used for fees and rewards, while only native tokens can be
customly created.

## Using ada for administrative operations

Ada is Cardano’s principal currency. It is essential to hold ada (besides other
currencies) to transfer multi-asset tokens between addresses, since each address
must hold a
[minimum ada value](https://cardano-ledger.readthedocs.io/en/latest/explanations/min-utxo-alonzo.html)
(`min-ada-value`).

As a consequence of this design, the following apply:

1. It is impossible to create outputs that contain only custom tokens.
2. The number of each kind of token in an output does not affect the
   `min-ada-value` of the output, but the **number of types of tokens**
   contained in an output increases the `min-ada-value`. _(The reason for this
   is that the names and policy IDs of each of the types of tokens take up
   additional space in the output.)_
3. Sending custom tokens to an address _always_ involves sending the
   `min-ada-value` of ada to that address alongside the custom tokens (by
   including the ada in the same output). If the address is not spendable by the
   user that is sending the tokens, the ada sent alongside the tokens no longer
   belongs to the sender.

:::note

Before transferring custom tokens, users may choose to use off-chain
communication to negotiate who supplies the ada to cover the min-ada-value in
the output made by the transferring transaction.

:::

4. To recover the ada stored alongside custom tokens in an output O, the user
   must either:

- Spend the output O, and _burn_ the custom tokens that are stored therein
- Spend an output O and an output O’, and consolidate the tokens therein with
  the same collection of types of custom tokens stored in another output (spent
  within the same transaction)

> For example: `(CryptoDoggiesPolicy, poodle, 1)` contained in O can be
> consolidated with `(CryptoDoggiesPolicy, poodle, 3)` in O’, for a total of
> `(CryptoDoggiesPolicy, poodle, 4)` in a new output that is made by the
> consolidating transaction.

5. Splitting custom tokens into more outputs than they were contained in before
   the transaction was processed requires more total ada to cover the
   `min-ada-value`, since some ada must be supplied in each output.

## Token bundles

A token bundle is a heterogeneous (‘mixed’) collection of tokens. Any tokens can
be bundled together. Token bundles are the standard - and only - way to
represent and store assets on the Cardano blockchain.

Token bundles organize tokens into a particular kind of data structure (see
example and explanation below), so that which tokens are fungible with which
other tokens explicitly adheres to this organization.

In previous versions of the Cardano ledger, ada amounts were specified in
transaction and UTXO outputs. With the introduction of multi-asset support,
these amounts have been extended with token bundles, which can specify an ada
amount alongside quantities of other assets in a single output.

Token bundles are contained in outputs and mint fields of transactions, and the
outputs in the UTXO set tracked by the ledger. Note that certain fields of a
transaction must still explicitly specify ada amounts, such as the fee field.

**Token bundle example**

Here is an example of a token bundle, let’s call it **TB_Example**:

```
{
NFLPlayerCardsPolicyID {(SomeNFLPlayerCard, 1),
                        (SomeOtherNFLPlayerCard, 1),
                        (YetAnotherNFLPlayerCard, 1)}


RushConcertPolicyID {(Tickets, 500),
                     (VIPTickets, 50)}
}
```

**How and where are token bundles stored?**

Token bundles can be found:

1. As the mint field of a transaction, indicating that the transaction is
   creating the tokens in the bundle.
2. In an output of a transaction or an output in the current UTXO tracked by the
   ledger, alongside the address of the output, eg
   `Multi { MyAddress, value: TB_Example }`

**Splitting and combining token bundles**

Transactions can arbitrarily split and combine token bundles into different
bundles. For example, we can split the bundle `TB_Example` into two:

_TB_Example_Part1:_

```
NFLPlayerCardsPolicyID {(SomeNFLPlayerCard, 1)}


RushConcertPolicyID {(Tickets, 200),
                     (VIPTickets, 20)}
```

_TB_ExamplePart2:_

```
NFLPlayerCardsPolicyID {(SomeOtherNFLPlayerCard, 1),
                        (YetAnotherNFLPlayerCard, 1)}

RushConcertPolicyID {(Tickets, 300),
                     (VIPTickets, 30)}
```

## Comparison with ERC20 tokens

ERC20 is an Ethereum token standard, widely used for the purpose of token
issuance on various platforms today. The peculiarity of this token type lies in
the fact that it can represent value and serve for such purposes as payments,
value transfer, exchange, rewards or incentives, access to services and
products, represent voting rights, etc. Also, these tokens can hold both utility
and security features, which opens a range of possible use cases for businesses,
applications, and enterprises.

On Cardano, users can create native tokens that will serve the above-mentioned
purposes and in addition, it is possible to create _unique_ (non-fungible)
assets representing value like real estate or intellectual rights, for example
(in Ethereum, this functionality requires a separate standard, ERC721).

Unlike ERC20 tokens, the tracking and accounting of native tokens is supported
by the ledger natively (ERC20 tokens require smart contracts to achieve the same
thing). An important benefit of using native tokens is that they do not require
smart contracts to transfer their value and can be transferred alongside other
token types. Also, unlike ERC20, native tokens do not require special transfer
fees or additional event-handling logic to track transactions.

Another advantage of native tokens over ERC20 is security. ERC20 tokens have
proven vulnerable to a wide range of
[security issues](https://peckshield.medium.com/alert-new-batchoverflow-bug-in-multiple-erc20-smart-contracts-cve-2018-10299-511067db6536).
This is conditioned by the fact that ERC20 token creation requires manual
modification of the contract standard, which can result in errors and possible
bugs. Creating and transacting tokens natively removes the possibility of human
error, since the ledger itself handles the token logic. Additionally, over- and
under-flow vulnerabilities that are present for ERC20 are eliminated for native
tokens, as Cardano’s scripting language does not have fixed-size integers and
the ledger itself (rather than the ERC20 user code) tracks token movement.

## Minting policy

**Overview**

A minting policy is the set of rules that govern the minting and burning of
assets scoped under that policy. The point of a minting policy is to specify the
conditions under which tokens are minted (or burned). For example, the rules
might specify who has control over the asset supply through minting and burning.

Minting policies are defined by the users who want to create a new asset. For
example, a user might wish to only allow themselves to mint any more of a
certain kind of token. This would be specified in the policy.

Minting rules can be expressed:

As very basic set of rules that is made up of (ANDs and ORs of):

1.  A specification of what signatures are needed to allow the mint (eg, a
    multisig specification, where no code is needed).
2.  A specification of during what slot the script can be spent from (eg, after
    slot 15 and before slot 20) With a Plutus Core script.

Adherence to minting policies is checked by the node at the time a transaction
is processed, by running the code or checking the relevant signatures.
Transactions must adhere to all the minting policies of all assets that the
transaction is attempting to mint.

## Important points about minting policies and assets

- All assets necessarily have a minting policy. For example, the minting policy
  of ada is 'new ada can never be minted'.
- A token is associated with (eg, scoped under) exactly one minting policy.
- A single policy specifies both minting and burning conditions of tokens scoped
  under it. Adherence to it is checked both at the time of minting as well as
  burning.
- An asset cannot ever change its associated minting policy. This association is
  permanent. In other words, existing tokens cannot be associated with a new
  policy. Users can, however, buy back and burn all existing tokens and mint new
  ones, with a new minting policy. Note that this is a feature, not a bug!
- If an existing asset on the ledger is scoped under a particular policy, it is
  guaranteed that it was originally minted according to that policy.
- Unless tokens of a given policy are being minted in a transaction, the actual
  policy is irrelevant. It is just used as an identifier of the asset.
- Assets associated with different minting policies are never fungible with one
  another. They can be traded in the same way one may use USD to buy CAD: the
  amount of CAD you can buy with a fixed amount of USD depends on the exchange
  rate of the place where you do the trade.

## Association between an asset and its minting policy

The association between an asset and its minting policy is permanent for safety
reasons: this feature protects the users and the system from illegitimately
minted tokens.

If the minting policy of a token changes, it is not really the same token any
more, and its value cannot be compared to that of the original token. This
permanent asset-policy association scheme is integral to defining high-assurance
policies. Loosening this identification opens our MA scheme to various attacks.
Having a permanent association between these allows us to guarantee that every
token was minted in accordance with its minting policy, and not any other policy
which it might have previously been associated with.

Note that this is not as restrictive as it sounds. In a loose parallel with the
US monetary policy, we can say that the policy is 'government and laws set the
policy', and this is a policy which requires looking up the current laws (which
themselves could change), and only minting money in adherence to them.

## Minting policy examples

- Single-issuer policy
- Time-locked mint policy
- One-time mint policy.

Note: There are many other types of minting policies.

**Single-issuer policy**

A single-issuer minting policy specifies that only the entity holding a
particular set of keys is allowed to mint tokens of the particular asset group.
For example, the set of keys specified in the minting policy must have signed
the minting transaction.

An example of an asset group that would use a single-issuer policy would be
tokens representing baseball cards. The company manufacturing legitimate
collectors' cards would publish the keys required by the minting script to mint
new baseball cards. This would mean that no new baseball card tokens can be
minted without the company's signatures. This type of policy can be implemented
without Plutus smart contracts.

**Time-locked minting policy (token-locking)**

This type of policy can be used to specify when tokens can be spent from an
address. In particular,

- only in or after a specified time slot
- only before a specified time slot.

This type of policy is usually not used by itself. Usually, it is in conjunction
with the multi-signature or single issuer policy. For example, this output can
be spent after slot _s_ and only by a transaction signed by key _k_.

This type of policy can be implemented without Plutus smart contracts.

**One-time minting policy**

In a one-time mint policy, the complete set of tokens of a given asset group is
minted by one specific transaction. This means that no more tokens in that
particular asset group will ever be minted. This type of policy needs Plutus
smart contracts to be implemented.

One-type mint policies would be useful for generating concert ticket tokens for
a specific concert, for example. The venue capacity is known ahead of time, so
there'll be no need to ever allow more tickets to be minted.

**Minting transactions**

To introduce new quantities of new tokens on the ledger (minting) or to remove
existing tokens (burning), each transaction features a mint field. The
transactions where the mint field is not empty are known as minting
transactions. The use of this field needs to be tightly controlled to ensure
that the minting and burning of tokens occur according to the token's minting
policy.

Apart from the mint field, minting transactions must also carry the minting
policies for the tokens they are minting, so that these tokens can be checked
during validation.

The outcome of processing a minting transaction is that the ledger will contain
the assets included in the mint field, which is included in the balancing of the
transaction: if the field is positive, then the outputs of the transaction must
contain more assets than the inputs provide; if it is negative then they must
contain fewer.

It is important to highlight that a single transaction might mint tokens
associated with multiple and distinct minting policies. For example,
`(Policy1, SomeTokens)` or `(Policy2, SomeOtherTokens)`. Also, a transaction
might simultaneously mint some tokens and burn others.

## The native token lifecycle

The native token lifecycle consists of five main phases:

1. minting
2. issuing
3. using
4. redeeming
5. burning.

The following diagram outlines the interaction between the system components:

![Multi-asset](https://ucarecdn.com/75b79657-9f94-41b9-9426-7a65245f14ee/multiassetdiagram.png)

Each of these logical phases involves transactions on the Cardano blockchain,
which may incur fees in ada. The main groups of actors are:

- **Asset controllers**, who define the policy for the asset class, and
  _authorise_ token issuers to mint/burn tokens. They may also retain co-signing
  rights for any tokens that are issued/burnt.
- **Token issuers**, who mint new tokens, maintain the reserve of tokens in
  circulation, issue them to token holders, and burn tokens when they are no
  longer of use.
- **Token holders**, who hold tokens, send them to other users, use them for
  payment, and who may redeem them with the issuers when they have finished
  using them. Token users may include normal users, exchanges etc.

The lifecycle of multi-asset tokens starts with their creation – **_minting_**,
which refers to the process whereby new tokens are created by one or more _token
issuers_ in accordance with the _monetary policy script_ that the _asset
controller_ has defined. New tokens will usually be created to fulfill specific
purposes. For example, _fungible_ or _non-fungible_ (unique) tokens may be
created to be used for specific payment, purchasing, or exchange needs. When a
new token is minted, the total _token supply_ for that token increases, but
there is no impact on the _ada supply_. Minting coins and transferring them to
new addresses may require an ada deposit to be paid, which may be proportional
to the number of different tokens that are held, for example.

Token holders will hold tokens in their wallets, may pass them on to other
users, exchange them for items of value (including non-native tokens), etc. in
exactly the same way that they can use ada. When a user has finished using the
token, they may choose to **_redeem_** them. This means that tokens are returned
to an issuer (perhaps in return for a product, service, or some other currency,
for instance). Once redeemed, tokens could then be re-issued to other users as
needed. Token holders will need to maintain some ada in their wallets to pay for
transaction fees.

When tokens become redundant, they can be **_burned_**, if desired, in
accordance with the underlying monetary policy script. The process of burning
destroys these tokens (removes them from circulation), and the total token
supply decreases. Any deposits will be returned at this point. The burning
process is identical for fungible and non-fungible tokens.

:::note

Note: The multi-asset token lifecycle potentially allows tokens to be obtained
and reissued by other parties - token holders who act as _reissuers_ for the
token. This can be done to eg, enable trading in multiple asset classes,
maintain liquidity in one or more tokens (by acting as a broker), or to
eliminate the effort/cost of token minting, issuing or metadata server
maintenance. Thus, both reissuers and issuers can gain from such a deal -
eliminating cost and effort, maintaining separation and integrity, and injecting
value into the asset class.

:::

---
title: Welcome
metaTitle: Developer resources
sidebar_position: 1
collapsible: true
collapsed: true
---

Welcome to the developer resources section! Here you will find references to the
resources that will help you with developing on Cardano.

## Getting started

- To get started, head over to the
  [Developer portal](https://developers.cardano.org/). There, you will find
  guides and tutorials on how to
  [install](https://developers.cardano.org/docs/operate-a-stake-pool/node-operations/installing-cardano-node)
  and [run](https://developers.cardano.org/docs/operate-a-stake-pool/node-operations/running-cardano) the
  Cardano node and
  [create simple transactions](https://developers.cardano.org/docs/get-started/create-simple-transaction).
- Alternatively, explore the
  [Cardano course](https://cardano-course.gitbook.io/cardano-course) material.

## Interfacing with Cardano

There are various ways to interact with the Cardano node:

- Via the official command-line interface (CLI) –
  [cardano-cli](https://github.com/IntersectMBO/cardano-cli)
- Via a proxy interface like [Ogmios](https://ogmios.dev/) (JSON/WebSocket) or
  the
  [submit-api](https://github.com/IntersectMBO/cardano-node/tree/master/cardano-submit-api)
  (CBOR/HTTP)
- Via an SDK/library such as
  [cardano-api](https://github.com/IntersectMBO/cardano-api) (Haskell),
  [Pallas](https://github.com/txpipe/pallas) (Rust),
  [Yaci](https://github.com/bloxbean/yaci) (Java), or
  [ouroboros-network-js](https://github.com/StricaHQ/ouroboros-network-js)
  (JavaScript).

You can also rely on third-party services and managed API query layers such as
[Blockfrost](https://blockfrost.io/), [Koios,](https://www.koios.rest/) or
[Maestro](https://www.gomaestro.org/). 

For more tools, refer to the most commonly used
[builder tools](https://developers.cardano.org/tools) on the developer portal or
see this
[list of community-built developer tools](https://www.essentialcardano.io/article/a-list-of-community-built-developer-tools-on-cardano).

:::note

Please note that this information is intended for informational purposes only
and **does not** constitute an endorsement or recommendation of any specific
tool or service.

:::

## Native tokens

To start working with native tokens, see:

- [Developer portal native token tutorials](https://developers.cardano.org/docs/native-tokens/)
- [Ledger explanations about native tokens](https://cardano-ledger.readthedocs.io/en/latest/explanations/index.html).

You can also explore Cardano assets using a variety of [explorer tools](/about-cardano/new-to-cardano/cardano-tracking-tools#exploring-assets).


## Smart contracts

To start working with smart contracts, see:

- [Developer portal smart contract tutorials](https://developers.cardano.org/docs/smart-contracts/)
- [Plutus Core documentation](https://plutus.readthedocs.io/en/latest/index.html).

There are also many languages you can use to develop smart contracts:

- [Aiken](https://github.com/aiken-lang/aiken) (DSL)
- [Plutarch](https://github.com/Plutonomicon/plutarch-plutus) (Haskell-based eDSL)
- [PlutusTx](https://github.com/IntersectMBO/plutus/tree/master/plutus-tx) (Haskell)
- [Plu-ts](https://github.com/HarmonicLabs/plu-ts) (TypeScript)
- [OpShin](https://github.com/OpShin/opshin) (Python)
- [Scalus](https://github.com/nau/scalus) (Scala)
- [Marlowe](https://github.com/input-output-hk/marlowe-cardano) (DSL).  

## Tools

Explore Cardano's ecosystem:

- [A list of community-built developer tools on Cardano](https://www.essentialcardano.io/article/a-list-of-community-built-developer-tools-on-cardano)
- [Builder tools](https://developers.cardano.org/tools/)
- [Cardano Cube](https://www.cardanocube.com/cardano-ecosystem-interactive-map)
- [CTimelines](https://www.ctimelines.io)
- [Built on Cardano](https://builtoncardano.com/).

## Going further

- If you’re beginning your developer journey with Cardano, check out
  the [Cardano course](https://cardano-course.gitbook.io/cardano-course/)
  learning material. It includes simple tutorials on how to get started.
- And if you want to know more about blockchain, distributed ledgers, and
  Cardano, check out the
  [Cardano Academy video course](https://learn.academy.cardanofoundation.org/landing)!

Keep navigating this section to learn more about native tokens, smart contracts,
and scalability solutions on Cardano. You will also find release notes, links
to the ecosystem builder tools, and weekly development updates.

---
title: Community education initiatives
metaTitle: Community education
sidebar_position: 1
---

This section aims to highlight the diverse educational efforts led by the Cardano community.

This space is dedicated to showcasing resources, programs, and projects that support learning and skill development across the ecosystem. Whether it’s tutorials, workshops, or other community-driven content, this section aims to reflect the collective knowledge and commitment to growing Cardano’s global reach.

:::tip

Contributions are encouraged! Share your educational resources to expand this library and help others explore and understand Cardano. Please check the [contribution guidelines](/about-cardano/contributions/) and submit a pull request.

:::

## Community education resources

-   [Cardano Academy](https://academy.cardanofoundation.org/) – learn about blockchain – empowering the digital architects of the future
-   [IO Academy YouTube channel](https://www.youtube.com/channel/UCX9j__vYOJu00iqBrCzecVw) – explore learning resources and check out [this section](/pioneer-programs/education/) for more details
-   [Gimbalabs](https://www.gimbalabs.com/) – a space for people to learn about and participate in the Cardano blockchain
-   [NMKR Docs](https://docs.nmkr.io/) – helping users find their way around NMKR to mint non-fungible tokens (NFTs) and fungible tokens on Cardano
-   [CTimelines](https://ctimelines.io/) – [Cardano Timeline](https://ctimelines.io/cardano-timeline) and [Research Timeline](https://ctimelines.io/research-timeline) infographics documenting the ecosystem’s history in a visual, accessible format for learners and the wider community

---
title: Input | Output education
metaTitle: Education overview
sidebar_position: 2
---

## IO initiatives

Education is a cornerstone of fostering a knowledgeable and engaged global community. By prioritizing education, IO aims to share its expertise and empower individuals to contribute meaningfully to the Cardano ecosystem. Whether you're a developer, an academic, or a business professional, IO educational offerings are designed to equip you with the skills and knowledge needed to thrive in the blockchain space.

The team brings together extensive experience in curriculum design, project management, and the technical intricacies of Cardano, Haskell, and smart contracts. This collective skill set ensures that educational programs are comprehensive, practical, and tailored to diverse learners' needs.

## Collaborations

The IO education team has collaborations with various universities and educational institutions worldwide to deliver high-quality education, including:

-   [University of Edinburgh](https://www.ed.ac.uk/), where there is a blockchain laboratory run by IO’s chief scientist, [Prof Aggelos Kiayias](https://iohk.io/en/leadership/aggelos-kiayias), and his research team
-   [University of Athens](https://en.uoa.gr/)
-   [University of West Indies](https://www.uwi.edu/)
-   [University of Wyoming](https://www.uwyo.edu/index.html)
-   [Carnegie Mellon University](https://www.cmu.edu/)
-   [European Business University of Luxembourg](https://ebulux.lu/)
-   [University of Malta](https://www.um.edu.mt/)
-   [University of Cantabria](https://web.unican.es/en/Pages/default.aspx)

Additionally, the team has worked on curriculum design for blockchain programs with Yeovil College in the UK and Consilium Academy in South Africa.

## Course offerings

### Mission-based education

-   **Cardano Days**: interactive events providing a deep dive into the Cardano platform, covering its unique features and applications.
-   **Mastering Cardano**: comprehensive book designed to provide an in-depth understanding of the Cardano blockchain. This work is in progress, so stay tuned for updates.

The education team has held several Cardano Day events across different locations:

-   [ITESO University](https://www.iteso.mx/en/), Guadalajara, Mexico
-   [University of Celaya,](https://www.udec.edu.mx/en/) Guanajuato, Mexico
-   [University of Malta](https://www.um.edu.mt/), Valletta Campus, Valletta, Malta
-   [University of Wyoming](https://www.uwyo.edu/index.html), Laramie, USA.
-   [University of Cantabria](https://web.unican.es/en/Pages/default.aspx), Santander, Spain

    
These events cover the basics of blockchain technology, Cardano, and smart contracts and prove very popular, with a typical NPS score of 92.

More events are continually planned, and if you would like to know more about hosting a Cardano Days event, please [get in touch](mailto:education@iohk.io).

### Developer education

-   [Cardano Developer course](https://github.com/iog-edu-argentina-2024/cardano-dev-2024): a program to train developers in building smart contracts and decentralized applications (DApps) on the Cardano platform.
-   [Plutus Pioneer program](https://docs.cardano.org/pioneer-programs/plutus-pioneers/): focuses on Plutus, Cardano’s smart contract platform, offering hands-on experience in writing and deploying smart contracts.
-   [DRep Pioneer program](https://sancho.network/drep-pioneer-program/drep-pp/overview): prepares participants to become delegate representatives, playing a crucial role in Cardano’s governance.
-   [Haskell Bootcamp](https://github.com/input-output-hk/haskell-course?tab=readme-ov-file#haskell-course): an immersive self-paced Haskell course. This course provides a stepping stone for people to upskill on Haskell before enrolling in the Plutus Pioneer program. It consists of videos and interactive lessons and has received positive feedback and engagement.
-   **Hackathons**: support for hackathons in universities and at various blockchain events. Previous hackathons have been held in Mongolia, the USA, and Argentina.


### Internal training

-   **Plutus internal training**: based on the Plutus Pioneer program, this course offers insights into the core smart contract functionality.
-   **IO Maths Academy**: basic category theory course, which teaches how to build stronger code with mathematical precision to gain knowledge about the functional approach to mathematics. The advanced course is planned for release in 2025.
    
### Academic and business collaborations

IO also engages in academic and business collaborations, offering specialized lectures and workshops on blockchain and Cardano. These programs bridge the gap between theoretical knowledge and practical application, fostering innovation and real-world problem-solving skills.

Some example initiatives include:

-   Smart contract lectures (virtual and in-person) at the [University of Malta](https://www.um.edu.mt/)
-   Cardano Developer course with the [National Technological University](https://utn.edu.ar/es/) in Buenos Aires.
    
### Resources

For more updates, please subscribe to the [IO Academy YouTube channel,](https://www.youtube.com/channel/UCX9j__vYOJu00iqBrCzecVw) where the team regularly publishes new lectures and educational content.

---
title: Plutus Pioneer program
metaTitle: Plutus Pioneer program
sidebar_position: 3
---

:::info

Before starting, note that Plutus Core, Cardano’s native smart contract language, is written in Haskell. Since Plutus contracts are Haskell programs, a basic understanding of Haskell is essential for the Plutus Pioneer program.

Therefore, it is recommended that users complete this self-paced [Haskell Bootcamp](https://github.com/input-output-hk/haskell-course?tab=readme-ov-file#haskell-course) course, which includes unique code examples, engaging exercises, and prepared solutions to learn Haskell fundamentals.

:::

## What is the Plutus Pioneer program?

It is a program to recruit and train developers in Plutus for the Cardano
ecosystem. When you join this program, you will become part of a group with
access to a set of lectures that teach you the core principles of how to code in
both Haskell and Plutus. It is highly interactive, with weekly videos,
exercises, and Q&A sessions, along with exclusive access to the creators and key
experts in the language. You will also be able to join a dedicated community
channel, created to help pioneers connect with each other as you learn.

## What prior experience do I need?

This course is not for coding beginners. While you do not need to be an expert
in formal methods, programming experience and a general aptitude for logical and
mathematical thinking are highly desirable. Some prior knowledge of Haskell or
functional programming is also recommended, as Plutus is heavily based on
Haskell and includes advanced features like Template Haskell, type-level
programming, and effect systems. We recommend that you work through **IO's**
official [Haskell Course](https://github.com/input-output-hk/haskell-course).
You can also read the [Learn You a Haskell](http://learnyouahaskell.com/) book.

## What can I expect to learn?

This course involves approximately ten hours a week of your time and effort. It
covers the building blocks of Haskell and Plutus, including:

- Functions and data types
- Type classes
- Monads
- Template Haskell
- Extended UTXO model
- Working with Plutus (on and off the chain)
- Minting policies
- Some case studies and practical exercises.

As with all learning experiences, the more you put in, the more you will get out!

## When can I join?

We have recently completed a very successful fourth cohort of this program and
will be launching another course later this year. Please register your interest
today using the form below to find out more details.

## How can I register for the Plutus Pioneer program?

If you are interested in joining a future cohort of this program, please
complete the registration form below and we will be in touch soon.

## Will I be certified?

Yes, we are working on certification for those pioneers who successfully
complete the entire program. Certificates will be represented as non-fungible
tokens (NFTs) (on the testnet) and locked by a Plutus contract. Pioneers can
demonstrate their qualification by constructing an appropriate transaction to
unlock their individual token.

## Yes, I'm interested!

**[REGISTER NOW](https://services.iohk.io/plutus-pioneers-registration)**

Please share your details and we will be in touch.

---
title: Operating a stake pool
metaTitle: Operating a stake pool
sidebar_position: 1
---

Welcome to the stake pool operations section!

Here, you will find explanations about stake pools, operators, and owners, along
with core concepts such as node connectivity, keys, operational certificates,
metadata management, performance, and ranking.

:::info

To get started with stake pool operations, refer to:

- [Developer portal SPO tutorials](https://developers.cardano.org/docs/operate-a-stake-pool/)
- [Cardano course SPO tutorials](https://cardano-course.gitbook.io/cardano-course/handbook/create-a-stake-pool)
- [The Guild Operators' tutorials](https://cardano-community.github.io/guild-operators/).

:::

## Useful resources

|                        |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| ---------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Node details           | [Release notes](/developer-resources/release-notes/release-notes), [compatibility matrix](/developer-resources/release-notes/comp-matrix), [Cardano node repository](https://github.com/input-output-hk/cardano-node#overview-of-the-cardano-node-repository), [Cardano node video course (includes SPO explainers).](/developer-resources/cardano-node-course/)                                                                                                                                                                                                                                 |
| Testnets               | [Cardano testnet environments](/cardano-testnets/environments), [testnets faucet](/cardano-testnets/tools/faucet/).                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| Tools                  | [Cardano ecosystem tools](https://developers.cardano.org/tools/), [community-built developer tools](https://www.essentialcardano.io/article/a-list-of-community-built-developer-tools-on-cardano).                                                                                                                                                                                                                                                                                                                                                                                               |
| Explorers              | [AdaStat](https://adastat.net/transactions), [Adatools](https://adatools.io/), [Cardanoscan](https://cardanoscan.io/transactions), [Cexplorer](https://cexplorer.io/), [Cardano Assets](https://cardanoassets.com/), [Pool.pm](https://pool.pm/tokens), [ADAtainment](https://www.adatainment.com/index.php?page=home&lang=en), [Cardano PoolTool](https://pooltool.io/).                                                                                                                                                                                                                        |
| SPO alliances          | [Guild Operators](https://cardano-community.github.io/guild-operators/): a group of well-known and experienced community members that provides information about guild tools that simplify various stake operations, [Cardano Single Pool Alliance (SPA)](https://singlepoolalliance.net/): a group of separate SPOs who have vowed to run a single stake pool for the sole purpose of providing the Cardano ecosystem with true decentralization, [xSPO Alliance](https://www.xspo-alliance.org/): a group of small SPOs that joined with a total live stake of less than one million ada each. |
| Communication channels | [SPO Telegram announcements channel](https://t.me/SPOannouncements), [IOG Technical Community Discord](https://discord.com/invite/5kaErDKDRq), [support](https://iohk.zendesk.com/hc/en-us).                                                                                                                                                                                                                                                                                                                                                                                                     |

---
title: About stake pools, operators, and owners
metaTitle: About stake pools, operators, and owners
---

A [stake pool](/about-cardano/learn/stake-pools) is a reliable server node that
represents the combined stake of various stakeholders in a single entity. Stake
pools use the [Cardano node](https://docs.cardano.org/learn/cardano-node/) to
form the backbone of the Cardano network by processing transactions and
validating and producing blocks.

Stake pool roles include a stake pool operator and one or more stake pool
owner(s). It is important to note the conceptual differences between the two:

- A _stake pool operator_ is the owner of the cold and VRF signing keys used to
  create the stake pool. The operator configures and maintains stake pool
  infrastructure and is responsible for monitoring it. The operator can
  register, re-register, or retire a stake pool, update certificates, and ensure
  that the block-producing node can sign and validate blocks and process
  transactions.
- A _stake pool owner_ is determined by the stake signing keys used in stake
  pool creation. The owner's
  [pledge stake](/about-cardano/learn/pledging-rewards) is intended to provide
  protection against Sybil attacks.

Usually, the stake pool operator and the owner are the same person. However, a
stake pool can have multiple owners that choose to combine their pledge. It
should be noted that the stake pool's declared pledge must be met by the
combined stake in the owner's wallets and failure to do so will result in the
pool being unable to produce blocks.

All operator and owner rewards are paid out into a single rewards account
associated with the reward address of the pool. If a stake pool has multiple
owners, it is up to the owner of the rewards account to distribute rewards to
the operator and other owners.

An agreement is likely needed to define when and how the accumulated rewards in
a shared account should be split. For example, they can agree to have the
operator control the shared account, or they can choose to use a multisig
account.

To run a pool effectively, a bi-directional relationship and trust are crucial.
If this trust is broken, other parties can lose in regards to accumulated or
potential rewards, or reputation for the operator.

:::info

Developer resources:

- [Stake pool operations overview](https://developers.cardano.org/docs/operate-a-stake-pool/)
- [Cardano course: creating a stake pool](https://cardano-course.gitbook.io/cardano-course/handbook/create-a-stake-pool)

:::

---
title: Establishing connectivity between the nodes
metaTitle: Establishing connectivity between the nodes
---

Nodes connect to each other within the Cardano networking layer, and this
connection is essential for information exchange about transactions and new
block creation.

## Cardano networking

Cardano's networking evolved from its initial federated structure to hybrid and
the Dynamic peer-to-peer (P2P) model. Networking will keep evolving with future
additions such as Ouroboros Genesis and peer-sharing. See
[this explainer](/about-cardano/explore-more/cardano-network/p2p-networking) for
more details about network evolution and P2P.

In a previous hybrid setup, SPOs could configure a node to connect to, for
example, 50 other SPO nodes. This is a pretty large number, since many nodes can
be offline, change their addresses, etc. While 20 connections are enough for
efficient communication, SPOs had to over-provision because of the static
configuration.

In a Dynamic P2P setup, SPOs can target to configure 20 connections, which can
be picked from thousands of SPO relays, not just 50. And if any are offline, or
go offline, the setup will automatically pick new ones, to meet the set target.
This means that the configuration is no longer limited to a static pool of 50
peers.

## Connecting core and relay nodes

As a stake pool operator, you will have two types of nodes: core nodes and relay
nodes. Each core node must be accompanied by one or more relay nodes.

The difference between the two types is that core nodes are responsible for
producing blocks, while relays are responsible for communicating with other
relays in the network and broadcasting blocks. This difference determines how
they are configured and how they are connected to the network.

A core node is configured with various key pairs and an operational certificate
required for block generation. It only connects to its relay nodes.

:::info

Developer resources:

- [Stake pool networking](https://developers.cardano.org/docs/operate-a-stake-pool/stake-pool-networking)
- [Cardano relay configuration](https://developers.cardano.org/docs/operate-a-stake-pool/relay-node-configuration)
- [Configurations and setup](https://cardano-course.gitbook.io/cardano-course/handbook/create-a-stake-pool/the-setup).

:::

---
title: Logging and monitoring
metaTitle: Logging and monitoring
---

Regular monitoring of active nodes is crucial to determine if the node is running optimally, if the system is performant, or if fine-tuning is required.

:::info

Developer resources:

- [Cardano course: monitoring the nodes](https://cardano-course.gitbook.io/cardano-course/handbook/monitoring-our-nodes)
- [Monitoring with gLiveView](https://developers.cardano.org/docs/operate-a-stake-pool/monitoring-gLiveView)
- [Grafana dashboard tutorial](https://developers.cardano.org/docs/operate-a-stake-pool/grafana-dashboard-tutorial)

:::

---
title: Logging and monitoring
metaTitle: Logging and monitoring
---

Regular monitoring of active nodes is crucial to determine if the node is running optimally, if the system is performant, or if fine-tuning is required.

:::info

Developer resources:

- [Cardano course: monitoring the nodes](https://cardano-course.gitbook.io/cardano-course/handbook/monitoring-our-nodes)
- [Monitoring with gLiveView](https://developers.cardano.org/docs/operate-a-stake-pool/monitoring-gLiveView)
- [Grafana dashboard tutorial](https://developers.cardano.org/docs/operate-a-stake-pool/grafana-dashboard-tutorial)

:::

---
title: Creating keys and operational certificates
metaTitle: Creating keys and operational certificates
---

## About the stake pool operator keys

It is the responsibility of the operator to manage both the hot (online), and
cold (offline) keys for the pool. Cold keys must be secure and should not reside
on a device that has internet connectivity. It is recommended that you have
multiple backups of your cold keys.

The keys that you need as a stake pool operator are:

- stake pool cold key
- stake pool hot key (KES key)
- stake pool VRF key.

The KES key, or hot key as mentioned above, is a node operational key that
authenticates who you are. You specify the validity of the KES key using the
start time and key period parameters and this KES key needs to be updated every
90 days. The VRF key is a signing verification key and is stored within the
operational certificate. 

:::info

Developer resources:

- [Generating wallet keys](https://developers.cardano.org/docs/operate-a-stake-pool/generating-wallet-keys)
- [Generating block-producing keys](https://developers.cardano.org/docs/operate-a-stake-pool/block-producer-keys)
- [Cardano course: generating keys](https://cardano-course.gitbook.io/cardano-course/handbook/create-a-stake-pool/generate-keys).

:::


## Creating an operational certificate and registering a stake pool

Stake pool operators must provide an operational certificate to verify that the
pool has the authority to run. The certificate includes the operator’s signature
and important information about the pool (addresses, keys, etc.)

Operational certificates represent the link between the operator’s offline key
and their operational key. A certificate’s job is to check whether or not an
operational key is valid, to prevent malicious interference. The certificate
identifies the current operational key, and is signed by the offline key.

Certificates are generated with an issue counter number and included in the
header of each block the node generates. 

Certificates include a kes-period (start date), which indicates the period
within which the certificate is valid before you need to create another one.

Certificates are generated on the offline machine using the offline/cold keys,
before being copied over to the node to validate the KES keys used to sign the
blocks.

:::info

Developer resources:

- [Registering a stake address](https://developers.cardano.org/docs/operate-a-stake-pool/register-stake-address)
- [Registering a pool](https://developers.cardano.org/docs/operate-a-stake-pool/register-stake-pool)
- [Cardano course: registering a stake pool](https://cardano-course.gitbook.io/cardano-course/handbook/create-a-stake-pool/register-stake-pool).

:::

---
title: Stake pool operations and maintenance
metaTitle: Stake pool operations and maintenance
---

After a successful stake pool registration and its further operation, SPOs might
be interested in obtaining certain information about their pools’ activity.
Information about those pools that are operating on mainnet can be fetched using
different explorer tools. See
[introduction](/stake-pool-operators/01-operating-a-stake-pool.mdx) for useful
resources.

In certain cases, exchanges and stake pool operators, in particular, are looking
for in-depth information about their mainnet _and_ testnet pools. To access data
that is stored on the Cardano blockchain, it is recommended to use
[cardano-node](https://github.com/input-output-hk/cardano-node),
[cardano-db-sync](https://github.com/input-output-hk/cardano-db-sync), or
[cardano-graphql](https://github.com/input-output-hk/cardano-graphql).

Each of the above integration components can be installed and easily deployed
using Docker.

:::info

Developer resources:

- [Cardano course: operations and maintenance](https://cardano-course.gitbook.io/cardano-course/handbook/create-a-stake-pool/pool-maintainance)
- [Developer portal: operator tools](https://developers.cardano.org/docs/operate-a-stake-pool/guild-ops-suite)

:::

---
title: Pool metadata management
metaTitle: Pool metadata management
---

Stake pools are registered on-chain, and their on-chain data (such as information required to calculate rewards) is critical to the operation of the ledger. Stake pools also possess metadata that helps users make a rational choice of a stake pool to delegate to. This metadata is stored off-chain as it might reflect sensitive content, and such an approach allows for a degree of decentralized censorship.

On the other hand, off-chain metadata storage prerequisites a challenge of open access by different users. On-chain stake pool registrations contain a URL pointer to the off-chain metadata and a content hash that can be fetched from a specific stake pool. This might cause both performance and privacy issues. Another crucial aspect to address is the stake pool’s 'ticker' name, which is the short name a stake pool is recognized by. Ticker names might reflect prominent brands or trademarks which should not be duplicated as this leads to confusion. Stake pool operators (SPOs) running multiple pools might want to use the same metadata for all their pools and then, this might also lead to these pools appearing with the same ticker name.

## Use cases

A stake pool metadata aggregation server (SMASH) is introduced to address metadata performance and privacy issues. Delegators, SPOs, exchanges, or wallets can deploy and use SMASH to ensure a higher level of metadata accountability and maintenance. SMASH aggregates metadata from existing stake pools and provides an efficient way to fetch it and store it in a semi-centralized environment. SMASH operators (exchanges, wallets, SPOs) are then enabled to validate and manage this metadata curating it for censorship via the delisting feature.

## Run your own

The first generation of the SMASH server has been deployed by Input Output Global (IOG). It aggregated stake pool metadata and offered a list of valid stake pools with reviewed content and ticker names. Nowadays, various solutions exist to synchronize data from the chain and a SMASH server is provided out-of-the-box as part of the cardano-db-sync project.

For running your own, see [these Cardano DB Sync instructions](https://github.com/IntersectMBO/cardano-db-sync/blob/master/doc/smash.md#smash-characteristics).

---
title: Stake pool performance
metaTitle: Stake pool performance
---

A stake pool elected as a slot leader is responsible for producing new blocks
for the Cardano network. If the stake pool does not produce a block, the slot
will remain empty and the blockchain will _not_ be extended. The Cardano
blockchain can tolerate a number of missing blocks, but the majority of blocks
expected to be produced (at least 50% + 1) must be generated during an epoch.

Although occasionally missed blocks don’t affect the extension of the blockchain
as a whole, an unresponsive elected stake pool decreases the overall performance
of the network, which is undesirable.

Stake pool performance is calculated according to its expected activity as the
ratio of a number of blocks a stake pool produces in a given epoch versus the
number it was capable to produce. For instance, if the stake pool could produce
100 blocks (based on its stake and the chance to be elected) in an epoch, but it
actually produced just 50 blocks, its performance would be 50%.

Poor stake pool performance decreases the number of rewards that a pool and its
members receive, thereby decreasing its attractiveness to delegators. To improve
its performance, a stake pool should have good network connectivity, be run on a
reliable system, and actively participate in block production and verification.

The higher the pool’s performance, the more attractive it will be to delegators
since it will offer higher rewards and incentives.

---
title: Stake pool ranking
metaTitle: Stake pool ranking
---

In the Daedalus wallet and Cardano Explorer, stake pools are ranked based on the
amount of rewards that users will receive if they choose to delegate to those
pools. Ranking shows pool saturation level, simplifying the pool choice. From a
delegator's perspective, once a pool reaches a certain saturation point, it will
no longer be beneficial to delegate to it. The most attractive stake pools are
displayed first and ordered from the top down.

The ranking system is designed to provide users with the ability to choose the
most suitable stake pool for a higher return on investment (ROI), so reliable
stake pool operators can maintain and support the system and maximize
decentralization.

### Ranking parameters

A pool's ranking is based on several parameters: cost and margin, the pool's
performance, and the amount of stake that the pool has already accumulated.
These factors promote reliable stake pools that are not yet saturated and
provide low cost and margin.

---
title: Guidelines for operating large stake pools
metaTitle: Guidelines for operating large stake pools
---

This section provides guidelines for operators of large stake pools,
specifically how to manage the risks and complexity of maintaining significant
stake or multiple pools. Operators of smaller stake pools may also find much of
this advice useful to them.

### Main recommendations

1. **Consider reliability and robustness**. Stake pools require high-reliability
   servers with:
   - Resilient compute capability.
   - Robust networking capability.
2. **Consider networking requirements** carefully:
   - High bandwidth connections are essential to run a relay node (e.g., 5
     Mbit/s + 0.1 Mbit/s per downstream peer as a capacity planning number).
3. **Operate sufficient relay nodes** as well as stake pools:
   - Relay nodes are necessary for the maintenance of the network.
   - Provision two relay nodes for each active stake pool.
4. **Be aware of system and network performance**, especially if using
   virtualized or shared servers:
   - Each stake pool may need its own dedicated hardware resources (compute,
     memory, and network).
   - It is recommended to use different servers for stake pools and relay nodes.
   - Virtualization may complicate the process of determining whether stake
     pools have adequate resources.
5. **Do not replicate stake pools** on the network:
   - Stake pool replication is adversarial behavior, which may lead to blocks
     being rejected.
   - Use high-reliability techniques to switch between duplicate or back-up
     servers without exposing both simultaneously to the network.
6. **Stake pools should have limited network connections**:
   - This reduces stake pool exposure to denial-of-service (DOS) attacks and
     improves performance.
   - Relay nodes should handle the larger portion of network traffic.
   - Stake pools should only be connected to trusted nodes (relay and/or stake
     generating).
   - Stake pools and relay nodes should restrict the services that are exposed
     (e.g., using a firewall).
7. **Security is paramount**:
   - Use _airgapping_ when signing transactions – do not store cold keys on an
     online server (including the one that is running a stake pool or a relay
     node).
   - Use hardware wallets for high-value private keys, where possible.
   - Where private keys cannot be stored in hardware wallets (e.g., cold keys),
     store them offline.
   - Be aware of and plan for enforced key rotations using the key evolving
     signature (KES) scheme.

### Managing risks and complexity

Operating stake pools effectively is crucial to ensure the long-term health and
liveliness of decentralization on Cardano. When a stake pool operator (SPO)
operates multiple stake pools (or has a single pool that directly controls a
significant percentage of the total staked ada), they may have a significant
effect on the overall system throughput as a consequence of the “proof-of-stake”
principle.

The Cardano design and security analysis assume that each stake pool operates in
a broadly independent and mutually supportive (non-adversarial) manner. This
means that large SPOs have a particular responsibility to ensure that their
operation supports the needs of the network as a whole. For this, it is
essential to evaluate and address common risks that may be experienced in stake
pool operations.

**Virtualization and containerization risks**

Because stake pools and relay nodes have specific real-time requirements, it is
generally not recommended to run Cardano nodes on virtualized resources without
undertaking careful performance, reliability, and security analyses. The mapping
of Cardano nodes to the underlying physical infrastructure must consider timing
issues for block production and propagation.

**Security and common-mode failures**

The risks of operating a stake pool using container-based virtualization include
increased chances of common-mode system failures, resource contention, and
increased exposure to security risks (including DOS attacks and loss of private
keys). The use of containerized environments, where relay nodes share the same
physical infrastructure as stake pool nodes, may impact real-time requirements
on block production and networking infrastructure, thus reducing stake pool
income. Moreover, concentrating network connections (as may happen with
virtualized or containerized services) increases the chances of a DOS attack, as
well as reduces network redundancy in non-obvious ways. In a virtualized or
shared environment, a single NIC/cable failure or DOS attack, for instance,
might then affect multiple stake pools or relay nodes, including those that
might be run by different SPOs.

**Shared resources**

The diffusion of a newly minted block causes a “pulse” of activity to occur.
Stake pools and relay nodes that share physical computing resources will be
forced to compete for shared CPU, memory, storage, networking, and other
resources. The demand for these resources is not smooth, it is correlated by the
block diffusion. This can negatively affect block production or result in
blockchain synchronization failures. Under some conditions, performance can
degrade in a non-linear way (e.g., adding a second node may reduce performance
by more than 50%). To eliminate such risks, it is important to analyze
performance requirements not only for the typical load but also for limit cases.

**Maintenance and upgrade**

System maintenance and upgrades should be always taken into consideration.
Although migration of a virtualized instance to new hardware, or duplication of
a running instance might seem easy, this commonly results in some timing
disruption. This is more significant in a real-time setting (such as Cardano)
than for typical data processing applications, which often have high levels of
replication and redundancy. System upgrades may also have unexpected effects on
the virtualized system’s performance, or occasionally, functionality. SPOs,
therefore, need to be aware of underlying system maintenance and take
appropriate action to avoid losing blocks (and income).

**Geographical and physical concentration**

Virtualized systems are often concentrated into a few large data centers. This
creates potential points of common network failures, including reliance on
specific portions of national infrastructures (internet backbones, power grids,
etc). Large SPOs should aim to disperse their operations across multiple
regions, and very large operators should aim for a global presence.

### Provisioning

To ensure overall network resilience and robustness, SPOs that operate large
stake pools must take special care of:

**Stake pool configuration**

1. Network configurations should ensure sufficient connectivity to other stake
   pools (including those that are run by other operators).
2. The effects of virtualization should be considered carefully: given the
   pulsed-nature of block diffusion, it is easy to overload physical compute,
   memory, and networking resources. Unlike a typical database or web service
   operation, Cardano stake pools have real-time compute and networking
   requirements. Failure to observe these requirements may have an impact on
   stake pool returns. It is generally not recommended to share server resources
   between multiple stake pools or relay nodes.
3. Stake pools should always be supported by sufficient relay nodes (two or
   more). This reduces the risk of a single relay node failure, which can
   potentially isolate a block-producing node. Failure of a block-producing node
   should only affect block production for that single stake pool. It is
   particularly important that large SPOs take on this responsibility since they
   are receiving a higher proportion of the operating rewards for the network.
4. To support the relay capacity scaling, it is desirable to use DNS names that
   map to multiple IP addresses and/or use multiple on-chain DnsName entries.
   Ideally, these entries should be supported over disjointed network
   infrastructure (e.g., different ISPs or physical paths).

**Security preferences**

1. Cold keys should never be stored on active servers (especially cloud
   servers).
2. Payment keys should be kept separate from block signing keys. Airgaps should
   be maintained between stake pools and systems used for transaction signing.
3. Hardware wallets or other secure means should be used to protect cold keys.
   One hardware wallet may be needed per set of keys (e.g., per stake pool), and
   these may need to be physically secured.
4. Back-up keys should be also securely maintained for each stake pool.

**General advice**:

1. Consider hardware performance, including memory, storage, and networking
   capabilities.
2. Perform failure-mode-effects analysis to ensure that single failures of a
   delivery component are suitably constrained.
3. Carefully consider containerization and virtualization performance to
   eliminate excessive contention for common resources (e.g., CPU cores).
4. Submit stake pool certificates that include both the IP address and DNS
   names.
5. Ensure that suitable monitoring is in place:
   - Receiving the vast majority of (>90%) blocks within 4,000ms of their
     associated slot time.
   - Track block production times to ensure that allocated resources remain
     sufficient (this will increase as transaction rates and block sizes
     increase).
6. Plan the expansion of relay nodes (potentially at a different location) that
   are associated with your stake registration (DDoS mitigation / rapid
   increases in load due to increase of delegation to SPO’s stake pools, etc.).
7. Plan for regular maintenance events (to minimize stake pool downtime and/or
   to perform maintenance when the leadership schedule indicates a good
   interval).
8. Plan and exercise disaster recovery every 3 to 6 months.
9. When operating multiple stake pools, spread out your block producer nodes
   across multiple continents to limit block production disruption and eliminate
   large-scale network outages.

#### Example system and relay topology configurations

There is no standard system configuration as every stake pool has different
operational requirements and preferences. It is the choice of an SPO on how to
configure the topology.

However, taking into account the earlier advice, it is recommended that an SPO
maintains at least two and an additional relay node(s) per a stake pool. In the
case of running multiple stake pools, it is best that SPOs use geographically
diverse peers, spread relay nodes across the world, and reach out to other SPOs
(particularly other large ones) making agreements to add each other's relay
nodes. The more SPOs they have peer-sharing agreements with, the more likely
their blocks will propagate and get included in the chain.

Monitoring is important for all SPOs, and it is essentially a responsibility of
an operator to ensure the quality of their pools’ functionality. As an example
of a monitoring process that reflects Prometheus rules alerting on thresholds,
one can take a look at the
[cardano-ops repository here](https://github.com/input-output-hk/cardano-ops/blob/master/modules/monitoring-cardano.nix#L13).

**Example relay topology**

_Please note that IOHK relay nodes are outlined as examples._

---
Frequently Asked Questions:

Stake Pool Fees: 
# What are the 340 ada fee and the percentage fee? Will I be charged this fee to stake my ada?
You are not charged this fee to stake your ada. The only fees you are charged to stake your ada are:

• A transaction fee (currently around 0.17 ada) to authorize the delegation transaction.
• 2 ada deposit which you can claim back if you decide to stop staking.

For each 5-day epoch, rewards are distributed to all ada holders who delegate to a stake pool, providing the stake pool produced a block. These rewards consist of:

• A set percentage of the ada reserves (undistributed ada). As this is a set percentage this is gradually declining over time.
• All of the transaction fees during the epoch. Currently, this is a small addition to the reward pot but as Cardano sees more activity this will increase the staking annual percentage rate (APR).

The amount of ada received by a pool of delegators from the reserves for each new block is currently around 500 ada. If a stake pool produces 1 block it will receive around 500 ada, 2 blocks around 1000 ada, and so on. The number of blocks a stake pool produces is proportional to the amount of delegation in the Stake Pool. From the total amount of ada that a pool of delegators is awarded, the stake pool operator (SPO) first takes the fixed fee of 340 ada, or maybe more if they have set a higher fixed fee. The SPO is then also awarded the margin fee that they have set, this is a percentage of the ada that remains after the fixed fee is taken. The ada that is left after these fees are taken is then shared between all of the delegators in the stake pool, weighted towards how much ada they have delegated. All of this happens automatically by the protocol, and is therefore trustless.

The lowest fees that an SPO can currently set for their stake pool is a 340 fixed fee and a 0% margin fee.

# Where can I get support for Marlowe?
For Marlowe support, please see this contact form ("https://docs.marlowe.iohk.io/docs/support/contact"). Join online discussions about Marlowe here. ("https://docs.marlowe.iohk.io/docs/support/dev-discussions")

# What is the Marlowe CLI tool?
Marlowe CLI is a command line tool that provides access to Marlowe capabilities on testnet environments and mainnet. It is specifically built for running Marlowe contracts directly without needing a web browser or mobile app.

Just as the cardano-cli tool enables plain transactions, simple scripts, and Plutus scripts, the Marlowe CLI tool facilitates the ability to interact with and develop Marlowe contracts. Users can measure transaction size, submit transactions, test wallet integrations, and debug validators. It provides a very concrete representation of Marlowe contracts that is quite close to what is occurring on-chain.

Users can create their own workflow that operates Marlowe, or their own toolset to wrap the Marlowe CLI tool in the way that developers have wrapped cardano-cli to create services such as libraries, faucets, and marketplaces.

# What is the Marlowe Pioneer program?
The Marlowe Pioneer program trains developers and anyone interested in how to write smart contracts using the Marlowe tools. The program has offered courses on building, simulating, analyzing and running smart contracts using Marlowe. The course has involved weekly videos, exercises, Q&A sessions, and access to the course creators and key experts in the field. 

The Marlowe team is currently reassessing the strategy to educate and support Marlowe pioneers. Some pioneers did not complete the program, so the team will be investing in educational courses, programs, and workshops tailored to specific audiences.

To best shape the educational programs to your needs, please send your input to tell what you think would be most helpful for you.

To hear about more upcoming educational opportunities as they emerge, please monitor Discord announcements and explore IOG Academy resources.

# How is Marlowe different from Plutus?
Marlowe and Plutus are both languages for writing smart contracts on the Cardano blockchain. 

Plutus is a general purpose language that can handle any type of logic, and is intended for developers who are familiar with the Haskell functional programming language. 

Marlowe, on the other hand, is a Plutus-based domain-specific language. The Marlowe language is designed to be accessible by both developers and non-developers. 

See ‘How are Marlowe contracts special?’ for more details.

# Where can I find Marlowe documentation and tutorials?
See the Marlowe documentation("https://docs.marlowe.iohk.io/docs/introduction") and tutorials("https://docs.marlowe.iohk.io/tutorials") for more details about Marlowe.

#How to get started with Marlowe?
Marlowe offers the following resources to help you get started. If you are already familiar with the Marlowe environment, take a look at the following:

Learn how to write and deploy smart contracts with Marlowe from basics to production by going through the tutorial concepts, guides, playbooks, and videos here("https://docs.marlowe.iohk.io/tutorials"). 

Find out more about the Marlowe Playground("https://docs.marlowe.iohk.io/docs/developer-tools/playground").

Leverage the Marlowe starter kit("https://docs.marlowe.iohk.io/docs/getting-started/marlowe-starter-kit").

# What is Marlowe Runtime?
Marlowe Runtime is the application backend for managing Marlowe contracts on the Cardano blockchain. It provides easy-to-use, higher-level APIs and complete backend services that enable developers to build and deploy enterprise and Web3 DApp solutions using Marlowe, but without having to assemble the ‘plumbing’ that manually orchestrates a backend workflow for a Marlowe-based application. 

Marlowe has a refined view of the Cardano ledger model. Runtime’s job is to map between the Marlowe conceptual model and the Cardano ledger model in both directions. Runtime takes commands relevant to the Marlowe ledger and maps them to the Cardano ledger. This can also be done with the REST API.

Primarily, you can do two types of things with Runtime:

Discovering and querying on-chain Marlowe contracts

Creating Marlowe transactions

# What languages does Marlowe support?
Marlowe supports the Marlowe language, built on top of Plutus. It also supports Haskell, JavaScript, and TypeScript. 

Additionally, Marlowe supports Blockly, a visual drag-and-drop programming tool.

# What happens when I move to a new stake pool? Will I lose rewards?
The switchover is seamless when you move from one stake pool to another and you will continue to receive staking rewards from your current pool until you start receiving rewards from your new stake pool. Every 5 days there is a stake snapshot on the boundary of each epoch. Your staked ada will be active in your new stake pool after two epoch boundaries and you will receive rewards from your new stake pool after the fourth epoch boundary.

You also do not need to withdraw your staking rewards before switching stake pools. See the following link for more information:

When should I withdraw my staking rewards?

# What is a spending password and what should I do if I've forgotten it?
When setting up your hot wallet, i.e. a wallet that is not using a hardware wallet, you are prompted to choose a spending password. This spending password is used to encrypt your wallet keys which are stored on your device. Your spending password is the only thing protecting the keys to your wallet so it's important that you choose a secure password.

If you forget your spending password you will need to remove your wallet from your wallet interface (Eternl, Yoroi, Flint, Typhon etc) and restore your wallet using your seed phrase which is the list of 15 or 24 words that you were prompted to write down when initially setting up your wallet.

Should I get a hardware wallet?

# I've lost the seed or recovery phrase for my wallet, what can I do?
Your seed phrase is the list of words, normally 15 or 24, that you are prompted to write down when you create a wallet using a wallet interface such as Eternl, Yoroi, Daedalus, Flint etc. This list of words should be stored very securely, preferably in more than one location, on paper or maybe even punched into a metal sheet and stored in a secret location. There are many products available that enable you to store your seed phrase in a way that it is safe even in the most extreme conditions. If you loose access to your wallet, maybe your phone or laptop breaks or is stolen, or maybe you have forgotten your spending password, restoring your wallet in a wallet interface using your seed phrase is the only way that you will be able to regain access to it.

If you have lost access to your seed phrase but still have access to your wallet then you should immediately create a new wallet, storing the seed phrase securely, then transfer all of your funds to your new secure wallet. If you have been staking your ada then please see the following link:

How do I move all of my ₳ to a new wallet and claim my 2 ₳ stake key deposit?

If you have lost your seed phrase and do not have access to your wallet there is no way to retrieve your funds.

# How can I delegate to more than one Stake Pool?
There are a a few reasons you may want to split your ada delegation between multiple stake pools. For example, to participate in multiple ISPOs, or you feel ethically aligned with more than one stake pool.

Currently there are two ways you can split your delegation.

Create new wallets
This option is pretty straightforward and you will be familiar with this from when you set up your first wallet. You can simply create extra wallets which will have their own seed phrase, transfer funds between these wallets and delegate each wallet to a different stake pool.

Add additional accounts under the same wallet
It is possible to add multiple additional accounts to your existing wallet. From a user perspective accounts act like separate wallets but they are recoverable using the same seed phrase or hardware wallet. Each account has its own receive address, transaction history, delegation history and stake key. Once you have set up an additional account you can send ada between accounts and delegate each account to a different Stake Pool if you wish. The multiple account feature is currently available in Eternl, Flint and some other wallets but not currently available in Daedalus or Yoroi.

How to set up and access additional accounts in Eternl
Open the wallet for which you would like to add additional accounts. On the main account screen select ‘account list’ then ‘add accounts’ and select which accounts you would like to add. To switch between accounts select ‘account list’ then select ‘activate’ underneath the account you would like to access.

How to set up and access additional accounts in Flint
Select the ‘wallets’ icon in the top right corner then select ‘add account’ under the wallet for which you would like to add an account and follow the on-screen instructions. To switch accounts select the ‘wallet’ icon in the top right then select the account you would like to access.

For instructions on how to set up additional accounts in other wallet interfaces please see their documentation.

It’s important to note that when restoring your wallet you will have to remember to add the accounts you want to access.

# When should I withdraw my staking rewards?
Staking rewards left in the rewards section of your wallet count towards your total balance staked. They therefore automatically compound without the need to withdraw them to the main part of your wallet. Each time you withdraw your staking rewards you will pay a transaction fee, which at the time of writing this is around 0.17 ada. This might not seem like a lot but it could add up over time. So it's best to leave your rewards in your rewards address until you need them.

As your staking rewards are held in the rewards address of your wallet, and not by the stake pool, you do not need to withdraw your staking rewards when delegating from one stake pool to another. You also do not need to withdraw your rewards when switching to a new wallet interface.

Some wallets offer an option to withdraw rewards at the same time as making a transaction. This reduces the withdrawal fee to a fraction of what you would normally pay to withdraw your rewards.

# Is my ada safe when staking?
This is a common question from people who are new to Cardano, who are familiar with staking on other blockchains, or who are moving their funds from an exchange to a self-custodial wallet such as Daedalus, Eternl, Flint, Typhon, Yoroi, and others.

As Cardano is a proof of stake (PoS) blockchain, it relies on holders delegating their ada to stake pools to secure the blockchain. In return they delegators are rewarded with ada. As Cardano relies on delegators taking part in this process it has been designed to be very secure.

When a delegator stakes their ada on Cardano their funds do not leave their wallet, the staking mechanism therefore does not put the delegator's ada at risk.

Staking is liquid, which means ada is not locked up for any period, delegators can move or spend their ada whenever they like.

The worst thing that could happen is that a stake pool underperforms, in which case a delegator may not receive all of the staking rewards. Even if this happens, the ada in the delegator's wallet is not at risk of being slashed or penalized in any way.

In line with the ethos of cryptocurrency and blockchain technology, staking is completely trustless and staking rewards are distributed automatically by the protocol.

A delegator can move to a new stake pool whenever they wish.

You can stake while using a hardware wallet and it is advisable to use one for extra security. See 'Should I get a hardware wallet?'

# Should I get a hardware wallet?
If you're wondering whether you need a hardware wallet, the answer is probably yes. If you have an amount of cryptocurrency that you worry about losing, you should probably get one.

Using a Cardano wallet interface such as Yoroi, Daedalus, Flint, Eternl, etc. without a hardware wallet means that the cryptographic keys to your on-chain wallet are stored on your computer or mobile phone. The only thing protecting your wallet keys in this scenario is your spending password, and as the device is connected to the internet this leaves your wallet vulnerable to hacking attempts. A hacker could potentially decrypt your wallet keys and sign a transaction, sending all of your funds to themselves.

If you connect a hardware wallet such as a Ledger or Trezor device to one of the wallet interfaces mentioned above, the cryptographic keys to your wallet are stored on the hardware wallet itself. In this scenario, your cryptographic keys are never exposed to the internet-connected device and physical access to the device is required to sign a transaction. This makes your cryptocurrency wallet a lot more secure.

Hardware wallets also allow you to switch between wallet interfaces without having to type in your seed phrase. Every time you type your seed phrase into an internet-connected device, you are putting your wallet at risk. With a hardware wallet you can just pair it to the new wallet interface and you're good to go.

A very important point to remember is that your hardware wallet is the only place you should ever enter your hardware wallet seed phrase. Never type your hardware wallet seed phrase into any Internet connected device such as your laptop or phone.

# What is an ISPO & what are the different types of ISPO?
ISPO is an acronym for Initial Stake Pool Offering. There are two main types but in both cases they enable projects that are building on Cardano to distribute their tokens to stake pool delegators. The tokens that are distributed generally have some sort of utility such as governance privileges in the project. The tokens can be held by the delegator for the utility or, if listed on one, can be traded on an exchange.

Community building ISPO

This type of ISPO uses the existing stake pool operator community to help raise awareness of and build a community around a project. A number of stake pools are selected to partner with the project and delegators of the partnered stake pools receive, or are able to claim, the project's token and also receive their regular staking rewards in ada. Stake pools are able to attract new delegators by being able to offer this extra incentive to join them. In an attempt to attract delegators, the stake pool operators would promote the project that they are partnered with, which would raise the profile of the project.

Fundraising ISPO

This type of ISPO is used to raise funds to finance the build and development of a project. In a fundraising ISPO, the project would run their own stake pools and set a high margin fee, sometimes 100%. The delegators would forfeit some or all of their normal staking rewards in ada in return for the promise of some of the project's tokens at a later date.

# Should I get a hardware wallet?
If you're wondering whether you need a hardware wallet, the answer is probably yes. If you have an amount of cryptocurrency that you worry about losing, you should probably get one.

Using a Cardano wallet interface such as Yoroi, Daedalus, Flint, Eternl, etc. without a hardware wallet means that the cryptographic keys to your on-chain wallet are stored on your computer or mobile phone. The only thing protecting your wallet keys in this scenario is your spending password, and as the device is connected to the internet this leaves your wallet vulnerable to hacking attempts. A hacker could potentially decrypt your wallet keys and sign a transaction, sending all of your funds to themselves.

If you connect a hardware wallet such as a Ledger or Trezor device to one of the wallet interfaces mentioned above, the cryptographic keys to your wallet are stored on the hardware wallet itself. In this scenario, your cryptographic keys are never exposed to the internet-connected device and physical access to the device is required to sign a transaction. This makes your cryptocurrency wallet a lot more secure.

Hardware wallets also allow you to switch between wallet interfaces without having to type in your seed phrase. Every time you type your seed phrase into an internet-connected device, you are putting your wallet at risk. With a hardware wallet you can just pair it to the new wallet interface and you're good to go.

A very important point to remember is that your hardware wallet is the only place you should ever enter your hardware wallet seed phrase. Never type your hardware wallet seed phrase into any Internet connected device such as your laptop or phone.

# How do I move all of my ₳ to a new wallet and claim my 2 ₳ stake key deposit?
There are many reasons you may want to move your ₳ to a new wallet, for example you may have just bought a hardware wallet to better secure your ₳ or your seed phrase may have become compromised.

If you unstake (or deregister your stake key) to claim back the 2 ₳ stake key deposit you will loose any pending rewards for the current and previous epoch. You will also loose any pending rewards for participating in Project Catalyst voting. By following these steps you will be able to receive your pending rewards and transfer all of your ₳ to your new wallet.

Leaving 1 ₳ in your wallet, send everything else including all of your Cardano Native Tokens, NFTs etc to your new wallet. This small amount of ₳ that you left in your wallet will be used to pay transaction fees later.

Delegate your new wallet to a stake pool straight away to ensure you don't miss out on any staking rewards.

Wait three epoch boundaries (or 15 days) for your pending stake rewards to arrive in your wallet. If you have recently voted in Project Catalyst you may want to wait until you have also received your voting rewards.

Claim your remaining rewards and unstake or deregister your stake key to receive your 2 ₳ stake key deposit that you paid when you registered your stake key.

Send all of the remaining ₳ to your new wallet.

You may want to keep your old wallet set up or keep hold of your seed phrase in case you need access to the information for tax purposes.

# My light wallet is showing an incorrect balance or says I'm not delegated to a Stake Pool
Cardano light wallets such as Yoroi, Eternl, Flint, Typhon, and others rely on servers to keep the blockchain information up to date. If you notice incorrect balances in your wallets, if your wallet says you are not delegated to a stake pool, or if you have some other issue, it may just be that the servers behind the wallet are experiencing sync issues.

Think of the aforementioned wallets more as wallet interfaces. Your wallet is on the blockchain and Yoroi, Eternl, Flint, Typon, and others are just software applications you can use to view and interact with your on-chain wallet.

To view your actual balance, stake pool delegation, and more, you can enter your address (starting with 'addr1') or your stake key (starting with 'stake1') into cardanoscan.io. The 'Controlled Stake' section on the platform will show your whole balance including staking rewards.

To fix your wallet interface issues you could try resyncing. You can find a resync option in the settings of most light wallets. If this doesn't fix your issue and your wallet interface is still showing a different balance to cardanoscan.io, then you can use your seed phrase or hardware wallet to set up your wallet in a different wallet interface. You can have your wallet set up in multiple wallet interfaces at the same time, this also enables you to try the interfaces out and see which one is best for your needs.

Switching between wallet interfaces is a lot more secure when using a hardware wallet such as a Ledger or Trezor as you avoid the need to enter your seed phrase each time. See 'Should I get a hardware wallet?'.

# What is Cardano's monetary policy?
Cardano's monetary policy addresses two issues:

The necessity to offer rewards for people who participate in the network
Funding the treasury
Rewards

The expansion and future improvement of the Cardano blockchain will be greatly influenced by its community, who need to be incentivized through rewards to participate in Cardano’s development.

Staking rewards for delegators and stake pool operators come from two sources:

Transaction fees - fees from every transaction from all blocks produced during every epoch go into a virtual 'pot'. A fixed percentage (ρ) of the remaining ada reserves is added to that pot.
Monetary expansion - a certain percentage (τ) of the pot is sent to the treasury, and the rest is used as epoch rewards.
Funding the Treasury

The Treasury's goal is the provision of funds to develop Cardano activities through a voting process. This necessitates a process whereby funds are regularly sent to the Treasury to ensure that funds are always available.

Cardano's monetary policy aims to keep the protocol sustainable in the long term ensuring its decentralization and security. Monetary policy must provide sufficient economic incentive to maintain the protocol and develop the ecosystem. More than 75% of ada is already in circulation. The protocol will distribute the remaining coins periodically each epoch in the coming years as rewards for pool operators and delegators. The Cardano protocol has a capped number of coins and will only depend on fees in the future. There is no never-ending inflation of ada coins.

# What is the Cardano network?
The Cardano network is a technical infrastructure combining Cardano nodes and their relative interactions in one unified system. It consists of a collection of nodes that communicate with each other to maintain the distributed ledger. These nodes validate blocks, add blocks to the chain, and distribute transactions.

The networking layer is the driving force for delivering information exchange requirements for establishing a better data flow. Cardano nodes maintain connections with peers that have been chosen via a custom peer selection process.

A set of mini-protocols is used to enable communication between different nodes. Each mini-protocol implements a basic information exchange requirement, such as:

informing peers of the latest block

sharing blocks as needed

sharing new transactions around the Cardano network.

For connection purposes, mini-protocols are determined by the version of the network protocol. For example, there are two protocol suites: node-to-node and node-to-client. The node-to-client protocol suite is used by wallets and chain consumers. Protocol suites use different sets of mini-protocols and the version is negotiated when a new connection is established. 

Find out more:

The Cardano network

Networking protocol design

Peer-to-peer (P2P) networking

# What is a cryptocurrency?
A cryptocurrency is a digital asset, which is stored on the ledger and is designed to serve as a medium of exchange for goods or services. It is otherwise called crypto.

Blockchain ledgers serve as the underlying technology for cryptocurrency creation in a decentralized environment. Blockchain protocols use rigorous cryptography techniques to enable the minting (creation) of cryptocurrency and to secure and verify crypto ownership and fund movement records. 

The price of cryptocurrency is not controlled by a government or centralized financial institution. It is defined by its value, correlation to real-world figures, and is driven by market supply and demand.

Ada is the native (underlying) currency on Cardano.

# Where do staking rewards come from?
There is a reward pot. Each epoch, all transaction fees and 0.3% of the remaining ada reserves are put into this pot.

20% of the pot reserves is sent to the treasury to support development through the Catalyst voting process.

80% is used as staking rewards.

Over time, transaction fees will become the main source of staking rewards.

# What is a sidechain?
A sidechain is a blockchain that runs independently alongside the main chain and is linked to it. Transactions are transferred to the sidechain for processing and the results are sent back, thus taking the workload off the main chain. This improves speed, lowers execution fees, and increases overall throughput.

# What is Hydra?
Hydra is a family of protocols that overlay the layer 1 Cardano blockchain to process transactions off the main chain. Hydra uses the main ledger as the secure settlement layer, boosts throughput, minimizes the delay in starting to process transactions, incurs low to no costs, and greatly reduces storage requirements.

# What’s the difference between layer 1 and layer 2 solutions?
Layer 1, which is the main blockchain ledger, operates on the underlying consensus protocol. This layer includes protocol parameters that control capabilities such as scalability and throughput. Layer 2 is an additional, off-chain protocol that works on top of the layer 1 blockchain. Parties can securely transfer funds from the blockchain into an off-chain protocol, settle transactions in this protocol independently of the underlying chain, and safely transfer funds back to the underlying chain as needed. Layer 2 protocols improve overall throughput and scalability because they reduce network congestion.

# What is scalability?
Scalability is the ability of a system to handle more and more work efficiently. This is a vital property for a blockchain or it will become slower and more expensive to use. Addressing the scaling problems of earlier blockchains was a founding aim of Cardano and is the focus of the Basho stage of development. Read more about how Cardano scales in 2022.

# What is the Catalyst Circle?
The Catalyst Circle is the representative body for groups participating in Project Catalyst. The Circle monitors the current state and future plans regarding governance in Catalyst. It detects and discusses concerns, objections, and opportunities arising within the Catalyst ecosystem. The Circle might discuss, for example, the definition of amounts allocated to challenges Fund over Fund; changes or conditions to incentive parameters; the Catalyst API, etc.

# What is Project Catalyst?
Project Catalyst is a decentralized innovation fund for Cardano projects. Since 2020, 30,000 members have set hundreds of projects underway. Project Catalyst marked the start of Cardano’s Voltaire phase and is one of the world’s largest examples of on-chain governance. See more FAQs about Project Catalyst.

# What is a CIP?
CIP stands for Cardano Improvement Proposal and is a participatory model system that gives decision-making power to the community. A CIP has an expected format: the structure of the proposal is templated to facilitate discussion and review. This enables other community members to jump in and discuss specific proposals, or individual points in a proposal. The proposals and their history are publicly available and maintained on the Cardano Foundation CIP GitHub repository.

# What is Cardano governance about?
Cardano’s decentralized governance model grants all ada holders the ability to decide what changes should be made for the ecosystem to grow and mature. Since individuals in the Cardano ecosystem are most affected by the decisions made about the protocol, it is important for them to understand how those decisions are made and how they are paid for, as well as how to participate in that process. Voltaire is the phase in Cardano's development that deals with decentralized governance and decision-making. It focuses on the Cardano community’s ability to decide on software updates, technical improvements, and project funding. To participate in the decision-making process, all ada holders can suggest a change through the Cardano improvement proposal (CIP) system, or participate in Project Catalyst to vote on what changes should be made.

# What is the Plutus Pioneer program?
The Plutus Pioneer program is a scheme to recruit and train developers to write smart contracts in Plutus on Cardano. Recruits are given access to courses about the principles of coding in both Haskell and Plutus. The course is highly interactive, with weekly videos, exercises, and Q&A sessions, along with exclusive access to Plutus creators and experts. There is also a dedicated community channel to help pioneers connect with each other. Find out more.

# What is Plutus?
Plutus is a set of programming tools for writing smart contracts on Cardano. Plutus includes Plutus Core – the software that runs Plutus transactions and contracts – and the Plutus programming language, which is based on Haskell.

# How are Marlowe contracts special?
Marlowe is based on original, peer-reviewed research conducted by the Marlowe team, initially at the University of Kent supported by a research grant from Input Output Global (IOG), and latterly by an internal engineering team. Marlowe is also a joint collaboration with Wyoming Advanced Blockchain R&D Laboratory (WABL) at the University of Wyoming.

A Marlowe contract is built by combining a small number of building blocks that describe making a payment, making an observation of something in the ‘real world,’ waiting until a certain condition becomes true, and other similar types of concepts.

Formal proofs, extensive testing, and analysis tools provide strong assurances for the safety of Marlowe contracts.

Beyond the notable benefit of being usable by non-programmers, Marlowe contracts are written in the Marlowe language, which has many additional advantages:

Ensures that certain sorts of flawed programs cannot even be written by designing those possibilities out of the language.
Avoids some of the unanticipated exploits that have been a problem for existing blockchains.
More easily checks that programs have the intended properties.
Makes sure that the contract will never fail to make a payment that it should.
Helps people write programs in the language using special-purpose tools.
Emulates how a contract will behave before it is run for real on the blockchain, ensuring that the contract performs as intended through static analysis.
Provides valuable diagnostics to potential participants before they commit to a contract.
Formally proves properties of Marlowe contracts, giving the highest level of assurance that contracts behave as intended through using logic tools.

# What is the Marlowe Playground?
The Marlowe Playground is a testing platform where developers can experiment with Marlowe contracts, including development, simulation, and testing. Marlowe contracts can be written in JavaScript or Haskell. The Playground also offers Blockly, a visual drag-and-drop programming tool.

# What is Marlowe?
Marlowe is a complete set of open source tools for developers to easily create, test, and deploy smart contracts on Cardano in a variety of programming languages. Marlowe offers developers intuitive solutions to create, use, and monetize secure smart contracts with ease – regardless of their expertise in software development.

# What is DApp certification?
DApp certification and assurance help ensure that products meet certain quality standards. While voluntary (Cardano is open and decentralized), certification benefits both developers and users because it includes security checks that help with auditing smart contracts. There are three levels of certification, each of which is complementary to the others.

# What are DApps?
DApp stands for decentralized application. That is a software app that works on blockchain. There are various types of DApps, including DeFi products, NFT markets, wallets, exchanges, games, and more.

# What is decentralized finance (DeFi)?
Decentralized finance or DeFi is a blockchain-based form of finance that addresses the same needs as traditional finance. You can send and receive payments, pay for products or services, or invest in cryptocurrency projects instead of bonds or stocks. DeFi uses smart contracts to settle deals fairly and does not depend on any intermediary.

# What is an oracle?
An oracle is a way to communicate with real-world data. Oracles connect with trusted external data sources that enable smart contracts to execute by referencing datasets such as exact timing, the weather, election results, sports statistics, and cryptocurrency prices. Oracles ensure confidence in timely, accurate, and untampered data.

# How do I write a smart contract?
Smart contracts are usually written by developers using a dedicated language. In Cardano, there is Plutus, and Marlowe for non-programmers. Haskell can also be used. Visit developers.cardano.org to find out more about writing smart contracts.

# What is a smart contract?
Smart contracts are automated digital agreements, written in code, that track, verify, and execute the binding parts of a deal between various parties. The contract executes automatically once pre-determined conditions are met.

# What is EUTXO?
EUTXO stands for Extended UTXO model. Cardano's EUTXO combines and matures Bitcoin's security and Ethereum's programmability. This model is vastly superior to the account-based model used by other blockchains because it ensures:

Enhanced security: every transaction uses a different address, which makes it impossible to track the address or find out the user’s overall balance.

Scalability: UTXO ledgers allow for transaction parallelization, which reduces congestion.

Interoperability: due to the implementation of off-chain and sidechain protocols, it is easier to establish interoperability between different blockchains.

Determinism: on the UTXO ledger, a user can predict the cost and validity of a transaction before it is processed on the chain. Transaction costs are also much lower in the UTXO model as there are no ‘gas’ fees.

# What is an account-based model?
Account-based accounting models use an account (which can be controlled by a private key or a smart contract) to hold a coin balance. In this model, assets are represented as balances within users’ accounts, and the balances are stored as a global state of accounts, kept by each node, and updated with every transaction.

In many respects, account-based chains (such as Ethereum) operate in a similar fashion to traditional bank accounts. The wallet's balance increases when coins are deposited, and decreases when coins are transferred elsewhere. This poses a security risk since the recipient’s address can be tracked to reveal its balances, which is not possible with UTXOs.

# What is a UTXO?
UTXO stands for unspent transaction output. This model allows keeping track of users’ balances after sending or receiving funds on the blockchain. In the UTXO model, a transaction has inputs and outputs, where the inputs are unspent outputs from previous transactions. As soon as an output is used as input in a transaction, it becomes spent and can never be used again. 

UTXO is, in many ways, similar to cash. A good analogy is this: Imagine you have $50 in your wallet. This amount could be made up with several combinations: two $20 bills and one $10, four $10 bills and two $5 bills, and many others. But regardless of the permutations, the amount ($50) remains equal. UTXOs work in the same way. Whatever balance you have in your blockchain wallet (say, 150 coins) could be made up with many different UTXO combinations, based on previous transactions, but the balance amount remains the same. In other words, the balance held in a given wallet address is the sum of all unspent UTXOs from previous transactions.

# What is an accounting model?
Blockchain is an accounting technology, and accounting models are crucial to keep track of funds' movements, asset ownership, and balance accuracy. Two major accounting models exist in the blockchain space: UTXO-based blockchains (Bitcoin or Cardano, for instance), and account-based chains (Ethereum, and others).

# Why is Haskell so important?
Haskell is vital to Cardano because its language tools allow extremely thorough testing of program logic and then the actual code. Such extensive testing tools are not available in languages such as C++ or Java.

# Which language is used for Cardano?
Haskell is the programming language that lies at the core of Cardano development. It is a 'functional' programming language, which means that all computations are treated as mathematical functions. The logic of a program is first described as an equation for testing using mathematical tools. Once correct, coding can begin.

# What is a stake pool operator pledge?
Pledging is an important mechanism that encourages the growth of a healthy ecosystem within the Cardano blockchain. When you register a stake pool you can choose to pledge some, or all, of your ada to the pool, to make it more attractive to people that want to delegate. Although pledging is not required when setting up a stake pool, it can make the stake pool more attractive to delegators. The higher the amount of ada that is pledged, the higher the rewards that will be paid out.

# What is the 340 fixed fee?
Providing a stake pool produces at least one block during an epoch, the stake pool operator will earn the minimum fixed fee of 340 ada. This is a constant value to help operators with the running costs of maintaining a pool.

# What is a profit margin?
A profit margin is the percentage of total ada rewards that the stake pool operator takes before sharing the rest of the rewards among all the delegators to the pool. A lower profit margin for the operator means they are taking less, which means that delegators can expect to receive more of the rewards for their delegated stake. A private pool is a pool with a profit margin of 100%, meaning that all the rewards will go to the operator and none to the delegators.

# What is the saturation point?
Saturation is a term used to indicate that a particular stake pool has more stake delegated to it than is ideal for the network. Saturation is displayed as a percentage. Once a stake pool reaches 100% saturation, it will offer diminishing rewards. The saturation mechanism was designed to prevent centralization by encouraging delegators to delegate to different stake pools, and operators to set up alternative pools so that they can continue earning maximum rewards. Saturation, therefore, exists to preserve the interests of both ada holders delegating their stake and stake pool operators.

# When will I receive my rewards?
Rewards are distributed at the end of each epoch (every five days). If this is the first time you delegated your funds to a pool, the process will take longer. If you delegate your ada during the first epoch, the pool that you delegate to can produce blocks two epochs later. Two epochs after that, you will begin receiving rewards. In this case, you will begin receiving staking rewards after 20 days.

# What happens when I switch delegation from one pool to another? Will I miss any rewards?
If you delegate to a different pool, you will still receive rewards from the previous one, but the delegation process will take effect after the current and next epochs.

# What other wallets support staking?
You can stake your ADA by using Daedalus or Yoroi wallets, products created by founding entities IOG and Emurgo respectively.

You can also choose from a growing range of other wallets, including:

Flint

Nami

Gero

Exodus

Adalite

Typhon

Eternl

NuFi

Hardware wallets Ledger and Trezor can also be used. Note that some of them won't let you choose the pool you want. This means that you will be able to delegate your ada only to the pool that supports a certain wallet.

# Can I stake with hardware wallets?
It is possible to stake using a hardware wallet by connecting it to Yoroi or Daedalus wallets.

# Can I stake on exchanges? Is it bad?
Staking on exchanges means that ada is kept on exchanges, which incurs security risks. Given that exchanges own a large amount of ada, this also poses a centralization issue. It is important for Cardano to maintain a healthy distribution of stake across many pool operators. The best way is to choose a stake pool that best meets the goals and values that a person supports and shares. You can discover multiple pools and their websites in Daedalus or use sources such as adapools.org, for example, to choose the pool for delegation.

# What is stake delegation?
Most ada holders do not have the knowledge or desire to run a pool, so they can delegate their stake to a stake pool. Delegation means letting the pool use the stake of owned ada. The more there is staked in a pool, the higher the rewards (until it reaches saturation).

# Who is a stake pool operator (SPO)?
A stake pool operator is a person or organization that takes responsibility for setting up and keeping a pool running. This usually entails owning or renting a server, holding the key to the pool, and maintaining and monitoring the node. The operator earns rewards for validating transactions on Cardano. These rewards are distributed between the operator and the ada holders who delegated their stake to the pool.

# What is a stake pool?
A stake pool is a virtual ‘pot’ that holds the combined stake of various holders. It is maintained by an operator, who is responsible for transaction validation on Cardano.

# What is stake?
Stake is the amount of ada a person owns.

# What is staking and delegation on Cardano?
Every ada holder owns a stake that is based on the amount of ada they have. A developer or a tech-savvy person can set up a stake pool and run it to help verify Cardano transactions and create new blocks getting rewards for this. Everyone can delegate their funds to a stake pool to earn a share of these rewards. There is no risk to this and no ada leaves your wallet. Ada can be delegated from your wallet or spent at any time.

# Hot wallets v cold wallets: what is the difference?
Hot wallets on a phone or computer are connected to the internet. Cold hardware or paper wallets are not online, so are even more secure, though prone to physical damage.

# What is a cold wallet?
A cold wallet is one that is not online. It can be in the form of a hardware device, similar to a USB stick, or a printed paper wallet. It is harder to hack (but may be easier to lose).

# What is a paper wallet?
A paper wallet includes scannable QR codes or crypto keys that you can use to access, send, or receive coins.

# What is a light wallet?
A light wallet does not download the full blockchain when a transaction is made. Instead, it relies on a website host, but is faster and needs less computing power. Light wallets are better suited for smartphones.

# What is a full-node wallet?
A full-node wallet downloads the full blockchain when opened, so needs specific software and storage requirements.

# What is the difference between Daedalus and Yoroi wallets?
Daedalus is a full-node wallet for desktop or laptop computers. It downloads and synchronizes the entire Cardano blockchain, which takes more time and requires significant storage space. Yoroi is a light-node wallet, which means that it gets data from the source with full blockchain access. This eliminates the need to download the full blockchain history, which makes a light wallet faster and easier to use.

# What is Yoroi?
Yoroi is a light wallet for daily use on Cardano. It is developed by Emurgo.

# On which systems does Daedalus run?
Daedalus runs on Windows, Mac, and Linux operating systems. You should download it only from the official Daedalus website.

# What is Daedalus?
Daedalus is one of Cardano's official wallets, along with Yoroi. Developed by IOG, Daedalus is the open-source desktop software wallet of choice for storing ada. It's a full-node wallet, which means the full Cardano blockchain needs to be downloaded, and each transaction is verified for maximum user security.

# How to store ada?
It is highly not recommended to store ada or any cryptocurrency on exchanges, as this increases the risk of funds loss. Daedalus and Yoroi are official Cardano wallets that are safe and secure for storing ada. They also provide users with a possibility to delegate ada to stake pools and earn passive rewards. Many other wallets also support ada.

# What is a cryptocurrency wallet?
A cryptocurrency (blockchain) wallet is software on a mobile phone or computer that allows you to send, receive, and store cryptocurrency. On Cardano, Daedalus or Yoroi can be used to manage your ada holdings and delegate your stake.

# What is a non-fungible token (NFT)?
A non-fungible token (NFT) is a unique token on a blockchain. It might be a digital work of art, or an object bought in an online game. NFTs confer proof of ownership and are bought and sold online.

# What does fungible mean?
A fungible item is identical to many others. Shares in a company, gold, and currencies are all fungible. Cryptos are usually fungible, but there are also unique, non-fungible tokens (NFTs).

# How to create Cardano native tokens?
Unlike Ethereum-based tokens created using smart contracts, Cardano native tokens run on the same blockchain layer as the ada cryptocurrency. Cardano’s architecture makes native tokens more secure and reduces the fees associated with transactions. Go to developers.cardano.org to find out how to create tokens on Cardano.

# What are native tokens?
'Native tokens' is Cardano’s feature that enables the creation of multi-purposed assets. Users can create their own tokens that interact with the blockchain just like ada. Tokens can be fungible (interchangeable) or non-fungible (unique), and act as payment units, rewards, tradable assets, or information holders. There is no need to create smart contracts to handle native tokens, which removes a layer of added complexity and potential for errors.

# How is ada mined? What can I do with ada? 
Mining ada isn't possible because mining is the feature of proof-of-work blockchains. On Cardano, owners can stake and delegate ada to earn rewards. By delegating their stake, users help run the Cardano blockchain. All rewards are distributed in ada every 5 days.

# Is ada a coin or a token?
Both coins and tokens represent a unit of cryptocurrency on a blockchain. Yet there is a difference between the two – a ‘coin’ has its own native blockchain whereas a ‘token’ is created on an existing blockchain. 

Ada (‘Ticker: ADA’) is the native cryptocurrency on Cardano and is the only currency used on Cardano to pay fees, make deposits and distribute rewards. In this sense, ada is a coin because it is native to Cardano in the same way that bitcoin is a coin on the Bitcoin blockchain and ether is a coin on Ethereum. 

Coins represent one type of digital assets. Other types of digital assets may still represent a footprint of value but may be designed for different purposes. These include tokens, NFTs, stablecoins, and other digital assets which represent value in the real world, such as a property deed, a government or corporate bond, an education certificate, ownership of a basketball boot, information, or a vote in an election. On Cardano, all of these can be represented as a ‘native token’. This means an NFT or another type of asset can be ‘minted’ (created) directly on the blockchain, without having to use a smart contract. This native capability is not available on other blockchains.

# What is ada? What’s in a name?
Ada is Cardano’s native currency. It is named after Ada Lovelace, the English mathematician and programmer. Ada became the first cryptocurrency to run on Cardano, in 2017. One ada equals 1,000,000 lovelaces. Lovelace is the smallest unit of ada. A lovelace is to ada what a satoshi is to bitcoin.

# How does Ouroboros work?
Ouroboros randomly elects a slot leader from among the stake pools to create block within a slot. A slot is the primary unit of time used by the Ouroboros algorithm. The more stake a pool controls, the greater the chance it has of being elected as a slot leader to produce a new block. When validating a transaction, a slot leader needs to ensure that the sender has included enough funds to pay for that transaction and must also ensure that the transaction’s parameters are met. If the transaction meets all these requirements, the slot leader will record it as a part of a new block, which will then be added to the chain.

# What is Ouroboros?
Ouroboros is Cardano's proof-of-stake consensus protocol. It is the first consensus protocol proven to be secure through academic peer review. The name comes from an ancient symbol that represents eternity and symbolizes the theoretical eternity of a blockchain. There have been several versions of Ouroboros: Classic (Byron phase, 2017), BFT (Byron/Shelley phases, 2020), Praos (Shelley phase, 2020), Genesis (planned for 2022), Chronos (in planning), Crypsinous (no deployment planned for Cardano).

# What is mining?
Mining is the process of creating coins in proof-of-work blockchains such as Bitcoin and Ethereum. Mining rigs (dedicated computers) consume huge amounts of energy to solve mathematical puzzles.

# What is proof of work (PoW)?
In proof-of-work (PoW) blockchains, such as Bitcoin and Ethereum, coins are created by miners who use huge amounts of energy doing ‘work’ – in this case, solving worthless mathematical problems. Later blockchains, such as Cardano, use proof of stake.

# What is proof of stake (PoS)?
In proof-of-stake (PoS) blockchains, such as Cardano, stake pool operators validate network activities. Operators (or slot leaders) are elected based on their holdings (stake) in the associated cryptocurrency (ada).

One of the key features of PoS is that as an operator's value increases, the opportunity to maintain the ledger also increases. This means a higher chance to produce new blocks that can be added to the blockchain and timestamped accordingly. The creator of a new block is chosen based on a combination of random selection and a determination of their stake, or wealth. A type of leader election occurs within the chain. Within a proof-of-stake protocol, participants get rewards for helping maintain network activities. This approach encourages the steady and stable growth of the blockchain incentivizing participants at the same time.

Primary advantages of proof of stake

Some of the primary advantages of PoS over PoW include:

rigorous security protocols are incorporated into a PoS protocol

reduced centralization - the risk of centralization is reduced by protocol parameters eliminating power accumulation for one participant

energy efficiency - energy consumption is extremely efficient as a smaller amount of electricity, as well as hardware resources, are needed to produce and run the blockchain

cost efficiency - PoS currencies are far more cost-effective than those operating on PoW protocols

# What is consensus?
Consensus means agreement. A consensus protocol is a way for blockchain participants to agree on transaction validity. The consensus protocol for Cardano is Ouroboros.

In a traditional setting, a central entity (like a bank) controls individuals’ funds and financial activity. This entity decides what kind of activity an individual can do, to whom they can send funds, or put a limit on certain operations. 

In a decentralized setting, no single entity is in control of individuals’ financial activity. That is why it is crucial to ensure that decisions made within the system are true, valid, and reached without the common pattern of centralized leadership management.

To ensure trust and security in financial operations, blockchains use the consensus algorithm. In essence, a consensus is what controls the laws and parameters governing the behavior of blockchains. Think of consensus as a ruleset that each network participant adheres to. It is the process of reaching a majority opinion by everyone involved in running the blockchain. An agreement must be made on which blocks to produce, which chain to adopt, and how to determine the single state of the network. 

The consensus protocol has three main responsibilities:

perform a leader check and decide if a block should be produced 

handle chain selection 

verify produced blocks

Blockchains create consensus by allowing participants to bundle transactions that others have submitted to the system in blocks, and add them to their chain (sequence of blocks). Determining who is allowed to produce a block when, and what to do in case of conflicts, (such as two participants adding different blocks at the same point of the chain), is the purpose of the consensus protocol.

# What are Babel fees?
This is a mechanism that allows transaction fees to be paid in tokens other than ada. This ensures enhanced interoperability and convenience.

# What are Cardano fees?
A fee is the amount of ada charged for processing a transaction on Cardano. Fees contribute to the network's financial health and development, and prevent economic attacks. Cardano’s deterministic nature ensures that fees are stable, predictable, and low in comparison to such proof-of-work blockchains like Ethereum, for example.

# What is a hard fork combinator?
A hard fork is a radical change to a blockchain protocol, which commonly results in one or more new protocols. The history of previous transactions is then lost and a new ‘forked’ protocol becomes the main one. Cardano’s unique hard fork combinator technology enables smooth protocol upgrades without disruption for users and saves the chain history of all operations.

# What is a genesis block?
The genesis block is the first block of a blockchain. All subsequent blocks can trace their lineage back to the genesis block.

# What are blocks, slots, and epochs? 
Time on Cardano is divided into epochs. Each epoch is divided into slots – a short period of time in which a block can be created. Blocks carry information about recent transactions and their data, and are linked to the previous and next block creating an immutable chain of records. A Cardano epoch includes 432,000 slots (5 days).

# Is Cardano independently verified?
Development on Cardano is driven by peer-reviewed research. This means that before any feature is implemented, it undergoes scientific review and verification by academics. In addition, Quantstamp, a blockchain security specialist, has independently audited Cardano for quality and safety. After the audit, Quantstamp’s chief executive Richard Ma said Cardano had ‘one of the best code bases we have seen’.

# Why is Cardano called ‘the green blockchain’?
People say Cardano is ‘green’ because it is a proof-of-stake blockchain that uses little energy and runs on basic hardware. Proof-of-work blockchains are more energy-intensive and generate large amounts of hardware waste.

# How does Cardano work?
Cardano is a fully decentralized blockchain platform. This means that thousands of people’s computers (nodes) cooperate to agree if a transaction is valid. This process is enabled by the proof-of-stake consensus protocol Ouroboros. On Cardano, people can send and receive ada (Cardano’s native currency), participate in staking and delegation to earn rewards, create multi-purposed assets (native tokens) and non-fungible tokens (NFTs). Cardano also has different smart contract languages and ensures increased security guarantees using the first provably secure proof-of-stake protocol Ouroboros.

# What makes Cardano a third-generation blockchain?
Bitcoin and Ethereum are first- and second-generation proof-of-work blockchains. Cardano is known as the third-generation blockchain because it uses greener proof-of-stake technology and aims to resolve other issues with the previous generations. In particular, Cardano aims to ensure higher performance, resolve scalability and interoperability issues, and ensure that the blockchain is self-sustainable.

# What is the history of Cardano's development?
Cardano was launched in 2017 to address problems with earlier blockchains such as high energy use, limited interoperability, and scalability. Cardano is being developed in five phases: Byron (completed 2019), Shelley (decentralization, 2020), Goguen (smart contracts, 2021-), Basho (performance improvements, 2021-), Voltaire (treasury and governance, 2021-).

# What is Cardano? 
Cardano is an open-source, proof-of-stake, public blockchain, the first to be founded on peer-reviewed research and development through evidence-based methods. Cardano combines pioneering technologies to provide unparalleled security and sustainability to decentralized applications, systems, and societies. Cardano exists to redistribute power from unaccountable structures to the margins – to individuals – and be an enabling force for positive change and progress.

# What is a private blockchain?
Private blockchains are usually established for a predefined number of users by closed organizations, or specific projects. Such blockchains can solve specific business problems relating to efficiency, security, or speed of transaction processing. Private blockchains are better suited for enterprises that seek to enhance their business processes without sharing information publicly.

# What is a public blockchain?
A public (or permissionless) blockchain is one that anyone can use without seeking permission. For example, to buy and sell ada or bitcoin, transact, or make payments. Such systems can be used for daily financial activities or by companies for commercial purposes.

# What is a blockchain?
A blockchain, also known as a distributed ledger, is a digital ‘book’ of records. Unlike traditional financial systems, blockchains are decentralized and are not regulated by any central authority. In a blockchain, nodes (people's computers) agree on the validity of any given transaction using what is called a consensus mechanism. Transactions are grouped together and stored in blocks that are added to the chain in set periods of time called slots. These transactions are visible to everyone, and once validated, records cannot be altered. This immutability guarantees transparency and trust between users.

-----
CARDANO IMPROVEMENT PROPOSALS

# Cardano Improvement Proposals (CIPs)
A Cardano Improvement Proposal (CIP) is a formalised design document for the Cardano community and the name of the process by which such documents are produced and listed. A CIP provides information or describes a change to the Cardano ecosystem, processes, or environment concisely and in sufficient technical detail. In this CIP, we explain what a CIP is; how the CIP process functions; the role of the CIP Editors; and how users should go about proposing, discussing and structuring a CIP.

The Cardano Foundation intends CIPs to be the primary mechanisms for proposing new features, collecting community input on an issue, and documenting design decisions that have gone into Cardano. Plus, because CIPs are text files in a versioned repository, their revision history is the historical record of significant changes affecting Cardano.

# Cardano Problem Statements (CPS)
A Cardano Problem Statement (CPS) is a formalised document for the Cardano ecosystem and the name of the process by which such documents are produced and listed. CPSs are meant to complement CIPs and live side-by-side in the CIP repository as first-class citizens.

# CIP-0001
Open
CIP Process
Status
Active
Category
Meta
Created
March 21, 2020
License
CC-BY-4.0
Authors
Frederic Johnson
Sebastien Guillemot
Matthias Benkort
Duncan Coutts
Michael Peyton Jones
Robert Phair
Implementors
N/A
Discussions
Discussion 1
Discussion 2
Discussion 3
Discussion 4
Abstract
A Cardano Improvement Proposal (CIP) is a formalised design document for the Cardano community and the name of the process by which such documents are produced and listed. A CIP provides information or describes a change to the Cardano ecosystem, processes, or environment concisely and in sufficient technical detail. In this CIP, we explain what a CIP is, how the CIP process functions, the role of the CIP Editors, and how users should go about proposing, discussing, and structuring a CIP.

The Cardano Foundation intends CIPs to be the primary mechanisms for proposing new features, collecting community input on an issue, and documenting design decisions that have gone into Cardano. Plus, because CIPs are text files in a versioned repository, their revision history is the historical record of significant changes affecting Cardano.

Motivation: why is this CIP necessary?
CIPs aim to address two challenges mainly:

The need for various parties to agree on a common approach to ease the interoperability of tools or interfaces.
The need to propose and discuss changes to the protocol or established practice of the ecosystem.
The CIP process does not by itself offer any form of governance. For example, it does not govern the process by which proposed changes to the Cardano protocol are implemented and deployed. Yet, it is a crucial, community-driven component of the governance decision pipeline as it helps to collect thoughts and proposals in an organised fashion. Additionally, specific projects may choose to actively engage with the CIP process for some or all changes to their project.

This document outlines the technical structure of the CIP and the technical requirements of the submission and review process. The history, social features and human elements of the CIP process are described the CIP repository Wiki.

Specification
Document
Structure
A CIP is, first and foremost, a document which proposes a solution to a well-defined problem. Documents are Markdown files with a Preamble and a set of pre-defined sections. CIPs authors must abide by the general structure, though they are free to organise each section as they see fit.

The structure of a CIP file is summarised in the table below:

Name	Description
Preamble	Headers containing metadata about the CIP (see below).
Abstract	A short (~200 word) description of the proposed solution and the technical issue being addressed.
Motivation: why is this CIP necessary?	A clear explanation that introduces a proposal's purpose, use cases, and stakeholders. If the CIP changes an established design, it must outline design issues that motivate a rework. For complex proposals, authors must write a Cardano Problem Statement (CPS) as defined in CIP-9999 and link to it as the Motivation.
Specification	The technical specification should describe the proposed improvement in sufficient technical detail. In particular, it should provide enough information that an implementation can be performed solely based on the design outlined in the CIP. A complete and unambiguous design is necessary to facilitate multiple interoperable implementations.

This section must address the Versioning requirement unless this is addressed in an optional Versioning section.

If a proposal defines structure of on-chain data it must include a CDDL schema.
Rationale: how does this CIP achieve its goals?	The rationale fleshes out the specification by describing what motivated the design and what led to particular design decisions. It should describe alternate designs considered and related work. The rationale should provide evidence of consensus within the community and discuss significant objections or concerns raised during the discussion.

It must also explain how the proposal affects the backward compatibility of existing solutions when applicable. If the proposal responds to a CPS, the 'Rationale' section should explain how it addresses the CPS and answer any questions that the CPS poses for potential solutions.
Path to Active	Organised in two sub-sections (see Path to Active for detail):
Acceptance Criteria
Describes what are the acceptance criteria whereby a proposal becomes 'Active'.
Implementation Plan
Either a plan to meet those criteria or N/A if not applicable.
optional sections	May appear in any order, or with custom titles, at author and editor discretion:
Versioning: if Versioning is not addressed in Specification
References
Appendices
Acknowledgements
Copyright	The CIP must be explicitly licensed under acceptable copyright terms (see below).
Note

Each of these section titles (Abstract onward) should be an H2 heading (beginning with markdown ##). Subsections like Versioning or Acceptance Criteria should be H3 headings (e.g. ### Versioning). Don't include a H1 title heading (markdown #): for web friendly contexts, this will be generated from the Preamble.

Header Preamble
Each CIP must begin with a YAML key:value style header preamble (also known as front matter data), preceded and followed by three hyphens (---).

Field	Description
CIP	The CIP number (without leading 0), or "?" before being assigned
Title	A succinct and descriptive title. If necessary, use a - delimiter to begin with an applicable classification (see Naming CIPs with similar subjects). Don't use backticks (`) in titles since they disrupt formatting in other contexts.
Category	One of the editorially accepted categories covering one area of the ecosystem.
Status	Proposed | Active | Inactive (..reason..)
Authors	A list of authors' real names and email addresses (e.g. John Doe john.doe@email.domain)
Implementors	A list of implementors committed to delivering an implementation of the proposal, when applicable. N/A when not applicable and [] when there's currently no implementor.
Discussions	A list of links where major technical discussions regarding this CIP happened. Links should include any discussion before submission, and must include a link to the pull request that created the CIP and any pull request that modifies it.
Solution-To	A list of CPS that this CIP addresses, if any. Omitted when not applicable.
Created	Date created on, in ISO 8601 (YYYY-MM-DD) format
License	Abbreviation of an approved license(s)
For example:

---
CIP: 1
Title: CIP Process
Category: Meta
Status: Active
Authors:
    - Frederic Johnson <frederic.johnson@cardanofoundation.org>
    - Sebastien Guillemot <sebastien@dcspark.io>
    - Matthias Benkort <matthias.benkort@cardanofoundation.org>
    - Duncan Coutts <duncan.coutts@iohk.io>
Implementors: N/A
Discussions:
    - https://github.com/cardano-foundation/CIPs/pull/366
Created: 2020-03-21
License: CC-BY-4.0
---
Especially because Markdown link syntax is not supported in the header preamble, labels can be added to clarify list items; e.g.:

Discussions:
    - Original-PR: https://github.com/cardano-foundation/CIPs/pull/366
Tip

A reference template is available in .github/CIP-TEMPLATE.md

Repository Organization
A CIP must be stored in a specific folder named after its number (4-digit, left-padded with 0) and in a file called README.md. Before a number is assigned, a placeholder folder name should be used (e.g. CIP-my-new-feature). After a number has been assigned, rename the folder.

Additional supporting files (such as diagrams, binary specifications, dialect grammars, JSON schemas etc.) may be added to the CIP's folder under freely chosen names.

For example:

CIP-0010
├── README.md
├── registry.json
└── registry.schema.json
 
Translations
While CIPs are mainly technical documents meant to be read primarily by developers -- and thus often written in English; some may be translated into various languages to increase their outreach. Any file in a CIP folder may also include translated content satisfying the following rules:

Any translated file shall share a common basename with its original source.
Translation file basenames must have a suffix using an ISO 639-1 language code, separated by a dot . character. (e.g. README.fr.md).
When no language code is provided as suffix, the default language for the document is assumed to be English (UK/US).
Translated CIPs (i.e. README files), must not include the original preamble. They must, however, include the following preamble as yaml frontmatter data:
Field	Description
CIP	The CIP number (without leading 0)
Source	The canonical GitHub link to the original CIP document
Title	A translation of the CIP's title
Revision	Whenever possible, the commit hash (7 first hex-digits, e.g. 12ab34c) of the source in the main repository. If the source was not committed to the main repo, use the best known translation date in ISO format (if unknown, use the date posted in the translation's PR branch).
Translators	A list of translators names and email addresses (e.g. John Doe <john.doe@email.domain>)
Language	An ISO 639-1 code of the target language (e.g. fr)
Translated CIPs inherit the same licensing terms as their original sources.
Versioning
CIPs must indicate how the defined Specification is versioned. Note this does not apply to the CIP text, for which annotated change logs are automatically generated and available through the GitHub UI as a history of CIP files and directories.

Authors are free to describe any approach to versioning that allows versioned alterations to be added without author oversight. Stipulating that the proposal must be superseded by another is also considered to be valid versioning.

A single Versioning scheme can be placed either as a subsection of the Specification section or in an optional Versioning top-level section near the end. If the Specification contains multiple specification subsections, each of these can have a Versioning subsection within it.

Licensing
CIPs are licensed in the public domain. More so, they must be licensed under one of the following licenses. Each new CIP must identify at least one acceptable license in its preamble. In addition, each license must be referenced by its respective abbreviation below in the "Copyright" section.

Purpose	Recommended License
For software / code	Apache-2.0 - Apache License, version 2.0
For documentation	CC-BY-4.0 - Creative Commons Attribution 4.0 International Public License
Warning

All licenses not explicitly included in the above lists are not acceptable terms for a Cardano Improvement Proposal unless a later CIP extends this one to add them.

Statuses
CIPs can have three statuses: Proposed, Active or Inactive. The CIP Process section highlights how CIPs move through these statuses; no CIP should be given one of these statuses without satisfying the criteria described here below.

Note

There is no "draft" status: a proposal which has not been merged (and hence exists in a PR) is a draft CIP. Draft CIPs should include the status they are aiming for on acceptance. Typically, but not always, this will be 'Proposed'.

Status: Proposed
A 'Proposed' CIP is any CIP that meets the essential CIP criteria but is not yet 'Active'. The criteria that must meet a CIP to be merged as 'Proposed' are:

It must contain all the sections described in Structure.
The quality of the content must be to the Editors’ satisfaction. That means it must be grammatically sound, well-articulated and demonstrates noticeable efforts in terms of completeness and level of detail.
Its technical soundness should have been established. Where necessary, this may require review by particular experts and addressing their concerns. Note that the requirement is that the proposal makes sense (i.e. be technically sound), yet no consulted experts need to think it is a good idea.
It must have a valid Path to Active as defined below.
Status: Active
An 'Active' CIP has taken effect according to the criteria defined in its 'Path to Active' section. Said differently, each CIP defines by which criteria it can become 'Active' and those criteria are included in the review process. Exact criteria thereby depends on the nature of the CIP, typically:

For CIPs that relate to particular projects or pieces of technology, it becomes 'Active' by being implemented and released;
For changes to the Cardano protocol, a CIP becomes 'Active' by being live on the Cardano mainnet;
For ecosystem standards, it means gaining sufficient and visible adoption in the community.
It must have a valid Path to Active as defined below: even the CIP is already acknowledged as Active or being documented retroactively (after acceptance and implementation).
A proposal that is 'Active' is considered complete and is synonymous with "production readiness" when it comes to the maturity of a solution. 'Active' CIPs will not be updated substantially (apart from minor edits, proofreading and added precisions). They can, nevertheless, be challenged through new proposals if need be.

Status: Inactive
An 'Inactive' CIP describes any proposal that does not fit into the other types. A CIP can therefore be 'Inactive' for various reasons (e.g. obsolete, superseded, abandoned). Hence the status must indicate a justification between parentheses; for example:

Status: Inactive (superseded by CIP-0001)
Path to Active
This must be subdivided into two sub-sections:

'Acceptance Criteria' This sub-section must define a list of criteria by which the proposal can become active. Criteria must relate to observable metrics or deliverables and be reviewed by editors and project maintainers when applicable. For example: "The changes to the ledger rules are implemented and deployed on Cardano mainnet by a majority of the network", or "The following key projects have implemented support for this standard".
'Implementation Plan' This sub-section should define the plan by which a proposal will meet its acceptance criteria, when applicable. More, proposals that require implementation work in a specific project may indicate one or more implementors. Implementors must sign off on the plan and be referenced in the document's preamble. In particular, an implementation that requires a hard-fork should explicitly mention it in its 'Implementation Plan'.
Note

The statuses of Proposed and Active both require a Path to Active section, making this a required section for all viable proposals. Even if a CIP is edited or submitted with an Inactive status, it may still be helpful to have a Path to Active if there are conditions that might lead to its acceptance or implementation.

Categories
CIPs are classified into distinct categories that help organise (and thus, find) them. Categories are meant to be flexible and evolve as new domains emerge. Authors may leave the category as ? should they not be sure under which category their proposal falls; editors will eventually assign one or reject the proposal altogether should it relate to an area where the CIP process does not apply.

Submissions can be made to these categories whenever relevant, without following any particular submission guidelines:

Category	Description
Meta	Designates meta-CIPs, such as this one, which typically serve another category or group of categories
Wallets	For standardisation across wallets (hardware, full-node or light)
Tokens	About tokens (fungible or non-fungible) and minting policies in general
Metadata	For proposals around metadata (on-chain or off-chain)
Tools	A broad category for ecosystem features not falling into any other category
Additionally, representatives of ecosystem categories may explicitly enlist their categories (see next section) to suggest a closer relationship with the CIP process. The following categories are confirmed as enlisted according to CIPs which define that relationship:

Category	Description
Plutus	Changes or additions to Plutus, following the process described in CIP-0035
Ledger	For proposals regarding the Cardano ledger, following the process described in CIP-0084
These tentatively enlisted categories await CIPs to describe any enlistment relationship:

Category	Description
Consensus	For proposals affecting implementations of the Cardano Consensus layer and algorithms
Network	Specifications and implementations of Cardano's network protocols and applications
Project Enlisting
Project representatives intending to follow an "enlisted" category above agree to coordinate with related development by sharing efforts to review and validate new proposals. It should be noted that single organisations can no longer represent any ecosystem or development category, which makes these enlistment guidelines both decentralised and cooperative, including whenever possible:

allocating time to review proposals from actors of the community when solicited by editors (i.e. after one first round of reviews);
defining additional rules and processes whereby external actors can engage with their project as part of the CIP process;
defining boundaries within their project for which the CIP process does apply;
establishing points of contact and any designated reviews for a category;
agreeing upon how proposals move from the state of idea (i.e. CIP) to actual implementation work;
writing CIPs for significant changes introduced in their projects when it applies.
Any guidelines for this cooperation should be described by a dedicated CIP whenever possible. When such a CIP is posted or supersedes another one, it will be entered into the above table in the Categories section. Participants of enlisted categories should follow the requirements outlined in that CIP and should update such proposals whenever these requirements or relationships change.

Warning

A positive review by any enlisted project representative does not constitute a commitment to implement the CIP. It is still the CIP author's responsibility to create an implementation plan and identify implementors.

Editors occasionally invite representatives from enlisted categories to speak during review meetings and solicit them for ultimate approvals of proposals in their area of expertise.

Note Optionally, projects may show their enlisting using the following badge on their introductory README:https://github.com/cardano-foundation/CIPs

![https://github.com/cardano-foundation/CIPs](https://raw.githubusercontent.com/cardano-foundation/CIPs/master/.github/badge.svg)
Process
1. Early stages
1.a. Authors open a pull request
Proposals must be submitted to the cardano-foundation/CIPs repository as a pull request named after the proposal's title. The pull request title should not include a CIP number (and use ? instead as number); the editors will assign one. Discussions may precede a proposal: early reviews and discussions streamline the process down the line.

PRs should not contain commits that also appear in other repository PR's: usually the consequence of re-using a branch in your fork or submitting your work from your fork's master branch. To avoid this, please:

Don't submit your PR from your fork's master branch.
Create a new branch for every pull request that you intend to submit.
Tip

The CIP title in the pull request should be kept consistent with the CIP header Title:.

Important

Pull requests should not include implementation code: any code bases should instead be provided as links to a code repository.

Note

Proposals addressing a specific CPS should also be listed in the corresponding CPS header, in 'Proposed Solutions', to keep track of ongoing work.

Naming CIPs with similar subjects
When a CIP title and subject matter share a common element, begin the CIP title with that common element and end it with the specific portion, delimited with the - character. Example (CIP-0095):

Web-Wallet Bridge - Governance

CIP editors will help determine these common elements and, whenever necessary, rename both CIP document titles and PR titles accordingly. The objective is to provide commonly recognisable names for similar developments (e.g. multiple extensions to another CIP or scheme).

Link to proposal from PR first comment
In the original comment for your pull request, please include a link to the directory or the README.md for the CIP in your working branch, so readers and reviewers can easily follow your work. This makes it easier for editors and the community to read and review your proposal.

Note

If this link changes (e.g. from the CIP directory being renamed), please keep this link updated.

Follow a reviewer- and editor-friendly review process
As review progresses:

When editors and reviewers submit changes that you accept, commit them from the GitHub UI so these review points are resolved.
Even if resolving these in your own environemnt, mark any review points Resolved as they are resolved: otherwise your PR will appear stalled and merging will likely be delayed.
Don't "force push": which overwrites commit histories and disrupts change visibility during the review process. Instead, git merge the PR branch back into your local environment: which will preserve any collaborative editing history.
1.b. Authors seek feedback
Authors shall champion their proposals. The CIP process is a collaborative effort, which implies discussions between different groups of individuals. While editors may provide specific inputs and help reach out to experts, authors shall actively seek feedback from the community to help move their proposal forward.

Discussions and comments shall mainly happen on Github in pull requests. When discussed on other mediums, we expect authors or participants to report back a summary of their discussions to the original pull request to keep track of the most critical conversations in a written form and all in one place.

As much as possible, commenters/reviewers shall remain unbiased in their judgement and assess proposals in good faith. Authors have the right to reject comments or feedback but are strongly encouraged to address concerns in their 'Rationale' section. Ultimately, CIP editors shall make the last call concerning the various statements made on a proposal and their treatment by the author(s).

By opening pull requests or posting comments, commenters and authors agree to our Code of Conduct. Any comment infringing this code of conduct shall be removed or altered without prior notice.

Note

For acceptability guidelines, including a concise review checklist, see CIP Wiki > CIPs for Reviewers & Authors.

2. Editors' role
2.a. Triage in bi-weekly meetings
CIP editors meet regularly in a public Discord server to go through newly proposed ideas in a triage phase. As a result of a triage, editors acknowledge new CIPs, and briefly review their preamble. Editors also assign numbers to newly proposed CIPs during this phase. Incidentally, the triage allows new CIPs to get visibility for potential reviews.

2.b. Reviews
In every meeting, editors will also review in more depth some chosen CIPs (based on their readiness and the stability of the discussions) and assess if they meet the criteria to be merged in their aimed status.

During review sessions, editors will regularly invite project maintainers or actors from the ecosystem who are deemed relevant to the meeting's agenda. However, meetings are open and held in public to encourage anyone interested in participating.

A dedicated Discord channel may also be created for some long-running discussions to support quick chats and progress on particular topics (while still being regularly summarised on the repository).

3. Merging CIPs in the repository
Once a proposal has reached all requirements for its target status (as explained in Statuses) and has been sufficiently and faithfully discussed by community members, it is merged with its target status.

Warning

Ideas deemed unsound shall be rejected with justifications or withdrawn by the authors. Similarly, proposals that appear abandoned by their authors shall be rejected until resurrected by their authors or another community member.

CIPs are generally merged with the status 'Proposed' until they meet their 'Path to Active' requirements. In some rare cases (mainly when written after the facts and resulting in a broad consensus), proposals may be merged as 'Active' immediately.

Each proposal is unique and has a bespoke 'Path to Active', which must be reviewed case-by-case. There must be sufficient time between the first appearance of a proposal and its merge into the repository to ensure enough opportunity for community members to review it.

4. Implementors work towards Active status following their 'Implementation Plan'
Once merged, implementors shall execute the CIP's 'Implementation Plan', if any. If a proposal has no implementors or no 'Implementation Plan', it may simply remain as 'Proposed' in the repository.

Warning

It is perfectly fine to submit ideas in the repository with no concrete implementation plan, yet they should be treated as such: ideas.

Besides, once all of the 'Path to Active' requirements have been met, authors shall make another pull request to change their CIP's status to 'Active'. Editors may also do this on occasion.

Editors
For a full, current description of Editor workflow, see CIP Wiki > CIPs for Editors.

Missions
CIP Editors safeguard the CIP process: they form a group enforcing the process described in this document and facilitating conversations between community actors. CIP editors should strive to keep up to date with general technical discussions and Cardano proposals. For each new draft proposal submitted on cardano-foundation/CIPs an editor might review it as follows:

Read the proposal to check if it is ready, sound, and complete.
Check if it has been properly formatted.
Check if sufficient time has been allowed for proper discussion amongst the community.
Ensure the motivation behind the CIP is valid and that design choices have relevant justifications or rationale.
Confirm licensing terms are acceptable.
Assign a CIP number
Assign a given category to help with searching
Request wording/grammar adjustments
CIPs that do not meet a sufficient level of quality or don't abide by the process described in this document will be rejected until their authors address review comments.

Reviews
Note that editors may provide technical feedback on proposals in some cases, although they aren't expected to be the sole technical reviewers of proposals. CIPs are, before anything, a community-driven effort. While editors are here to facilitate the discussion and mediate debates, they aren't necessarily technical experts on all subjects covered by CIPs.

Therefore, CIPs authors are encouraged to reach out to known experts to demonstrate their good faith and openness when they champion a proposal. Editors may help with such efforts but cannot be expected to do this alone.

Nomination
Existing editors or the community may nominate new editors, provided they have proven to be already existing and active contributors to the CIP process and are ready to commit some of their time to the CIP process regularly.

The missions of an editor include, but aren't exclusively limited to, any of the tasks listed above. Active members that seek to become listed editors may also come forth and let it be known. Any application will take the form of a pull request updating this document with a justification as the pull request's description.

Current editors are listed here below:

Robert Phair
@rphair	Ryan Williams
@Ryun1	Adam Dean
@Crypto2099	Thomas Vellekoop
@perturbing
Emeritus editors:

Frederic Johnson - @crptmppt
Sebastien Guillemot - @SebastienGllmt
Matthias Benkort - @KtorZ
Duncan Coutts - @dcoutts
Rationale: how does this CIP achieve its goals?
Key changes from CIP-0001 (version 1)
Introduction of Cardano Problem Statements
A significant friction point regarding complex CIPs is often how the main problem is stated. The 'Motivation' is often insufficient (or simply underused) to describe various aspects of a problem, its scope, and its constraints. This lack of clarity leads, in the end, to poorly defined issues and debates over solutions that feel unclear amongst different participants.

The introduction of CIP-9999: Cardano Problem Statements addresses this gap by introducing a formal template and structure around problem statements. However, requiring any CIP to be preceded by a CPS would likely be overkill and become an obstacle to the overall adoption of the CIP process for more straightforward problems. At this stage, it is reasonable to think either (a) that CIP authors would foresee the complexity and state their problem as a CPS beforehand or (b) that editors or reviewers will require authors to write a CPS to clarify a perhaps ambiguous motivation on complex CIPs.

We also anticipate project maintainers or community actors will write standalone CPS to document well-known issues for which the design space is still to be explored.

Explicit enlisting
A recurring pain point with the previous CIP process was the lack of clear ownership/accountability of some proposals affecting remote parts of the ecosystem. On several occasions, proposals from community members have concerned, for example, core components of the Cardano architecture. However, those proposals have been hard to move forward with and to either reject or turn into concrete action steps. Authors usually do not have the technical proficiency required to implement them and rely on the core engineering team in charge of projects to do so. Thus, explicit compliance and collaboration of those engineering teams are necessary to propose changes affecting their work.

By asking teams to explicitly state their compliance with the CIP process with clear, accountable owners (as individuals), it becomes plausible to now establish a dialogue between community members and technical leadership responsible for specific ecosystem projects. Furthermore, projects that, on the contrary, do not seek to participate in CIP or receive contributions in the form of CIP/CPS are automatically taken out of this process, saving time and energy for both reviewers and authors.

Nomination of new editors
The 'Editors' section now details how to become a CIP editor. The process aims to be simple and define those involved the most with editorship tasks as editors. Thus, being an active contributor to the CIP process as a prerequisite only makes sense. We want to leave the room open for either existing editors to refer an existing community as an editor or for community members to formulate that request explicitly.

There are no delays or number of contributions necessary to pretend to become an editor. Those criteria are often less relevant than others and more subjective, such as the quality of one's participation or their relevance. Since editors also need to work with one another in the end, it seems logical that existing editors have their final say about whom they intend to work with.

Removal of
type
in the preamble
The type field in the header has shown to be:

confusing (often authors are getting it wrong);
not-too-useful (as a type tells you very little about the nature of the CIP).
An ad-hoc classification by non-rigid categories, which may evolve over time to reflect ecosystem areas, seems better suited. Therefore, we do not require authors to categorise their CIPs; instead, categories will be added and maintained by editors as a side task atop the CIP process.

Simplification of the statuses
Over time we've learnt that the valuable information a status should convey is really about the readiness of a CIP, especially regarding standards. For a long time, many CIPs have lived as Draft despite some being used in dozens of systems. Consequently, the status has lost a bit of its meaning. The frontier between Draft and Proposed hasn't always been clear, and it has proven challenging to come up with good statuses to describe all possible rejections. So instead, the current division of statuses is as simple-as-it-can-be and remains flexible regarding rejections.

Choice of CoC
The choice of a code of conduct follows other popular open source initiatives. It serves as a good, unilaterally accepted foundation which can be later revisited if necessary.

Path to Active
Acceptance criteria
 The proposal has been reviewed by the community and sufficiently advertised on various channels.
 Cardano Forum
 IOG Technical Community Discord
 Twitter
 Reddit
 Cardano Summit 2022
 IO ScotFest 2022
 All major concerns or feedback have been addressed. The document is as unambiguous as it can be and it outlines a process that a supermajority of reviewers is happy to follow.
Implementation Plan
 Rework existing draft CIPs to adopt this new format and process. In particular, CIPs affecting enlisted projects should be brought to the attention of the respective project maintainers.
 Edit / align old CIPs preambles and sections to at least reflect also this new format.
Copyright
This CIP is licensed under CC-BY-4.0.

---
CIP: 2
Title: Coin Selection Algorithms for Cardano
Category: Wallets
Authors:
    - Jonathan Knowles <jonathan.knowles@iohk.io>
Implementors: N/A
Discussions:
    - https://github.com/cardano-foundation/CIPs/pull/2
    - https://github.com/cardano-foundation/CIPs/issues/232
Status: Active
Created: 2020-05-04
License: CC-BY-4.0
---

## Abstract

This article describes, in _human-readable terms_, the coin selection
algorithms used by [Cardano
Wallet](https://github.com/cardano-foundation/cardano-wallet/) and other parts of
the Cardano ecosystem.

In the context of this article, **coin selection** refers to the process of
choosing _unspent coins_ from a wallet (or [UTxO set](#utxo-set)) in order to
pay money to one or more recipients.

## Motivation: why is this CIP necessary?

This document was written to help people gain an understanding of the coin
selection algorithms used in Cardano _without_ having to read through and
understand Cardano source code.

We aim to provide descriptions of algorithms that:

  - don't require prior experience with any particular programming language;
  - are understandable for people who are unfamiliar with coin selection;
  - are precise enough for software engineers to be able to reimplement these
    algorithms in their preferred programming languages.

Where appropriate, we also provide mathematical descriptions, for added clarity.

### Scope

Coin selection is a large, complex topic, with many difficult problems to
solve. However, all software that performs coin selection must ultimately deal
with at least the following pair of problems:

  - How to _generate_ a coin selection, by choosing unspent coins from a wallet
    (or [UTxO set](#utxo-set)) in order to pay money to one or more
    recipients.
  - How to _adjust_ a coin selection in order to pay for a _network fee_, so
    that the coin selection can be published as a transaction on the
    blockchain.

This article concerns itself with the _former_ problem of _how to generate_ a
coin selection.

Problems relating to network fees, and how to adjust coin selections to pay for
such fees, are outside the scope of this article.

### Background

This section introduces the fundamental concepts behind coin selection,
provides a discussion of why coin selection is a non-trivial problem, and
describes important goals of coin selection algorithms.

#### What is Coin Selection?

Coin selection is the process of choosing unspent coins from a wallet in order
to pay money to one or more recipients.

##### Coin Selection in the Physical World

In the familiar world of _physical_ money, our wallets hold value in the form
of _coins and banknotes_.

When making a payment to someone, we typically select a combination of coins
and banknotes from a wallet that, when added together, have enough value to
cover the amount required.

Ideally, we'd always be able to select _just enough_ to cover the exact amount.
However, given that coins and banknotes have fixed values (and cannot be
subdivided without destroying their value), it's often impossible to select the
exact amount required. In such cases, we typically give the recipient _more_
than the required amount, and then receive the excess value back as _change_.

> :bulb: **Example**
>
> Alice wishes to pay for her lunch.
>
> The total price comes to €2.50 (2 euros and 50 cents). In her wallet, she has
> **five** _one-euro_ coins, and **one** _ten-euro_ note.
>
> Note that there is _no_ combination of coins (or notes) in her wallet that
> when added together give a total of €2.50, but there _are_ several possible
> combinations that _exceed_ the total.
>
> To solve this problem, Alice selects _one_ of these combinations: **three**
> _one-euro_ coins. She uses the coins to make the payment, and then receives
> **one** 50-cent coin as change.

##### Coin Selection in Cardano

Similarly to how a physical wallet holds value in the form of _unspent coins
and banknotes_, a Cardano wallet holds value in the form of _unspent
transaction outputs_. An [unspent transaction
output](#unspent-transaction-output) is the result of a previous transaction
that transferred money to the wallet, where the value has not yet been spent by
another transaction. Each unspent transaction output has an associated [coin
value](#coin-value), and the total value of a wallet is the _sum_ of these coin
values. Collectively, the set of unspent transaction outputs is known as the
[UTxO set](#utxo-set).

When using a Cardano wallet to make a payment, the wallet software must select
a combination of unspent outputs from the wallet's [UTxO set](#utxo-set), so
that the total value of selected outputs is enough to cover the target amount.

Just as with physical coins and notes, unspent outputs from the UTxO set
_cannot_ be subdivided, and must either be spent completely in a given
transaction, or not be spent at all. Similarly to a transaction with physical
money, the wallet software must select a combination of unspent outputs whose
total value is _greater_ than the target amount, and then arrange that _change_
is paid back to the wallet.

Coin selection refers to the process of selecting a combination of unspent
outputs from a wallet's [UTxO set](#utxo-set) in order to make one or more
payments, and computing the set of change to be paid back to the wallet.

#### Why is Coin Selection Non-Trivial?

There are a number of issues which make the problem of coin selection more
complicated than would initially appear.

##### The Transaction Size Limitation

Each [transaction](#transaction) has a _maximum size_, as defined by the
protocol. The size of a transaction increases as we add more
[inputs](#transaction-input) or [outputs](#transaction-output).

Therefore, there's a practical limit on the number of coins we can select for
any given transaction.

##### The Problem of Dust

One simple strategy for *selecting coins* might be to mimic what we do when
making a payment with coins and banknotes in the physical world. By giving the
recipient an amount that's as close as possible to the amount they're expecting,
we can minimize the amount of change they need to return to us.

However, trying to replicate this strategy with a UTxO-based wallet has an
undesirable effect: minimizing the total value of selected coins will also
minimize the value of change returned to the wallet. When repeated over time,
this strategy will tend to cause an accumulation of tiny outputs in the
wallet's [UTxO set](#utxo-set) known as [**dust**](#dust-output).

Dust outputs are a problem, because even if the total value of dust in a wallet
is more than enough to cover a given target amount, if we attempt to include
that dust in a given transaction, we may run out of space (by reaching the
[transaction size limit](#the-transaction-size-limitation)) before we can cover
the target amount.

For more information on dust avoidance, see [Self Organisation in Coin
Selection](#self-organisation-in-coin-selection).

##### The Problem of Concurrency

One simple strategy for *generating change* might be to mimic what a shop
assistant does when accepting a payment in the real world, which is to minimize
the *number* of coins and banknotes that they return to the customer.  This is
beneficial for the shop, as it reduces the chances of them running out of
change, and beneficial for the customer, as it reduces the amount of change
that they have to carry around in their wallet.

Analogously, when generating change for a UTxO-based wallet, we might be
tempted to use the simple strategy of just creating a single [change
output](#change-output) with the exact excess value.

However, using this strategy has an undesirable effect: the repeated act of
minimizing the number of change outputs will tend (over time) to reduce the
number of entries in a wallet's [UTxO set](#utxo-set). This is bad for two
reasons:

1.  Having a small [UTxO set](#utxo-set) limits the number of future payments
    that we can make in parallel.

2.  The approach of coalescing all change into a single output is widely
    considered to have negative privacy implications.

#### Goals of Coin Selection Algorithms

In light of the issues described above, we'd ideally like for our coin selection
algorithms to be able to:

  * limit, over the course of time, the amount of [dust](#dust-output) that
    accumulates in the [UTxO set](#utxo-set).

  * maintain, over the course of time, a [UTxO set](#utxo-set) with _useful_
    outputs: that is, outputs that allow us to process future payments with a
    reasonably small number of [inputs](#transaction-input).

## Specification

### Structure

The [Background](#background) section introduces the fundamental concepts
behind coin selection, provides a discussion of why coin selection is
a non-trivial problem, and describes the goals of coin selection algorithms.

The [Interface](#interface) section gives a description of the common interface
unifying all coin selection algorithms used within Cardano Wallet, the standard
parameter types, result types, and error types used by this interface, and a
description of the properties that all conforming implementations are expected
to satisfy.

The [Algorithms](#algorithms) section gives detailed descriptions of each of
the individual coin selection algorithms used in Cardano Wallet, along with
step-by-step descriptions of the computations involved.

The [Reference Implementations](#reference-implementations) section provides
links to reference implementations of each algorithm in various languages.

### Contents

* [Abstract](#abstract)
* [Motivation](#motivation-why-is-this-cip-necessary)
  * [Scope](#scope)
  * [Background](#background)
    * [What is Coin Selection?](#what-is-coin-selection)
      * [Coin Selection in the Physical World](#coin-selection-in-the-physical-world)
      * [Coin Selection in Cardano](#coin-selection-in-cardano)
    * [Why is Coin Selection Non-Trivial?](#why-is-coin-selection-non-trivial)
      * [The Transaction Size Limitation](#the-transaction-size-limitation)
      * [The Problem of Dust](#the-problem-of-dust)
      * [The Problem of Concurrency](#the-problem-of-concurrency)
    * [Goals of Coin Selection Algorithms](#goals-of-coin-selection-algorithms)
* [Specification](#specification)
  * [Structure](#structure)
  * [Definitions](#definitions)
    * [Address](#address)
    * [Coin Value](#coin-value)
    * [Transaction](#transaction)
    * [Transaction Input](#transaction-input)
    * [Transaction Output](#transaction-output)
    * [Spent Transaction Output](#spent-transaction-output)
    * [Unspent Transaction Output](#unspent-transaction-output)
    * [UTxO Set](#utxo-set)
    * [Change Output](#change-output)
    * [Dust Output](#dust-output)
  * [Interface](#interface)
    * [Parameters](#parameters)
      * [Requested Output Set](#requested-output-set)
      * [Initial UTxO Set](#initial-utxo-set)
      * [Maximum Input Count](#maximum-input-count)
    * [Results](#results)
      * [Coin Selection](#coin-selection)
      * [Remaining UTxO Set](#remaining-utxo-set)
    * [Properties](#properties)
      * [Coverage of Payments](#coverage-of-payments)
      * [Correctness of Change](#correctness-of-change)
      * [Conservation of UTxO](#conservation-of-utxo)
      * [Conservation of Outputs](#conservation-of-outputs)
    * [Failure Modes](#failure-modes)
      * [UTxO Balance Insufficient](#utxo-balance-insufficient)
      * [UTxO Not Fragmented Enough](#utxo-not-fragmented-enough)
      * [UTxO Fully Depleted](#utxo-fully-depleted)
      * [Maximum Input Count Exceeded](#maximum-input-count-exceeded)
  * [Algorithms](#algorithms)
    * [Largest-First](#largest-first)
      * [State](#state)
        * [Available UTxO List](#available-utxo-list)
        * [Unpaid Output List](#unpaid-output-list)
        * [Accumulated Coin Selection](#accumulated-coin-selection)
      * [Computation](#computation)
    * [Random-Improve](#random-improve)
      * [Cardinality](#cardinality-1)
      * [State](#state-1)
        * [Available UTxO Set](#available-utxo-set)
        * [Accumulated Coin Selection](#accumulated-coin-selection-1)
      * [Computation](#computation-1)
        * [Phase 1: Random Selection](#phase-1-random-selection)
        * [Phase 2: Improvement](#phase-2-improvement)
      * [Termination](#termination-1)
* [Rationale: how does this CIP achieve its goals?](#rationale-how-does-this-cip-achieve-its-goals)
  * [Motivating Principles](#motivating-principles)
    * [LargestFirst](#largest-first-2)
    * [Random-Improve](#random-improve-2)
      * [Principle 1: Dust Management](#principle-1-dust-management)
      * [Principle 2: Change Management](#principle-2-change-management)
      * [Principle 3: Performance Management](#principle-3-performance-management)
  * [External Resources](#external-resources)
    * [Self Organisation in Coin Selection](#self-organisation-in-coin-selection)
* [Path to Active](#path-to-active)
  * [Acceptance Criteria](#acceptance-criteria)
  * [Implementation Plan](#implementation-plan)
    * [Reference Implementations](#reference-implementations)
      * [Largest-First](#largest-first-1)
      * [Random-Improve](#random-improve-1)
* [Copyright](#copyright)

### Definitions

This section defines common terms that are used throughout this document.

#### Address

An _address_ is a unique identifier that represents a payment recipient, a
destination for a payment.

Addresses are typically owned (and generated) by individual wallets.

In general, coin selection algorithms are agnostic to the type of addresses
used to identify payment recipients. Any address type may be used, so long as
the set of possible addresses is totally-ordered.

#### Coin Value

A _coin value_ is a non-negative integer value that represents a number of
[Lovelace](https://cardanodocs.com/cardano/monetary-policy/).

One [Ada](https://cardanodocs.com/cardano/monetary-policy/) is _exactly_ equal
to one million Lovelace.

#### Transaction

In a [UTxO](#utxo-set)-based blockchain, a _transaction_ is a binding between
[inputs](#transaction-input) and [outputs](#transaction-output).

```
input #1  >---+          +---> output #1
               \        /
input #2  >-----+------+
               /        \
input #3  >---+          +---> output #2
```

#### Transaction Input

A _transaction input_ is a _unique reference_ to a single
[output](#transaction-output) from a previous transaction.

In general, coin selection algorithms are agnostic to the type of references
used to identify outputs from previous transactions. Any type may be used, so
long as the set of possible references is totally-ordered, and so long as it is
possible to determine the [coin value](#coin-value) associated with any given
reference.

In the case of Cardano and other UTxO-based blockchains, this reference
generally consists of a pair of values (**_h_**, **_n_**), where:

  * **_h_** is a _unique identifier_ for an existing transaction **_t_**;
  * **_n_** is a 0-based integer index into the output list of transaction
    **_t_**.

#### Transaction Output

A _transaction output_ consists of a pair of values (**_a_**, **_v_**), where:

  * **_a_** is the [address](#address) of a recipient.
  * **_v_** is the [coin value](#coin-value) to pay to the recipient.

#### Spent Transaction Output

A _spent transaction output_ is an [output](#transaction-output) from an
existing [transaction](#transaction) that has already been referenced as an
[input](#transaction-input) within a later transaction on the blockchain.

In effect, the [coin value](#coin-value) associated with that transaction
output has been _spent_, and cannot be reused.

#### Unspent Transaction Output

An _unspent transaction output_ is an [output](#transaction-output) from an
existing [transaction](#transaction) that has not yet been referenced as an
[input](#transaction-input) within a later transaction.

In effect, the [coin value](#coin-value) associated with that transaction
output has _not yet_ been spent, and is still available.

#### UTxO Set

A _UTxO set_ is a set of [unspent transaction
outputs](#unspent-transaction-output).

This term is commonly used in two ways:

  * To describe the _complete set_ of all unspent transaction outputs within a
    _blockchain_.

  * To describe the _subset_ of unspent transaction outputs associated with
    a _wallet_. The UTxO set of a wallet represents the total unspent value
    associated with that wallet.

From the point of view of a coin selection algorithm, each member of a UTxO set
can be represented as a pair of the form (**_u_**, **_v_**), where:

  * **_u_** is a unique reference to an
    [unspent output](#unspent-transaction-output) from a previous transaction.
  * **_v_** is the [coin value](#coin-value) associated with **_u_**.

In general, coin selection algorithms are agnostic to the type of references
used to identify unspent outputs from previous transactions. Any type may be
used, so long as the set of possible references is totally-ordered.

In practice however, the type of each unique reference **_u_** is equivalent
to the type of a [transaction input](#transaction-input), as transaction inputs
are simply references to unspent outputs from previous transactions.

#### Change Output

In the context of a wallet, a _change output_ is a [transaction
output](#transaction-output) that transfers value _back_ to the wallet, rather
than to an external payment recipient. The [address](#address) associated with
a change output is generated by the wallet, and belongs to the wallet.

Change ouputs are necessary in a UTxO-based blockchain, as the value associated
with any given [transaction input](#transaction-input) must be spent _entirely_
by the transaction that includes it.

When selecting entries from a [UTxO set](#utxo-set) to include as inputs in a
transaction, a coin selection algorithm will generally not be able to select
inputs that precisely match the total value of all payments to external
recipients, and will therefore need to select more than is strictly required.
To avoid the destruction of value, selection algorithms create _change outputs_
to return the excess value back to the wallet.

#### Dust Output

A _dust output_ is a [transaction output](#transaction-output) with an
associated [coin value](#coin-value) that is:

  * small in comparison to payments typically made by the user of the wallet;
  * small in comparison to the marginal fee associated with including it in
    a transaction.

Dust outputs are a problem, because even if the total value of dust in a wallet
is more than enough to cover a given payment amount, if we attempt to include
that dust in a given transaction, we may run out of space (by reaching the
[transaction size limit](#the-transaction-size-limitation)) before we can cover
the target amount.

### Interface

All coin selection algorithms used by Cardano Wallet implement a
_common interface_.

At the most fundamental level, a coin selection algorithm is a _mathematical
function_ that when applied to a [requested output
set](#requested-output-set) and an [initial UTxO set](#initial-utxo-set),
will produce a [coin selection](#coin-selection): the basis for a
[transaction](#transaction) in a UTxO-based blockchain.

This section describes:

  * the [parameters](#parameters) accepted by all coin selection algorithms;
  * the [results](#results) they produce when successful;
  * the [error conditions](#failure-modes) that may occur on failure;
  * the [properties](#properties) that apply to all coin selection
    algorithms: mathematical laws governing the relationships between parameters
    and results.

In this section, the terms _coin selection algorithm_ and _coin selection
function_ will be used interchangeably.

#### Parameters

All coin selection functions accept the following parameters:

 1. **Requested Output Set**

    A list of payments to be made to recipient addresses, encoded as a list of
    [transaction outputs](#transaction-output).

 2. **Initial UTxO Set**

    A [UTxO set](#utxo-set) from which the coin selection algorithm can select
    entries, to cover payments listed in the [requested output
    set](#requested-output-set).

    In the context of a wallet, this parameter would normally be assigned with
    the wallet's complete [UTxO set](#utxo-set), giving the coin selection
    algorithm access to the total value associated with that wallet.

 3. **Maximum Input Count**

    An _upper bound_ on the number of UTxO entries that the coin selection
    algorithm is permitted to select from the [initial UTxO
    set](#initial-utxo-set).

    This parameter is necessary for blockchains that impose an upper limit on
    the size of transactions.

#### Results

All coin selection functions produce the following result values:

 1. **Coin Selection**

    A _coin selection_ is the basis for a [transaction](#transaction) in a
    UTxO-based blockchain.

    It is a record with three fields:

      * A set of **_inputs_**, equivalent to a subset of the
        [initial UTxO set](#initial-utxo-set).

        From the point of view of a _wallet_, this represents the value that
        has been selected from the wallet in order to cover the total payment
        value.

      * A set of **_outputs_** (see [transaction output](#transaction-output)).

        Represents the set of payments to be made to recipient addresses.

      * A set of **_change values_** (see [change output](#change-output)),
        where each change value is simply a [coin value](#coin-value).

        From the point of view of a _wallet_, this represents the change to be
        returned to the wallet.

 2. **Remaining UTxO Set**

    The _remaining UTxO set_ is a subset of the [initial UTxO
    set](#initial-utxo-set).

    It represents the set of values that remain after the coin selection
    algorithm has removed values to pay for entries in the [requested output
    set](#requested-output-set).

    In the context of a _wallet_, if a coin selection algorithm is applied to
    the wallet's _complete_ UTxO set, then the _remaining_ UTxO set represents
    the _updated_ UTxO set of that wallet.

#### Properties

All coin selection algorithms satisfy a common set of _properties_: general
rules that govern the relationship between the _parameters_ supplied to coin
selection functions and the _results_ they are allowed to produce.

##### Coverage of Payments

This property states that the total value of _inputs_ in the resulting [coin
selection](#coin-selection) result is sufficient to _cover_ the total value of
the [requested output set](#requested-output-set).

In particular:

  * **_v_**<sub>selected</sub> ≥ **_v_**<sub>requested</sub>

Where:

  * **_v_**<sub>requested</sub>

    is the total value of the [requested output set](#requested-output-set)


  * **_v_**<sub>selected</sub>

    is the total value of the _inputs_ field of the [coin
    selection](#coin-selection) result.

##### Correctness of Change

This property states that the correct amount of _change_ was generated.

In particular:

  * **_v_**<sub>selected</sub>
  = **_v_**<sub>requested</sub> + **_v_**<sub>change</sub>

Where:

  * **_v_**<sub>change</sub>

    is the total value of the _change_ field of the [coin
    selection](#coin-selection) result.

  * **_v_**<sub>requested</sub>

    is the total value of the [requested output set](#requested-output-set)

  * **_v_**<sub>selected</sub>

    is the total value of the _inputs_ field of the [coin
    selection](#coin-selection) result.

##### Conservation of UTxO

This property states that every entry in the [initial UTxO
set](#initial-utxo-set) is included in _either_ the inputs set of the generated
[coin selection](#coin-selection), _or_ in the [remaining UTxO
set](#remaining-utxo-set), but _not both_.

  * If a UTxO entry _is_ selected by the coin selection algorithm, it is
    included in the [coin selection](#coin-selection) inputs set.

  * If a UTxO entry is _not_ selected by the coin selection algorithm, it is
    included in the [remaining UTxO set](#remaining-utxo-set).

The following laws hold:

  * **_U_**<sub>initial  </sub> ⊃ **_U_**<sub>remaining</sub>
  * **_U_**<sub>initial  </sub> ⊇ **_U_**<sub>selected </sub>

And:

  * **_U_**<sub>remaining</sub> ∩ **_U_**<sub>selected </sub> = ∅
  * **_U_**<sub>remaining</sub> ⋃ **_U_**<sub>selected </sub> =
    **_U_**<sub>initial  </sub>

Where:

  * **_U_**<sub>initial</sub>

    is the [initial UTxO set](#initial-utxo-set).

  * **_U_**<sub>remaining</sub>

    is the [remaining UTxO set](#remaining-utxo-set).

  * **_U_**<sub>selected</sub>

    is the value of the _inputs_ field of the [coin selection](#coin-selection)
    result.

##### Conservation of Outputs

This property states that the [requested output set](#requested-output-set)
is _conserved_ in the [coin selection](#coin-selection) result.

In particular, the _outputs_ field of the [coin selection](#coin-selection)
result should be _equal to_ the [requested output set](#requested-output-set).

#### Failure Modes

There are a number of ways in which a coin selection algorithm can fail:

  * **UTxO Balance Insufficient**

    This failure occurs when the total value of the entries within the [initial
    UTxO set](#initial-utxo-set) (the amount of money _available_) is _less
    than_ the the total value of all entries in the [requested output
    set](#requested-output-set) (the amount of money _required_).

  * **UTxO Not Fragmented Enough**

    This failure occurs when the _number_ of entries in the [initial UTxO
    set](#initial-utxo-set) is _smaller than_ the number of entries in the
    [requested output set](#requested-output-set), for algorithms that impose
    the restriction that a single UTxO entry can only be used to pay for _at
    most one_ output.

  * **UTxO Fully Depleted**

    This failure occurs if the algorithm depletes all entries from the [initial
    UTxO set](#initial-utxo-set) _before_ it is able to pay for all outputs in
    the [requested output set](#requested-output-set).

    This can happen _even if_ the total value of entries within the [initial
    UTxO set](#initial-utxo-set) is _greater than_ the total value of all
    entries in the [requested output set](#requested-output-set), due to
    various restrictions that coin selection algorithms impose on themselves
    when selecting UTxO entries.

  * **Maximum Input Count Exceeded**

    This failure occurs when another input must be selected by the algorithm in
    order to continue making progress, but doing so will increase the size of
    the resulting selection beyond an acceptable limit, specified by the
    [maximum input count](#maximum-input-count) parameter.

### Algorithms

This section describes the coin selection algorithms used by Cardano Wallet,
along with step-by-step descriptions of the computations involved.

All algorithms implement a _common interface_, as described in the
[Interface](#interface) section.

There are two main algorithms used by Cardano Wallet:

  * [Largest-First](#largest-first)
  * [Random-Improve](#random-improve)

In general, Cardano Wallet gives _priority_ to the
[Random-Improve](#random-improve) algorithm, as experimental evidence shows
that it performs better at [minimising dust](#goals) and maintaining a UTxO set
with [useful outputs](#goals). (See [Self Organisation in Coin
Selection](#self-organisation-in-coin-selection) for more details.)

However, in rare cases, the [Random-Improve](#random-improve) algorithm may
fail to produce a result. In such cases, Cardano Wallet will fall back to the
[Largest-First](#largest-first) algorithm.

#### Largest-First

The **Largest-First** coin selection algorithm considers UTxO set entries
in _descending order of value_, from largest to smallest.

When applied to a set of [requested outputs](#requested-output-set), the
algorithm repeatedly selects entries from the [initial UTxO
set](#initial-utxo-set) until the total value of selected entries is _greater
than or equal to_ the total value of requested outputs.

The name of the algorithm is taken from the idea that the **largest** UTxO
entry is always selected **first**. Specifically:

> A given UTxO entry **_u_<sub>1</sub>** with
> value **_v_<sub>1</sub>** can be selected if and only if there is no other
> unselected entry **_u_<sub>2</sub>** with value **_v_<sub>2</sub>** where
> **_v_<sub>2</sub>** > **_v_<sub>1</sub>**.

##### State

At all stages of processing, the algorithm maintains the following pieces of
state:

 1. **Available UTxO List**

    This is initially equal to the [initial UTxO set](#initial-utxo-set),
    sorted into _descending order of coin value_.

    The _head_ of the list is always the remaining UTxO entry with the _largest
    coin value_.

    Entries are incrementally removed from the _head_ of the list as the
    algorithm proceeds, until enough value has been selected.

 2. **Selected UTxO Set**

    This is initially equal to the empty set.

##### Computation

The algorithm proceeds according to the following sequence of steps:

  * **Step 1**

    If the [available UTxO list](#available-utxo-list) is _empty_:

      * Terminate with a [UTxO Balance
        Insufficient](#utxo-balance-insufficient) error.

    If the [available UTxO list](#available-utxo-list) is _not empty_:

      * Remove an UTxO entry from the head of the [available UTxO
        list](#available-utxo-list) and add it to the [selected UTxO
        set](#selected-utxo-set).

  * **Step 2**

    Compare the total size **_n_**<sub>selected</sub> of the [selected UTxO
    set](#selected-utxo-set) with the [maximum input
    count](#maximum-input-count) **_n_**<sub>max</sub>.

      * If **_n_**<sub>selected</sub> > **_n_**<sub>max</sub> then:

        * Terminate with a [Maximum Input Count
          Exceeded](#maximum-input-count-exceeded) error.

      * If **_n_**<sub>selected</sub> ≤ **_n_**<sub>max</sub> then:

        * Go to step 3.

  * **Step 3**

    Compare the total value **_v_**<sub>selected</sub> of the [selected UTxO
    set](#selected-utxo-set) to the total value **_v_**<sub>requested</sub> of
    the [requested output set](#requested-output-set):

      * If **_v_**<sub>selected</sub> < **_v_**<sub>requested</sub> then go to
        step 1.
      * If **_v_**<sub>selected</sub> ≥ **_v_**<sub>requested</sub> then go to
        step 4.

  * **Step 4**

    Return a [coin selection](#coin-selection) result where:

      * The _inputs_ set is equal to the [selected UTxO
        set](#selected-utxo-set).

      * The _outputs_ set is equal to the [requested output
        set](#requested-output-set).

      * If **_v_**<sub>selected</sub> > **_v_**<sub>requested</sub> then:

        * The _change_ set contains just a single [coin](#coin-value) of value
          (**_v_**<sub>selected</sub> − **_v_**<sub>requested</sub>).

      * If **_v_**<sub>selected</sub> = **_v_**<sub>requested</sub> then:

        * The _change_ set is empty.

#### Random-Improve

The **Random-Improve** coin selection algorithm works in _two phases_:

  * In the **first phase**, the algorithm iterates through each of the
    [requested outputs](#requested-output-set) in _descending order of coin
    value_, from _largest_ to _smallest_. For each output, the algorithm
    repeatedly selects entries _at random_ from the [initial UTxO
    set](#initial-utxo-set), until each requested output has been associated
    with a set of UTxO entries whose _total value_ is enough to pay for that
    ouput.

  * In the **second phase**, the algorithm attempts to _expand_ each
    existing UTxO selection with _additional_ values taken at random from the
    [initial UTxO set](#initial-utxo-set), to the point where the total value
    of each selection is as close as possible to _twice_ the value of its
    associated output.

After the above phases are complete, for each output of value
**_v_**<sub>output</sub> and accompanying UTxO selection of value
**_v_**<sub>selection</sub>, the algorithm generates a _single_ change output
of value **_v_**<sub>change</sub>, where:

> **_v_**<sub>change</sub>
>   = **_v_**<sub>selection</sub>
>   − **_v_**<sub>output</sub>

Since the goal of the second phase was to expand each selection to the point
where its total value is _approximately twice_ the value of its associated
output, this corresponds to a change output whose target value is
_approximately equal_ to the value of the output itself:

> **_v_**<sub>change</sub>
>   = **_v_**<sub>selection</sub>
>   − **_v_**<sub>output</sub>
>
> **_v_**<sub>change</sub>
>   ≈ <span>2</span>**_v_**<sub>output</sub>
>   − **_v_**<sub>output</sub>
>
> **_v_**<sub>change</sub>
>   ≈ **_v_**<sub>output</sub>

##### Cardinality

The Random-Improve algorithm imposes the following cardinality restriction:

  * Each entry from the [initial UTxO set](#initial-utxo-set) is used to pay
    for _at most one_ output from the [requested output
    set](#requested-output-set).

As a result of this restriction, the algorithm will fail with a [UTxO Not
Fragmented Enough](#utxo-not-fragmented-enough) error if the number of entries
in the [initial UTxO set](#initial-utxo-set) is _smaller than_ the number of
entries in the [requested output set](#requested-output-set).

##### State

At all stages of processing, the algorithm maintains the following pieces of
state:

 1. **Available UTxO Set**

    This is initially equal to the [initial UTxO set](#initial-utxo-set).

 2. **Accumulated Coin Selection**

    The accumulated coin selection is a [coin selection](#coin-selection) where
    all fields are initially equal to the _empty set_.

##### Computation

The algorithm proceeds in two phases.

- **Phase 1: Random Selection**

In this phase, the algorithm iterates through each of the [requested
outputs](#requested-output-set) in descending order of coin value, from
largest to smallest.

For each output of value **_v_**, the algorithm repeatedly selects entries at
**random** from the [available UTxO set](#available-utxo-set), until the _total
value_ of selected entries is greater than or equal to **_v_**. The selected
entries are then _associated with_ that output, and _removed_ from the
[available UTxO set](#available-utxo-set).

This phase ends when _every_ output has been associated with a selection of
UTxO entries.

- **Phase 2: Improvement**

In this phase, the algorithm attempts to improve upon each of the UTxO
selections made in the previous phase, by conservatively expanding the
selection made for each output in order to generate improved change
values.

During this phase, the algorithm:

  * processes outputs in _ascending order of coin value_.

  * continues to select values from the [available UTxO
    set](#available-utxo-set).

  * incrementally populates the
    [accumulated coin selection](#accumulated-coin-selection-1).

For each output of value **_v_**, the algorithm:

 1.  **Calculates a _target range_** for the total value of inputs used to
     pay for that output, defined by the triplet:

     (_minimum_, _ideal_, _maximum_) =
     (**_v_**, <span>2</span>**_v_**, <span>3</span>**_v_**)

 2.  **Attempts to improve upon the existing UTxO selection** for that output,
     by repeatedly selecting additional entries at random from the [available
     UTxO set](#available-utxo-set), stopping when the selection can be
     improved upon no further.

     A selection with value **_v_<sub>1</sub>** is considered to be an
     _improvement_ over a selection with value **_v_<sub>0</sub>** if **all**
     of the following conditions are satisfied:

      * **Condition 1**: we have moved closer to the _ideal_ value:

        abs (_ideal_ − **_v_<sub>1</sub>**) <
        abs (_ideal_ − **_v_<sub>0</sub>**)

      * **Condition 2**: we have not exceeded the _maximum_ value:

        **_v_<sub>1</sub>** ≤ _maximum_

      * **Condition 3**: when counting cumulatively across all outputs
        considered so far, we have not selected more than the _maximum_ number
        of UTxO entries specified by [Maximum Input
        Count](#maximum-input-count).

 3.  **Creates a _change value_** for the output, equal to the total value
     of the _improved UTxO selection_ for that output minus the value **_v_**
     of that output.

 4.  **Updates the [accumulated coin
     selection](#accumulated-coin-selection-1)**:

      * Adds the _output_ to the _outputs_ field;
      * Adds the _improved UTxO selection_ to the _inputs_ field;
      * Adds the _change value_ to the _change values_ field.

This phase ends when every output has been processed, **or** when the
[available UTxO set](#available-utxo-set) has been exhausted, whichever occurs
sooner.

##### Termination

When both phases are complete, the algorithm terminates.

The [accumulated coin selection](#accumulated-coin-selection-1) is returned
to the caller as the [coin selection](#coin-selection) result.

The [available UTxO set](#available-utxo-set) is returned to the caller as the
[remaining UTxO set](#remaining-utxo-set) result.

## Rationale

### Largest-First

\-

### Random-Improve

There are several motivating principles behind the design of the algorithm.

#### Principle 1: Dust Management

The probability that random selection will choose dust entries from a UTxO
set _increases_ with the proportion of dust in the set.

Therefore, for a UTxO set with a large amount of dust, there's a high
probability that a random subset will include a large amount of dust.

Over time, selecting entries randomly in this way will tend to _limit_ the
amount of dust that accumulates in the UTxO set.

#### Principle 2: Change Management

As mentioned in the [Goals](#goals-of-coin-selection-algorithms) section, it is
desirable that coin selection algorithms, over time, are able to create UTxO
sets that have _useful_ outputs: outputs that will allow us to process future
payments with a _reasonably small_ number of inputs.

If for each payment request of value **_v_** we create a change output of
_roughly_ the same value **_v_**, then we will end up with a distribution of
change values that matches the typical value distribution of payment
requests.

> :bulb: **Example**
>
> Alice often buys bread and other similar items that cost around €1.00 each.
>
> When she instructs her wallet software to make a payment for around
> €1.00, the software attempts to select a set of unspent transaction outputs
> with a total value of around €2.00.
>
> As she frequently makes payments for similar amounts, transactions created by
> her wallet will also frequently produce change coins of around €1.00 in value.
>
> Over time, her wallet will self-organize to contain multiple coins of around
> €1.00, which are useful for the kinds of payments that Alice frequently makes.

#### Principle 3: Performance Management

Searching the UTxO set for additional entries to _improve_ our change outputs
is _only_ useful if the UTxO set contains entries that are sufficiently
small enough. But it is precisely when the UTxO set contains many small
entries that it is less likely for a randomly-chosen UTxO entry to push the
total above the upper bound.

### External Resources

#### Self Organisation in Coin Selection

> | **Title** | Self Organisation in Coin Selection |
> |:--|:--|
> | **Author** | [Edsko de Vries](http://www.edsko.net/) |
> | **Year** | 2018 |
> | **Location** | https://iohk.io/en/blog/posts/2018/07/03/self-organisation-in-coin-selection/ |
>
> This article introduces the [Random-Improve](#random-improve) coin selection
> algorithm, invented by [Edsko de Vries](http://www.edsko.net/).
>
> It describes the three principles of self-organisation that inform the
> algorithm's design, and provides experimental evidence to demonstrate the
> algorithm's effectiveness at maintaining healthy UTxO sets over time.

## Path to Active

### Acceptance Criteria

- [x] There exists one or more reference implementations with appropriate testing illustrating the various properties of coin-selection stated in this document.

### Implementation Plan

#### Reference Implementations

##### Largest-First

Reference implementations of the [Largest-First](#largest-first) algorithm are
available in the following languages:

| _Language_ | _Documentation_ | _Source_ |
| -- | -- | -- |
| **Haskell** | [Documentation](https://hackage.haskell.org/package/cardano-coin-selection/docs/Cardano-CoinSelection-Algorithm-LargestFirst.html) | [Source](https://hackage.haskell.org/package/cardano-coin-selection/docs/src/Cardano.CoinSelection.Algorithm.LargestFirst.html) |

##### Random-Improve

Reference implementations of the [Random-Improve](#random-improve) algorithm
are available in the following languages:

| _Language_ | _Documentation_ | _Source_ |
| -- | -- | -- |
| **Haskell** | [Documentation](https://hackage.haskell.org/package/cardano-coin-selection/docs/Cardano-CoinSelection-Algorithm-RandomImprove.html) | [Source](https://hackage.haskell.org/package/cardano-coin-selection/docs/src/Cardano.CoinSelection.Algorithm.RandomImprove.html) |

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
CIP: 3
Title: Wallet Key Generation
Status: Active
Category: Wallets
Authors:
    - Matthias Benkort <matthias.benkort@cardanofoundation.org>
    - Sebastien Guillemot <seba@dcspark.io>
Implementors:
    - Matthias Benkort <matthias.benkort@cardanofoundation.org>
    - Sebastien Guillemot <seba@dcspark.io>
Discussions:
    - https://github.com/input-output-hk/implementation-decisions/pull/18
    - https://github.com/cardano-foundation/cips/pull/33
    - https://github.com/cardano-foundation/cips/pull/76
    - https://github.com/cardano-foundation/cips/pull/132
Created: 2020-05-07
License: CC-BY-4.0
---

## Abstract

Many wallets utilize some way of mapping a sentence of words (easy to read and write for humans) uniquely back and forth to a sized binary data (harder to remember).

This document outlines the various mapping algorithms used in the Cardano ecosystem.

## Motivation: why is this CIP necessary?

The philosophy of cryptocurrencies is that you are in charge of your own finances. Therefore, it is very anti-thematic for wallet software to lock in a user by explicitly describing the algorithm used to derive keys for a wallet (both the master key and key derivation)

To this end, this document outlines all the relevant key generation algorithms used in the Cardano ecosystem.

## Specification

### Recovery Phrase (mnemonic) Generation

Conversion from a recovery phrase to entropy is the same as described in [BIP39](https://github.com/bitcoin/bips/blob/master/bip-0039/bip-0039-wordlists.md).

### Hierarchical Deterministic Wallets

In Cardano, hierarchical deterministic (abbrev. HD) wallets are similar to those described in [BIP-0032](https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki). Notably, we use a variation called [ED25519-BIP32](https://github.com/input-output-hk/adrestia/raw/bdf00e4e7791d610d273d227be877bc6dd0dbcfb/user-guide/static/Ed25519_BIP.pdf). A reference implementation can be found [here](https://docs.rs/ed25519-bip32/).

### Master Key Generation

The master key generation is the mean by which on turns an initial entropy into a secure cryptographic key.

More specifically, the generation is a function from an initial seed to an extended private key (abbrev. XPrv) composed of:

- 64 bytes: an extended Ed25519 secret key composed of:
  - 32 bytes: Ed25519 curve scalar from which few bits have been tweaked according to ED25519-BIP32
  - 32 bytes: Ed25519 binary blob used as IV for signing
- 32 bytes: chain code for allowing secure child key derivation

#### History

Throughout the years, Cardano has used different styles of master key generation:

| Name                                    | Used by         | Address prefix in Byron | Is deprecated? | Is Recommended? |
|-----------------------------------------|-----------------|-------------------------|----------------|-----------------|
| [Byron](./Byron.md)                     | Daedalus        | Ddz                     | Yes            | No              |
| [Icarus](./Icarus.md)                   | Yoroi, Daedalus | Ae2                     | No             | Yes             |
| [Icarus-Trezor](./Icarus.md)            | Trezor          | Ae2                     | No             | No              |
| [Ledger/BitBox02](./Ledger_BitBox02.md) | Ledger/BitBox02 | Ae2                     | No             | No              |

## Rationale: how does this CIP achieve its goals?

This CIP is merely to document the existing standards and not to provide rationales for the various methods used.

However, you can learn more at the following links:

- [Adrestia documentation](https://cardano-foundation.github.io/cardano-wallet/concepts/master-key-generation)
- [SLIP-0010](https://github.com/satoshilabs/slips/blob/master/slip-0010.md)
- [SLIP-0023](https://github.com/satoshilabs/slips/blob/master/slip-0023.md)

## Path to Active

### Acceptance Criteria

- [x] Each generation method is documented and provides test vectors in a language-agnostic way.
- [x] There exists reference implementations in various languages for each method.
- [x] At least 2 Cardano wallets (e.g. Yoroi & Daedalus) implement these methods.

### Implementation Plan

- [x] Implementation of each algorithm will be carried out in Yoroi and Daedalus (via cardano-wallet) by Emurgo and Input Output respectively.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
CIP: 4
Title: Wallet Checksums
Status: Proposed
Category: Wallets
Authors:
  - Ruslan Dudin <ruslan@emurgo.io>
  - Sebastien Guillemot <seba@dcspark.io>
Implementors:
  - Ruslan Dudin <ruslan@emurgo.io>
  - Sebastien Guillemot <seba@dcspark.io>
Discussions:
  - https://forum.cardano.org/t/cip4-wallet-checksum/32819
  - https://github.com/cardano-foundation/CIPs/pull/4
Created: 2019-05-01
License: Apache-2.0
---

## Abstract

We introduce a checksum algorithm to help users verify they are restoring the right wallet before the restoration actually takes place.

## Motivation: why is this CIP necessary?

Users occasionally enter the wrong [mnemonic](https://github.com/bitcoin/bips/blob/master/bip-0039.mediawiki) for their wallet. In this case, they simply see a 0 ADA wallet after syncing is over. This not only wastes the user's time, in the worst case it makes them think they either lost all their ADA or think there is a bug in the wallet implementation.

To solve this, we introduce a checksum that can be computed without having to perform wallet restoration.

## Specification

First, it's important to note that the method for generating a checksum is heavily dependent on the type of wallet (ex: BIP44, etc.). We outline an algorithm that works with most, but not all, types of wallet.

### Requirements for checksum

1) Easily recomputed without access to mnemonic, private key or other similarly sensitive data
2) Does not reveal anything about the wallet (irreversible -- cannot tell addresses, private key, etc. from just seeing the checksum)
3) Negligible chance of collision
4) Easy to memorize for the user
5) Can be easily saved both digitally or on paper

### Implementation Outline

To satisfy (1), the checksum SHOULD be seeded from the public key for the wallet. Notably, in the [BIP44](https://github.com/bitcoin/bips/blob/master/bip-0044.mediawiki) case, it should come from the bip44 account derivation level's public key.
**Note**: For HD wallets, the public key used SHOULD contain the chaincode also because we need to make sure that not just the public key, but all its child keys also, are properly generated.

To satisfy (2) and (3), the a hash of the public key is used

To satisfy (4) and (5), we generate for an *ImagePart* and a *TextPart*. The brain can roughly remember images allowing you to quickly dismiss checksums that look totally different. However, since images can sometimes be similar, a *TextPart* is also provided for double-checking. Additionally, if the user does not have access to a printer, the text part can be easily written down by hand on a piece of paper to satisfy (5).

## Rationale: how does this CIP achieve its goals?

We first provide a template for the code, explain the template and then provide the parameterization we use for Cardano

```js
function calculateChecksum(publicKeyHash: string /* note: lowercase hex representation */) {
  const hash = hash1(publicKeyHash);
  const [a, b, c, d] = hash_to_32(hash); // get a 4 byte value from the hash
  const alpha = `ABCDEJHKLNOPSTXZ`; // take 16 letters from the alphabet that are easy to distinguish

  // construct the TextPart from a group of letters and a group of numbers
  const letters = x => `${alpha[Math.floor(x / 16)]}${alpha[x % 16]}`;
  const numbers = `${((c << 8) + d) % 10000}`.padStart(4, '0');
  const id = `${letters(a)}${letters(b)}-${numbers}`;

  return {
    hash, // used to generate the ImagePart
    id, // used as the TextPart
  };
}
```

### TextPart rationale

For ease of perception it seems that short alphanumeric sequences are the best for humans to remember, especially when letters and numbers are separated and not mixed together.

#### Letter part

For letters, we render the bytes in hex, but replace the alphanumeric used in hex with this letter-only alphabet:

`A B C D E J H K L N O P S T X Z`

This alphabet satisfies the following requirements:

1) Has exactly 16 letters (one-to-one mapping with 2 bytes in HEX)
1) Does not contain characters that look too much like each other
1) Minimizes occurrences of undesirable words in [this list](https://www.noswearing.com/fourletterwords.php).

#### Number part

The last two bytes are compressed to a 4-digit number. For this we will simply take the last 4 digits of the 16-bit integer number constructed from 2 bytes as `((A << 8) + B) % 10000` (zero-padded).

This above produces 10000 unique values across all possible values of A and B and giving maximum of 7 potential collisions per value and 6.5 average collisions per value, which is the minimum, given the fact that we reduce maximum potential number 65536 to 4 digits.
**Note**: resulting number is zero-padded to 4 digits.

### ImagePart rationale

For the image, we take the result of `hash1` and use it as the seed for the [blockies](https://github.com/ethereum/blockies) library.

This library in particular has the following benefits:

- Has been audited
- Used by other blockchains and therefore has common libraries for implementation

**Note**: This library internally re-hashes its input to a 128-bit entropy string

### Hash algorithms used in Byron + ITN

For `hash1`, we use `blake2b512`. [Blake2b](https://tools.ietf.org/html/rfc7693) is a standardized hash function that is used in Cardano for other purposes like key derivations. Reusing blake2b means one less dependency. We use `512` bytes of output to try and future-proof this algorithm (better to spread the entropy across more bits than needed than end up not capturing the full entropy in the future).

For `hash_to_32` we use CRC32. We hash a second time for the following:

1) The *TextPart* is constructed from 4 bytes (2 for letters, 2 for numbers) and so we need to project the result of `hash1` down to 4 bytes.
2) We don't want to simply take the last 4 bytes of `hash1` because that would reveal part of the input used to generate the *ImagePart*. Although strictly speaking this should not be of a concern (since the result of `hash1` doesn't reveal any information about the original key), we take this as a precaution.
3) `CRC32` is used in the Byron implementation of Cardano as a checksum for addresses, meaning no additional dependency has to be added.

Although there is no specification for CRC32 and many variations exist, in Cardano we use the CRC-32-IEEE variation. You can find a C implementation [here](https://github.com/cardano-foundation/ledger-app-cardano/blob/3f784d23c1b87df73cda552ef01428d3e2733237/src/crc32.c#L6)

### Hash algorithms used in Shelley mainnet

1) For `hash1`, we still use `blake2b512` but we now set the blake2b `personalization` to the the utf-8 byte equivalent of `wallets checksum` (exactly 16 utf-8 bytes in length) to avoid collision with any other standard that decides to hash a public key.
2) For `hash_to_32`, we no longer use `crc32` for the following reasons:

- It has multiple competing implementations all called `crc32` (easily to get the wrong implementation library)
- It requires building a lookup table, making it slower than other hashing algorithms for similar safety
- Cardano no longer uses `crc32` in the Shelley mainnet as addresses now use [BIP173 - bech32](https://github.com/bitcoin/bips/blob/master/bip-0173.mediawiki) which has its own checksum algorithm.

Instead, we replace it with [FNV-1a](https://tools.ietf.org/html/draft-eastlake-fnv-10) in 32-bit mode. FNV-1a is fast, easy to implement inline and gives good empirical distribution.

### When no public key is present

Note that a different construction is needed for wallet types which do not have a public key (such as a balance tracking application which simply manages a set of addresses). In the balanace tracking case, simply hashing the set of addresses used is possible, but it means that adding & removing an address would change the checksum (possibly unintuitive). Since the checksum is meant to represent the wallet itself, we also cannot run a checksum on the name of the wallet or any other user-inputted data.

## Path to Active

### Acceptance Criteria

- [x] There exists a reference implementation with test vectors.
- [ ] Checksums are adopted by two or more wallets.
  - [x] Yoroi

### Implementation Plan

- [x] Reference implementations:
  - [Javascript](https://github.com/Emurgo/CIP4)

## Copyright

This CIP is licensed under Apache-2.0

---
CIP: 5
Title: Common Bech32 Prefixes
Category: Tools
Status: Active
Authors:
  - Matthias Benkort <matthias.benkort@cardanofoundation.org>
Implementors: N/A
Discussions:
  - https://forum.cardano.org/t/cip5-common-bech32-prefixes/35189
  - https://github.com/cardano-foundation/CIPs/pull/427
  - https://github.com/cardano-foundation/CIPs/pull/342
  - https://github.com/cardano-foundation/CIPs/pull/251
  - https://github.com/cardano-foundation/CIPs/pull/177
  - https://github.com/cardano-foundation/CIPs/pull/31
Created: 2020-05-28
License: Apache-2.0
---

## Abstract

This CIP defines a set of common prefixes (or so-called human-readable part in the [bech32](https://github.com/bitcoin/bips/blob/master/bip-0173.mediawiki)) encoding format) for various bech32-encoded binary data across the Cardano eco-system.

## Motivation: why is this CIP necessary?

Many tools used within the Cardano eco-system are manipulating binary data. Binary data are typically encoded as hexadecimal text strings when shown in a user interface (might it be a console, a url or a structured document from a server). From the user perspective, it can be difficult to distinguish between various encoded data. From the tools developer perspective, it can also be difficult to validate inputs based only on raw bytes (in particular when encoded data often have the same length).

Therefore, we can leverage bech32 for binary data encoding, with a set of common prefixes that can be used across tools and software to disambiguate payloads.

## Specification

We define the following set of common prefixes with their corresponding semantic. Any software willing to represent binary data in a human-friendly way should abide by these guidelines. Should a data-type be missing, we encourage developers to update this CIP and register a new prefix.

### Keys

| Prefix             | Meaning                                                               | Contents                           |
| ---                | ---                                                                   | ---                                |
| `acct_sk`          | CIP-1852's account private key                                        | Ed25519 private key                |
| `acct_vk`          | CIP-1852's account public key                                         | Ed25519 public key                 |
| `acct_xsk`         | CIP-1852's extended account private key                               | Ed25519-bip32 extended private key |
| `acct_xvk`         | CIP-1852's extended account public key                                | Ed25519 public key with chain code |
| `acct_shared_sk`   | CIP-1854's account private key                                        | Ed25519 private key                |
| `acct_shared_vk`   | CIP-1854's account public key                                         | Ed25519 public key                 |
| `acct_shared_xsk`  | CIP-1854's extended account private key                               | Ed25519-bip32 extended private key |
| `acct_shared_xvk`  | CIP-1854's extended account public key                                | Ed25519 public key with chain code |
| `addr_sk`          | CIP-1852's address signing key                                        | Ed25519 private key                |
| `addr_vk`          | CIP-1852's address verification key                                   | Ed25519 public key                 |
| `addr_xsk`         | CIP-1852's address extended signing key                               | Ed25519-bip32 extended private key |
| `addr_xvk`         | CIP-1852's address extended verification key                          | Ed25519 public key with chain code |
| `addr_shared_sk`   | CIP-1854's address signing key                                        | Ed25519 private key                |
| `addr_shared_vk`   | CIP-1854's address verification key                                   | Ed25519 public key                 |
| `addr_shared_xsk`  | CIP-1854's address extended signing key                               | Ed25519-bip32 extended private key |
| `addr_shared_xvk`  | CIP-1854's address extended verification key                          | Ed25519 public key with chain code |
| `cc_cold_sk`       | CIP-1852’s constitutional committee cold signing key                  | Ed25519 private key                |
| `cc_cold_vk`       | CIP-1852’s constitutional committee verification signing key          | Ed25519 private key                |
| `cc_cold_xsk`      | CIP-1852’s constitutional committee cold extended signing key         | Ed25519-bip32 extended private key |
| `cc_cold_xvk`      | CIP-1852’s constitutional committee extended verification signing key | Ed25519 public key with chain code |
| `cc_hot_sk`        | CIP-1852’s constitutional committee hot signing key                   | Ed25519 private key                |
| `cc_hot_vk`        | CIP-1852’s constitutional committee verification signing key          | Ed25519 private key                |
| `cc_hot_xsk`       | CIP-1852’s constitutional committee hot extended signing key          | Ed25519-bip32 extended private key |
| `cc_hot_xvk`       | CIP-1852’s constitutional committee extended verification signing key | Ed25519 public key with chain code |
| `cvote_sk`         | CIP-36's vote signing key                                             | Ed25519 private key                |
| `cvote_vk`         | CIP-36's vote verification key                                        | Ed25519 public key                 |
| `drep_sk`          | CIP-1852’s DRep signing key                                           | Ed25519 private key                |
| `drep_vk`          | CIP-1852’s DRep verification key                                      | Ed25519 public key                 |
| `drep_xsk`         | CIP-1852’s DRep extended signing key                                  | Ed25519-bip32 extended private key |
| `drep_xvk`         | CIP-1852’s DRep extended verification key                             | Ed25519 public key with chain code |
| `gen_sk`           | Genesis signing key                                                   | Ed25519 private key                |
| `gen_vk`           | Genesis verification key                                              | Ed25519 public key                 |
| `gen_deleg_sk`     | Genesis delegate private key                                          | Ed25519 private key                |
| `gen_deleg_vk`     | Genesis delegate public key                                           | Ed25519 public key                 |
| `gen_utxo_sk`      | Genesis UTXO private key                                              | Ed25519 private key                |
| `gen_utxo_vk`      | Genesis UTXO public key                                               | Ed25519 public key                 |
| `kes_sk`           | KES signing key                                                       | KES signing key                    |
| `kes_vk`           | KES verification key                                                  | KES verification key               |
| `policy_sk`        | CIP-1855's policy private key                                         | Ed25519 private key                |
| `policy_vk`        | CIP-1855's policy public key                                          | Ed25519 public key                 |
| `pool_sk`          | Pool operator signing key                                             | Ed25519 private key                |
| `pool_vk`          | Pool operator verification key                                        | Ed25519 public key                 |
| `pool_xsk`         | Pool operator extended signing key                                    | Ed25519-bip32 extended private key |
| `pool_xvk`         | Pool operator extended verification key                               | Ed25519 public key with chain code |
| `root_sk`          | CIP-1852's root private key                                           | Ed25519 private key                |
| `root_vk`          | CIP-1852's root public key                                            | Ed25519 public key                 |
| `root_xsk`         | CIP-1852's extended root private key                                  | Ed25519-bip32 extended private key |
| `root_xvk`         | CIP-1852's extended root public key                                   | Ed25519 public key with chain code |
| `root_shared_sk`   | CIP-1854's root private key                                           | Ed25519 private key                |
| `root_shared_vk`   | CIP-1854's root public key                                            | Ed25519 public key                 |
| `root_shared_xsk`  | CIP-1854's extended root private key                                  | Ed25519-bip32 extended private key |
| `root_shared_xvk`  | CIP-1854's extended root public key                                   | Ed25519 public key with chain code |
| `stake_sk`         | CIP-1852's stake address signing key                                  | Ed25519 private key                |
| `stake_vk`         | CIP-1852's stake address verification key                             | Ed25519 public key                 |
| `stake_xsk`        | CIP-1852's extended stake address signing key                         | Ed25519-bip32 extended private key |
| `stake_xvk`        | CIP-1852's extended stake address verification key                    | Ed25519 public key with chain code |
| `stake_shared_sk`  | CIP-1854's stake address signing key                                  | Ed25519 private key                |
| `stake_shared_vk`  | CIP-1854's stake address verification key                             | Ed25519 public key                 |
| `stake_shared_xsk` | CIP-1854's extended stake address signing key                         | Ed25519-bip32 extended private key |
| `stake_shared_xvk` | CIP-1854's extended stake address verification key                    | Ed25519 public key with chain code |
| `vrf_sk`           | VRF signing key                                                       | VRF signing key                    |
| `vrf_vk`           | VRF verification key                                                  | VRF verification key               |

### Hashes

| Prefix                 | Meaning                                                               | Contents                                                                 |
| ---                    | ---                                                                   | ---                                                                      |
| `asset`                | Fingerprint of a native asset for human comparison                    | See [CIP-0014]                                                           |
| `pool`                 | Pool operator verification key hash (pool ID)                         | blake2b\_224 digest of an operator verification key                      |
| `script`               | Script hash                                                           | blake2b\_224 digest of a serialized transaction script                   |
| `addr_vkh`             | Address verification key hash                                         | blake2b\_224 digest of a payment verification key                        |
| `addr_shared_vkh`      | Shared address verification key hash                                  | blake2b\_224 digest of a payment verification key                        |
| `policy_vkh`           | Policy verification key hash                                          | blake2b\_224 digest of a policy verification key                         |
| `stake_vkh`            | Stake address verification key hash                                   | blake2b\_224 digest of a delegation verification key                     |
| `stake_shared_vkh`     | Shared stake address verification key hash                            | blake2b\_224 digest of a delegation verification key                     |
| `req_signer_vkh`       | Required signer verification key hash                                 | blake2b\_224 digest of a required signer verification key                |
| `vrf_vkh`              | VRF verification key hash                                             | blake2b\_256 digest of a VRF verification key                            |
| `datum`                | Output datum hash                                                     | blake2b\_256 digest of output datum                                      |
| `script_data`          | Script data hash                                                      | blake2b\_256 digest of script data                                       |
| `drep_vkh`             | Delegate representative verification key hash                         | blake2b\_224 digest of a delegate representative verification key        |
| `cc_cold_vkh`          | Constitutional committee cold verification key hash                   | blake2b\_224 digest of a constitutional committee cold verification key  |
| `cc_hot_vkh`           | Constitutional committee hot verification key hash                    | blake2b\_224 digest of a constitutional committee hot verification key   |

### Miscellaneous

| Prefix              | Meaning                                               | Contents                                                      |
| ---                 | ---                                                   | ---                                                           |
| `addr`              | Mainnet address                                       | Network tag, payment credential and optional stake credential |
| `addr_test`         | Testnet address                                       | Network tag, payment credential and optional stake credential |
| `stake`             | Mainnet stake address                                 | Network tag and stake credential                              |
| `stake_test`        | Testnet stake address                                 | Network tag and stake credential                              |
| `drep`              | DRep identifier                                       | DRep credential, see [CIP-0129]                               |
| `cc_cold`           | Constitutional committee cold identifier              | Constitutional committee cold credential, see [CIP-0129]      |
| `cc_hot`            | Constitutional committee hot identifier               | Constitutional committee hot credential, see [CIP-0129]       |
| `gov_action`        | Governance action identifier                          | Governance action ID, see [CIP-0129]                          |


### Deprecated Governance Prefixes

The some governance prefixes contained within the tables above were originally defined via [CIP-0105].
With the authoring of and acceptance of [CIP-0129], the community decide to replace some governance prefix definitions whilst depreciating others.

Further details on the depreciation and replacement of definitions can be seen via [CIP-105 Deprecated Governance ID Definition](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0105#deprecated-governance-id-definition).

Below is a table of entries of prefix definitions which were defined via [CIP-0105], but have now been marked as depreciated as [CIP-0129] defined replacements.\
The [CIP-0129] definitions are included within the tables above.

| Prefix             | Meaning                                                               | Contents                                                                 |
| ---                | ---                                                                   | ---                                                                      |
| `drep`             | Delegate representative verification key hash (DRep ID)               | blake2b\_224 digest of a delegate representative verification key        |
| `drep_script`      | Delegate representative script hash (DRep ID)                         | blake2b\_224 digest of a serialized delegate representative script       |
| `cc_cold`          | Constitutional committee cold verification key hash (cold credential) | blake2b\_224 digest of a constitutional committee cold verification key  |
| `cc_cold_script`   | Constitutional committee cold script hash (cold credential)           | blake2b\_224 digest of a serialized constitutional committee cold script |
| `cc_hot`           | Constitutional committee hot verification key hash (hot credential)   | blake2b\_224 digest of a constitutional committee hot verification key   |
| `cc_hot_script`    | Constitutional committee hot script hash (hot credential)             | blake2b\_224 digest of a serialized constitutional committee hot script  |

## Rationale: how does this CIP achieve its goals?

### About the `_test` suffix

Address already contains a discriminant tag, yet it requires one to peek at the internal binary payload. With Base58-encoded addresses, people have been used to look at first few characters and distinguish address this way. Not only this is cumbersome, but it is also made harder with both Shelley and Bech32-encoded addresses. On the one hand, the "common" part of the internal payload is much less than in Byron addresses and thus, the first bytes of the payload are varying much more. Plus, the bech32 prefix which can now be fixed makes it even more error-prone.

Therefore, having a clear human-readable indicator regarding the network discrimination is useful.

### About `addr`

Addresses probably are the most user-facing object in the current Cardano eco-system. Being able to clearly identify them

> :bulb: Open question: with side-chains and multi-currencies coming soon, would it make sense to include the currency in the bech32 prefix? e.g. `ada1...` or `ada_addr1.`

### About `stake`

Stake _addresses_ are references to reward account. They are used in many manipulation involving rewards (registering stake key, delegating, fetching reward balance etc..). We therefore make it a "first-class" object and assign it a dedicated prefix.

### About `sk` & `vk`

Both are rather transparent abbreviations for **s**igning **k**ey and **v**erification **k**ey.

### About `xsk` & `xvk`

The prefix `x` is typically used in cryptography to refer to e**x**tended keys (e.g. `xpub`, `xprv` ...). Following this convention, we prefix `sk` and `vk` as such when they refer to extended keys.

### About `vkh`

An abbreviation for **v**erification **k**ey **h**ash.

Verification key hashes are commonly utilized throughout the Cardano
eco-system. For example, they're used in stake pool registration and
retirement certificates, stake key registration, delegation, and
deregistration certificates, etc. As a result, it seems useful to have a
human-readable prefix by which one can discern the different kinds of
verification key hashes.

### Backwards compatibility

The only prior work done towards that direction has been [jcli](https://input-output-hk.github.io/jormungandr/jcli/introduction.html). Historically, prefixes evolved very organically and without any agreed-upon standard. jcli is however only compatible with Jörmungandr and is not intended to be compatible with the cardano-node. Therefore, there's little concern regarding compatibility here.

## Path to Active

### Acceptance Criteria

- [x] There is a variety of tools and services utilizing this standard:
  - [x] Trezor, Ledger
  - [x] cardano-cli
  - [x] cardano-wallet
  - [x] Blockfrost
  - [x] cardanoscan, cexplorer
  - [x] cardano-signer 
  - ... and more

### Implementation Plan

- Available JavaScript library: [cip5-js](https://www.npmjs.com/package/@dcspark/cip5-js)

## Changelog

In order to make it easy to keep up with updates to this CIP, we include the following table as a log of the changes sorted in decreasing order of date. Changes to the CIP should include an entry at the top of the table that includes a unique sequential identifier of the change, the date of the changes (in the format YYYY-MM-DD), a summary of the changes, and a link to the pull request that introduces the changes.

| ID  | Date        | Summary of changes                                              | Pull Request                                                  |
| --- | ---         | ---                                                             | ---                                                           |
| 2   | 2025-05-13  | Defined bech32 prefixes for extended pool operator keys         | [#1036](https://github.com/cardano-foundation/CIPs/pull/1036) |
| 1   | 2025-04-22  | Defined bech32 prefixes for genesis keys and created changelog. | [#1027](https://github.com/cardano-foundation/CIPs/pull/1027) |

## Copyright

This CIP is licensed under [Apache-2.0](https://www.apache.org/licenses/LICENSE-2.0).

[CIP-0014]: https://github.com/cardano-foundation/CIPs/blob/645243e30b5aae109a70ec2b47af70dcc808bc56/CIP-0014
[CIP-0105]: https://github.com/cardano-foundation/CIPs/blob/master/CIP-0105/README.md
[CIP-0129]: https://github.com/cardano-foundation/CIPs/blob/master/CIP-0129/README.md

---
CIP: 6
Title: Stake Pool Extended Metadata
Status: Active
Category: Tools
Authors:
  - Markus Gufler <gufmar@gmail.com>
  - Mike Fullman <mike@fullman.net>
Implementors:
  - pooltool.io
  - cexplorer.io
Discussions:
  - https://github.com/cardano-foundation/CIPs/pull/15
Created: 2020-07-20
License: CC-BY-4.0
---

## Abstract

This CIP defines the concept of extended metadata for pools that is referenced from the pool registration data stored on chain.

## Motivation: why is this CIP necessary?

As the ecosystem around Cardano stake pools proliferate so will the desire to slice, organize and search pool information dynamically.  Currently the metadata referenced on chain provides 512 bytes that can be allocated across the four information categories ([delegation-design-specification Section 4.2)](https://github.com/IntersectMBO/cardano-ledger/releases/latest/download/shelley-delegation.pdf):

| key           | Value                                |  Rules  |
| ---           | ---                                  |  ---  |
|  `ticker` | Pool ticker.  uppercase | 5 Characters Maximum, Uppercase letters and numbers |
|  `description` | Pool Description.  Text that describes the pool | 50 Characters Maximum |
|  `homepage` | A website URL for the pool  | 64 Characters Maximum, must be a valid URL |
|  `name` | A name for the pool | 50 Characters Maximum |

Many additional attributes can be envisioned for future wallets, pool explorers, and information aggregators.  The proposal below outlines an initial strategy for capturing this extended metadata.

## Specification

> **Note** Updated: 2020-11-24 2nd key-pair for validation 2021-02-08 json schema

### On Chain referenced (main) metadata file

We define two more fields for the on chain referenced metadata file that references another JSON file on a URL with the extended metadata.  The proposed metadata is as follows:

| key           | Value                                | Rules  |
| ---           | ---                                  | ---  |
|  `ticker`       | Pool ticker.  uppercase              | 5 Characters Maximum, Uppercase letters and numbers  |
|  `description` | Pool Description.  Text that describes the pool | 50 Characters Maximum |
|  `homepage` | A website URL for the pool| 64 Characters Maximum, must be a valid URL |
|  `name` | A name for the pool | 50 Characters Maximum |
| `extDataUrl` | A URL for extended metadata | optional, 128 Characters Maximum, must be a valid URL |
| `extSigUrl` | A URL with the extended metadata signature | optional, 128 Characters Maximum, must be a valid URL |
| `extVkey` | the public Key for verification | optional, 68 Characters |

In order to include the additional ext Field data, we suggest increasing the maximum size of the main metadata file from currently 512 to 1024 bytes.

### Extended Metadata - flexible but validable

In difference to the main metadata, the extended metadata should be updateable without having to use the cold key of the pool and without having to perform an on-chain transaction. The consumer of these data should still have the possibility to verify the authenticity of the data.

The operator notes all his additional pool information in the extended metadata (`extData.json`).

We propose the pool operator generate a new public-secret key-pair (`extData.skey` and `extData.vkey`)

```shell
cardano-cli node key-gen 
  --cold-verification-key-file extData.vkey 
  --cold-signing-key-file extData.skey 
  --operational-certificate-issue-counter-file extData.counter
```

Then a new (not available yet) `cardano-cli` command generate the signature (`extData.sign`) .

```shell
cardano-cli stake-pool rawdata-sign
  --raw-metadata-file extData.json
  --signing-key-file extData.skey
  --out-file extData.sign
```

The operator now:

- has the `extData.json` and `extData.sign` files
- will publish them at some https:// URL (probably same host as the main metadata)
- set the published `extData.json` URL in the main metadata `extDataUrl` field
- set the published `extData.sign` URL in then main metadata `extSigUrl` field
- set the `extData.vkey` string in the main metadata `extVkey` field
- re-register the extended main metadata file on chain

This re-registration of the main metadata file with the `extData.vkey` and the two URLs is only necessary once. Afterwards, the operator can update his extended metadata at any time, compute the new signature with the `cardano-cli stake-pool rawdata-sign` command, and publish both files at the existing `extDataUrl` and `extSigUrl`.

## Rationale: how does this CIP achieve its goals?

In the following we describe a first minimal version of the extended JSON file format.

Since this extended metadata file can be updated at any time by the pool operator, a **serial number** is useful for consuming applications and services to identify updates.

There are main thematic sections with respective subordinate data fields:

- the **itn** section is about the verifiable linking of an ITN pool ticker with its counterpart in Mainnet to identify fraudulent duplicates. (already used as not standardized extension)
- the **pool** section contains additional information about the pool instance
  - the pool.**contact** section contains information for additional information and contact data 
  - the pool.**media_assets** section contains additional information about the pools media files and colors
  - the pool.**itn** section is an optional section for ITN pool operators  

The full schema is given in annexe as [schema.json][]

<details>
  <summary>See JSON example</summary>

```json
{
  "serial": 870,
  "pool": {
    "id": "69579373ec20f2f82d2dc2360410350b308112f2939f92a",
    "country": "JPN",
    "status": "active",
    "contact": {
      "primary": "email",
      "email": "info@demopool.org",
      "facebook": "demopool12",
      "github": "demopooldev",
      "feed": "https://www.demopool.org/feed.xml",
      "telegram": "demopool",
      "twitter": "demopoolbird"
    },
    "media_assets": {
      "icon_png_64x64": "https://www.demopool.org/media/icon64.png",
      "logo_png": "https://www.demopool.org/media/logo.png",
      "logo_svg": "https://www.demopool.org/media/logo.svg",
      "color_fg": "#AABBCC",
      "color_bg": "#C0C0C0"
    },
    "itn": {
      "owner": "ed25519_pk1...",
      "witness": "ed25519_sig1..."
    }
  }
}
```
</details>

### Backwards compatibility

No fields are removed or changed in the current on chain metadata.  The new `ext...` fields are optional and not necessary to parse for any entities that do not need additional information about a pool

## Path to Active

### Acceptance Criteria

- [x] There exist at least two explorers which make use of this extended metadata structure or very close equivalent:
  - [x] pooltool.io
  - [x] cexplorer.io

### Implementation Plan

- [x] Provide direct support for this specification in stake pool explorers and other tools.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

[schema.json]: https://raw.githubusercontent.com/cardano-foundation/CIPs/master/CIP-0006/schema.json

---
CIP: 7
Title: Curve Pledge Benefit
Authors:
  - Shawn McMurdo <shawn_mcmurdo@yahoo.com>
Category: Ledger
Status: Proposed
Created: 2020-08-11
Discussions:
  - https://github.com/cardano-foundation/CIPs/pull/12
  - https://forum.cardano.org/t/protocol-parameters-pledge-and-sybil-resistance/35100
  - https://github.com/input-output-hk/cardano-node/issues/1518
Implementors: []
License: Apache 2.0
---

## Abstract

Modifying the current rewards calculation equation by substituting a n-root curved relationship between pledge and pledge benefit rewards for the current linear relationship will better achieve the original design goal of incentivizing pledge to help prevent Sybil attacks.
This also reduces the unfortunate side effect in the current equation that over rewards private pools which provide no additional security benefit.

## Motivation: why is this CIP necessary?

There are two main reasons for changing the current linear a0 pledge benefit factor in the rewards equation.

1. Pools pledging less than 1 million ADA see very little reward benefit.  This is not a strong incentive for pool operators as at current prices that is approximately $150,000 USD.

2. Private pools get massive reward benefit without providing any additional protection against Sybil attacks. Why should a private pool make 29% more rewards than a pool with 5m ADA pledge while doing the same work?

## Specification

This is a modification of the maxPool function defined in section 11.8 Rewards Distribution Calculation of “A Formal Specification of the Cardano Ledger”.

maxPool = (R / (1 + a0)) * (o + (s * a0 * ((o - (s * ((z0 - o) / z0))) / z0)))

where:
R = ((reserve * rho) + fees) * (1 - tau)
o = min(pool_stake / total_stake, z0) = z0 for fully saturated pool
s = pledge / total_stake
z0 = 1 / k
and the following are current protocol parameters:
k = 150
rho = 0.0022
a0 = 0.3
tau = .05

The idea is to replace s in the above equation with an n-root curve expression of pledge rather than the linear pledge value.

We use an expression called crossover to represent the point where the curve crosses the line and the benefit in the new and original equations is identical.
Because the a0 pledge benefit is spread over the pledge range from 0 to saturation there is a dependence on k and total_stake.
Since k and total_stake will likely change over time it is best to express crossover in terms of k and total_stake as follows:

crossover = total_stake / (k * crossover_factor)

where crossover_factor is any real number greater than or equal to 1.
So crossover_factor is essentially a divisor of the pool saturation amount.
For example, setting crossover_factor to 20 with k = 150 and total_stake = 31 billion gives a crossover of approximately 10.3 million.

Also, we can parameterize the n-root curve exponent.
This gives us:

s = pow(pledge, (1 / curve_root)) * pow(crossover, ((curve_root - 1) / curve_root)) / total_stake

The curve_root could be set to any integer greater than 0 and when set to 1 produces the current rewards equation.
The curve_root is n in n-root. For example, 1 = linear, 2 = square root, 3 = cube root, 4 = fourth root, etc.

By making this modification to the rewards equation we introduce two new protocol parameters, crossover_factor and curve_root, that need to be set thoughtfully.

### Test Cases

See rewards.php for some simple PHP code that allows you to try different values for crossover_factor and curve_root and compare the resulting rewards to the current equation.
For usage, run "php -f rewards.php help".

An interesting set of parameters as an example is:

curve_root = 3
crossover_factor = 8

Running "php -f rewards.php 3 8" produces:

Assumptions
Reserve: 14b
Total stake: 31.7b
Tx fees: 0
Fully Saturated Pool
Rewards available in epoch: 29.3m
Pool saturation: 211.3m

Curve root: 3
Crossover factor: 8
Crossover: 26.4m

Pledge	Rewards	Benefit	Alt Rwd	Alt Bnft
0k	150051	0%	150051	0%
10k	150053	0%	150458	0.27%
50k	150062	0.01%	150747	0.46%
100k	150073	0.01%	150928	0.58%
200k	150094	0.03%	151156	0.74%
500k	150158	0.07%	151551	1%
1m	150264	0.14%	151941	1.26%
2m	150477	0.28%	152432	1.59%
5m	151116	0.71%	153282	2.15%
10m	152181	1.42%	154122	2.71%
20m	154311	2.84%	155180	3.42%
50m	160702	7.1%	157012	4.64%
100m	171352	14.2%	158821	5.84%
211.3m	195067	30%	161305	7.5%

As you can see this gives meaningful pledge benefit rewards to pools pledging less than 1m ADA.

## Rationale: how does this CIP achieve its goals?

Using the n-root curve pledge benefit shows a much more reasonable distribution of pledge related rewards which will encourage meaningful pledges from more pool operators thus making the network more secure against Sybil attacks.
It also provides higher rewards for higher pledge without disproportionately rewarding a very few private pool operators who provide no additional security value to the network.
This modification maintains the general principles of the current rewards equation and does not introduce any hard limits.
It improves the incentives that were originally designed to make them more meaningful for the majority of pool operators.

### Backward Compatibility

This proposal is backwards compatible with the current reward function by setting the curve_root parameter to 1.

## Path to Active

### Acceptance Criteria

- [ ] The new equation is implemented in the ledger and enacted through a hard-fork.

### Implementation Plan

- [ ] Agreement by the Ledger team as defined in [CIP-0084](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0084) under _Expectations for ledger CIPs_ including "expert opinion" on changes to rewards & incentives.

- [ ] Author has offered to produce an implementation of this change as a pull request if shown where the current maxPool reward equation is implemented in the code.

## Copyright

2020 Shawn McMurdo. This CIP is licensed under [Apache-2.0](https://www.apache.org/licenses/LICENSE-2.0).

---
CIP: 8
Title: Message Signing
Status: Active
Category: Tools
Authors:
  - Sebastien Guillemot <sebastien@emurgo.io>
Implementors:
  - Mesh <https://meshjs.dev/>
  - Cardano Ballot <https://github.com/cardano-foundation/cf-cardano-ballot>
  - SundaeSwap governance <https://governance.sundaeswap.finance/>
  - Emurgo <https://www.emurgo.io/>
Discussions:
  - https://github.com/Emurgo/EmIPs/pull/5
  - https://forum.cardano.org/t/message-signing-specification/41032
  - https://cardano.ideascale.com/a/dtd/Create-message-signing-standard/323158-48088
Created: 2020-09-28
License: CC-BY-4.0
---

## Abstract

Private keys can be used to sign arbitrary data. If you have the public key, you can verify the data was signed by the owner of the private key. This is how transaction signing works internally but its utility is not limited to transactions. This document tries to set a standard for how to represent and verify signed messages for Cardano.

## Motivation: why is this CIP necessary?

Most common use cases:

1) Proving ownership of a set addresses (possibly to prove ownership of more then X Ada)
1) Proving ownership of addresses used in a transaction
1) Proving ownership of an identity or other off-chain data with a public key attached to it

## Specification

### Versions

| version | description     |
|---------|-----------------|
| 1       | initial version |

### Signing and Verification

#### Overview

First we will show a very basic example of the structure to help the reader understand the definitions that comes after

```
[
  bstr,               ; protected header
  { * label => any }, ; unprotected header
  bstr / nil,         ; message to sign
  [                   ; signature array
    [                     ; first signature
      bstr                ; protected
      { * label => any }, ; unprotected
      bstr                ; signature
    ]
  ]
]
```

You can see the structure has two layers -- both containing a `protected` and an `unprotected` section. Items inside `protected` are part of the data signed while `unprotected` is meant to annotate the COSE structure (for example add annotations as you pass a message across a stack). Items MUST NOT be duplicated.

You can find more complete definitions of `protected` and `unprotected` in [RFC 8152 section-3](https://tools.ietf.org/html/rfc8152#section-3) and you can see some the reserved entries (called `Generic_Headers`) in the maps in [RFC 8152 section-3.1](https://tools.ietf.org/html/rfc8152#section-3.1).

Note: `payload` can be `nil`. This means that the payload is known by both the signer and the verifier and therefore doesn't need to be encoded.

For your convenience, the structure is provided here:

```
empty_or_serialized_map = bstr .cbor header_map / bstr .size 0

header_map = {
    Generic_Headers,   ; reserved headers (see COSE section 3.1)
    * label => values  ; any number of int/string labels for application-specific purpose.
}

Headers = (
    protected : empty_or_serialized_map,
    unprotected : header_map
)

; signature layer
COSE_Signature =  [
    Headers,
    signature : bstr
]

; if signing with just ONE key
COSE_Sign1 = [
    Headers,
    payload : bstr / nil,
    signature : bstr
]

# if signing with >1 key
COSE_Sign = [
    Headers,
    payload : bstr / nil,
    signatures : [+ COSE_Signature]
]

signed_message = COSE_SIGN / COSE_Sign1
```

#### Signing and Verification target format

Instead of signing the full structure, we instead sign the following type which is derived from the structure

```
Sig_structure = [
  context : "Signature" / "Signature1" / "CounterSignature",    ; explained below
  body_protected : empty_or_serialized_map,                     ; protected from layer 1
  ? sign_protected : empty_or_serialized_map,                   ; not present in Sign1 case
  external_aad : bstr,                                          ; explanation below
  payload : bstr
]
```

The `external_aad` allows an application to ask the user to sign some extra data but NOT put it inside the COSE structure (only as part of the data to sign). Defaults to `h''`. You can read more about this at [RFC 8152 section-4.3](https://tools.ietf.org/html/rfc8152#section-4.3).

The `context` is meant to encode what structure was used for the COSE request. `CounterSignature` is explained in a later section of this specification.

#### Signing and Verification process

To be able to effectively verify we need two things:

1) P1 - (optional) knowledge of the relation of a public key and a Cardano address
1) P2 - Knowledge of which algorithm was used to sign

For `P1`, the mapping of public keys to addresses has two main cases:

1) for Shelley addresses, one payment key maps to many addresses (base, pointer, enterprise)
2) for Byron addresses, chaincode and address attributes need to be combined with the public key to generate an address

To resolve this, one SHOULD add the full address to the protected header when using a v2 address. The v2 addresses contain the chaincode + attributes and we can verify the address matches combining it with the verification public key.

```
? address: bstr,
```

For `P2`, we use the `alg` header and to specify which public key was used to sign, use the [cwt](https://tools.ietf.org/html/rfc8392) protected header.

### Encryption

Although COSE defines multiple ways to encrypt, we simplify our spec to the two following cases:

1) Encrypted with the recipient's public key (called `key transport` in COSE spec)
2) Encrypted with a user-chosen password (called `passwords` in COSE spec)

In order to facilitate implementations in wallets, we limit the usage of these to the following

```
ChachaPoly
Ed25519PubKey
```

We will explain what this means shortly but you can find the full list of the types of encryption allowed by COSE at [RFC 8152 section 5.1.1](https://tools.ietf.org/html/rfc8152#section-5.1.1)

#### Structure

The COSE specification is made to be composable -- that is you can have a plaintext that you wrap with a signature, then wrap with an encryption, then wrap with a signature again (and so on).

That means that for encryption in particular, it can either

1) Be used to encrypt plaintext directly
2) Be used to encrypt another COSE message

In this spec, we care about the case where you encrypt a `signed_message`

Here is the overall CBOR structure

```
; holds encrypted content itself
COSE_Encrypt = [
  Headers,
  ciphertext : bstr / nil, ; contains encrypted signed_message
  recipients : [+COSE_recipient]
]

; holds encrypted keys the receiver can use to decrypt the content
COSE_recipient = [
  Headers,
  ciphertext : bstr / nil, ; contains encrypted key to decrypt the COSE_Encrypt ciphertext
  ? recipients : [+COSE_recipient] ; in case you need multiple rounds to get decryption key
]
```

To encrypt the structure as a whole, we call our encryption method once for each level (root, recipient, etc.) recursively. For example, we encrypt the `signed_message` and put it in the `COSE_Encrypt` ciphertext, then we encrypt the decryption key and put it in the `COSE_recipient` ciphertext.

For the `Headers`,

- The `protected` fields MUST be empty. These are meant to be used with AEAD which we  don't need in this  specification (you can read more about it at [RFC 8152 section 5.3](https://tools.ietf.org/html/rfc8152#section-5.3)).

We define two ways to encrypt content:

##### Password-based encryption

For password-based encryption we don't need a receiver field (anybody who knows the password can decrypt) so we instead use the following (simplified) structure

```
COSE_Encrypt0 = [
    Headers,
    ciphertext : bstr / nil,
]

PasswordEncryption = 16 (COSE_Encrypt0)
```

The COSE spec uses the following parameters for ChaCha20/Poly1305 as specified in [10.3](https://tools.ietf.org/html/rfc8152#section-10.3):

- 256-bit key
- 128-bit tag
- 96-bit nonce

We RECOMMEND using `19162` iterations as this matches the existing password encryption in the [Yoroi encryption spec](https://github.com/Emurgo/yoroi-frontend/blob/737595fec5a89409aacef827d356c9a1605515c0/docs/specs/code/ENCRYPT.md)`.

##### Public key based encryption

We only allow encrypting based on `ED25519` public keys (the ones used for Cardano). To encrypt based on these public keys, you must

1) Compute a password consisting of 22 case-sensitive alphanumeric (a–z, A–Z, 0–9) characters (this gives you ~128 bits of entropy)

Now, for each receiver, you must

2) Compute an ephemeral key pair using `ED25519 extended`
1) Compute the shared DH secret between the private key from step (1) and the public key received (using the `exchange` functionality)
1) Use this as the password to encrypt the password in (1) using [the Yoroi encryption spec](https://github.com/Emurgo/yoroi-frontend/blob/737595fec5a89409aacef827d356c9a1605515c0/docs/specs/code/ENCRYPT.md)

The structure will look like the following:

```
COSE_Encrypt = [
  Headers,
  ciphertext : bstr / nil, ; contained signed_message encrypted with random password
  recipients : [+COSE_recipient]
]

COSE_recipient = [
  Headers,
  ciphertext : bstr / nil, ; contains random password encrypted with shared secret
]

PubKeyEncryption = 96 (COSE_Encrypt)
```

The `Headers` for the recipient MUST have a `epk` label containing the public key of the ephemeral keypair as described in the [CBOR Encoded Message Syntax](http://www.watersprings.org/pub/id/draft-schaad-cose-02.html).

#### Version

The `Headers` for the body MUST have `version: uint` in the `unprotected` field. See [Versions table](#versions) for possible version numbers.

#### Payload encoding

To solve `E3`, `signed_message` body header MUST contain `hashed: bool` as an `unprotected` header which defines whether or not we signed the `payload` OR the `Blake2b224` hash of the `payload`. The hash MUST be used in the following two cases

1) The size of the raw `payload` would otherwise be too big to fit in hardware wallet memory (see E1). Note that the exact size for which this is the case depends on the device.
1) The payload characters (ex: non-ASCII) that cannot be displayed on the hardware wallet device (see E3)

We RECOMMEND showing the user the full payload on the device if possible because it lowers the attack surface (otherwise the user has to trust that the hash of the payload was calculated correctly).

`Blake2b224` was chosen specifically because `224` bits is already a long string for hardware wallets.

### User-Facing Encoding

Once we have our top-level `encrypted_message` or `signed_message` we need to encode them in way that can be displayed to users (doesn't need to be stored and can be inferred just from the data)

We define the encoding in three parts `prefix || data || checksum` (where || means append)

### Prefix

We need a human-readable prefix. We use "CM" for "Cardano Message" followed by the message type:

- `encrypted_message`: `cme_`
- `signed_message`: `cms_`
- `mac_message`: `cmm_` (unused in this spec)

#### Data

Data is simply the `base64url` encoding of the message

#### Checksum

We use fnv32a on the data for the checksum and store it as the `base64url` encoding of its network byte order representation.

### Other remarks

We recommend usage of unprotected headers vs protected headers when possible. This is because we have to limit the amount of data passed to a hardware wallet  to satisfy E1. If the only effect of an adversary changing an unprotected header only leads to the signature not matching, then it's best to leave it unprotected.

The public key SHOULD NOT contain any chaincode information, as it could compromise child non-hardened keys. It is both a privacy and a security risk (see [here](https://bitcoin.org/en/wallets-guide#hierarchical-deterministic-key-creation) for more detail).

## Rationale: how does this CIP achieve its goals?

### Requirements

On top being usable for all cases mentioned in [Motivation](#motivation-why-is-this-cip-necessary), we also desire the following to ensure it works well with hardware wallets:

- E1 - Low runtime memory environment
- E2 - Low app size environment (cannot implement every cryptographic algorithm on the device or app size would be too big)
- E3 - Works well with limited display (some hardware wallets cannot display long text and cannot display UTF8)

### Related specifications

#### Requirement Levels [[RFC 2119](https://tools.ietf.org/html/rfc2119)]

The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT", "SHOULD", "SHOULD NOT", "RECOMMENDED", "NOT RECOMMENDED", "MAY", and "OPTIONAL" in this document are to be interpreted as described in [RFC 2119](https://tools.ietf.org/html/rfc2119).

#### Concise Binary Object Representation (CBOR) [[RFC 7049](https://tools.ietf.org/html/rfc7049)]

CBOR is a way to serialize structured data in a more compact way than what is allowed by JSON. It is widely used across the Cardano ecosystem and so we use it to encode the data for this specification.

#### *Concise Data Definition Language (CDDL)* [[RFC 8610](https://tools.ietf.org/html/rfc8610)]

CDDL is a human-readable CBOR notation format. CBOR schemas defined in this document are defined uinsg CDDL. We use `label = int / tstr` in several places.

#### *CBOR Object Signing and Encryption (COSE)* [[RFC 8152](https://tools.ietf.org/html/rfc8152)] 

This is a standard for how to use CBOR for message signing. It is based on the JSON equivalent *JSON Object Signing and Encryption* [RFC 7520](https://tools.ietf.org/html/rfc7520). 

We base our construction on COSE because all Cardano libraries already depend on CBOR due to its use in the base protocol (which means we don't need to introduce a new library). It is also more compact, which is useful in case data generated by this standard ever needs to be stored on-chain.

#### *CBOR Web Token (CWT)* [[RFC 8392](https://tools.ietf.org/html/rfc8392)]

This is a standard for pre-defined header elements for message signing based on the equivalent standard for JSON (*JWT* [[RFC 7519](https://tools.ietf.org/html/rfc7519)]). This allows us to standardize notions of concepts like expiration time of a signed message.

#### *Address Formats*

BECH32 ([BIP-173](https://github.com/bitcoin/bips/blob/master/bip-0173.mediawiki)) is a standard for encoding addresses such that they

- Are human readable (both in length and contain a common prefix)
- Can easily be displayed in a QR code
- Contain error detection through a BCH checksum

Cardano has several address types based on the era they were created in:
* `v1` - Legacy Daedalus (starts with Dd)
* `v2` - Legacy Icarus (starts with Ae2)
* `v3` - Shelley (bech32)


Although `v3` is relevant to us for encoding public keys into addresses, we do not use `bech32`'s scheme for encoding in this specification. This is because

- The payload may be too big to reasonably encode in a QR code so the benefits of using base32 are limited.
- BCH checksums are not made for large payloads and additionally the polynomial used has to be fine-tuned for the expected length (but the length of our payload varies too much in this spec)

#### fnv32a

Although Cardano Byron addresses use CRC32 (IEEE variation), due to `E1` and `E2` we use fnv32a for checksums.

This is because CRC32:

1) Has fairly few collisions compared to other hashes
2) Is moderately slower than other hashes
3) Requires more memory than alternatives (need to build a lookup table)
4) Is somewhat complex implementation (many alternatives exist)

while fnv32a:

1) Still has fairly few collisions
2) Is faster than CRC32 in general
3) Needs only O(1) memory requirement
4) Is simple to implement

In particular, using fnv32a over CRC32 frees up 1024 bytes of memory due to not having a lookup table which is significant on hardware wallets.

#### Blake2b [[RFC 7693](https://tools.ietf.org/html/rfc7693)]

Blake2b is a hash algorithm used commonly in Cardano. Notably, Blake2b-224, Blake2b-256 and Blake2b512 are used depending on the context.

#### base64url [[RFC 4648 section 5](https://tools.ietf.org/html/rfc4648#section-5)]

`base64url` allows encoding bytes in a human-readable format that is also safe to pass in URLs.

#### Other blockchain standards

Other blockchains have existing specifications for message signing, but they mostly revolve around scripts trying to validate messages. We don't leverage any of their work in particular but it may be of interest.

- [BIP-137](https://github.com/bitcoin/bips/blob/master/bip-0137.mediawiki) - simply scheme for message signing that works with P2PKH, P2PSH and bech32
- [BIP-322](https://github.com/kallewoof/bips/blob/master/bip-0322.mediawiki) - reuses Bitcoin script to process a generic signed message format

- [EIP-191](https://github.com/ethereum/EIPs/blob/master/EIPS/eip-191.md) - encode data for Ethereum contracts
- [EIP-712](https://github.com/ethereum/EIPs/blob/master/EIPS/eip-712.md) - encode structs for Ethereum contracts

### Existing Code

#### Cryptography

Cardano already allows message signing within the WASM bindings. Notably,

1) [sign](https://github.com/Emurgo/cardano-serialization-lib/blob/4792b1b121e728a51686d5fcbffd33489d65c903/rust/src/crypto.rs#L279)
2) [verify](https://github.com/Emurgo/cardano-serialization-lib/blob/4792b1b121e728a51686d5fcbffd33489d65c903/rust/src/crypto.rs#L322)

You can see an example of these two functions [here](https://repl.it/repls/FlusteredSimpleFunction)

Even if you use cryptographically secure `sign` and `verify` functions, you still have the following problems:

1) No human-recognizable prefix
0) No error detection
0) User could accidentally sign a transaction or a block thinking it's harmless data

We also have a risk of a few different kinds of replay attacks

4) A dapp asks person A to sign "BOB" and then another dapp asks user B to sign "BOB". B can just use the signature from A
0) A dapp asks person A to sign "BOB" on a testnet chain. Person B then sends this signed message to the same dapp running on mainnet (same argument applies to sidechains)

### Reference implementations

- [COSE-JS](https://github.com/erdtman/cose-js)
- [Rust message signing](https://github.com/Emurgo/message-signing)

### Unresolved

This specification provides no means of Revocation.

## Path to Active

### Acceptance Criteria

- [x] There are wallets supporting creation of signed messages as per this protocol (enumerated 2023-12-19 as per [CardanoBallot list of wallets](https://voting.summit.cardano.org/user-guide) supporting CIP-8 signed messages):
  - [x] Flint
  - [x] Eternl
  - [x] Nami
  - [x] Typhon
  - [x] Yoroi
  - [x] Nufi
  - [x] Gerowallet
  - [x] Lace
- [x] There exist one or more implementations in commonly used development libraries:
  - [x] [Mesh](https://meshjs.dev/)
  - [x] `@emurgo/cardano-message-signing-asmjs`
- [x] There exist CLI Tools supporting creation and verification of signed messages:
  - [x] [cardano-signer](https://github.com/gitmachtl/cardano-signer)
- [x] There exist one or more implementations in web sites and other tools:
  - [x] SundaeSwap Governance voting

### Implementation Plan

- [x] Make this standard available as well-supported means of message signing across Cardano wallets, dApps, and CLI tools.
- [x] Support this standard in a usable reference implementation ([`@emurgo/cardano-message-signing-asmjs`](https://www.npmjs.com/package/@emurgo/cardano-message-signing-asmjs)).

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode)

---
CIP: 9
Title: Protocol Parameters (Shelley Era)
Status: Active
Category: Ledger
Authors:
  - Kevin Hammond <kevin.hammond@iohk.io>
Implementors:
  - IOG <https://iog.io/>
Discussions:
  - https://github.com/cardano-foundation/CIPs/pull/45
Created: 2021-01-29
License: Apache-2.0
---

## Abstract

This CIP is an informational CIP that describes the initial protocol parameter settings for the Shelley era of the Cardano blockchain, plus the changes that have been made.
It is intended to serve as a historic record, allowing protocol parameter changes to be tracked back to the original settings.

## Motivation: why is this CIP necessary?

We need to provide a concise description of the initial protocol parameter choices, that can be used by the community as the base for future proposed protocol changes,
and that document the chain of changes to the parameters.


## Specification

### Proposing Protocol Parameter Changes

This CIP records only the changes to the protocol parameters that have actually been made.  Suggested changes to protocol parameters should be proposed by preparing and submitting a new CIP, rather than editing this CIP.  The following information should be included.

| Name of the Parameter   | New Parameter (Y/N)  | Deleted Parameter (Y/N) | Proposed Value   | Summary Rationale for Change |
|-----------------------  |--------------------  |------------------------ |---------------   | ---------------------------- |

Where necessary, the summary rationale should be supported by a few paragraphs of text giving the full rationale, plus references to any external documents that are needed to understand the proposal.

Protocol parameters are used to affect the operation of the Cardano Protocol.  They may be either **updatable** or **non-updatable**.
Updatable parameters can be tuned to vary the operation of the block producing protocol, impacting the proportion of pools that are federated/non-federated,
how much influence the "pledge" has etc.  Non-updatable parameters affect the fundamentals of the blockchain protocol, including defining the
genesis block, basic security properties etc.  Some non-updatable parameters may be embedded within the source code or implemented as software.
Each major protocol version defines its own sets of updatable/non-updatable parameters.


### Updatable Protocol Parameters

The initial **updatable** protocol parameter values are given below (in JSON format).  Any of these parameters may be changed by submitting
a parameter update proposal.  A change to the major protocol parameter version triggers a "hard fork" event.  This will require stake pool operators to
upgrade to a new software version that complies with the new chain production protocol as well as being able to verify the construction of the chain.

```
{
    "protocolVersion": {
        "major": 2,
        "minor": 0
    },
    "nOpt": 150,
    "a0": 0.3,
    "minPoolCost": 340000000,
    "decentralisationParam": 1.0,
    "maxBlockBodySize": 65536,
    "maxBlockHeaderSize": 1100,
    "maxTxSize": 16384,
    "tau": 0.2,
    "rho": 3.0e-3,
    "poolDeposit": 500000000,
    "keyDeposit": 2000000,
    "minFeeB": 155381,
    "minFeeA": 44,
    "minUTxOValue": 1000000,
    "extraEntropy": {
        "tag": "NeutralNonce"
    },
    "eMax": 18
}
```

The meaning of the fields is:

| Field                 	| Initial Value                                                          	| Description                                                                                                                                                                                                                      	|
|-----------------------	|------------------------------------------------------------------------	|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------	|
| protocolVersion       	| ```protocolVersion": {         "major": 2,         "minor": 2     }``` 	| Protocol version.  Minor versions indicate software updates (will generally be 0).  Major version 1 = Byron, 2 = Shelley                                                                                                         	|
| nOpt                  	| 150                                                                    	| "Target number of pools" ("k").  Impacts saturation threshold, encouraging growth in number of stake pools.                                                                                                                                                                  	|
| a0                     	| 0.3                                                                    	| "Influence Factor". Governs how much impact the pledge has on rewards.                                                   v                                                                                                                                                   	|
| minPoolCost           	| 340000000                                                              	| Minimum Pool Cost per epoch (in lovelace).  Enables pledge effect.                                                                                                                                                               	|
| decentralisationParam 	| 1.0                                                                    	| Level of decentralisation.  Starts at 1.  Block production is fully decentralised when this reaches 0.                                                                                                                           	|
| | |
| maxBlockBodySize      	| 65536                                                                  	| Maximum size of a block body.  Limits blockchain storage size, and communication costs.                                                                                                                                          	|
| maxBlockHeaderSize    	| 1100                                                                   	| Maximum size of the block header.  Should be significantly less than the maximum block size.                                                                                                                                     	|
| maxTxSize             	| 16384                                                                  	| Maximum size of a transaction.  Several transactions may be included in a block.  Must be strictly less than the max. block body size.                                                                                           	|
| | |
| tau                   	| 0.2                                                                    	| Treasury rate (0.2 = 20%).  Proportion of total rewards allocated to treasury each epoch before remaining rewards are distributed to pools.                                                                                                                                                                          |                                          	|
| rho                   	| 3.0e-3                                                                	| Monetary expansion rate per epoch.  Governs the rewards that are returned from reserves to the ecosystem (treasury, stake pools and delegators).                                                                                  |
| | |
| poolDeposit           	| 500000000                                                              	| Pool deposit (in lovelace)                                                                                                                                                                                                       	|
| keyDeposit            	| 2000000                                                                	| Deposit charged for stake keys (in Lovelace).  Ensures that unused keys are returned, so freeing resources.                                                                                                                      	|
| | |
| minFeeB               	| 155381                                                                 	| Base transaction fee (in lovelace).                                                                                                                                                                                              	|
| minFeeA               	| 44                                                                     	| Additional transaction fee per byte of data (in lovelace).                                                                                                                                                                       	|
| | |
| minUTxOValue          	| 1000000                                                                	| Minimum allowed value in a UTxO.  Security-related parameter used to prevent the creation of many small UTxOs that could use excessive resource to process.                                                                      	|
| | |
| extraEntropy          	| ```{         "tag": "NeutralNonce"     }```                            	| Should additional entropy be included in the initial phases.  This provides additional certainty that the blockchain has not been compromised by the seed key holders.  Redundant once the system is sufficiently decentralised. 	|
| eMax                  	| 18                                                                     	| Maximum number of epochs within which a pool can be announced to retire, starting from the next epoch.                                                                                                                 	|

### Non-Updatable Parameters

The initial non-updatable protocol parameters are given below (in JSON format):

```
  "activeSlotsCoeff": 0.05,
  ...
  "genDelegs": {
    "ad5463153dc3d24b9ff133e46136028bdc1edbb897f5a7cf1b37950c": {
      "delegate": "d9e5c76ad5ee778960804094a389f0b546b5c2b140a62f8ec43ea54d",
      "vrf": "64fa87e8b29a5b7bfbd6795677e3e878c505bc4a3649485d366b50abadec92d7"
    },
    ...
    }
  },
  "updateQuorum": 5,
  "networkId": "Mainnet",
  "initialFunds": {},
  "maxLovelaceSupply": 45000000000000000e,
  "networkMagic": 764824073,
  "epochLength": 432000,
  "systemStart": "2017-09-23T21:44:51Z",
  "slotsPerKESPeriod": 129600,
  "slotLength": 1,
  "maxKESEvolutions": 62,
  "securityParam": 2160
}
```

The meaning of the fields is:

| Field                 	| Initial Value                                                          	| Description
|-----------------------	|------------------------------------------------------------------------	|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------	|
| activeSlotsCoeff                  	| 0.05                                                                     	| The fraction of the total number of slots that will, on average, be selected to include a block in the chain.  Smaller numbers increase security, but reduce efficiency.                                                                                                                 	|
| genDelegs                  	| ...                                                                     	| Details of the public keys that have been selected by each of the genesis keys to act as a delegate for signing protocol updates etc. |
| updateQuorum                  	| 5                                                                     	| How many of the genesis delegate keys must endorse an update proposal.  |
| networkId                  	| "Mainnet"                                                                     	| Is this a testnet or mainnet  |
| initialFunds                  	|  {} | initial distribution of funds to addresses. |
| maxLovelaceSupply                  	| 45000000000000000                                                                    	| The limit on the maximum number of lovelace that can be in circulation. |
| networkMagic                  	| 764824073                                                                    	| A magic number used to distinguish different networks. |
| epochLength                  	|  432000                                                                   	| The number of slots in an epoch. |
| SystemStart                  	|  "2017-09-23T21:44:51Z"                                                                   	| When did the system originally start operation. |
| slotsPerKESPeriod             | 129600                                                                   	| After how many slots will a pool's operational key pair evolve (Key Evolving Signatures). |
| slotLength             | 1                                                                   	| The length of each slot (in seconds). |
| maxKESEvolutions             | 62                                                                   	| What is the maximum number of times a KES key pair can evolve before a new KES key pair must be generated from the master keys. |
| securityParam             | 2160                                                                   	| After how many blocks is the blockchain considered to be final, and thus can no longer be rolled back (i.e. what is the maximum allowable length of any chain fork).  |



### Pre-Shelley Protocol Parameters

The original protocol parameters are given in the Byron genesis file.  These parameters need to be included in any operational stake pool so that the Byron portion
of the chain can be verified, but they can no longer be altered.

```
{
    "avvmDistr": {
    ...
    },
    "blockVersionData": {
    ...
    },
    "ftsSeed": "76617361206f7061736120736b6f766f726f64612047677572646120626f726f64612070726f766f6461",
    "protocolConsts": {
    ...
    },
    "startTime": 1506203091,
    "bootStakeholders": {
    ...
    },
    "heavyDelegation": {
    ...
    }
    },
    "nonAvvmBalances": {},
    "vssCerts": {
    ...
    }
```

### Process for Making Changes to Protocol Parameters

#### Governance

Changes will affect many stakeholders and must therefore be subject to open community debate and discussion.

Ultimately, the Voltaire protocol voting mechanism will be used to achieve fully automated, decentralised and transparent governance.
In the interim, the CIP process will be used.


#### Signalling Protocol Parameter Changes

Changes to the parameters need to be signalled to the community well in advance, so that they can take appropriate action.  For the most significant parameters, a minimum of 4-6 weeks
elapsed time between announcement and enactment is appropriate.  This period must be included in the CIP.  Announcements will be made as soon
as practical after the conclusion of the vote.


#### Applying Protocol Parameter Changes

Protocol parameter changes must be submitted and endorsed within the first 24 hours of the epoch before they are required to come into effect.
For example, a change that is intended for epoch 300 must be submitted and endorsed in the first 24 hours of epoch 299.
Once a change has been submitted and endorsed by a sufficient quorum of keyholders (currently 5 of the 7 genesis keys), it cannot be revoked.

#### Voiding Proposed Protocol Parameter Changes

Once a protocol parameter change has been announced, it can only be overridden through the voting process (CIP, Voltaire etc.).  Any vote must be
completed before the start of the epoch in which the change is to be submitted.

### Change Log

#### Changes to the Updatable Parameters since the Shelley Hard Fork Event

Following the Shelley hard fork event, the ``decentralisationParam`` parameter has been gradually decreased from ``1.0`` to ``0.3``, with the goal of ultimately decreasing it to ``0`` (at which point
it can be removed entirely as an updatable parameter).  This has gradually reduced the impact of the federated block producing nodes, so ensuring that the network moves to become a distributed collection of increasingly decentralised stake pools.
The parameter was frozen at ``0.32`` between epochs 234 and 240.   The ``nOpt`` parameter was changed from ``150`` to ``500`` in epoch 234.


| Epoch |  Date       | Decentralisation | nOpt|
| ----- |  ---------- | ---------------- | ---- |
| 208 |	2020-07-29 |	1 |	150|
| 209 |	2020-08-03 |	1 |	150|
| 210 |	2020-08-08 |	1 |	150|
| 211 |	2020-08-13 |	0.9 |	150|
| 212 |	2020-08-18 |	0.8 |	150|
| 213 |	2020-08-23 |	0.78 |	150|
| ... |	... 	   |	... |	...|
| 227 |	2020-11-01 |	0.5 |	150|
| ... |	... 	   |	... |	...|
| 233 |	2020-12-01 |	0.38 |	150|
| 234 |	2020-12-06 |	0.32 |	500|
| 235 |	2020-12-11 |	0.32 |	500|
| ... |	... 	   |	... |	...|
| 239 |	2020-12-31 |	0.32 |	500|
| 240 |	2021-01-05 |	0.32 |	500|
| 241 |	2021-01-10 |	0.3 |	500|
| ... | ...	   |	... |	...|


#### The Allegra Hard Fork Event

The Allegra Hard Fork Event on 2020-12-16 (epoch 236) introduced token locking capabilities plus some other small changes to the protocol.  No parameters were
added or removed.

```
{
    "poolDeposit": 500000000,
    "protocolVersion": {
        "minor": 0,
        "major": 3
    },
    "minUTxOValue": 1000000,
    "decentralisationParam": 0.32,
    "maxTxSize": 16384,
    "minPoolCost": 340000000,
    "minFeeA": 44,
    "maxBlockBodySize": 65536,
    "minFeeB": 155381,
    "eMax": 18,
    "extraEntropy": {
        "tag": "NeutralNonce"
    },
    "maxBlockHeaderSize": 1100,
    "keyDeposit": 2000000,
    "nOpt": 500,
    "rho": 3.0e-3,
    "tau": 0.2,
    "a0": 0.3
}
```

#### The Mary Hard Fork Event

The Mary Hard Fork Event will introduce multi-asset token capability.  It is not expected that any parameter will be added or removed.

```
{
    "poolDeposit": 500000000,
    "protocolVersion": {
        "minor": 0,
        "major": 4
    },
    "minUTxOValue": 1000000,
    "decentralisationParam": 0.32,
    "maxTxSize": 16384,
    "minPoolCost": 340000000,
    "minFeeA": 44,
    "maxBlockBodySize": 65536,
    "minFeeB": 155381,
    "eMax": 18,
    "extraEntropy": {
        "tag": "NeutralNonce"
    },
    "maxBlockHeaderSize": 1100,
    "keyDeposit": 2000000,
    "nOpt": 500,
    "rho": 3.0e-3,
    "tau": 0.2,
    "a0": 0.3
}
```

#### The Alonzo Hard Fork Event

See [CIP-0028: Protocol Parameters (Alonzo Era)](../CIP-0028).


## Rationale: how does this CIP achieve its goals?

The initial parameter settings were chosen based on information from the Incentivised Testnet, the Haskell Testnet, Stake Pool Operators plus benchmarking and security concerns.  This parameter choice was deliberately conservative,
in order to avoid throttling rewards in the initial stages of the Cardano mainnet, and to support a wide range of possible stake pool operator (professional, amateur, self, etc.).
Some parameter choices (``systemStart``, ``securityParam``) were required to be backwards compatible with the Byron chain.


### Key Behavioural Parameters


The key  parameters that govern the behaviour of the system are ``nOpt``, ``a0``, ``decentralisationParam`` and ``minPoolCost``.
Changes to these parameters need to be considered as a package -- there can be unintended consequences when changing a single parameter in isolation.

It is expected that the following changes to these parameters are likely in the near to medium term:

* increasing ``nOpt`` to align more closely with the number of active pools
* increasing ``a0`` to increase the pledge effect
* decreasing ``minPoolCost`` (e.g. in line with growth with the Ada value)
* decreasing ``decentralisationParam`` to 0 (to enable full decentralisation of block production)

Further adjustments are likely to be required to tune the system as it evolves.


### Economic Parameters

Four parameters govern the economics of the system:  ``tau``, ``rho``, ``minFeeA`` and ``minFeeB``.
The first two concern the rate of rewards that are provided to stake pools, delegators and the treasury.
The others concern transaction costs.


### Transaction and Block Sizes

Three parameters govern block and transaction sizes: ``maxBlockBodySize``, ``maxBlockHeaderSize``, ``maxTxSize``.
Their settings have been chosen to ensure the required levels of functionality, within
constrained resource restrictions (including long-term blockchain size and real-time worldwide exchange of blocks).
Changes to these parameters may impact functionality, network reliability and performance.


### Backward Compatibility

This CIP describes the initial set of protocol parameters and the changes to date, so backwards compatibility is not an issue.
Future proposals may change any or all of these parameters.
A change to the major protocol version indicates a major change in the node software.
Such a change may involve adding/removing parameters or changing their semantics/formats.
In contrast, minor protocol changes are used to ensure key software updates without changing
the meaning of any protocol parameters.

## Path to Active

### Acceptance Criteria

- [x] The Shelley ledger era is activated.
- [x] Documented parameters are in operational use by Cardano Node and Ledger.

### Implementation Plan

- [x] Original (Shelley) and subsequent ledger era parameters are deemed correct by working groups at IOG.

## Copyright

This CIP is licensed under [Apache-2.0](https://www.apache.org/licenses/LICENSE-2.0). 

---
CIP: 10
Title: Transaction Metadata Label Registry
Status: Active
Category: Metadata
Authors:
  - Sebastien Guillemot <sebastien@emurgo.io>
Implementors: N/A
Discussions:
  - https://github.com/cardano-foundation/CIPs/pull/34
  - https://forum.cardano.org/t/cip10-transaction-metadata-label-registry/41746
Created: 2020-10-31
License: CC-BY-4.0
---

## Abstract

Cardano transaction metadata forces metadata entries to namespace their content using an unsigned integer key. This specification is a registry of which use cases has allocated which number to avoid collisions.

## Motivation: why is this CIP necessary?

The top level of the transaction metadata CBOR object is a mapping of `transaction_metadatum_label` to the actual metadata where the `transaction_metadatum_label` represents an (ideally unique) key for a metadata use case. This allows enables the following:

1) Fast lookup for nodes to query all transactions containing metadata that uses a specific key
2) Allows a single transaction to include multiple metadata entries for different standards

## Specification

### Terminology

Transaction metadata refers to an optional CBOR object in every transaction since the start of the Shelley era. It is defined as the follow CDDL data structure

```
transaction_metadatum =
    { * transaction_metadatum => transaction_metadatum }
  / [ * transaction_metadatum ]
  / int
  / bytes .size (0..64)
  / text .size (0..64)

transaction_metadatum_label = uint

transaction_metadata =
  { * transaction_metadatum_label => transaction_metadatum }
```

### Structure

These are the reserved `transaction_metadatum_label` values

`transaction_metadatum_label` | description
----------------------------  | -----------------------
0 - 15                        | reserved\*
65536 - 131071                | reserved - private use

For the registry itself, please see [registry.json](./registry.json) in the machine-readable format. Please open your pull request against
this file.

\* It's best to avoid using `0` or any a similar number like `1` that other people are very likely to use. Prefer instead to generate a random number

## Rationale: how does this CIP achieve its goals?

Creating a registry for `transaction_metadatum_label` values has the following benefit:

1) It makes it easy for developers to know which `transaction_metadatum_label` to use to query their node if looking for transactions that use a standard
2) It makes it easy to avoid collisions with other standards that use transaction metadata

## Path to Active

### Acceptance Criteria

- [x] Consistent, long-term use by Cardano implementors of the metadata label registry by all applications requiring a universally acknowledged metadata label.
- [x] Consistent, long-term use in the CIP editing process: tagging, verifying, and merging new label requirements.

### Implementation Plan

- [x] Confirmed interest and cooperation in this metadata labelling standard and its `registry.json` convention by Cardano implementors: including NFT creators, data aggregators, and sidechains.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode)

---
CIP: 11
Title: Staking key chain for HD wallets
Status: Active
Category: Wallets
Authors:
  - Sebastien Guillemot <sebastien@emurgo.io>
  - Matthias Benkort <matthias.benkort@iohk.io>
Implementors: N/A
Discussions:
  - https://github.com/cardano-foundation/CIPs/pull/33
  - https://forum.cardano.org/t/staking-key-chain-for-hd-wallets/41857
  - https://github.com/cardano-foundation/CIPs/pull/37
Created: 2020-11-04
License: CC-BY-4.0
---

## Abstract

Starting with the Shelley hardfork, Cardano makes use of both the *UTXO model* and the *account model*. To support both transaction models from the same master key, we allocate a new chain for [CIP-1852].

## Motivation: why is this CIP necessary?

Generally it's best to only use a cryptographic key for a single purpose, and so it's best to make the staking key be separate from any key used for UTXO addresses.

## Specification

> **Note** The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT", "SHOULD", "SHOULD NOT", "RECOMMENDED", "NOT RECOMMENDED", "MAY", and "OPTIONAL" in this document are to be interpreted as described in [RFC 2119](https://tools.ietf.org/html/rfc2119).

Recall that [CIP-1852] specifies the following derivation path:

```
m / purpose' / coin_type' / account' / chain / address_index
```

We set `chain=2` to indicate the *staking key chain*. Keys in this chain MUST follow the accounting model for transactions and SHOULD be used for *reward addresses*

### *address_index* value

We RECOMMEND wallets only use `address_index=0` for compatibility with existing software. This also avoids the need for staking key discovery.

Wallets that use multiple staking keys are REQUIRED to use sequential indexing with no gaps. This is to make detection of mangle addresses (addresses where the payment key belongs to the user, but the staking key doesn't) easier.

*Note*: an observer looking at the blockchain will be able to tell if two staking keys belong to the same user if they are generated from the same wallet with different `address_index` values because the payment keys inside the *base addresses* will be the same.

### Test vectors

recovery phrase
```
prevent company field green slot measure chief hero apple task eagle sunset endorse dress seed
```

private key (including chaincode) for `m / 1852' / 1815' / 0' / 2 / 0`
```
b8ab42f1aacbcdb3ae858e3a3df88142b3ed27a2d3f432024e0d943fc1e597442d57545d84c8db2820b11509d944093bc605350e60c533b8886a405bd59eed6dcf356648fe9e9219d83e989c8ff5b5b337e2897b6554c1ab4e636de791fe5427
```

reward address (with `network_id=1`)
```
stake1uy8ykk8dzmeqxm05znz65nhr80m0k3gxnjvdngf8azh6sjc6hyh36
```

## Rationale: how does this CIP achieve its goals?

### Meaning of *account*

The term "account" is unfortunately an overloaded term so we clarify all its uses here:

#### 1) "Account" as a BIP44 derivation level

BIP44 uses the term "account" as one derivation level to mean the following

> This level splits the key space into independent user identities, so the wallet never mixes the coins across different accounts.
To differentiate this from other usage, we sometimes refer to it as an `account'` (the bip32 notation) or a BIP44 Account.

#### 2) "Account" as a transaction model

Blockchains like Ethereum does not use the UTXO model and instead uses the [*Account model*](https://github.com/ethereum/wiki/wiki/Design-Rationale#accounts-and-not-utxos) for transactions.

## Path to Active

### Acceptance Criteria

- [x] All notable wallet and tooling providers follow this method of key derivation.

### Implementation Plan

- [x] This method of key derivation has been agreed as canonical and has been included in [CIP-1852].
- [x] This method of key derivation has been supported by all wallet and tool providers beginning with the Shelley ledger era.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

[CIP-1852]: https://github.com/cardano-foundation/CIPs/blob/master/CIP-1852/README.md

---
CIP: 12
Title: On-chain stake pool operator to delegates communication
Status: Proposed
Category: Metadata
Authors:
  - Marek Mahut <marek.mahut@fivebinaries.com>
  - Sebastien Guillemot <sebastien@emurgo.io>
  - Ján Hrnko <jan.hrnko@fivebinaries.com>
Discussions:
  - https://forum.cardano.org/t/on-chain-stake-pool-operator-to-delegates-communication/42229
  - https://github.com/cardano-foundation/CIPs/pull/44
Created: 2020-11-07
License: CC-BY-4.0
---

## Abstract

Standard format for metadata used in an on-chain communication of stake pool owner towards their delegates.

## Motivation: why is this CIP necessary?

Stake pool owners and their delegates lack an on-chain communication standard between them.

[CIP-0006](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0006/README.md) already defines an external feed of a stake pool within the extended metadata. However, there is need for a more verifiable on-chain communication standard that will also provide additional cost associated with such communication to prevent its abuse.

## Specification

### Terminology

We define two types of communication metadata, which are distinguished by transaction metadata label as defined in [CIP-0010: Transaction metadata label registry](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0010/README.md):

 * *Message board communication* is a type of metadata that has been included in an on-chain transaction between two base addresses associated with a stake pool operator owner address. Given the onetime fee for this communication, we are considering this as a message board of a stake pool, as it also enables delegates to easier access historical metadata communication.

 * *Direct delegate communication* is a type of metadata that has been included in an on-chain transaction between a stake pool owner account and a delegate's account. This type of communication is more expensive for the stake pool owner, preventing higher abuse and therefore enables wallets to implement notification granularity. It might be suitable for targeting specific delegates, such as messaging only new joined delegates, loyal delegates, high-amount delegates etc.

As per CIP-0010, we assign:

* *Message board communication* transaction metadata label `1990`,
* *Direct delegate communication* transaction metadata label `1991`.

### Metadata

Metadata are written in JSON format and maximum size of metadata around 16KB.

The root object property is a 3 bytes UTF-8 encoded string representing the ISO 639-3
language code of the content.

| key                    | Value                                        | Rules                                      |
| ---------------------- | -------------------------------------------- | ------------------------------------------ |
| `title` *(required)*   | Title of the communication                   | 64 bytes UTF-8 encoded string              |
| `content` *(required)* | Content of the communication                 | An array of 64 bytes UTF-8 encoded strings |
|                        |                                              |
| `valid` *(optional)*   | Slot number the communication becomes valid  | Unsigned integer                           |
| `expires` *(optional)* | Slot number until the communication is valid | Unsigned integer                           |

#### Metadata JSON schema

The [schema.json](./schema.json) file defines the metadata.

#### Metadata example including the transaction metadata label

```
{
  "1991": [ {
    "lat": {
      "title": "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do",
      "content": [
        "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do ",
        "eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut e",
        "nim ad minim veniam, quis nostrud exercitation ullamco laboris n",
        "isi ut aliquip ex ea commodo consequat. Duis aute irure dolor in",
        " reprehenderit in voluptate velit esse cillum dolore eu fugiat n",
        "ulla pariatur. Excepteur sint occaecat cupidatat non proident, s",
        "unt in culpa qui officia deserunt mollit anim id est laborum."
      ],
      "valid": 10661033,
      "expire": 10669033
    }
   },
   {
    "eng": {
      "title": "But I must explain to you how all this mistaken idea",
      "content": [
        "But I must explain to you how all this mistaken idea of denounci",
        "ng of a pleasure and praising pain was born and I will give you ",
        "a complete account of the system, and expound the actual teachin",
        "gs of the great explorer of the truth, the master-builder of hum",
        "an happiness. No one rejects, dislikes, or avoids pleasure itsel",
        "f, because it is pleasure, but because those who do not know how",
        " to pursue pleasure rationally encounter consequences."
      ],
      "valid": 10661033,
      "expire": 10669033
    }
   }
  ]
}
```

## Rationale: how does this CIP achieve its goals?

The format of the `content` field is required to be an array of 64 bytes chunks, as this is the maximum size of a JSON field in the Cardano ledger. Tools, such as wallets, are required to recompose the content of the message.

The current Cardano protocol parameter for maximum transaction size, that will hold the metadata, is around 16KB.

### Backwards compatibility

No backwards compatibility breaking changes are introduced.

## Path to Active

### Acceptance Criteria

 * [ ] Indications that more than one wallet or backend supports this standard, including:
   * [ ] Yoroi (in progress from [Implement CIP12 to Yoroi backends](https://www.lidonation.com/en/proposals/implement-cip12-to-yoroi-backends))

### Implementation Plan

 * [x] Develop reference implementation ([CIP12 communication tool examples](https://github.com/fivebinaries/cip-metadata-communication-example))
 * [x] Offer this standard for implementation in downstream tools and wallets: pending their own decisions about whether and how to display communication messages.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
CIP: 13
Title: Cardano URI Scheme
Status: Proposed
Category: Wallets
Authors:
    - Robert Phair <rphair@cosd.com>
    - Sebastien Guillemot <sebastien@emurgo.io>
    - Vicente Almonacid <vicente@emurgo.io>
Implementors: N/A
Discussions:
    - https://github.com/Emurgo/EmIPs/pull/2
    - https://forum.cardano.org/t/cip-cardano-payment-uri-scheme/41457
    - https://github.com/cardano-foundation/CIPs/pull/25
    - https://github.com/cardano-foundation/CIPs/pull/61
    - https://github.com/cardano-foundation/CIPs/pull/86
    - https://forum.cardano.org/t/cip-stake-uri-scheme-for-pools-delegation-portfolios/40594
    - https://forum.cardano.org/t/cip-generalized-cardano-urls/57464
    - https://github.com/cardano-foundation/CIPs/pull/546
    - https://github.com/cardano-foundation/CIPs/pull/559
Created: 2020-09-22
License: CC-BY-4.0
---

## Abstract

This describes a general standard URI scheme with two specific protocols to handle Ada transfers and links to weighted lists of stake pools.

## Motivation: why is this CIP necessary?

### In general:

Developers of protocols that use URI schemes should be able to choose unique protocol keywords indicating how these links are handled by applications.

Beyond the two earliest defined protocols below, protocols using distinct keywords (e.g. `//stake`) can be defined in other CIPs and implemented without ambiguity by applications which interpret those particular URI protocols.

### For payment URIs:

Users who create community content often want donations as a financial incentive. However, forcing users to open their wallet and copy-paste an address lowers the amount of people likely to send tokens (especially if they have to sync their wallet first).

If donating was as simple as clicking a link that opens a light wallet with pre-populated fields, users may be more willing to send tokens. URI schemes would enable users to easily make payments by simply clicking links on webpages or scanning QR Codes.

### For stake pool URIs:

Centralised sources of information have led a growing amount of stake to be disproportionately assigned to pools pushed near & beyond the saturation point.

Stake pool URIs will provide an additional means for small pools to acquire delegation and maintain stability, supporting diversity and possibly fault-tolerance in the Cardano network through a more even distribution of stake.

Interfaces that connect delegators with pools beyond the highly contested top choices of the in-wallet ranking algorithms are important to avoid saturation and maintain decentralization.

Larger pools and collectives can also use these URIs to link to, and spread delegation between, a family of pools they own to avoid any one of their pools becoming saturated.

Pool links allow for interfaces to initiate delegation transactions without requiring any code modifications to the wallets themselves.

URIs for weighted stake pool lists provide alternatives to using a JSON file to implement *delegation portfolios* in a way that may better suit certain platforms, applications, or social contexts.

## Specification

The core implementation should follow the [BIP-21](https://github.com/bitcoin/bips/blob/master/bip-0021.mediawiki) standard (with `bitcoin:` replaced with `web+cardano:`)

Examples:
```
<a href="web+cardano:Ae2tdPwUPEZ76BjmWDTS7poTekAvNqBjgfthF92pSLSDVpRVnLP7meaFhVd">Donate</a>
<a href="web+cardano://stake?c94e6fe1123bf111b77b57994bcd836af8ba2b3aa72cfcefbec2d3d4">Stake with us</a>
<a href="web+cardano://stake?POOL1=3.14159&POOL2=2.71828">Split between our 2 related pools</a>
<a href="web+cardano://stake?COSD">Choose our least saturated pool</a>
<a href="web+cardano://claim/v1?faucet_url=https%3A%2F%2Fclaim.hosky.io&code=consensus2023">Claim $HOSKY</a>
```

The protocol term (e.g. `//stake`) is called the _authority_ as defined in [Wikipedia > Uniform Resource Identifier > Syntax](https://en.wikipedia.org/wiki/Uniform_Resource_Identifier#Syntax).

### Choice of URI scheme name

`cardano:` is chosen over `ada:` because other projects that implement this standard tend to take the project name over the currency name (this makes sense if we consider this protocol as a generic way for interacting with the blockchain through wallets and dApps - as opposed to a simple payment system).

Depending on the protocol registration method (see Rationale), browsers generally enforce a `web+` or `ext+` prefix for non-whitelisted protocols (note: `bitcoin:` was whitelisted; see [registerProtocolHandler > Permitted schemes](https://developer.mozilla.org/en-US/docs/Web/API/Navigator/registerProtocolHandler#permitted_schemes)). The prefix `ext+` is recommended for extensions, but not mandatory (see [protocol_handlers](https://developer.mozilla.org/en-US/docs/Mozilla/Add-ons/WebExtensions/manifest.json/protocol_handlers)).

### Grammar & interpretation

This top-level definition is mainly to allow switching to a particular protocol for each separately defined `authority`, with a payment link being the default:

* When `authority` is unspecified, it is a payment URI (with an address and an optional amount parameter;
* When `authority` is explicit (containing `//` followed by the authority keyword), it is defined in the `//stake` case below or in a separate CIP for that protocol.

```
cardanouri = "web+cardano:" (paymentref | authorityref)

authorityref = (stakepoolref | otherref)
otherref = "//" authority query
```

For grammar reference, see:

  - [Wikipedia > Augmented Backus–Naur form](https://en.wikipedia.org/wiki/Augmented_Backus%E2%80%93Naur_form)
  - [RFC 2234: Augmented BNF for Syntax Specifications: ABNF](https://datatracker.ietf.org/doc/html/rfc2234)
  - [Unicode in ABNF](https://tools.ietf.org/html/draft-seantek-unicode-in-abnf-00)

#### Payment URI queries

```
paymentref = cardanoaddress [ "?" amountparam ]
cardanoaddress = *(base58 | bech32)
amountparam = "amount=" *digit [ "." *digit ]
```

The amount parameter must follow the [same rules](https://github.com/bitcoin/bips/blob/master/bip-0021.mediawiki#transfer-amountsize) described in BIP-21, namely, it must be specified in decimal ADA, without commas and using the period (.) decimal separator.

#### Stake pool URI queries

```
stakepoolref = "//stake" query
query = ( "?" stakepoolpair) *( "&" stakepoolpair)
stakepoolpair = stakepool [ "=" proportion]

stakepool = poolhexid | poolticker
poolhexid = 56HEXDIG
poolticker = 3*5UNICODE

proportion = *digit [ "." *digit ]
```

For brevity, essential in many Internet contexts, `poolticker`  must be supported here in addition to the unambiguous `poolhexid`.

##### Interpretation of `proportion`

* If only one stake pool is specified, any proportion is meaningless and ignored.
* If all stake pools have a numerical proportion, each component of the resulting stake distribution will have the same ratio as the provided `proportion` to the sum of all the propotions.
* Any missing `proportion` is assigned a precise value of `1`.
* If a stake pool is listed multiple times, the URI is rejected as invalid.

##### Handling stake pool links

When there is more than one pool registered with any of the specified `poolticker` parameters (whether for pool groups which have the same ticker for all pools, or for separate pools using the same ticker), the choice to which pool(s) to finally delegate is left to the user through the wallet UI.

The wallet UI should always confirm the exact delegation choice even when it is unambiguous from the URI.  When the user has multiple wallets, the wallet UI must select which wallet(s) the user will be delegating from.

If, during a wallet or other application's development process, it is still only able to support single pool links, these parameters in the URI query string should (by preference of the wallet UI designers) *either* be ignored *or* generate a warning message, to avoid leading the user to believe they are implementing a currently unsupported but perhaps popularly referenced multi-pool delegation list:

* any value for the first URI query argument;
* any URI query argument beyond the first.

#### Other URI queries

An ABNF grammar should be specified and explained similarly for each CIP that defines a new Cardano URI authority by explicitly defining the terms `authority` and `query` as for the "Stake pool" case above.

### Security Considerations

1. For payment links, we cannot prompt the user to send the funds right away as they may not be fully aware of the URI they clicked or were redirected to. Instead, it may be better to simply pre-populate fields in a transaction.
2. For either payment or staking links, we should be wary of people who disguise links as actually opening up a phishing website that LOOKS like that corresponding part of the wallet UI.
3. If wallets *create* stake pool links, the actual ada or lovelace balance should not be used literally as the `proportion` figure, to avoid revealing the identity of the wallet owner who is creating the portfolio (e.g. the proportions could be scaled to normalise the largest to `1`).

## Rationale: how does this CIP achieve its goals?

### Rationale for general URI scheme

#### Why not use Universal links, deep links or other non-protocol-based Solution?

An alternative solution to the original problem described above is to use standard URL links in combination with a routing backend system. The routing system is used to redirect to the app's URI. The advantage of this scheme is that it allows to provide a fallback mechanism to handle the case when no application implementing the protocol is installed (for instance, by redirecting to the App Store or Google Play). This is the approach behind iOS Universal Links and Android App Links. In general, it provides a better user experience but requires a centralized system which makes it unsuitable for as a multi-app standard.

For background, see

  - [Android Developer Docs > Add intent filters for incoming links](https://developer.android.com/training/app-links/deep-linking#adding-filters)
  - [Apple Developer Docs > Defining a custom URL scheme for your app](https://developer.apple.com/documentation/xcode/defining-a-custom-url-scheme-for-your-app)
  - [React Native > Linking](https://reactnative.dev/docs/linking.html)

### Rationale for payment links

#### Why confine payment links to address and amount like BIP-21?

BIP-21 is limited to only features Bitcoin supports. A similar feature for Ethereum would, for example, also support gas as an extra parameter. BIP-21 is easily extensible but we have to take precaution to avoid different wallets having different implementations of these features as they become available on Cardano. To get an idea of some extra features that could be added, consider this (still under discussion) proposal for Ethereum: [EIP-681](https://eips.ethereum.org/EIPS/eip-681)

### Rationale for stake pool links

#### How do URI delegation portfolio links supplement use of JSON files for the same purpose?

URIs facilitate the "social element" of delegated staking and pool promotion through a socially familiar, easily accessible, and less centralised convention for sharing stake pool references and potential delegation portfolios without having to construct or host a JSON file.

The processing of a JSON file delivered by a web server will depend highly on a user's platform and might not even be seen by the wallet application at all.  With a properly associated `web+cardano:` protocol, developers and users have another option available in case JSON files are not delivered properly to the wallet application.

For a CIP based on this principle, see [CIP-0017](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0017/README.md).

## Path to Active

### Acceptance Criteria

- [x] There exist one or more wallets supporting Payment URIs.
  - [x] Yoroi
  - [x] Begin Wallet
- [x] There exist one or more wallets supporting Stake Pool URIs.
  - [ ] TBD
- [x] There exist other CIPs or drafts defining additional URI protocols.
  - [x] [CIP-0099](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0099/README.md)  
- [x] There exist one or more wallets supporting additional URI protocols.
  - [x] Yoroi (CIP-0099)
  - [x] Begin Wallet (CIP-0099)
  - [x] VESPR (CIP-0099)  

### Implementation Plan

Encourage wallet and dApp developers to support all currently defined URI protocols, keeping in mind these are each likely to be considered separately:

- Payment URIs
- Stake Pool URIs
- all other URI schemes defined in separate CIPs

Education and advocacy about these standards should be done by:

- Developers of applications and standards requiring new URI schemes
- Cardano sponsoring companies
- Community advocates
- CIP editors

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
CIP: 14
Title: User-Facing Asset Fingerprint 
Status: Active
Category: Tokens
Authors:
  - Matthias Benkort <matthias.benkort@iohk.io>
  - Rodney Lorrimar <rodney.lorrimar@iohk.io>
Implementors: N/A
Discussions:
  - https://github.com/cardano-foundation/CIPs/pull/64
Created: 2020-02-01
License: CC-BY-4.0
---

## Abstract

This specification defines a user-facing asset fingerprint as a bech32-encoded blake2b-160 digest of the concatenation of the policy id and the asset name.

## Motivation: why is this CIP necessary?

The Mary era of Cardano introduces the support for native assets. On the blockchain, native assets are uniquely identified by both their so-called policy id and asset name. Neither the policy id nor the asset name are intended to be human-readable data. 

On the one hand, the policy id is a hash digest of either a monetary script or a Plutus script. On the other hand, the asset name is an arbitrary bytestring of up to 32 bytes (which does not necessarily decode to a valid UTF-8 sequence). In addition, it is possible for an asset to have an empty asset name, or, for assets to have identical asset names under different policies. 

Because assets are manipulated in several user-facing features on desktop and via hardware applications, it is useful to come up with a short(er) and human-readable identifier for assets that user can recognize and refer to when talking about assets. We call such an identifier an _asset fingerprint_.

## Specification

We define the asset fingerprint in pseudo-code as:

```
assetFingerprint := encodeBech32
  ( datapart = hash
    ( algorithm = 'blake2b'
    , digest-length = 20
    , message = policyId | assetName
    )
  , humanReadablePart = 'asset'
  )
```

where `|` designates the concatenation of two byte strings. The `digest-length` is given in _bytes_ (so, 160 bits).

### Reference Implementation

#### Javascript

[cip14-js](https://www.npmjs.com/package/@emurgo/cip14-js)

#### Haskell (GHC >= 8.6.5)

<details>
  <summary>Language Extensions</summary>

```hs
{-# LANGUAGE OverloadedStrings #-}
{-# LANGUAGE QuasiQuotes #-}
{-# LANGUAGE TypeApplications #-}
```
</details>

<details>
  <summary>Imports</summary>

```hs
-- package: base >= 4.0.0
import Prelude
import Data.Function
    ( (&) )

-- package: bech32 >= 1.0.2
import qualified Codec.Binary.Bech32 as Bech32

-- package: bech32-th >= 1.0.2
import Codec.Binary.Bech32.TH
    ( humanReadablePart )

-- package: bytestring >= 0.10.0.0
import Data.ByteString
    ( ByteString )

-- package: cryptonite >= 0.22
import Crypto.Hash
    ( hash )
import Crypto.Hash.Algorithms
    ( Blake2b_160 )

-- package: memory >= 0.14
import Data.ByteArray
    ( convert )

-- package: text >= 1.0.0.0
import Data.Text
    ( Text )
```
</details>

```hs
newtype PolicyId = PolicyId ByteString
newtype AssetName = AssetName ByteString
newtype AssetFingerprint = AssetFingerprint Text

mkAssetFingerprint :: PolicyId -> AssetName -> AssetFingerprint
mkAssetFingerprint (PolicyId policyId) (AssetName assetName)
    = (policyId <> assetName)
    & convert . hash @_ @Blake2b_160
    & Bech32.encodeLenient hrp . Bech32.dataPartFromBytes
    & AssetFingerprint
  where
    hrp = [humanReadablePart|asset|]
```

### Test Vectors

> :information_source: `policy_id` and `asset_name` are hereby base16-encoded; their raw, decoded, versions should be used when computing the fingerprint.

```yaml
- policy_id: 7eae28af2208be856f7a119668ae52a49b73725e326dc16579dcc373
  asset_name: ""
  asset_fingerprint: asset1rjklcrnsdzqp65wjgrg55sy9723kw09mlgvlc3

- policy_id: 7eae28af2208be856f7a119668ae52a49b73725e326dc16579dcc37e
  asset_name: ""
  asset_fingerprint: asset1nl0puwxmhas8fawxp8nx4e2q3wekg969n2auw3

- policy_id: 1e349c9bdea19fd6c147626a5260bc44b71635f398b67c59881df209
  asset_name: ""
  asset_fingerprint: asset1uyuxku60yqe57nusqzjx38aan3f2wq6s93f6ea

- policy_id: 7eae28af2208be856f7a119668ae52a49b73725e326dc16579dcc373
  asset_name: 504154415445
  asset_fingerprint: asset13n25uv0yaf5kus35fm2k86cqy60z58d9xmde92

- policy_id: 1e349c9bdea19fd6c147626a5260bc44b71635f398b67c59881df209
  asset_name: 504154415445
  asset_fingerprint: asset1hv4p5tv2a837mzqrst04d0dcptdjmluqvdx9k3

- policy_id: 1e349c9bdea19fd6c147626a5260bc44b71635f398b67c59881df209
  asset_name: 7eae28af2208be856f7a119668ae52a49b73725e326dc16579dcc373
  asset_fingerprint: asset1aqrdypg669jgazruv5ah07nuyqe0wxjhe2el6f

- policy_id: 7eae28af2208be856f7a119668ae52a49b73725e326dc16579dcc373
  asset_name: 1e349c9bdea19fd6c147626a5260bc44b71635f398b67c59881df209
  asset_fingerprint: asset17jd78wukhtrnmjh3fngzasxm8rck0l2r4hhyyt

- policy_id: 7eae28af2208be856f7a119668ae52a49b73725e326dc16579dcc373
  asset_name: 0000000000000000000000000000000000000000000000000000000000000000
  asset_fingerprint: asset1pkpwyknlvul7az0xx8czhl60pyel45rpje4z8w
```

## Rationale: how does this CIP achieve its goals?

### Design choices

- The asset fingerprint needs to be _somewhat unique_ (although collisions are plausible, see next section) and refer to a particular asset. It must therefore include both the policy id and the asset name.

- Using a hash gives us asset id of a same deterministic length which is short enough to display reasonably well on small screens.

- We use bech32 as a user-facing encoding since it is both user-friendly and quite common within the Cardano eco-system (e.g. addresses, pool ids, keys).

### Security Considerations

- With a 160-bit digest, an attacker needs at least 2^80 operations to find a collision. Although 2^80 operations is relatively low (it remains expansive but doable for an attacker), it 
  is considered safe within the context of an asset fingerprint as a mean of _user verification_ within a particular wallet. An attacker may obtain advantage if users can be persuaded 
  that a certain asset is in reality another (which implies to find a collision, and make both assets at the reach of the user). 

- We recommend however that in addition to the asset fingerprint, applications also show whenever possible a visual checksum calculated from the policy id and the asset name as specified in [CIP-YET-TO-COME](). Such generated images, which are designed to be unique and easy to distinguish, in combination with a readable asset fingerprint gives strong verification means to end users. 

## Path to Active

### Acceptance Criteria

- [x] Asset fingerprints as described have been universally adopted in: wallets, blockchain explorers, query layers, token minting utilities, NFT specifications, and CLI tools.

### Implementation Plan

- [x] Reference implementations available in both Javascript and Haskell.
- [x] Public presentation with confirmed interest in adopting this standard in advance of Mary ledger era.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
CIP: 15
Title: Registration Transaction Metadata Format
Category: Metadata
Status: Active
Authors:
    - Sebastien Guillemot <sebastien@dcspark.io>, 
    - Rinor Hoxha <rinor.hoxha@iohk.io>, 
    - Mikhail Zabaluev <mikhail.zabaluev@iohk.io>
Implementors:
    - Daedalus <https://daedaluswallet.io/>,
    - DcSpark <https://www.dcspark.io/>,
    - Eternl <https://eternl.io/>,
    - Flint <https://flint-wallet.com/>,
    - Project Catalyst <https://projectcatalyst.io/>,
    - Typhon <https://typhonwallet.io/>,
    - Yoroi <https://yoroi-wallet.com/>
Discussions:
    - https://forum.cardano.org/t/cip-catalyst-registration-metadata-format/44038
    - https://github.com/cardano-foundation/cips/pulls/58
Created: 2020-01-05
License: CC-BY-4.0
---

## Abstract

This CIP details a transaction metadata format, used by Cardano's [Project Catalyst](https://projectcatalyst.io) for voter registrations.

## Motivation: why is this CIP necessary?

Project Catalyst uses a sidechain for it's voting system.
One of the desirable properties of this sidechain is that even if its' safety is compromised, it doesn't cause a loss of funds on Cardano mainnet. 
To achieve this, instead of using Cardano wallets' recovery phrase on the sidechain, we introduce the "voting key".

However, since Catalyst uses stake-based voting, a user needs to associate their mainnet Ada to their voting key. 
This can be achieved through a voter registration transaction.

We therefore need a voter registration transaction that serves three purposes:

1. Registers a voting key to be included in the sidechain
2. Associates Mainnet ada to this voting key
3. Declare an address to receive Catalyst voting rewards

## Specification

### Voting key format

A voting key is simply an ED25519 key. 
How this key is created is up to the wallet, although it is not recommended that the wallet derives this key deterministicly from a mnemonic used on Cardano.

### Registration metadata format

A Catalyst registration transaction is a regular Cardano transaction with a specific transaction metadata associated with it.

Notably, there should be four entries inside the metadata map:

Voting registration example:

```cddl
61284: {
  // voting_key - CBOR byte array
  1: "0xa6a3c0447aeb9cc54cf6422ba32b294e5e1c3ef6d782f2acff4a70694c4d1663",
  // stake_pub - CBOR byte array
  2: "0xad4b948699193634a39dd56f779a2951a24779ad52aa7916f6912b8ec4702cee",
  // reward_address - CBOR byte array
  3: "0x00588e8e1d18cba576a4d35758069fe94e53f638b6faf7c07b8abd2bc5c5cdee47b60edc7772855324c85033c638364214cbfc6627889f81c4",
  // nonce
  4: 5479467
}
```

The entries under keys 1, 2, 3, and 4 represent the Catalyst voting key, the staking key on the Cardano network, the address to receive rewards, and a nonce, respectively. 
A registration with these metadata will be considered valid if the following conditions hold:

- The nonce is an unsigned integer (of CBOR major type 0) that should be monotonically rising across all transactions with the same staking key. 
  - The advised way to construct a nonce is to use the current slot number.
  - This is a simple way to keep the nonce increasing without having to access the previous transaction data.
- The reward address is a Shelley address discriminated for the same network this transaction is submitted to.

To produce the signature field, the CBOR representation of a map containing a single entry with key `61284` and the registration metadata map in the format above is formed, designated here as `sign_data`.
This data is signed with the staking key as follows: first, the blake2b-256 hash of `sign_data` is obtained. 
This hash is then signed using the Ed25519 signature algorithm. The signature metadata entry is added to the transaction under key 61285 as a CBOR map with a single entry that consists of the integer key 1 and signature as obtained above as the byte array value.

Signature example:

```cddl
61285: {
  // signature - ED25119 signature CBOR byte array
  1: "0x8b508822ac89bacb1f9c3a3ef0dc62fd72a0bd3849e2381b17272b68a8f52ea8240dcc855f2264db29a8512bfcd522ab69b982cb011e5f43d0154e72f505f007"
}
```
### Versioning

This CIP is not to be versioned using a traditional scheme, rather if any large technical changes require a new proposal to replace this one.
Small changes can be made if they are completely backwards compatible.

### Changelog

Catalyst Fund 3: 
- added the `reward_address` inside the `key_registration` field.

Catalyst Fund 4:
- added the `nonce` field to prevent replay attacks;
- changed the signature algorithm from one that signed `sign_data` directly to signing the Blake2b hash of `sign_data` to accommodate memory-constrained hardware wallet devices.

It was planned that since Fund 4, `registration_signature` and the `staking_pub_key` entry inside the `key_registration` field will be deprecated.
This has been deferred to a future revision of the protocol.

## Rationale: how does this CIP achieve its goals?

The described metadata format allows for the association of a voting key with a stake credential on a Cardano network.

### Associating stake with a voting key

Cardano uses the UTxO model so to completely associate a wallet's balance with a voting key (i.e. including enterprise addresses), we would need to associate every payment key to a voting key individually.
Although there are attempts at this (see [CIP-0008]), the resulting data structure is a little excessive for on-chain metadata (which we want to keep small).

Given the above, we choose to only associate staking keys with voting keys.
Since most Cardano wallets only use base addresses for Shelley wallet types, in most cases this should perfectly match the user's wallet.

### Future development

A future change of the Catalyst system may make use of a time-lock script to commit ADA on the mainnet for the duration of a voting period.
The voter registration metadata in this method will not need an association with the staking key.
Therefore, the `staking_pub_key` map entry and the `registration_signature` payload with key `61285` will no longer be required.

## Path to Active

### Acceptance Criteria

- [x] This metadata format is implemented by at least 3 wallets
  - Deadalus <https://daedaluswallet.io/>
  - Eternl <https://eternl.io/>,
  - Flint <https://flint-wallet.com/>,
  - Typhon <https://typhonwallet.io/>,
  - Yoroi <https://yoroi-wallet.com/>
- [x] This metadata format is used by Catalyst for at least 3 funds
  - This format has been used up to and included Catalyst fund 10

### Implementation Plan

- [x] Author(s) to provide a schema cddl file
  - See the [schema file](./schema.cddl)
- [x] Author(s) to provide a test vectors file
  -  See [test vector file](./test-vector.md)
- [x] Author(s) to provide a npm package to support the creation of this metadata format
  - [catalyst-registration-js](https://www.npmjs.com/package/@dcspark/catalyst-registration-js)

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

[CIP-0008]: https://github.com/cardano-foundation/CIPs/blob/master/CIP-0008/README.md


---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0015/README.md
---

- --
CIP: 15
Title: Registration Transaction Metadata Format
Category: Metadata
Status: Active
Authors:
- Sebastien Guillemot <sebastien@dcspark.io>,
- Rinor Hoxha <rinor.hoxha@iohk.io>,
- Mikhail Zabaluev <mikhail.zabaluev@iohk.io>
Implementors:
- Daedalus <https://daedaluswallet.io/>,
- DcSpark <https://www.dcspark.io/>,
- Eternl <https://eternl.io/>,
- Flint <https://flint-wallet.com/>,
- Project Catalyst <https://projectcatalyst.io/>,
- Typhon <https://typhonwallet.io/>,
- Yoroi <https://yoroi-wallet.com/>
Discussions:
- https://forum.cardano.org/t/cip-catalyst-registration-metadata-format/44038
- https://github.com/cardano-foundation/cips/pulls/58
Created: 2020-01-05
License: CC-BY-4.0
- --

## Abstract

This CIP details a transaction metadata format, used by Cardano's [Project Catalyst](https://projectcatalyst.io) for voter registrations.

## Motivation: why is this CIP necessary?

Project Catalyst uses a sidechain for it's voting system.
One of the desirable properties of this sidechain is that even if its' safety is compromised, it doesn't cause a loss of funds on Cardano mainnet.
To achieve this, instead of using Cardano wallets' recovery phrase on the sidechain, we introduce the "voting key".

However, since Catalyst uses stake-based voting, a user needs to associate their mainnet Ada to their voting key.
This can be achieved through a voter registration transaction.

We therefore need a voter registration transaction that serves three purposes:

1. Registers a voting key to be included in the sidechain
2. Associates Mainnet ada to this voting key
3. Declare an address to receive Catalyst voting rewards

## Specification

### Voting key format

A voting key is simply an ED25519 key.
How this key is created is up to the wallet, although it is not recommended that the wallet derives this key deterministicly from a mnemonic used on Cardano.

### Registration metadata format

A Catalyst registration transaction is a regular Cardano transaction with a specific transaction metadata associated with it.

Notably, there should be four entries inside the metadata map:

Voting registration example:

```cddl
61284: {
  // voting_key - CBOR byte array
  1: "0xa6a3c0447aeb9cc54cf6422ba32b294e5e1c3ef6d782f2acff4a70694c4d1663",
  // stake_pub - CBOR byte array
  2: "0xad4b948699193634a39dd56f779a2951a24779ad52aa7916f6912b8ec4702cee",
  // reward_address - CBOR byte array
  3: "0x00588e8e1d18cba576a4d35758069fe94e53f638b6faf7c07b8abd2bc5c5cdee47b60edc7772855324c85033c638364214cbfc6627889f81c4",
  // nonce
  4: 5479467
}
```

The entries under keys 1, 2, 3, and 4 represent the Catalyst voting key, the staking key on the Cardano network, the address to receive rewards, and a nonce, respectively.
A registration with these metadata will be considered valid if the following conditions hold:

- The nonce is an unsigned integer (of CBOR major type 0) that should be monotonically rising across all transactions with the same staking key.
- The advised way to construct a nonce is to use the current slot number.
- This is a simple way to keep the nonce increasing without having to access the previous transaction data.
- The reward address is a Shelley address discriminated for the same network this transaction is submitted to.

To produce the signature field, the CBOR representation of a map containing a single entry with key `61284` and the registration metadata map in the format above is formed, designated here as `sign_data`.
This data is signed with the staking key as follows: first, the blake2b-256 hash of `sign_data` is obtained.
This hash is then signed using the Ed25519 signature algorithm. The signature metadata entry is added to the transaction under key 61285 as a CBOR map with a single entry that consists of the integer key 1 and signature as obtained above as the byte array value.

Signature example:

```cddl
61285: {
  // signature - ED25119 signature CBOR byte array
  1: "0x8b508822ac89bacb1f9c3a3ef0dc62fd72a0bd3849e2381b17272b68a8f52ea8240dcc855f2264db29a8512bfcd522ab69b982cb011e5f43d0154e72f505f007"
}
```
### Versioning

This CIP is not to be versioned using a traditional scheme, rather if any large technical changes require a new proposal to replace this one.
Small changes can be made if they are completely backwards compatible.

### Changelog

Catalyst Fund 3:
- added the `reward_address` inside the `key_registration` field.

Catalyst Fund 4:
- added the `nonce` field to prevent replay attacks;
- changed the signature algorithm from one that signed `sign_data` directly to signing the Blake2b hash of `sign_data` to accommodate memory-constrained hardware wallet devices.

It was planned that since Fund 4, `registration_signature` and the `staking_pub_key` entry inside the `key_registration` field will be deprecated.
This has been deferred to a future revision of the protocol.

## Rationale: how does this CIP achieve its goals?

The described metadata format allows for the association of a voting key with a stake credential on a Cardano network.

### Associating stake with a voting key

Cardano uses the UTxO model so to completely associate a wallet's balance with a voting key (i.e. including enterprise addresses), we would need to associate every payment key to a voting key individually.
Although there are attempts at this (see [CIP-0008]), the resulting data structure is a little excessive for on-chain metadata (which we want to keep small).

Given the above, we choose to only associate staking keys with voting keys.
Since most Cardano wallets only use base addresses for Shelley wallet types, in most cases this should perfectly match the user's wallet.

### Future development

A future change of the Catalyst system may make use of a time-lock script to commit ADA on the mainnet for the duration of a voting period.
The voter registration metadata in this method will not need an association with the staking key.
Therefore, the `staking_pub_key` map entry and the `registration_signature` payload with key `61285` will no longer be required.

## Path to Active

### Acceptance Criteria

- [x] This metadata format is implemented by at least 3 wallets
- Deadalus <https://daedaluswallet.io/>
- Eternl <https://eternl.io/>,
- Flint <https://flint-wallet.com/>,
- Typhon <https://typhonwallet.io/>,
- Yoroi <https://yoroi-wallet.com/>
- [x] This metadata format is used by Catalyst for at least 3 funds
- This format has been used up to and included Catalyst fund 10

### Implementation Plan

- [x] Author(s) to provide a schema cddl file
- See the [schema file](./schema.cddl)
- [x] Author(s) to provide a test vectors file
- See [test vector file](./test-vector.md)
- [x] Author(s) to provide a npm package to support the creation of this metadata format
- [catalyst-registration-js](https://www.npmjs.com/package/@dcspark/catalyst-registration-js)

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

[CIP-0008]: https://github.com/cardano-foundation/CIPs/blob/master/CIP-0008/README.md

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0016/README.md
---

- --
CIP: 16
Title: Cryptographic Key Serialisation Formats
Status: Active
Category: Tools
Authors:
- Luke Nadur <luke.nadur@iohk.io>
Implementors:
- jcli <https://github.com/input-output-hk/catalyst-core/tree/main/src/jormungandr/jcli>
- cardano-signer <https://github.com/gitmachtl/cardano-signer>
- cardano-serialization-lib <https://github.com/Emurgo/cardano-serialization-lib>
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/57
Type: Standards
Created: 2020-12-21
License: Apache-2.0
- --

## Abstract

This CIP defines serialisation formats for the following types of
cryptographic keys across the Cardano eco-system:

- Regular Ed25519 keys

- [BIP32-Ed25519](https://ieeexplore.ieee.org/document/7966967) extended keys
  (Ed25519 extended keys with BIP32-style derivation)

## Motivation

Throughout the Cardano eco-system, different projects have used different
serialisation formats for cryptographic keys.

For example, for BIP32-Ed25519 extended signing keys, the
[`cardano-crypto`](https://github.com/input-output-hk/cardano-crypto)
implementation supports a 128-byte binary serialization format, while
[`jcli`](https://input-output-hk.github.io/jormungandr/jcli/introduction.html)
and
[`cardano-addresses`](https://github.com/input-output-hk/cardano-addresses)
supports a 96-byte binary serialization format.

Another example would be
[`cardano-cli`](https://github.com/input-output-hk/cardano-node) which
supports a custom JSON format, referred to as "text envelope", (which can be
used for serialising keys) that isn't supported by other projects in the
eco-system.

This has introduced compatibility problems for both users and developers:

- Users cannot easily utilize their keys across different tools and software
  in the Cardano eco-system as they may be serialized in different ways.

- Developers wanting to support the different serialisation formats may need
  to write potentially error-prone (de)serialisation and conversion
  operations.

Therefore, this CIP aims to define standard cryptographic key serialisation
formats to be used by projects throughout the Cardano eco-system.

## Specification

### Verification Keys

For the verification (public) key binary format, we simply use the raw 32-byte
Ed25519 public key data.

This structure should be Bech32 encoded, using one of the appropriate `*_vk`
prefixes defined in CIP-0005.

### Extended Verification Keys

For extended verification (public) keys, we define the following 64-byte
binary format:

```
- -----------------------+-----------------------+
| Public Key (32 bytes) | Chain Code (32 bytes) |
- -----------------------+-----------------------+
```

That is, a 32-byte Ed25519 public key followed by a 32-byte chain code.

This structure should be Bech32 encoded, using one of the appropriate `*_xvk`
prefixes defined in CIP-0005.

### Signing Keys

For the signing (private) key binary format, we simply use the raw 32-byte
Ed25519 private key data.

This structure should be Bech32 encoded, using one of the appropriate `*_sk`
prefixes defined in CIP-0005.

### Extended Signing Keys

For extended signing (private) keys, we define the following 96-byte binary
format:

```
- ---------------------------------+-----------------------+
| Extended Private Key (64 bytes) | Chain Code (32 bytes) |
- ---------------------------------+-----------------------+
```

That is, a 64-byte Ed25519 extended private key followed by a 32-byte chain
code.

This structure should be Bech32 encoded, using one of the appropriate `*_xsk`
prefixes defined in CIP-0005.

## Rationale

### Extended Signing Key Format

As mentioned in the [Abstract](#abstract), the original
[`cardano-crypto`](https://github.com/input-output-hk/cardano-crypto)
implementation defined a 128-byte binary serialization format for
BIP32-Ed25519 extended signing keys:

```
- ---------------------------------+-----------------------+-----------------------+
| Extended Private Key (64 bytes) | Public Key (32 bytes) | Chain Code (32 bytes) |
- ---------------------------------+-----------------------+-----------------------+
```

However, as it turns out, keeping around the 32-byte Ed25519 public key is
redundant as it can easily be derived from the Ed25519 private key (the first
32 bytes of the 64-byte extended private key).

Therefore, because other projects such as
[`jcli`](https://input-output-hk.github.io/jormungandr/jcli/introduction.html)
and
[`cardano-addresses`](https://github.com/input-output-hk/cardano-addresses)
already utilize the more compact 96-byte format, we opt to define that as the
standard.

## Path to Active

### Acceptance Criteria

- [x] Confirm support by applications and tools from different developers:
- [x] [jcli](https://github.com/input-output-hk/catalyst-core/tree/main/src/jormungandr/jcli)
- [x] [cardano-signer](https://github.com/gitmachtl/cardano-signer)
- [x] [cardano-serialization-lib](https://github.com/Emurgo/cardano-serialization-lib)

### Implementation Plan

N/A

## Copyright

This CIP is licensed under [Apache-2.0](https://www.apache.org/licenses/LICENSE-2.0).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0017/README.md
---

- --
CIP: 17
Title: Cardano Delegation Portfolio
Status: Inactive (abandoned for lack of interest)
Category: Tools
Authors:
- Matthias Benkort <matthias.benkort@cardanofoundation.org>
Implementors: []
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/82
Created: 2020-04-02
License: CC-BY-4.0
- --

## Abstract

This document details a common format for sharing Cardano delegation portfolio across various tools and wallets.

## Motivation: why is this CIP necessary?

Stakeholders have indicated the desire to split their stake in various sizes and delegate to n pools from a single wallet/mnemonic. Albeit there are no monetary incentive for users to do this, the desire to drive decentralisation is sufficiently prevalent to justify it. Furthermore, stakeholders want to introduce a certain social element to this activity by sharing their delegation portfolio with other stakeholders. This specification should help to standardize the representation of portfolios across tools for more interoperability.

## Specification

### Overview

We'll use JSON as a data format for it is commonly used and supported across many programming languages, and is also relatively readable on itself. Portfolios should abide by the [JSON schema given in appendix](CIP-0017.json).

At minima, a portfolio should cover a list of delegation choices (pools and weights) and have a human-readable name for easier identification.

### Weights

For each pool, we demand a `weight` which can capture a certain stake proportion within the portfolio. The value is an integer, and relative to other weights in the portfolio. For example, a portfolio with two pools and respective weights of `1` and `2` means that we expect users to assign twice more stake to the second pool than the first. Fundamentally, this means that for every 3 Ada, 1 Ada should go to the first pool, and 2 Ada should go to the second. Note that this is equivalent to having weights of `10`/`20` or `14` / `28`. Weights are ultimately interpreted as fractions.

Portfolios which treat all stake pools equally should use the same weight (e.g. `1`) for each pool.

### Example

```json
{ "name": "Metal 🤘"
, "description": "Pools supporting Metal music across the world."
, "pools":
  [ { "id": "d59123f4dce7c62fa74bd37a759c7ba665dbabeb28f08b4e5d4802ca"
    , "name": "Dark Tranquility"
    , "ticker": "DARK"
    , "weight": 42
    }
  , { "id": "5f3833027fe8c8d63bc5e75960d9a22df52e41bdf62af5b689663c50"
    , "ticker": "NITRO"
    , "weight": 14
    }
  , { "id": "a16abb03d87b86f30bb743aad2e2504b126286fe744d3d2f6a0b4aec"
    , "name": "Loudness"
    , "ticker": "LOUD"
    , "weight": 37
    }
  , { "id": "9f9bdee3e053e3102815b778db5ef8d55393f7ae83b36f906f4c3a47"
    , "weight": 25
    }
  ]
}
```

## Rationale: how does this CIP achieve its goals?

1. JSON is widely used, widely supported and quite lightweight. Makes for a reasonable choice of data format.

2. Using JSON schema for validation is quite common when dealing with JSON and it's usually sufficiently precise to enable good interoperability.

3. The portfolio should only capture information that are not subject to radical change. That is, stake pools parameters like pledge or fees are excluded since they can be changed fairly easily using on-chain certificate updates.

4. The JSON schema doesn't enforce any `additionalProperties: false` for neither the top-level object definition nor each stake pool objects. This allows for open extension of the objects with custom fields at the discretion of applications implementing this standard. The semantic of well-known properties specified in this document is however fixed.

5. Since the portfolio format isn't _immediately user-facing_, we favor base16 over bech32 for the pool id's encoding for there's better support and tooling for the former.

### Backwards Compatibility

#### Adafolio

The format used by [Adafolio](https://adafolio.com) share a lot of similarities with the proposed format in this CIP. In order to power its frontend user interface, Adafolio contains however several fields which we consider _too volatile_ and unnecessary to the definition of a portfolio. This doesn't preclude the format used by Adafolio as a valid portfolio format (see also point (4). in the rationale above).

The only point of incompatibility regards the `pool_id` field (in Adafolio) vs the `id` field (in this proposal) which we deem more consistent with regards to other field.

## Path to Active

### Acceptance Criteria

- [ ] At least one pair of applications (wallets, explorers or other tools) together support the following:
- [ ] generation of the specified portfolio file format
- [ ] interpretation and use of the specified portfolio file format

### Implementation Plan

- [ ] Provide a reference implementation and/or parsing library to read and/or write files in this schema.

# Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0018/README.md
---

- --
CIP: 18
Title: Multi-Stake-Keys Wallets
Status: Proposed
Category: Wallets
Authors:
- Matthias Benkort <matthias.benkort@iohk.io>
Implementors: []
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/83
Created: 2020-03-18
License: CC-BY-4.0
- --

## Abstract

This document describes how to evolve sequential wallets from Cardano to support multiple stake keys. This proposal is an extension of [CIP-1852] and [CIP-0011].

## Motivation: why is this CIP necessary?

Cardano wallets originally approached stake delegation by considering a single stake key per wallet. While this was beneficial in terms of ease of implementation and simplicity of reasoning, this is unsuitable for many users with large stakes. Indeed, the inability to split out the stake into multiple pools often leads to over-saturation of existing pools in case of delegation. The only workaround so far requires users to split their funds into multiple accounts and manage them independently. This can be quite cumbersome for a sufficiently large number of accounts.

Even for smaller actors, it can be interesting to delegate to multiple pools to limit risks. Pools may underperform or simply change their parameters from a day to another (for which wallets still do not warn users about). Delegating to more pools can reduce the impact of pool failure (for one or more epochs) or unattended changes in pool fees. It may as well be a matter of choice if users do want to delegate to several independent entities for various other reasons.

Another concern regards privacy leaks coming with the existing wallet scheme. Since the same stake key is associated with every single address of the wallet, it creates a kind of watermark that allows for tracing all funds belonging to the same wallet very easily (it suffices to look at the stake part of addresses). By allowing the wallet to hold multiple stake keys, rotating through them when creating changes does make the traceability a bit harder. One could imagine using a hundred stake keys delegated to the same pool.

## Specification

### Overview

The restriction from [CIP-0011] regarding the derivation index reserved for stake key is rendered obsolete by this specification. That is, one is allowed to derive indexes beyond the first one (index = 0) to effectively administrate multi-stake keys accounts. The creation of new stake keys is however tightly coupled to the registration of associated stake keys to allow wallet software to automatically discover stake keys on-chain. In this design, stake key indexes form **at all time a contiguous sequence with no gap**.

### Key registration

We introduce the concept of _UTxO stake key pointer_ to reliably keep track of stake keys on the blockchain. The gist is to require that every key registration and/or deregistration consume and create a special UTxO which in itself is pointing to the next available stake key of the wallet. Such pointer allows piggybacking on the existing UTxO structure to cope with concurrency issues and rollbacks that are inherent to a distributed system such as Cardano. Plus, this mechanism only demands a low overhead for wallet software and may be recognized as a special spending pattern by hardware devices.

In more details, we require that beyond the first stake key (index = 0), all registrations must satisfy the following rules:

1. Stake keys must be derived sequentially, from 0 and onwards.
1. Every stake key registration must be accompanied by a matching delegation certificate.
1. Every registration transaction must create a special output of exactly `minUTxOValue` Ada such that its address is an enterprise address with a single **payment part** using the stake key hash of the next available stake key of the wallet to be registered after processing the registration transaction.
1. Unless no key beyond the first one is registered, every registration transaction must consume the special UTxO stake key pointer corresponding to the previous key registration (resp. de-registration) for that wallet.
> **Note** The `minUTxOValue` is fixed by the protocol. It is defined as part of the Shelley genesis and can be updated via on-chain protocol updates. At the moment, this equals 1 Ada on the mainnet.

#### Example

For example, a wallet that has already registered stake keys 0, 1 and 2 have a UTxO for which the payment part is a hash of the stake key at index=3. If the wallet wants to register two new stake keys at index 3 and 4, it'll do so in a single transaction, by consuming the pointer UTxO and by creating a new one for which the payment part would be a hash of the stake key at index=5.

Note that this only requires two signatures from stake keys at indexes 3 and 4.

### Key de-registration

Key de-registrations work symmetrically and require that:

1. Stake keys are de-registered sequentially, from the highest and downwards.
1. Unless the first key of the wallet is being de-registered, every de-registration transaction must create a special output of exactly `minUtxOValue` Ada such that its address is an enterprise address with a single **payment part** using the stake key hash of the next stake key of the wallet after processing the de-registration transaction.
1. Every de-registration must consume the special UTxO stake key pointer corresponding to the previous key de-registration (resp. registration) for that wallet.

### Backwards Compatibility

As stated in the introduction, this proposal is built on top of [CIP-0011] such that backward compatibility is preserved when a single key is used. In fact, The management of the first stake key at index 0 remains unchanged and does not require any pointer. This preserves backward compatibility with the existing design for a single stake key wallet and offers a design that can be implemented on top, retro-actively.

Nevertheless, we do assume that existing wallets following [CIP-0011] are already fully capable of discovering addresses using stake keys not belonging to the wallet. Some may even report them as _mangled_.
> **Note** An address is said _mangled_ when it has a stake part, and the stake part isn't recognized as belonging to its associated wallet. That is, the payment part and the stake part appear to come from two different sources. This could be the case if the address has been purposely constructed in such a way (because the stake rights and funds are managed by separate entities), or because the stake part refers to a key hash which is no longer known of the wallet (because the associated key registration was rolled back).

As a result, this extension would not incapacitate existing wallets since the payment ownership is left untouched. However, wallets not supporting the extension may display addresses delegated to keys beyond the first one as mangled and may also fail to report rewards correctly across multiple keys.

We deem this to be an acceptable and fairly minor consequence but encourage existing software to raise awareness about this behaviour.


## Rationale: how does this CIP achieve its goals?

- Carrying an extra UTxO pointer makes it possible to not worry (too much) about concurrency issues and problems coming with either, multiple instances of the wallet (like many users do between a mobile and desktop wallet) or the usual rollbacks which may otherwise create gaps in the indexes. By forcing all registration (resp. deregistration) transactions to be chained together, we also enforce that any rollbacks do maintain consistency of the index state: if any intermediate transaction is rolled back, then transactions they depend on are also rolled back.

- The first registration induces an extra cost for the end-user for the wallet needs to create a new UTxO with a minimum value. That UTxO is however passed from registration to registration afterwards without any extra cost. It can also be fully refunded upon de-registering the last stake key. So in practice, it works very much like a key deposit.

- We do not allow mixing up key registration and key deregistration as part of the same transaction for it makes the calculation of the pointer trickier for wallet processing transactions. A single transaction either move the pointer up or down.

- There's in principle nothing preventing someone from sending money to the special key-registration tracking address. Wallets should however only keep track of UTxOs created as part of transactions that register stake keys (and have therefore been authorized by the wallet itself). Applications are however encouraged to collect any funds sent to them in an ad-hoc manner on such keys.

## Path to Active

### Acceptance Criteria

- [ ] There exists one or more reference implementations with appropriate testing illustrating the viability of this approach and specification.

### Implementation Plan

- [ ] Update this proposal to account for the Conway Ledger era, which brings new types of certificates for registering stake keys.

- [ ] Develop the proposed Reference Implementation as suggested when this CIP was originally published (see Discussion link in header for history).
- [ ] Contact wallet and dApp representatives in the community to develop and maintain interest in their support for this specification.

# Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

[CIP-1852]: https://github.com/cardano-foundation/CIPs/blob/master/CIP-1852
[CIP-0011]: https://github.com/cardano-foundation/CIPs/blob/master/CIP-0011

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0019/README.md
---

- --
CIP: 19
Title: Cardano Addresses
Status: Active
Category: Ledger
Authors:
- Matthias Benkort <matthias.benkort@cardanofoundation.org>
Implementors: N/A
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/78
Created: 2020-03-25
License: CC-BY-4.0
- --

## Abstract

This specification describes the structure of addresses in Cardano, covering both addresses introduced in the Shelley era and the legacy format from the Byron era.

## Motivation: why is this CIP necessary?

Document design choices for posterity. Most applications interacting with the Cardano blockchain will likely not have any need for this level of details, however, some might. This CIP is meant to capture this knowledge.

## Specification

### Introduction

In Cardano, an address is a **sequence of bytes** that conforms to a particular format, which we describe below.

However, users will typically come into contact with addresses only after these addresses have been **encoded** into sequences of human-readable characters. In Cardano, the [Bech32][] and [Base58][] encodings are used to encode addresses, as opposed to standard hexadecimal notation (Base16, example `0x8A7B`). These encoded sequence of characters have to be distinguished from the byte sequences that they encode, but lay users will (and should) perceive the encoded form as "the" address.

### User-facing Encoding

By convention, **Shelley** and stake addresses are encoded using **[Bech32][]**, with the exception that Cardano does not impose a length limit on the sequence of characters. The human-readable prefixes are defined in [CIP-0005][]; the most common prefix is `addr`, representing an address on mainnet. Bech32 is the preferred encoding, as its built-in error detection may protect users against accidental misspellings or truncations.

Again by convention, **Byron** addresses are encoded in **[Base58][]**.

Historically, Byron addresses were introduced before the design of Bech32, which solves various issues of the Base58 encoding format (see [Bech32's motivation](https://github.com/bitcoin/bips/blob/master/bip-0173.mediawiki#motivation) for more detail). Byron addresses were however kept as Base58 to easily distinguish them from new addresses introduced in Shelley, massively making use of Bech32 for encoding small binary objects.

Cave: In principle, it is possible for a Shelley address to be encoded in Base58 and a Byron address to be encoded in Bech32 (without length limit). However, implementations are encouraged to reject addresses that were encoded against convention, as this helps with the goal that lay users only encounter a single, canonical version of every address.

Examples of different addresses encoded in different eras:

| Address Type | Encoding | Example                                                                                                              |
| ---          | ---      | ---                                                                                                                  |
| Byron        | Base58   | `37btjrVyb4KDXBNC4haBVPCrro8AQPHwvCMp3RFhhSVWwfFmZ6wwzSK6JK1hY6wHNmtrpTf1kdbva8TCneM2YsiXT7mrzT21EacHnPpz5YyUdj64na` |
| Shelley      | bech32   | `addr1vpu5vlrf4xkxv2qpwngf6cjhtw542ayty80v8dyr49rf5eg0yu80w`                                                         |
| stake        | bech32   | `stake1vpu5vlrf4xkxv2qpwngf6cjhtw542ayty80v8dyr49rf5egfu2p0u`                                                        |

### Binary format

In Cardano, the sequence of bytes (after decoding with Bech32 or Base58) that represents an address  comprises two parts, a one-byte **header** and a **payload** of several bytes. Depending on the header, the interpretation and length of the payload varies.

In the header-byte, bits [7;4] indicate the type of addresses being used; we'll call these four bits the **header type**. The remaining four bits [3;0] are either unused or refer to what we'll call the **network tag**. There are currently 11 types of addresses in Cardano which we'll divide into three categories: [Shelley addresses], [stake addresses], and [Byron addresses].

```
  1 byte     variable length
 <------> <------------------->
┌────────┬─────────────────────┐
│ header │        payload      │
└────────┴─────────────────────┘
    🔎
    ╎          7 6 5 4 3 2 1 0
    ╎         ┌─┬─┬─┬─┬─┬─┬─┬─┐
    ╰╌╌╌╌╌╌╌╌ │t│t│t│t│n│n│n│n│
              └─┴─┴─┴─┴─┴─┴─┴─┘
```

See also the more detailed [ABNF grammar in annex].

#### Network Tag

Except for [Byron addresses] (type 8 = `1000`), the second half of the header (bits [3;0]) refers to the network tag which can have the following values and semantics. Other values of the network tag are currently reserved for future network types. In the case of [Byron addresses], bits [3;0] have a completely separate definition detailed in the section below.

Network Tag (`. . . . n n n n`)   | Semantic
- --                               | ---
`....0000`                        | Testnet(s)
`....0001`                        | Mainnet


#### Shelley Addresses

There are currently 8 types of Shelley addresses summarized in the table below:

Header type (`t t t t . . . .`) | Payment Part     | Delegation Part
- --                             | ---              | ---
(0) `0000....`                  | `PaymentKeyHash` | `StakeKeyHash`
(1) `0001....`                  | `ScriptHash`     | `StakeKeyHash`
(2) `0010....`                  | `PaymentKeyHash` | `ScriptHash`
(3) `0011....`                  | `ScriptHash`     | `ScriptHash`
(4) `0100....`                  | `PaymentKeyHash` | `Pointer`
(5) `0101....`                  | `ScriptHash`     | `Pointer`
(6) `0110....`                  | `PaymentKeyHash` | ø
(7) `0111....`                  | `ScriptHash`     | ø

- `PaymentKeyHash` and `StakeKeyHash` refer to `blake2b-224` hash digests of Ed25519 verification keys. How keys are obtained is out of the scope of this specification. Interested readers may look at [CIP-1852] for more details.

- `ScriptHash` refer to `blake2b-224` hash digests of serialized monetary scripts. How scripts are constructed and serialized is out of the scope of this specification.

- `Pointer` is detailed in the section below.

##### Payment part

Fundamentally, the first part of a Shelley address indicates the ownership of the funds associated with the address. We call it the **payment part**. Whoever owns the payment parts owns any funds at the address. As a matter of fact, in order to spend from an address, one must provide a witness attesting that the address can be spent. In the case of a `PaymentKeyHash`, it means providing a signature of the transaction body made with the signing key corresponding to the hashed public key (as well as the public key itself for verification). For monetary scripts, it means being able to provide the source script and meet the necessary conditions to validate the script.

##### Delegation part

The second part of a Shelley address indicates the owner of the stake rights associated with the address. We call it the **delegation part**. Whoever owns the delegation parts owns the stake rights of any funds associated with the address. In most scenarios, the payment part and the delegation part are owned by the same party. Yet it is possible to construct addresses where both parts are owned and managed by separate entities. We call such addresses **mangled addresses** or **hybrid addresses**.

Some addresses (types 6 and 7) carry no delegation part whatsoever. Their associated stake can't be delegated. They can be used by parties who want to prove that they are not delegating funds which is typically the case for custodial businesses managing funds on the behalf of other stakeholders. Delegation parts can also be defined in terms of on-chain [pointers].

##### Pointers

> **Note**
> From the Conway ledger era, new pointer addresses cannot be added to Mainnet.

In an address, a **chain pointer** refers to a point of the chain containing a stake key registration certificate. A point is identified by 3 coordinates:

- An absolute slot number
- A transaction index (within that slot)
- A (delegation) certificate index (within that transaction)

These coordinates form a concise way of referring to a stake key (typically half the size of a stake key hash). They are serialized as three variable-length positive numbers following the ABNF grammar here below:

```abnf
POINTER = VARIABLE-LENGTH-UINT ; slot number
        | VARIABLE-LENGTH-UINT ; transaction index
        | VARIABLE-LENGTH-UINT ; certificate index

VARIABLE-LENGTH-UINT = (%b1 | UINT7 | VARIABLE-LENGTH-UINT)
                     / (%b0 | UINT7)

UINT7 = 7BIT
```

#### Stake Addresses

Like [Shelley addresses], stake addresses (also known as **reward addresses**) start with a single header byte identifying their type and the network, followed by 28 bytes of payload identifying either a stake key hash or a script hash.

Header type (`t t t t . . . .`) | Stake Reference
- --                             | ---
(14) `1110....`                  | `StakeKeyHash`
(15) `1111....`                  | `ScriptHash`

- `StakeKeyHash` refers to `blake2b-224` hash digests of Ed25519 verification keys. How keys are obtained is out of the scope of this specification. Interested readers may look at [CIP-1852] for more details.

- `ScriptHash` refers to `blake2b-224` hash digests of serialized monetary scripts. How scripts are constructed and serialized is out of the scope of this specification.

#### Byron Addresses

Before diving in, please acknowledge that a lot of the supported capabilities of Byron addresses have remained largely unused. The initial design showed important trade-offs and rendered it unpractical to sustain the long-term goals of the network. A new format was created when introducing Shelley and Byron addresses were kept only for backward compatibility. Byron addresses are also sometimes called **bootstrap addresses**.


Like many other objects on the Cardano blockchain yet unlike Shelley addresses, Byron addresses are [CBOR]-encoded binary objects. Conveniently enough, the first 4 bits of their first byte are always equal to `1000....` which allows us to land back on our feet w.r.t to the address type. Their internal structure is however vastly different and a bit unusual.

```
┌────────┬──────────────┬────────┐
│  root  │  attributes  │  type  │
└────────┴──────────────┴────────┘
  ╎        ╎              ╎
  ╎        ╎              ╰╌╌ Standard
  ╎        ╎              ╰╌╌ Redeem
  ╎        ╎
  ╎        ╰╌╌ Derivation Path
  ╎        ╰╌╌ Network Tag
  ╎
  ╎                   ┌────────┬─────────────────┬──────────────┐
  ╰╌╌╌╌ double-hash ( │  type  │  spending data  │  attributes  │ )
                      └────────┴─────────────────┴──────────────┘
                                 ╎
                                 ╰╌╌ Verification Key
                                 ╰╌╌ Redemption Key
```

The address `root` uniquely identifies the address and is a double-hash digest (SHA3-256, and then Blake2b-224) of the address type, spending data, and attributes.

Then comes the address attributes which are both optional. The network tag is present only on test networks and contains an identifier that is used for network discrimination. The [derivation path] (detailed below) was used by legacy so-called random wallets in the early days of Cardano and its usage was abandoned with the introduction of Yoroi and so-called **Icarus addresses**.

Finally, the address type allows for distinguishing different sub-types of Byron addresses. **Redeem addresses** are used inside the Byron genesis configuration and were given to early investors who helped to fund the project.

A full and more detailed [CDDL specification of Byron addresses] is given in the annex to the CIP.

##### Derivation path

Historically, Cardano wallets have been storing information about the wallet structure directly within the address. This information comes in the form of two derivation indexes (in the sense of child key derivation as defined in [BIP-0032]) which we call **derivation path**. To protect the wallet's anonymity, the derivation path is stored encrypted using a ChaCha20/Poly1305 authenticated cipher.

### Test Vectors

All test vectors below use the following payment key, stake key, script and pointer:

- `addr_vk1w0l2sr2zgfm26ztc6nl9xy8ghsk5sh6ldwemlpmp9xylzy4dtf7st80zhd`
- `stake_vk1px4j0r2fk7ux5p23shz8f3y5y2qam7s954rgf3lg5merqcj6aetsft99wu`
- `script1cda3khwqv60360rp5m7akt50m6ttapacs8rqhn5w342z7r35m37`
- `(2498243, 27, 3)`

```yaml
mainnet:
    type-00: addr1qx2fxv2umyhttkxyxp8x0dlpdt3k6cwng5pxj3jhsydzer3n0d3vllmyqwsx5wktcd8cc3sq835lu7drv2xwl2wywfgse35a3x
    type-01: addr1z8phkx6acpnf78fuvxn0mkew3l0fd058hzquvz7w36x4gten0d3vllmyqwsx5wktcd8cc3sq835lu7drv2xwl2wywfgs9yc0hh
    type-02: addr1yx2fxv2umyhttkxyxp8x0dlpdt3k6cwng5pxj3jhsydzerkr0vd4msrxnuwnccdxlhdjar77j6lg0wypcc9uar5d2shs2z78ve
    type-03: addr1x8phkx6acpnf78fuvxn0mkew3l0fd058hzquvz7w36x4gt7r0vd4msrxnuwnccdxlhdjar77j6lg0wypcc9uar5d2shskhj42g
    type-04: addr1gx2fxv2umyhttkxyxp8x0dlpdt3k6cwng5pxj3jhsydzer5pnz75xxcrzqf96k
    type-05: addr128phkx6acpnf78fuvxn0mkew3l0fd058hzquvz7w36x4gtupnz75xxcrtw79hu
    type-06: addr1vx2fxv2umyhttkxyxp8x0dlpdt3k6cwng5pxj3jhsydzers66hrl8
    type-07: addr1w8phkx6acpnf78fuvxn0mkew3l0fd058hzquvz7w36x4gtcyjy7wx
    type-14: stake1uyehkck0lajq8gr28t9uxnuvgcqrc6070x3k9r8048z8y5gh6ffgw
    type-15: stake178phkx6acpnf78fuvxn0mkew3l0fd058hzquvz7w36x4gtcccycj5

testnet:
    type-00: addr_test1qz2fxv2umyhttkxyxp8x0dlpdt3k6cwng5pxj3jhsydzer3n0d3vllmyqwsx5wktcd8cc3sq835lu7drv2xwl2wywfgs68faae
    type-01: addr_test1zrphkx6acpnf78fuvxn0mkew3l0fd058hzquvz7w36x4gten0d3vllmyqwsx5wktcd8cc3sq835lu7drv2xwl2wywfgsxj90mg
    type-02: addr_test1yz2fxv2umyhttkxyxp8x0dlpdt3k6cwng5pxj3jhsydzerkr0vd4msrxnuwnccdxlhdjar77j6lg0wypcc9uar5d2shsf5r8qx
    type-03: addr_test1xrphkx6acpnf78fuvxn0mkew3l0fd058hzquvz7w36x4gt7r0vd4msrxnuwnccdxlhdjar77j6lg0wypcc9uar5d2shs4p04xh
    type-04: addr_test1gz2fxv2umyhttkxyxp8x0dlpdt3k6cwng5pxj3jhsydzer5pnz75xxcrdw5vky
    type-05: addr_test12rphkx6acpnf78fuvxn0mkew3l0fd058hzquvz7w36x4gtupnz75xxcryqrvmw
    type-06: addr_test1vz2fxv2umyhttkxyxp8x0dlpdt3k6cwng5pxj3jhsydzerspjrlsz
    type-07: addr_test1wrphkx6acpnf78fuvxn0mkew3l0fd058hzquvz7w36x4gtcl6szpr
    type-14: stake_test1uqehkck0lajq8gr28t9uxnuvgcqrc6070x3k9r8048z8y5gssrtvn
    type-15: stake_test17rphkx6acpnf78fuvxn0mkew3l0fd058hzquvz7w36x4gtcljw6kf
```
## Rationale: how does this CIP achieve its goals?

As stated in [Motivation](#motivation-why-is-this-cip-necessary) this CIP is provided for informational purposes regarding a single deliberate design. Further rationale and motivation for this design are available in the [Design Specification for Delegation and Incentives in Cardano - Section 3.2 :: Addresses & Credentials ](https://github.com/intersectmbo/cardano-ledger/releases/latest/download/shelley-delegation.pdf).

### Reference Implementation(s)

- [IntersectMBO/cardano-addresses (Byron & Shelley)](https://github.com/IntersectMBO/cardano-addresses)

- [IntersectMBO/cardano-ledger-specs (Byron)](https://github.com/IntersectMBO/cardano-ledger-specs/blob/d5eaac6c4b21a8e69dc3a5503a72e3c3bfde648e/byron/ledger/impl/src/Cardano/Chain/Common/Address.hs)

- [IntersectMBO/cardano-ledger-specs (Shelley)](https://github.com/IntersectMBO/cardano-ledger-specs/blob/1e7e6e03a46e8118b318ed105214767aec0f3976/shelley/chain-and-ledger/executable-spec/src/Shelley/Spec/Ledger/Address.hs)

## Path to Active

### Acceptance Criteria

- [x] Confirmation by consensus, with no reported dispute since publication, that this document fully descibes how Cardano addresses are universally implemented.

### Implementation Plan

- [x] Publish this documentation for confirmation that it accurately describes conventionals of universal use.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

[ABNF grammar in annex]: https://raw.githubusercontent.com/cardano-foundation/CIPs/master/CIP-0019/CIP-0019-cardano-addresses.abnf
[base58]: https://tools.ietf.org/id/draft-msporny-base58-01.html
[bech32]: https://github.com/bitcoin/bips/blob/master/bip-0173.mediawiki
[BIP-0032]: https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki
[Byron addresses]: #byron-addresses
[CBOR]: https://www.rfc-editor.org/rfc/rfc8949
[CDDL specification of Byron addresses]: https://raw.githubusercontent.com/cardano-foundation/CIPs/master/CIP-0019/CIP-0019-byron-addresses.cddl
[CIP-0005]: https://github.com/cardano-foundation/CIPs/tree/master/CIP-0005
[CIP-1852]: https://github.com/cardano-foundation/CIPs/blob/master/CIP-1852
[derivation path]: #derivation-path
[pointers]: #pointers
[Shelley addresses]: #shelley-addresses
[Stake addresses]: #stake-addresses

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0020/README.md
---

- --
CIP: 20
Title: Transaction message/comment metadata
Status: Active
Category: Metadata
Authors:
- Martin Lang <martin@martinlang.at>
- Ola Ahlman <ola@ahlnet.nu>
- Andrew Westberg <andrewwestberg@gmail.com>
Implementors:
- CNTools <https://cardano-community.github.io/guild-operators/#/Scripts/cntools>
- JorManager <https://bitbucket.org/muamw10/jormanager/>
- StakePoolOperator Scripts <https://github.com/gitmachtl/scripts>
- Cardanoscan.io <https://cardanoscan.io>
- AdaStat.net <https://adastat.net>
- Eternl Wallet <https://eternl.io>
- CardanoWall <https://cardanowall.com>
- Nami Wallet <https://namiwallet.io>
- CNFT <https://cnft.io>
- Cardano Explorer <https://cexplorer.io>
- SundaeSwap <https://https://sundaeswap.finance/>
- Minswap <https://minswap.org/>
- MuesliSwap <https://muesliswap.com/>
- DripDropz.io <https://dripdropz.io/>
- Typhon Wallet <https://typhonwallet.io/>
- Ledger Live <https://www.ledger.com/>
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/100
- https://github.com/cardano-foundation/CIPs/pull/394
Created: 2021-06-13
License: CC-BY-4.0
- --

## Abstract

This describes a basic JSON schema to add messages/comments/memos as transaction metadata by using the metadatum label **674**:
allowing informational, commerical, or any other text to be included in a transaction on the Cardano blockchain.

## Motivation: why is this CIP necessary?

We have the utilities on the cardano blockchain now since the introduction of the "allegra-era". A simple consens about adding messages, comments or memos to transactions is still missing.
So the CIP authors came together to form a first implementation of this. It is straight and simple, additional keys and content can be added later.
The IOG main wallet `Daedalus` can now also directly show attached metadata information in the transaction details view. This CIP is the missing link to bring it together.

Some of the current Tools/Sites/Explorers that have implemented it already:
- [CNTools](https://cardano-community.github.io/guild-operators/#/Scripts/cntools)
- [JorManager](https://bitbucket.org/muamw10/jormanager/)
- [StakePoolOperator Scripts](https://github.com/gitmachtl/scripts)
- [Cardanoscan.io](https://cardanoscan.io)
- [AdaStat.net](https://adastat.net)
- [Eternl Wallet](https://eternl.io)
- [CardanoWall](https://cardanowall.com)
- [Nami Wallet](https://namiwallet.io)
- [CNFT](https://cnft.io)
- [Cardano Explorer](https://cexplorer.io)
- [SundaeSwap](https://https://sundaeswap.finance/)
- [Minswap](https://minswap.org/)
- [MuesliSwap](https://muesliswap.com/)
- [DripDropz.io](https://dripdropz.io/)
- [Typhon Wallet](https://typhonwallet.io/)
- [Ledger Live](https://www.ledger.com/)

## Specification

The specification for the individual strings follow the general design specification for JSON metadata, which is already implemented and in operation on the cardano blockchain.
The used metadatum label is **`"674":`**, this number was choosen because it is the T9 encoding of the string "msg".
The message content has the key **`"msg":`** and consists of an **array** of individual **message-strings**.
The number of theses **message-strings** must be at least one for a single message, more for multiple messages/lines. Each of theses individual **message-strings** array entries must be at most 64 bytes when UTF-8 encoded.

### Format:
```
{
  "674":
         {
           "msg":
                  [
                    "message-string 1" //Optional: ,"message-string 2","message-string 3" ...
                  ]
         }
}
```

### Example for a single message/comment/memo:
``` json
{
  "674":
         {
           "msg":
                  [
                    "This is a comment for the transaction xyz, thank you very much!"
                  ]
         }
}
```

### Example for multiple messages/comments/memos:
``` json
{
  "674":
         {
           "msg":
                  [
                    "Invoice-No: 1234567890",
                    "Customer-No: 555-1234",
                    "P.S.: i will shop again at your store :-)"
                  ]
         }
}
```

&nbsp;<p>

### Some Integration examples

- *Ledger Live** is offering a memo field
![image](https://user-images.githubusercontent.com/47434720/204649383-c34ae733-e136-41b8-8fa8-619dde978621.png)

- *Daedalus** shows the metadata text (could be improved if CIP is implemented):
![image](https://user-images.githubusercontent.com/47434720/121822100-85b38a80-cc9d-11eb-9d13-1869746a69b2.png)

- *Cardanoscan.io**, **Adastat.net** and other tools implemented it already, to show messages along transactions:
![image](https://user-images.githubusercontent.com/47434720/204633595-d865c7ee-0c30-4af1-bb55-3c0ad323b58c.png)
![image](https://user-images.githubusercontent.com/47434720/204634111-256c6c18-974a-41f5-a6e4-b9edee8f9d62.png)

- *eternl.io** has added it with a message field on the sending-page, and shows it also on the transactions-page:
![image](https://user-images.githubusercontent.com/47434720/204632224-5be33098-00f6-41da-a2f0-7c138b28354f.png)
![image](https://user-images.githubusercontent.com/47434720/204632802-33f1afa5-d9b2-494f-84fe-d7f0594a7f1b.png)

- *StakePool Operator Scripts**: It works on the commandline like any other script of the collection by just adding the "msg: ..." parameter to a transaction. This automatically generates the needed metadata.json structure and attaches it to the transaction itself.
![image](https://user-images.githubusercontent.com/47434720/129110626-6bc5b3c3-102d-4793-b508-7d4190b31cf7.png)

- *CNTools**:<br>
![image](https://user-images.githubusercontent.com/47434720/130353491-fc0f3a69-1937-4e72-b680-c04cc069b5c4.png)

## Rationale: how does this CIP achieve its goals?

This design is simple, so many tools on the cardano blockchain can implement it easily. The array type was choosen to have consistency, no need to switch between a string or
an array format, or testing against a string or array format. Updates in the future are possible, like adding a versioning key `"ver":`, adding a key `"utxo":` to provide specific data for every tx-out#idx in the transaction, adding the `"enc":` key like for encrypted messages, making subarrays in the message-strings, etc. But for now, we need a common agreement to provide general messages/comments/memos with this CIP. The starting design war choosen as simple as possible to keep the additional transaction fees as low as possible.

### Wallet Implementation

Would be a good idea to hide the message/comment/note behind a "show unmoderated content" button/drop-down. Like the Metadata display on the Cardano Explorer. Also, it should be displayed as plain-text non-clickable. To enhance security further, URLs could be automatically deleted or hidden from such comments, to not welcome bad actors with phishing attempts. Another solution to start with would be to really limit the character space for display in Wallets, like limiting it to `a-zA-z0-9` and a handful of special chars like `+-_#()[]:` without a `.<>"/\` chars, so a domain or html code would not work. Last points are worth for discussions of course, because it would also filter out unicode.

### Handling ill-formed 674 metadata

It is up to the wallet-/display-/receiver-implementor to parse and check the provided metadata. As for the current state, its not possible to have the same label "674" more than once in a cardano transaction. So a check about that can be ignored at the moment. This CIP provides the correct implementation format, the parsing should search for the "674" metadata label and the "msg" key underneath it. There should also be a check, that the provided data within that "msg" key is an array. All other implementations like a missing "msg" key, or a single string instead of an array, should be marked by the display-implementor as "invalid". Additional keys within the "674" label should not affect the parsing of the "msg" key. As written above, we will likely see more entries here in the future like a "version" key for example, so additional keys should not harm the parsing of the "msg" key.

### Implementation conclusion

A transaction message should be considered valid if the following apply:

- Label = 674.
- has property "msg".
- msg property contains an array of strings, even for a single-line message.
- Each line has a maximum length of 64 characters.
- If there are additional properties, they don't invalidate the message. They can just be ignored.

If any of the above is not met, ignore the metadata as a transaction message. Can still be displayed as general metadata to the transaction.

_Optional to consider for the implementer:_

- For message creation both single-line and multi-line input should be considered valid. The wallet/tool isn't required to support multi-line input.
- Message display in explorers/wallets should however preferably support multi-line messages even if it only supports single-line on creation. Not a requirement but should at least indicate that there are more data if only the first line is displayed. Maybe a link to explorer etc in the case it's not possible to solve in UI in a good way.

## Path to Active

### Acceptance Criteria

- [x] There exist a variety of wallet-based, dApp, and CLI implementations of this standard, developed by a a wide variety of providers, and is in regular use.

### Implementation Plan

As per the first two Discussion links:
- [x] The format in this CIP has been the ground base for supporting transaction messages / comments / memos.
- [x] The format and its interpretation have been considered and implemented by both creator/sender implementations and wallet/receiver/display implementations.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode)

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0021/README.md
---

- --
CIP: 21
Title: Transaction requirements for interoperability with hardware wallets
Status: Active
Category: Wallets
Authors:
- Gabriel Kerekes <gabriel.kerekes@vacuumlabs.com>
- Rafael Korbas <rafael.korbas@vacuumlabs.com>
- Jan Mazak <jan.mazak@vacuumlabs.com>
Implementors: N/A
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/107
Created: 2021-06-15
License: CC-BY-4.0
- --

## Abstract

This CIP describes all the restrictions applicable to Cardano transactions which need to be signed by hardware wallets.

## Motivation: why is this CIP necessary?

Due to certain limitations of hardware (abbrev. HW) wallets, especially very small memory and a limited set of data types supported by Ledger, HW wallets are not able to process all valid transactions which are supported by Cardano nodes.

The limitations also result in an inability of HW wallets to see the whole transaction at once. Transaction data are streamed into HW wallets in small chunks and they compute a rolling hash of the transaction body which is signed at the end. Consequently, a HW wallet only provides witness signatures, and the transaction body which was signed has to be reconstructed by the client. We thus need a common transaction serialization format which will allow no ambiguity. In addition, the format must define ordering of map keys in such a way that it’s possible to check for duplicate keys by HW wallets.

Several of the restrictions also stem from security or UX concerns, e.g. the forbidden combination of pool registration certificates and withdrawals in a single transaction (see reasoning below).

To ensure interoperability, SW wallets and other tools working with HW wallets should only use transactions which conform to the following rules.

## Specification

Certain transaction elements, described as allowed in this document, might not be supported by some HW wallets. Support also depends on HW wallet firmware or app versions. If transaction signing fails for a transaction which is built according to this specification, make sure to check the documentation of the HW wallet you are using.

> **Note:** It might take some time for recent Cardano ledger spec changes to be implemented for HW wallets. Thus it might happen that further restrictions might apply on top of the restrictions mentioned in this CIP until the changes are implemented on HW wallets.

### Canonical CBOR serialization format

Transactions must be serialized in line with suggestions from [Section 3.9 of CBOR specification RFC](https://datatracker.ietf.org/doc/html/rfc7049#section-3.9). In particular:

- Integers must be as small as possible.
- The expression of lengths in major types 2 through 5 must be as short as possible.
- The keys in every map must be sorted from lowest value to highest.
- Indefinite-length items must be made into definite-length items.

See the RFC for details.

### Tags in sets

Conway introduced optional 258 tags in certain items that are considered sets semantically but encoded as arrays in CBOR. HW wallets will support this optional encoding, but it must be consistent across the transaction: either there are no tags 258 in sets, or there are such tags everywhere (as described in the CDDL specification).

### Transaction body

#### Unsupported entries

The following transaction body entries must not be included:
- `6 : update`
- `20 : proposal procedures`

#### Integers

HW wallets support at most `int64` for signed integers and `uint64` for unsigned integers i.e. larger integers are not supported overall. Additionally, any integer value must fit in the appropriate type.

#### Numbers of transaction elements

The number of the following transaction elements individually must not exceed `UINT16_MAX`, i.e. 65535:

- inputs in transaction body
- outputs in transaction body
- asset groups (policy IDs) in an output or in the mint field
- tokens (asset names) in an asset group
- certificates in transaction body
- pool owners in a pool registration certificate
- pool relays in a pool registration certificate
- withdrawals in transaction body
- collateral inputs in transaction body
- required signers in transaction body
- reference inputs in transaction body
- the total number of witnesses

For voting procedures, it is only allowed to include a single voter with a single voting procedure.

#### Optional empty lists and maps

Unless mentioned otherwise in this CIP, optional empty lists and maps must not be included as part of the transaction body or its elements.

Since Conway, the [CDDL specification](https://github.com/intersectmbo/cardano-ledger/tree/master/eras/conway/impl/cddl-files) is stricter, so many arrays are now treated as non-empty sets, and some maps are required to be non-empty. HW wallets enforce this in many cases.

#### Outputs

A new, "post Alonzo", output format has been introduced in the Babbage ledger era which uses a map instead of an array to store the output data. For now, both the "legacy" (array) and "post Alonzo" (map) output formats are supported by HW wallets but we encourage everyone to migrate to the "post Alonzo" format as support for the "legacy" output format might be removed in the future. Both formats can be mixed within a single transaction, both in outputs and in the collateral return output.

##### Legacy outputs

Outputs containing no multi-asset tokens must be serialized as a simple tuple, i.e. `[address, coin, ?datum_hash]` instead of `[address, [coin, {}], ?datum_hash]`.

##### Post Alonzo outputs

If the `data` of `datum_option` is included in an output, it must not be empty. `script_ref` (reference script) must also not be empty if it is included in an output.

#### Multiassets

Since multiassets (`policy_id` and `asset_name`) are represented as maps, both need to be sorted in accordance with the specified canonical CBOR format. Also, an output or the mint field must not contain duplicate `policy_id`s and a policy must not contain duplicate `asset_name`s.

#### Certificates

Certificates of the following types are not supported and must not be included:
- `genesis_key_delegation`
- `move_instantaneous_rewards_cert`
- `stake_vote_deleg_cert`
- `stake_reg_deleg_cert`
- `vote_reg_deleg_cert`
- `stake_vote_reg_deleg_cert`

If a transaction contains a pool registration certificate, then it must not contain:

- any other certificate;
- any withdrawal;
- mint entry;
- any output containing datum, datum hash or reference script;
- script data hash;
- any collateral input;
- any required signer;
- collateral return output;
- total collateral;
- reference inputs;
- voting procedures;
- treasury value;
- donation value.

It is allowed to arbitrarily combine other supported certificate types.

#### Withdrawals

Since withdrawals are represented as a map of reward accounts, withdrawals also need to be sorted in accordance with the specified canonical CBOR format. A transaction must not contain duplicate withdrawals.

#### Auxiliary data

HW wallets do not serialize auxiliary data because of their complex structure. They only include the given auxiliary data hash in the transaction body. The only exception is Catalyst voting registration because it requires a signature computed by the HW wallet.

In this exceptional case, auxiliary data must be encoded in their "tuple" format:

```
[ transaction_metadata: { * transaction_metadatum_label => transaction_metadatum }, auxiliary_scripts: [ * native_script ]]
```

The `auxiliary_scripts` must be an array of length 0.

## Rationale: how does this CIP achieve its goals?

### Canonical CBOR serialization format

As HW wallets don't return the whole serialized transaction, a common CBOR serialization is needed so that software wallets and other tools interacting with HW wallets are be able to deterministically reproduce the transaction body built and signed by the HW wallet.

The specified canonical CBOR format is consistent with how certain other data are serialized (e.g. Plutus script data in Alonzo) and allows the use of standard CBOR libraries out of the box.

### Transaction body

#### Credentials

Generally, HW wallets require that any key hash credential (and withdrawal address too) is given by the derivation path of the key (otherwise the user will not be aware that the key belongs to his wallet). This does not apply to Plutus transactions where HW wallets instead aim for maximum flexibility at the cost of users being potentially misled. (It is very hard to foresee how Plutus script authors would use various transaction elements and any restriction applied by HW wallets might break a use case which is otherwise perfectly sound and safe.)

When signing a transaction, Ledger and Trezor use a _transaction signing mode_ that describes upfront what the intent is
(the software wallet is responsible for choosing an appropriate mode). The transaction is then validated according to the mode.
There are, in principle, four options:
- _Stake pool registration transaction_. Stake pool registration certificates are signed on their own, the transaction should contain nothing that is not necessary.
- _Ordinary transaction_. Credentials must be given as key paths.
- _Multisig transaction_. Credentials must be script hashes; only [multisig keys](https://github.com/cardano-foundation/CIPs/blob/master/CIP-1854/README.md) are allowed.
- _Plutus transaction_. The only mode that allows elements related to running Plutus scripts (script data hash etc.). No extra restrictions on transaction elements or their combinations. The drawback is that more is shown to the user (e.g. witnesses are not hidden as in ordinary transactions). Please only use this mode if no other mode is sufficient.

This brief description does not aim to capture the full complexity of signing modes; always verify that transactions you aim to construct are supported by other tools you will rely on (hardware wallets, software wallets, command-line tools like [`cardano-hw-cli`](https://github.com/vacuumlabs/cardano-hw-cli) etc.).

#### Multiassets

Allowing duplicate `policy_id`s (or `asset_name`s) might lead to inconsistencies between what is displayed to the user and how nodes and other tools might interpret the duplicate keys, i.e. all policies (or asset names) would be shown to the user, but nodes and other tools might eventually interpret only a single one of them.

#### Certificates

Combining withdrawals and pool registration certificates isn't allowed because both are signed by staking keys by pool owners. If it was allowed to have both in a transaction then the witness provided by a pool owner might inadvertently serve as a witness for a withdrawal for the owner's account.

#### Withdrawals

Similarly to multiassets, allowing duplicate withdrawals might lead to inconsistencies between what is displayed to the user and how nodes and other tools might interpret the duplicate keys.

#### Auxiliary data

The specified auxiliary data format was chosen in order to be compatible with other Cardano tools, which mostly use this serialization format.

### Stake key signing vs other keys

A DRep witness can serve for both certificates and votes at the same time. Unlike with stake keys (where combining pool registration with e.g. withdrawals is forbidden), no restriction is imposed on the combination of certificates and votes.
We think that votes and DRep certificates are rare and substantially distinguished parts of a transaction, signed by DRep keys which are likely to only be used by users with deep enough understanding (and, unlike stake keys, are always visible when providing witnesses). A single vote or a DRep certificate is unlikely to have a major effect (esp. not on the loss of funds). If submitting unintended votes turns out to be a problem, it is likely better to solve it on the level of Cardano blockchain ledger by providing a mechanism allowing for replacing or cancelling votes.

## Path to Active

### Acceptance Criteria

- [x] Confirmation (by default if no ongoing incompatibilities) since Alonzo ledger era that this interoperability between software and hardware wallets has been generally achieved.

### Implementation Plan

- [x] Tools exist which can be used to validate or transform transactions into a HW wallet compatible format if possible:
- [x] [`cardano-hw-interop-library`](https://github.com/vacuumlabs/cardano-hw-interop-lib)
- [x] [`cardano-hw-cli`](https://github.com/vacuumlabs/cardano-hw-cli) (which uses the interop library)

## Restrictions for specific hardware devices

The following list of features with missing support on particular hardware devices is subject to occasional changes. Some features might be added, but some could also be removed (e.g. if they take too much space needed for other features).

#### Ledger: Nano S Plus, Nano X, Stax

Everything described here as allowed should (eventually) work on these devices.

#### Ledger: Nano S

Missing features:
- signing operational certificates
- derivation of native script hashes
- stake pool registration and retirement
- display of certain details of Byron addresses (though addresses themselves are supported)

#### Trezor

Missing features:
- derivation of stake pool cold keys
- signing operational certificates
- signing pool registration certificates as operator (only as owner is allowed)
- derivation of DRep and constitutional committee keys
- DRep certificates (registration, retirement, update)
- constitutional committee certificates
- voting procedures
- treasury and donation elements of transactions

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0022/README.md
---

- --
CIP: 22
Title: Pool operator verification
Status: Active
Category: Tools
Authors:
- Andrew Westberg <andrewwestberg@gmail.com>
- Martin Lang <martin@martinlang.at>
- Ola Ahlman <ola@ahlnet.nu>
Implementors:
- CNCLI
- JorManager
- StakePoolOperator Scripts
- CNTools
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/102
Created: 2021-06-21
License: CC-BY-4.0
- --

## Abstract

This proposal describes a method allowing a stakepool operator to provide credentials to verify that they are the rightful manager for their stakepool.

## Motivation: why is this CIP necessary?

Many websites such pooltool.io, adapools.org, and others need to allow pool operators special access to modify the way their pool appears on the website. SPOCRA and other organizations also have a need to allow voting on proposals and ensure that each vote cast is from a valid pool operator. Today, these sites and organizations all use different techniques for validating pool operators.

pooltool.io - Validates operators by receiving 1 ada spent from the pool's registered rewards account

adapools.org - Validates operators by requesting that the operator include a special generated value in their extended pool metadata json file.

This proposal is to simplify and streamline a single approach that all can reference in order to verify that a pool operator is who they say they are.

## Specification

In order to achieve the goals of this CIP, the pool operator needs to provide some credential or credentials to the validating party which cannot be spoofed. The VRF pool keys and VRF signature algorithm implemented in libsodium are chosen to build and provide this credential/signature. This signature can then be validated and the operator verified without ever exposing any of the pool's private key information. This technique is very similar to verifying that a block produced by another pool is valid. The only difference is that instead of validating the slot seed for a given pool, we're validating a pre-determined message hash.

### Verification Steps:

1. Stakepool Operator (SPO) sends their pool_id and public pool.vrf.vkey to the Validating Server (VS)
2. VS validates that the vrf hash in the pool's registration certificate on the blockchain matches the blake2b hash of the sent vkey. Note: The VS should use the latest registration certificate on the chain for matching as the VRF is a "hot" key and can be changed at any time by the pool operator. A single point-in-time verification is sufficient to properly identify the pool operator.
3. The VS sends a challenge request to the SPO which is the domain of the VS and a random 64-byte nonce.
4. The SPO creates a blake2b hash of "cip-0022{domain}{random_nonce}" and then signs this with their private VRF key.
5. The SPO sends this to VS as the challenge response within a 5-minute window to the VS
6. The VS validates the signed challenge response

### Code Example (Validating server):

```kotlin
// Server side, create inputs for a challenge. Store this and only allow responses
// within 5 minutes to be successful.
val random = SecureRandom()
val domain = "pooltool.io"
val nonce = ByteArray(64)
random.nextBytes(nonce)
println("domain: $domain, nonce: ${nonce.toHexString()}")
```

### Code Example (Pool Operator side):

```kotlin
// Node operational VRF-Verification-Key: pool.vrf.vkey
//{
//    "type": "VrfVerificationKey_PraosVRF",
//    "description": "VRF Verification Key",
//    "cborHex": "5820e0ff2371508ac339431b50af7d69cde0f120d952bb876806d3136f9a7fda4381"
//}
//
// Node operational VRF-Signing-Key: pool.vrf.skey
//{
//    "type": "VrfSigningKey_PraosVRF",
//    "description": "VRF Signing Key",
//    "cborHex": "5840adb9c97bec60189aa90d01d113e3ef405f03477d82a94f81da926c90cd46a374e0ff2371508ac339431b50af7d69cde0f120d952bb876806d3136f9a7fda4381"
//}

// We assume the pool operator has access to the pool's vrf secret key
val skeyCbor = "5840adb9c97bec60189aa90d01d113e3ef405f03477d82a94f81da926c90cd46a374e0ff2371508ac339431b50af7d69cde0f120d952bb876806d3136f9a7fda4381".hexToByteArray()
val vrfSkey = (CborReader.createFromByteArray(skeyCbor).readDataItem() as CborByteString).byteArrayValue()
val vkeyCbor = "5820e0ff2371508ac339431b50af7d69cde0f120d952bb876806d3136f9a7fda4381".hexToByteArray()
val vrfVkey = (CborReader.createFromByteArray(vkeyCbor).readDataItem() as CborByteString).byteArrayValue()

// Client side, construct and sign the challenge
val challengeSeed = "cip-0022${domain}".toByteArray() + nonce
val challenge = SodiumLibrary.cryptoBlake2bHash(challengeSeed, null)
println("challenge: ${challenge.toHexString()}")

val signature = SodiumLibrary.cryptoVrfProve(vrfSkey, challenge)
println("signature: ${signature.toHexString()}")
```

### Code Example (Validating server):

```kotlin
// Server side, verify the message based on only knowing the pool_id, public vkey, signature, and constructing
// the challenge ourselves the same way the client should have.
val challengeSeed = "cip-0022${domain}".toByteArray() + nonce
val challenge = SodiumLibrary.cryptoBlake2bHash(challengeSeed, null)

// Get the vkeyHash for a pool from the "query pool-params" cardano-cli command
// This comes from the pool's registration certificate on the chain.
val vkeyHash = "f58bf0111f8e9b233c2dcbb72b5ad400330cf260c6fb556eb30cefd387e5364c".hexToByteArray()

// Verify that the vkey from the latest minted block on the blockchain (or the client supplied if they
// haven't yet minted a block) is the same as the one on-chain in the pool's registration certificate
val vkeyHashVerify = SodiumLibrary.cryptoBlake2bHash(vrfVkey, null)
assertThat(vkeyHash).isEqualTo(vkeyHashVerify)

// Verify that the signature matches
val verification = SodiumLibrary.cryptoVrfVerify(vrfVkey, signature, challenge)
println("verification: ${verification.toHexString()}")

println("Verification SUCCESS!")
```

### Code Example output:

```
vrfSkey: adb9c97bec60189aa90d01d113e3ef405f03477d82a94f81da926c90cd46a374e0ff2371508ac339431b50af7d69cde0f120d952bb876806d3136f9a7fda4381
vrfVkey: e0ff2371508ac339431b50af7d69cde0f120d952bb876806d3136f9a7fda4381
domain: pooltool.io, nonce: c936ab102a86442c7120f75fa903b41d9f6f984a9373a6fa0b7b8cb020530318bdec84512468681c7d8454edf3a0e0bf21f59c401028030a8fb58117edc8b03c
challenge: 6977c480a3acb4c838ba95bb84d1f4db1c2591ea6ebe5805ed0394f706c23b05
signature: a3c9624aa14f6f0fba3d47d3f9a13bb55f0790eacd7bad9a89ce89fecb9e7eb8ca0d19aea8b6a7be39ae3e8b9768211b4d8aa789e82c1e150826fe15a0b0323f08e18635deb94c49d7f4421750d44903
signatureHash: 9ca4c7e63ba976dfbe06c7a0e6ec4aec5a5ef04b721ffc505222606dfc3d01572ddce3b55ac5c9470f061f137dafe31669794ea48118d1682d888efbe0cb4d1a
verification: 9ca4c7e63ba976dfbe06c7a0e6ec4aec5a5ef04b721ffc505222606dfc3d01572ddce3b55ac5c9470f061f137dafe31669794ea48118d1682d888efbe0cb4d1a
Verification SUCCESS!
```

## Rationale: how does this CIP achieve its goals?

Implementing this simplifies and commonizes the process for verifying that a pool operator is who they say they are in 3rd party systems. Having a common way of verify pool operators also allows simple integration into pool management tools.

There is also some overlap with [CIP-0006](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0006/README.md#extended-metadata---flexible-but-validable) and the `rawdata-sign` command although it specifies generating a new key instead of utilizing the pool's existing `vrf.skey` to sign like this proposal.

## Path to Active

### Acceptance Criteria

- [x] Tools that have implemented, or are implementing, this proposal:
- [x] [CNCLI](https://github.com/AndrewWestberg/cncli)
- [x] [JorManager](https://bitbucket.org/muamw10/jormanager/)
- [x] [StakePoolOperator Scripts](https://github.com/gitmachtl/scripts)
- [x] [CNTools](https://cardano-community.github.io/guild-operators/#/Scripts/cntools)

### Implementation Plan

- [x] Consensus between providers of the most popular tools and CLIs for stake pool operators that this approch is viable and desirable.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0023/README.md
---

- --
CIP: 23
Title: Fair Min Fees
Authors:
- Shawn McMurdo <shawn_mcmurdo@yahoo.com>
- Ryan Wiley <rian222@gmail.com>
Category: Ledger
Status: Proposed
Created: 2021-02-04
Discussions:
- https://forum.cardano.org/t/fair-min-fees-cip/47534
- https://github.com/cardano-foundation/CIPs/pull/66
Implementors: []
License: CC-BY-4.0
- --

## Abstract

This CIP introduces a new protocol parameter, `minPoolMargin`, which specifies a lower bound on the variable fee (margin) a stake pool may set. The parameter is introduced initially set to `0` to avoid disrupting existing pool certificates.  This proposal does not change or reduce the existing minimum fixed pool fee (`minPoolCost`).

## Motivation: why is this CIP necessary?

The current minimum fixed pool fee places a large and unfair burden on delegators to pools with smaller amounts of stake.
This incentivizes people to delegate to pools with higher stake causing centralization and creating an unequal playing field for stake pool operators.

Using a minimum variable pool fee reduces the imbalance between stake pools with less or more stake to a more reasonable range that allows for more fair competition between stake pools and more fair rewards for delegators to stake pools with less stake.

This creates a more fair marketplace for all stake pool operators and increases decentralization, which is a goal of Cardano.

## Specification

This CIP introduces a new protocol parameter, `minPoolMargin`, which represents the
minimum variable fee (margin) that a stake pool can set. This parameter is
distinct from the existing `minPoolCost`, which represents the minimum fixed
fee a pool can set. Both limits are enforced independently by the ledger.

- `minPoolMargin` defines the lower bound for the pool margin (variable fee), i.e.,
  the minimum allowable percentage of rewards a pool can take. Pool
  registration and update certificates MUST have `margin >= minPoolMargin`.
- `minPoolCost` retains its current meaning and enforcement as the minimum
  fixed fee a pool can set.

This CIP does not prescribe specific values for either parameter. Concrete
values for `minPoolMargin` and `minPoolCost` are to be chosen and enacted through
the standard protocol parameter update process.

### Backward Compatibility

To maintain compatibility for existing pool certificates whose current margin is below the new `minPoolMargin`, the ledger's rewards calculation should treat the protocol parameter `minPoolMargin` as the effective margin for those pools. In other words, if a pool's margin is less than `minPoolMargin`, the protocol-level `minPoolMargin` overrides the pool's registered `margin` during reward calculation. This minimizes disruption and lets legacy pool certificates remain valid while ensuring the ledger enforces the new minimum fee during reward distribution.

It is also recommended to introducce the hard-fork with `minPoolMargin` initially set to `0`. Doing so minimizes migration friction for stake pool operators and gives governance time to raise the parameter to its target value through the normal paramater change governance action process.

Should this clamping approach prove infeasible, pool certificates with a margin lower than `minPoolMargin` would need to be re-registered with compliant values, but the goal is to avoid disruption as much as possible.

## Rationale: how does this CIP achieve its goals?

The PHP code in minfees.php in the pull request allows exploration of the effects of choosing different values for the minimum fixed and variable fees.
Running minfees without any arguments gives the usage message as below.

```
php minfees.php

Usage: php minfees.php <min_fixed_fee> <min_rate_fee> <pledge>
  min_fixed_fee:  Minimum fixed fee in ADA.
               An integer greater than or equal to 0.
  min_rate_fee: Minimum rate fee in decimal. Example: 1.5% = .015
                    A real number greater than or equal to 0.
  pledge: Optional pledge amount in ADA. Defaults to 100000
                    A real number greater than or equal to 0.
```

Running minfees with the proposed values gives the following comparison of current and proposed pool operator and staker results at various pool stake levels.

```
php minfees.php 50 .015 100000

Reserve: 13b
Total stake: 32b
Tx fees: 0
Rewards available in epoch: 31.2m
Pool saturation: 64m
Pledge: 100k
Staker Delegation: 100k
Current Fixed Fee: 340
Current Rate: 0%
New Fixed Fee: 50
New Rate: 1.5%

- ---------- Current ----------+ +-------- Proposed ---------+
Pool    Total   Pool    Staker  Staker  Current Pool    Staker  Staker  New
Stake   Rewards Cur Fee Cur Fee Cur Rew Fee %   New Fee New Fee New Rew Fee %
2m      1501    340     17      58.1    22.7%   71.8    3.6     71.5    4.8%
5m      3752    340     6.8     68.2    9.1%    105.5   2.1     72.9    2.8%
10m     7503    340     3.4     71.6    4.5%    161.8   1.6     73.4    2.2%
20m     15007   340     1.7     73.3    2.3%    274.4   1.4     73.7    1.8%
30m     22511   340     1.1     73.9    1.5%    386.9   1.3     73.7    1.7%
64m     48023   340     0.5     74.5    0.7%    769.6   1.2     73.8    1.6%
```

Definitions:
Pool Stake - Total stake delegated to pool.
Total Rewards - Total rewards generated by the pool in one epoch.
Pool Cur Fee - The total amount of fees taken by the pool with current parameters.
Staker Cur Fee - The amount of fees paid by a staker who delegates 100k ADA  with current parameters.
Staker Cur Rew - The amount of rewards received by a staker who delegates 100k ADA  with current parameters.
Current Fee % - The percentage of rewards taken by the pool as fees  with current parameters.
Pool New Fee - The total amount of fees taken by the pool with proposed parameters.
Staker New Fee - The amount of fees paid by a staker who delegates 100k ADA with proposed parameters.
Staker New Rew - The amount of rewards received by a staker who delegates 100k ADA with proposed parameters.
New Fee % - The percentage of rewards taken by the pool as fees with proposed parameters.
Note: All amounts other than %s are in ADA.

The table above shows that currently a delegator staking 100k ADA to a stake pool with 2m ADA total delegation to the pool is paying an exorbitant 22.7% in fees while the same delegator staking with a fully saturated pool would only pay 0.7% in fees.
This is a substantial and unfair advantage that large pools have in the stake pool marketplace.
This is a strong incentive to centralize stake to fewer larger pools which reduces the resiliency of the network.

The proposed minimum fees bring this imbalance into a more reasonable range of 1.6% to 4.8%.
It is much more likely that a small stake pool with other advantages or selling points would be able to convince a delegator to accept about 2 less ADA in rewards per epoch for their 100k delegation than about 17 ADA as in the current case.
This is particularly true as the price of ADA increases.
At current price of $0.90 USD, a delegator staking 100k ADA is giving up over $1000 USD per year by delegating to a small pool!
This does not even include the amount lost by comounding rewards being staked over the year.

16.5 ADA/epoch * 73 epochs/year =  1204.5 ADA/year
1204.5 ADA/year * $0.90 USD/ADA = $1084.05 USD/year

With proposed parameters the same delegator would only be giving up about $150 USD per year to support a small pool.

2.3 ADA/epoch * 73 epochs/year =  167.9 ADA/year
167.9 ADA/year * $0.90 USD/ADA = $151.11 USD/year

The calculations below show that given the price increase in ADA compared to when the protocol parameters were first set, we can maintain viable funding for stake pool operators with the proposed parameter changes.

Annual pool operator funding given initial parameters:
340 ADA/epoch * $0.08 USD/ADA = $27.20 USD/epoch
$27.20 USD/epoch * 73 epochs/year = $1985.60 USD/year

Annual pool operator funding given proposed parameters for stake pool with 2 million ADA delegation:
71.8 ADA/epoch * $0.90 USD/ADA = $64.62 USD/epoch
$64.62 USD/epoch * 73 epochs/year = $4717.26 USD/year

Annual pool operator funding given proposed parameters for fully saturated stake pool:
769.6 ADA/epoch * $0.90 USD/ADA = $692.64 USD/epoch
$692.64 USD/epoch * 73 epochs/year = $50,562.72 USD/year

In summary, the proposed parameter changes would create a more fair marketplace for stake pools, provide more fair rewards for delegators to smaller pools and would lower incentives for centralization providing a more resilient network.

### Test Cases

See the minfees.php code to test different potential values of the parameters.

## Path to Active

### Acceptance Criteria

- Consensus on initial parameter value – An initial value for the new protocol parameter `minPoolMargin` must be agreed upon before hard-fork combinator (HFC) activation. The choice should consider operational viability, empirical analyses, and community feedback.
- Endorsement by Technical Bodies – The Cardano Parameter-Change Proposals (PCP) Committee and the Intersect Technical Steering Committee (TSC) should both recommend the proposal as technically sound and aligned with the protocol’s long-term roadmap.
- Stakeholder Concurrence – A majority of stake pool operators (SPOs), ecosystem tooling maintainers, dReps, and other infrastructure providers must signal readiness to upgrade.
- Governance Ratification – The on-chain Hard-Fork Governance Action must pass the requisite dRep and Constitutional Committee thresholds, establishing legal-constitutional legitimacy and stakeholder support for the change.

### Implementation Plan

- Community Deliberation (Preparation Phase)
- Publish the finalized CIP revision and present it to the PCP committee, TSC, CIP Editors, and wider community channels (Discord, X, Cardano Forum, etc.).
- Collect structured feedback, particularly on candidate values for the new parameter values and iterate until broad technical consensus emerges.
- Specification & Code Integration (Development Phase)
- Once initial parameter values are determined, integrate the new rewards calculation logic and governance features for the new parameter into cardano-node and related libraries (ledger, CLI, wallet APIs).
- Determine the best method to deal with existing pool registration certificates that currently have a variable fee lower than what the new `minPoolMargin` parameter allows.
- Submit pull requests to the canonical repositories; obtain code reviews from IOG, CF, and community contributors.
- Release a new protocol version that includes the changes made in this CIP.
- Use a dedicated pre-production testnet that mirrors main-net parameters but enforces the new changes, allowing SPOs and exchanges to test end-to-end flows.
- Readiness Sign-off (Testing Phase)
- Require at least two weeks of uninterrupted testnet stability plus green results from regression and property-based tests.
- Monitor ecosystem dApps and tooling to confirm that major node implementations, explorers, wallets, and exchange integrations support the new rule set.
- On-chain Governance (Ratification Phase)
- File the Hard-Fork Governance Action on-chain with the agreed initial parameter value tagged for the next hard fork event.
- Modify the existing Cardano Constitution to include definitions and guardrails for the new protocol parameters and have it ratified by the tripartite government of Cardano.
- Mobilize dRep outreach to ensure quorum and super-majority passage; concurrently, the Constitutional Committee validates procedural compliance.
- Hard-Fork Activation (Deployment Phase)
- Upon successful vote, the hard fork event is automatically triggered upon epoch turnover.
- Monitor main-net metrics during the changeover epoch; provide real-time support for any late-upgrading SPOs.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode)

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0024/README.md
---

- --
CIP: 24
Title: Non-Centralizing Rankings
Authors:
- Shawn McMurdo <shawn_mcmurdo@yahoo.com>
Category: Wallets
Status: Proposed
Discussions:
- https://forum.cardano.org/t/how-to-improve-daedalus-rankings/40478
- https://github.com/cardano-foundation/CIPs/pull/21
Implementors: []
Created: 2020-09-15
License: CC-BY-4.0
- --

## Abstract

Modify the current Daedalus ranking system by removing the centralizing Nash Equilibrium goal of the ranking methodology in order to give more fair rankings and improve the viability of the stake pool operator community and the network overall.  To do this we need to remove the stated goal of having k fully saturated pools and all other pools having no stake other than owner pledge, which goes against the Cardano goal of decentralization.

## Motivation: why is this CIP necessary?

There are two main reasons for changing the current ranking methodology:

1. Allow for more than k successful stake pools.

2. Provide better decentralization away from a very few stake pool operators creating many pools.

## Specification

This is a modification of the ranking methodology defined in section 5.6 Non-Myopic Utility of “Shelley Ledger: Delegation/Incentives Design Spec. (SL-D1 v.1.20, 2020/07/06)” as follows:

1. Remove the following statement from section 5.6:

"The idea is to first rank all pools by “desirability”, to then assume that the k most desirable
pools will eventually be saturated, whereas all other pools will lose all their members, then to
finally base all reward calculations on these assumptions."

2. Remove the following statement from section 5.6.1:

"We predict that pools with rank ≤ k will eventually be saturated, whereas pools with rank
> k will lose all members and only consist of the owner(s)."

3. Add the following to section 5.6.1:

For all pools with proposed_pool_stake greater than saturation_warning_stake add k to their rank.
Where:
proposed_pool_stake = pool_live_stake + proposed_user_stake
saturation_warning_stake = (total_stake / k) * saturation_warning_level
saturation_warning_level is a real number greater than 0 representing the percent of saturation which is undesirable.  A proposed value for saturation_warning_level is 0.95 meaning 95% saturated.

For example, if a pool has non-myopic desirability rank of 3, pool_live_stake of 207m ADA, proposed_user_stake of 100k ADA with total_stake of 31.7b ADA, k = 150 and saturation_warning_level = 0.95, we would calculate:
207m + 100k > (31.7b / 150) * 0.95
and see that
207.1m > 200.8m
is true so we would change the pool rank to 153 (3 + k) and all pools previously ranked 4 through 153 would move up 1 rank.

4. Remove secion 5.6.2.

5. Remove section 5.6.3.

6. Remove section 5.6.4.

7. Add to secion 5.6.5.

For example, apparent performance, desirability and ranking can be made non-myopic for ranking purposes as follows:

dnm[n] :=
 average(d[1]...d[n],and[n + 1]...and[i])  if n < i
 average(d[1]...d[n])  if n = i
 (dnm[n - 1] * h) + (d[n] * (1 - h))  otherwise.

where:
n = epoch number beginning at n = 1 in the first epoch that the pool is eligible for potential rewards.
dnm[n] = the non-myopic desirability of the pool in the nth epoch.
d[n] = the desirability in the nth epoch unaware of historical desirability.
and[n] = the average desirability of the network as a whole in the nth epoch unaware of historical desirability.
h = historical influence factor, which is any real number between 0 and 1 exlusive.
i = integer(1 / h) which is the initial number of epochs during which we use the average desirability

As an example, setting h to 0.1 would mean that the initial number of epochs for using the averaging functions (i) would be 10.  If a pool has been eligible to receive rewards (n) for 3 epochs then we use the average of the pool's desirability for those 3 epochs and the overall network desirability for the prior 7 epochs.  After the 10th epoch we would use 90% of the previous epoch's non-myopic historical desirability and 10% of the current epoch's desirability to arrive at the new non-myopic desirability.

This gives a more reasonable ranking for newer pools that do not have enough historical data to provide fair rankings.

## Rationale: how does this CIP achieve its goals?

Using this non-centralizing ranking methodology gives a more fair ranking of stake pools based on performance, pledge and saturation which will encourage delegators to choose better pools.
It will also bring the rankings more in line with the general Cardano principle of increasing decentralization.

## Path to Active

### Acceptance Criteria

- [ ] One or more wallet software implements this new ranking approach.

### Implementation Plan

- [ ] Author has offered to produce an implementation of this change as a cardano-wallet / Daedalus pull request if shown where the current desirability equation is implemented in the code.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).


---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0025/README.md
---

- --
CIP: 25
Title: Media Token Metadata Standard
Status: Active
Category: Tokens
Authors:
- Alessandro Konrad <alessandro.konrad@live.de>
- Smaug <smaug@pool.pm>
Implementors: N/A
Discussions:
- https://forum.cardano.org/t/cip-nft-metadata-standard/45687
- https://www.reddit.com/r/CardanoDevelopers/comments/mkhlv8/nft_metadata_standard/
- https://github.com/cardano-foundation/CIPs/pull/85
- https://github.com/cardano-foundation/CIPs/pull/267
- https://github.com/cardano-foundation/CIPs/pull/341
- https://github.com/cardano-foundation/CIPs/pull/527
- https://github.com/cardano-foundation/CIPs/pull/593
Created: 2021-04-08
License: CC-BY-4.0
- --

## Abstract

This proposal defines an Media Token Metadata Standard for Native Tokens.

## Motivation: why is this CIP necessary?

Tokens on Cardano are a part of the ledger. Unlike on Ethereum, where metadata can be attached to a token through a smart contract, this isn't possible on Cardano because tokens are native and Cardano uses a UTxO ledger, which makes it hard to directly attach metadata to a token.
So the link to the metadata needs to be established differently.

Cardano has the ability to send metadata in a transaction, allowing the creation of a link between a token and the metadata. To make the link unique, the metadata should be appended to the same transaction, where the token forge happens:

> Given a token in a EUTXOma ledger, we can ask “where did this token come from?” Since tokens
> are always created in specific forging operations, we can always trace them back through their
> transaction graph to their origin.

—Section 4 in the paper [UTXOma:UTXO with Multi-Asset Support](https://iohk.io/en/research/library/papers/utxomautxo-with-multi-asset-support/)

That being said, we have unique metadata link to a token and can always prove that with 100% certainty. No one else can manipulate the link except if the policy allows it to ([update mechanism](#update-metadata-link-for-a-specific-token)).

## Specification

This is the registered `transaction_metadatum_label` value

| transaction_metadatum_label | description  |
| --------------------------- | ------------ |
| 721                         | Token Metadata |

### General structure

The structure allows for multiple token mints, also with different policies, in a single transaction.

```
{
  "721": {
    "<policy_id>": {
      "<asset_name>": {
        "name": <string>,

        "image": <uri | array>,
        "mediaType": image/<mime_sub_type>,

        "description": <string | array>,

        "files": [{
          "name": <string>,
          "mediaType": <mime_type>,
          "src": <uri | array>,
          <other_properties>
        }],

        <other properties>
      }
    },
    "version": <version_id>
  }
}
```

### CDDL

[Version 1](./cddl/version_1.cddl)\
[Version 2](./cddl/version_2.cddl)

- In version `1` the **`asset_name`** must be `utf-8` encoded and in text format for the key in the metadata map. In version `2` the the raw bytes of the **`asset_name`** are used.

- In version `1` the **`policy_id`** must be in text format for the key in the metadata map. In version `2` the the raw bytes of the **`policy_id`** are used.

- The  **`name`** property is marked as required.
- The **`image`** property is required and must be a valid [Uniform Resource Identifier (URI)](https://www.rfc-editor.org/rfc/rfc3986) pointing to a resource with mime type `image/*`.  Note that this resource is used as thumbnail or the actual link if the token is an image (ideally <= 1MB). If the string representing the resource location is >64 characters, an array may be used in place of a simple JSON string type, which viewers will automatically concatenate to create a single URI.
- Please note that if distributed storage systems like IPFS or Arweave are used it is required to use a URI containing the respective scheme (e.g., `ipfs://` or `ar://`) and not merely the content identifier (CID) as token viewers may not be able to locate the file.
- Valid identifiers would include:
- `"https://cardano.org/favicon-32x32.png"`
- `"ipfs://QmbQDvKJeo2NgGcGdnUiUFibTzuKNK5Uij7jzmK8ZccmWp"`
- `["ipfs://", "QmbQDvKJeo2NgGcGdnUiUFibTzuKNK5Uij7jzmK8ZccmWp"]`
- Invalid identifiers would include:
- `"cardano.org/favicon-32x32.png"`
- `"QmbQDvKJeo2NgGcGdnUiUFibTzuKNK5Uij7jzmK8ZccmWp"`
- `["Qm", "bQDvKJeo2NgGcGdnUiUFibTzuKNK5Uij7jzmK8ZccmWp"]`
- If an inline base64-encoded image will be used, the data must be prepended with a valid `data:<mime_type>;base64` prefix as specified by the [data URL scheme standard](https://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP/Data_URLs) to indicate the image is an inline data object
- See [the OpenSea "IPFS and Arweave URIs" section in their reference guide](https://docs.opensea.io/docs/metadata-standards#ipfs-and-arweave-uris) for more helpful information on the topic.

- The **`description`** property is optional.

- The **`mediaType`** and **`files`** properties are optional.<br /> **`mediaType`** is however required inside **`files`**. The **`src`** property inside **`files`** is an URI pointing to a resource of this mime type. If the mime type is `image/*`, **`mediaType`** points to the same image, like the on in the **`image`** property, but in an higher resolution.

- The **`version`** property is also optional. If not specified the version is `1`. It will become mandatory in future versions if needed.

- This structure really just defines the basis. New properties and standards can be defined later on for varies uses cases. That's why there is an "other properties" tag.

- The retrieval of the metadata should be the same for all however.

Optional fields allow to save space in the blockchain. Consequently the minimal structure for a single token is:

#### Version 1

```
{
  "721": {
    "<policy_id>": {
      "<asset_name>": {
        "name": <string>,
        "image": <uri | array>
      }
    }
  }
}
```

#### Version 2

```
{
  "721": {
    "<policy_id>": {
      "<asset_name>": {
        "name": <string>,
        "image": <uri | array>
      }
    },
    "version": 2
  }
}
```

### References

- Mime types: [RFC6838: Media Type Specifications and Registration Procedures](https://tools.ietf.org/html/rfc6838)
- CIP about reserved labels: [CIP-0010: Transaction Metadata Label Registry](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0010)
- [EIP-721](https://eips.ethereum.org/EIPS/eip-721)
- URI/URL standards: [RFC3986](https://tools.ietf.org/html/rfc3986), [RFC2397](https://tools.ietf.org/html/rfc2397)

## Rationale: how does this CIP achieve its goals?

### Retrieve valid metadata for a specific token

As mentioned above this metadata structure allows to have either one token or multiple tokens with also different policies in a single mint transaction. A third party tool can then fetch the token metadata seamlessly. It doesn't matter if the metadata includes just one token or multiple. The procedure for the third party is always the same:

1. Find the latest mint transaction with the label 721 in the metadata of the specific token that mints a positive amount of the token
2. Lookup the 721 key
3. Lookup the Policy Id of the token
4. Lookup the Asset name of the token
5. You end up with the correct metadata for the token

### Update metadata link for a specific token

Using the latest mint transaction with the label 721 as valid metadata for a token allows to update the metadata link of this token. As soon as a new mint transaction is occurring including metadata with the label 721 and a positive amount of the token, the metadata link is considered updated and the new metadata should be used. This is only possible if the policy allows to mint or burn the same token again.

Since modern token policies or ledger rules should generally make burning of tokens permissionless, the metadata update is restricted to minting (as in positive amounts) transaction and excludes burning transactions explicitly.

### Backward Compatibility

To keep token metadata compatible with changes coming up in the future, we use the **`version`** property.
A future version will introduce [schema.org](https://schema.org).

## Path to Active

### Acceptance Criteria

- [x] Support of this NFT definition in a commercially significant number and variety of NFT-related services and wallets.
- [x] Evolution of this document and standard beyond its early adoption and use cases (up through the point when alternative NFT standards have emerged).

### Implementation Plan

- [x] Promulgation of this standard among NFT creators, minting services, token analytic / query services, and wallets.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0026/README.md
---

- --
CIP: 26
Title: Cardano Off-Chain Metadata
Status: Active
Category: Metadata
Authors:
- Matthias Benkort <matthias.benkort@iohk.io>
- Michael Peyton Jones <michael.peyton-jones@iohk.io>
- Polina Vinogradova <polina.vinogradova@iohk.io>
Implementors:
- cardano-token-registry <https://github.com/cardano-foundation/cardano-token-registry>
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/112
Created: 2021-08-10
License: CC-BY-4.0
- --

## Abstract

We introduce a standard for off-chain metadata that can map opaque on-chain identifiers to metadata suitable for human consumption. This will support user-interface components, and allow resolving trust issues in a moderately decentralized way.

## Motivation: why is this CIP necessary?

On the blockchain, we tend to refer to things by hashes or other opaque identifiers. But these are not very human friendly:
- In the case of hashes, we often want to know the preimage of the hash, such as
- The script corresponding to an output locked by a script hash
- The public key corresponding to a public key hash
- We want other metadata as appropriate, such as
- A human-friendly name
- The name of the creator, their website, an icon, etc. etc.
- We would like such metadata to be integrated into the UI of our applications
- For example, if I’ve accepted a particular name for a currency, I’d like to see that name everywhere in the UI instead of the hash
- We want the security model of such metadata to be sound
- For example, we don’t want users to be phished by misleading metadata

We think there is a case for a metadata distribution system that would fill these needs in a consistent fashion. This would be very useful for Plutus, multi-asset support, and perhaps even some of the existing Cardano infrastructure. Moreover, since much of the metadata which we want to store is not determined by the blockchain, we propose a system that is independent of the blockchain, and relies on non-blockchain trust mechanisms.

The Rationale section provides additional justifications for the design decisions in this document.

### Use Cases

#### Hashed Content

Many pieces of information on the Cardano blockchain are stored as hashes, and only revealed at later stages when required for validation. This is the case for example of scripts (Plutus or phase-1), datums, and public keys. It is likely that (some) users will want to know the preimages of those hashes in a somewhat reliable way and, before they are revealed on-chain.

#### Off-Chain Metadata

Unlike some (un)popular opinions suggest, a blockchain is a poor choice for a content database. Blockchains are intrinsically ledgers and they are good at recording events or hashes. Yet, there are several elements for which hashes aren't providing a great user experience and to which we would rather attach some human-readable metadata (like names, websites, contact details etc...). This is the case for stake pool for instance for which [SMASH](https://github.com/input-output-hk/smash/) already provides a solution. This is also the case for monetary policies and scripts. In both cases, having the ability to attach extra metadata to _some_ hash with a way to ensure the authenticity of the data is useful.

## Specification

This specification covers some parts of a bigger system likely involving multiple components. What part is being implemented and by who is considered out of the scope of this specification. We however envision a setup in which users have access to a client application (a.k.a the wallet), which itself is able to connect to some remote server. We assume the server to also offer a user-interface (either via a graphical user-interface or a application programming interface) for accepting content.

> **Note** The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT", "SHOULD", "SHOULD NOT", "RECOMMENDED", "NOT RECOMMENDED", "MAY", and "OPTIONAL" in this document are to be interpreted as described in [RFC 2119](https://tools.ietf.org/html/rfc2119).

### Hash Function

There are several places in this document where we need an arbitrary hash function. We will  henceforth refer to this simply as “hash”. The hash function MUST be **Blake2b-256** (unless explicitly said otherwise). The hash of a string is the hash of the bytes of the string according to its encoding.

### Metadata Structure

Metadata consists of a mandatory _metadata subject_, and a number of _metadata properties_ which describes that subject. Each property consists of a mapping from _property names_ to _property values_, _sequence numbers_ and _signatures_.

Metadata subjects, property names, and property values must all be represented as UTF-8 encoded strings. In addition, property values must parse as valid JSON.

There is no particular interpretation attached to a metadata subject: it can be anything (see however the special-case of phase-1 monetary policy below). We anticipate however that the primary use-case for it will be something that appears on the blockchain, like the hash of a script.

We will refer to a whole metadata as a _metadata object_ and to a particular property assignment for a particular metadata subject as a _metadata entry_. We will say that a metadata object is _well-formed_ when it validates according to the [JSON-schema specification given in annex][schema.json]. To be valid, a metadata object MUST be (at least) well-formed.

```json
{
  "subject": "a5408d0db0d942fd80374",
  "contact": {
    "value": "Cid Kramer",
    "sequenceNumber": 0,
    "signatures": [
      {
        "signature": "79a4601",
        "publicKey": "bc77d04"
      }
    ]
  }
}
```

#### Sequence Numbers

Metadata entries MUST have a `sequenceNumber` associated with them. This is a monotonically increasing integer which is used to ensure that clients and servers do not revert to “earlier” versions of an entry. Upon receiving new metadata objects, servers SHOULD verify the sequence number for each entry already known for that subject and reject submissions with a lower sequence number.

#### Attestation Signatures

Metadata entries MAY have attestations `signatures` associated with them, in the form of an array of objects. Attestation signatures are indeed annotated. An annotated signature for a message is an object with of a `publicKey`, and a `signature` of a specified message (see below) by the corresponding private key.

> <sub>░ note ░</sub>
>
> When we say “signature” in the rest of this document we mean “annotated signature”.

An attestation signature for an entry is a signature of the entry message:

```js
hash(
  hash(CBOR(subject)) +
  hash(CBOR(name)) +
  hash(CBOR(value)) +
  hash(CBOR(sequenceNumber))
)
```

where `+` designates the concatenation of two bytestrings and `CBOR` designates a function which encodes its input into binary according to [RFC-8949](https://www.rfc-editor.org/rfc/rfc8949). That is, JSON strings are encoded as major type 3, JSON integers as major types 0 or 1, JSON floats as major type 7, JSON arrays as major type 4, JSON objects as major type 5, JSON booleans as major type 7 and JSON null as major type 7; according to the specification.

For example, the attestation message for the example entry above is:

```js
hash(
  hash(0x75613534303864306462306439343266643830333734) +  // text "a5408d0db0d942fd80374"
  hash(0x67636F6E74616374) +                              // text "contact"
  hash(0x6A436964204B72616D6572) +                        // text "Cid Kramer"
  hash(0x00)                                              // uint 0
)
// cd731afcc904c521e0c6b3cc0b560b8157ee29c3e41cd15f8dc8984edf600029
```

The `publicKey` and the `signature` MUST be base16-encoded and stored as JSON strings. All signatures must be verifiable through the **Ed25519 digital signature** scheme and public keys must therefore be 32-byte long Ed25519 public keys.


### Well-known Properties

The following properties are considered well-known, and the JSON in their values MUST have the given structure and semantic interpretation. New properties can be added to this list by amending this CIP. The role of well-known properties is to facilitate integration between applications implementing this CIP. Nevertheless, registries are encouraged to **not** restrict properties to only this limited set but, registries (or metadata servers) MUST verify the well-formedness of those properties when present in a metadata object.

<details>
  <summary><code>preimage</code></summary>

```json
{
    "type": "object",
    "description": "A hashing algorithm identifier and a base16-enocoded bytestring, such that the bytestring is the preimage of the metadata subject under that hash function.",
    "requiredProperties": [ "alg", "msg" ],
    "properties": {
        "alg": {
            "type": "string",
            "description": "A hashing algorithm identifier. The length of the digest is given by the subject.",
            "enum": [ "sha1", "sha", "sha3", "blake2b", "blake2s", "keccak", "md5" ]
        },
        "msg": {
            "type": "string",
            "description": "The actual preimage.",
            "encoding": "base16"
        }
    }
}
```
</details>


<details>
  <summary><code>name</code></summary>

```json
{
    "type": "string",
    "description": "A human-readable name for the metadata subject, suitable for use in an interface or in running text.",
    "maxLength": 50,
    "minLength": 1
}
```
</details>


<details>
  <summary><code>description</code></summary>

```json
{
    "type": "string",
    "description": "A longer description of the metadata subject, suitable for use when inspecting the metadata subject itself.",
    "maxLength": 500
}
```
</details>

<details>
  <summary><code>ticker</code></summary>

```json
{
  "type": "string",
  "description": "A short identifier for the metadata subject, suitable to show in listings or tiles.",
  "maxLength": 9,
  "minLength": 2
}
```
</details>

<details>
  <summary><code>decimals</code></summary>

```json
{
  "type": "integer",
  "description": "When the metadata subject refers to a monetary policy, refers to the number of decimals of the currency.",
  "minimum": 0,
  "maximum": 19
},
```
</details>

<details>
  <summary><code>url</code></summary>

```json
{
    "type": "string",
    "description": "A universal resource identifier pointing to additional information about the metadata subject.",
    "format": "uri",
    "maxLength": 250
}
```
</details>

<details>
  <summary><code>logo</code></summary>

```json
{
  "type": "string",
  "description": "An `image/png` object which is 64KB in size at most.",
  "encoding": "base64",
  "maxLength": 87400
}
```
</details>

### Verifying Metadata

#### General Case

Applications that want to display token metadata MUST verify signatures of metadata entries against a set of trusted keys for certain subjects. We will call such applications _"clients"_. Conceptually we expect clients to maintain a mapping of many subjects to many verification keys. In case where a metadata entry contains no signatures, when none of the provided signatures was produced by a known key for the corresponding subject or when none of the provided signatures verifies: the metadata entry MUST be considered invalid and not be presented to end-users.

Note that:

- In this scenario, a single valid signature is sufficient to consider a metadata entry valid but there can be many signatures (invalid or valid). So long as one is valid, the entry is considered verified.

- The verification is done per _entry_. That is, a metadata object may contain both verified and unverified entries. Plus, entries under the same subject may be verified by different keys.

The way by which the trusted keys are registered into clients is unspecified although we already consider the following, **non-overlapping**, **complementary**, options:

1. Clients MAY explicitly prompt the consumer / end-user about whether to accept a certain entry. While this is unpractical for some cases (e.g. token metadata), it may be relevant for some.

1. Clients MAY come with a set of pre-configured well-known trusted keys chosen at the discretion of the application editor.

2. End-users SHOULD have the ability to add/remove keys from their trusted set. This allows end-users to introduce trusted keys they know before they end up in the pre-configured set (which likely follow the application release cycle). In this context, keys are advertised by the signing authority by some means, for instance, on social media or on another form of public key registry (e.g. [keybase.io](http://keybase.io/))

3. Mappings of subjects to keys MAY be recorded on-chain, using transaction metadata and an appropriate label registered on [CIP-0010]. In some scenarios, the context within which the transaction is signed may be enough to reliably trust the legitimacy of a mapping. For example, in the case of a monetary policy, one could imagine _registering trusted keys_ in the same transaction minting tokens. Because the transaction is inserted in the ledger, it must have been signed by the token issuer and therefore, the specified keys are without doubt acknowledged by the token issuer. As a result, clients having access to on-chain data can automatically discover new mappings from observing the chain.

#### Special Case: Phase-1 Monetary Policies

> <sub>░ note ░</sub>
>
> This approach is now considered superseded by the more general approach. While it has some nice properties, new applications should consider sticking to the general case **even for phase-1 monetary policies**. Servers who seek backward-compatibility should implement both.

We consider the case of phase-1 monetary policies introduced during the Allegra era of Cardano. Such policies are identified by a simple (native) script which is validated by the ledger during the so-called phase-1 validations. Such scripts are made of key hashes, combined via a set of basic first-order logic primitives (ALL, ANY-OF, N-OF...) such that, it is possible to statically verify whether a set of signatures would validate the script.

These scripts therefore make relatively good verification mechanisms for metadata associated with phase-1 monetary policies. Hence, we introduce the following well-known property:

<details>
  <summary><code>policy</code></summary>

```json
{
  "type": "string",
  "description": "A CBOR-serialized phase-1 monetary policy, used as a pre-image to produce a policyId.",
  "encoding": "base16",
  "minLength": 56,
  "maxLength": 120
}
```
</details>

<br/>

Metadata objects that contain an extra top-level property `policy` MUST therefore abide by the following rules:

1. Their subject MUST be an `assetId`, encoded in base16; where the `assetId` is the concatenation of a `policyId` (28 bytes) and an `assetName` (up to 32 bytes).
2. The `policy` MUST therefore re-hash (through blake2b-224) into the first 28 bytes of the metadata's subject (the policy id).
3. Every metadata entry MUST have a set of signatures such that, the monetary script given in policy can be validated using the provided signatures, irrespective of the time constraints. Said differently, the public keys from the annotated signature must re-hash into key hashes present in the policy AND each key must verify its associated signature AND the provided signatures must
be sufficient to validate the monetary script according to the semantic given by the cardano ledger without considering the time constraints.

For example, consider the following phase-1 monetary policy, represented in JSON:

```json
{
    "type": "all",
    "scripts": [
        {
            "keyHash": "2B0C33E73D2A70733EDC971D19E2CAFBADA1692DB2D35E7DC9453DF2",
            "type": "sig"
        },
        {
            "keyHash": "E2CAFBADA1692DB2D35E7DC9453DF22B0C33E73D2A70733EDC971D19",
            "type": "sig"
        }
    ]
}
```

To validate such a policy, each entry would require 2 signatures:

- from `2B0C33E73D2A70733EDC971D19E2CAFBADA1692DB2D35E7DC9453DF2`
- and from `E2CAFBADA1692DB2D35E7DC9453DF22B0C33E73D2A70733EDC971D19`

The policy MAY also contain some additional time constraints (VALID-AFTER, VALID-BEFORE) specifying a certain slot number. For the sake of verifying policies, these should be ignored and consider _valid_.

### Recommendations For Metadata Servers

The following section gives some recommendations to application developers willing to implement a metadata server / registry. Following these recommendations will facilitate interoperability between applications and also, provide some good security foundation for the server. In this context, what we refer to as a _metadata server_ is a web server that exposes the functionality of a simple key-value store, where the keys are metadata subjects and property names, and the values are their property values.

##### Querying Metadata

The metadata server SHOULD implement the following HTTP methods:

```
GET /metadata/{subject}/property/{property name}
```

This SHOULD return the property values for the given property name (if any) associated with the subject. This is returned as a array of JSON objects whose key is the property name and return the complete JSON entries for that subject+name (including `value`, `sequenceNumber` and `signatures`).

The metadata server SHOULD set the Content-Length header to allow clients to decide if they wish to download sizeable metadata.

```
GET /metadata/{subject}/properties
```

This SHOULD return all the property names which are available for that subject (if any). These are returned as a JSON list of strings.

```
GET /metadata/{subject}
```

This SHOULD return a array of the full metadata objects associated with this subject, including the subject and all properties associated with it.

```
POST /metadata/query
REQUEST BODY : a JSON object with the keys:
  “subjects” : A list of subjects, encoded as strings in the same way as the queries above.
  “properties” : An optional list of property names, encoded as strings in the same way as the query above.
```

This endpoint provides a way to batch queries, making several requests of the server with only one HTTP request.

If only `“subjects”` is supplied, this query SHOULD return a list of subjects with all their properties. The response format will be as similar as possible to the `GET /metadata/{subject}` request, but nested inside a list.

If `“subjects”` and `“properties”` are supplied, the query will return a list of subjects, with their properties narrowed down to only those specified by `“properties”`.

> <sub>░ suggestion ░</sub>
>
> Metadata servers MAY provide other methods of querying metadata, such as:
>
> * Searching for all mappings whose “name” property value is a particular string
> * Searching for all metadata items which are signed by a particular cryptographic key, or uploaded by a particular user

##### Modifying metadata

The metadata server needs some way to add and modify metadata entries. The method for doing so is largely up to the implementor, but recommend to abide by the following rules:

1. The server MUST only accept updates for metadata entries that have a higher sequence number than the previous entry.
1. The server MUST always verify well-formedness of metadata entries before accepting them.
1. The server MUST always cryptographically verify metadata entries' signatures before accepting them.
1. The server MAY reject entries with no signatures.
1. The server MAY support the POST, PUT, and DELETE verbs to modify metadata entries.

##### Authentication

If the server supports modifications to metadata entries, it SHOULD provide some form of authentication which controls who can modify them. Servers MUST NOT use the attestation signatures on metadata entries as part of authentication (with the exception of phase-1 monetary policy). Attestation signatures are per-entry, and are orthogonal to determining who controls the metadata for a subject.

Simple systems may want to use little more than cryptographic signatures, but more sophisticated systems could have registered user accounts and control access in that way.

##### Audit

The metadata server MAY provide a mechanism auditing changes to metadata, for example by storing the update history of an entry and allowing users to query it (via the API or otherwise).

#### Hardening

Allowing unrestricted updates to non-verifiable metadata would allow malicious users to “squat” or take over another user’s metadata subjects. This is why we recommend that server implementations have some kind of authentication system to mitigate this.

One approach is to have a system of “ownership” of metadata subjects. This notion of ownership is vague, although in some cases there are obvious choices (e.g. for a multisig minting policy, the obvious owners are the signatories who can authorize minting). It is up to the server to pick a policy for how to decide on owners, and enforce security; or indeed to take a different approach entirely.

See the earlier “Authentication” section for a description of a possible approach to managing ownership.

Depending on the authentication mechanism they use, servers may also need to worry about replay attacks, where an attacker submits a (correctly signed) old record to “revert” a legitimate change.  However, these can be unconditionally prevented by correctly implementing sequence numbers as described earlier, which prevents old entries from being accepted.

### Recommendations For Metadata Clients

Similar to the recommendations for metadata servers, this section gives some recommendations to application developers willing to implement a metadata client. Following these recommendations will facilitate interoperability between applications and also, provide some good security foundation for the clients. A metadata client refers to the component that communicates with a metadata server and maintains the user’s trusted metadata mapping. This may be implemented as part of a larger system, or may be an independent component (which could be shared between multiple other systems, e.g. a wallet frontend and a blockchain explorer).

- The metadata client MUST allow the metadata server (or servers) that it references to be configured by the end-user.
- The metadata client MUST maintain a local mapping of trusted keys and metadata entries.
- The metadata client MUST only accept updates with a higher sequence number than the entry it currently has.
- The metadata client MUST always verify well-formedness or metadata entries before accepting them.
- The metadata client MUST always verify any signatures on metadata entries before accepting them.
- The metadata client SHOULD allow browsing of its trusted keys.
- The metadata client SHOULD allow modifications of its trusted keys by the end-user.
- The metadata client SHOULD use a Trust On First Use (TOFU) strategy for updating this store. When an update is requested for a metadata entry, the client should query the server, but not trust the resulting metadata entry unless the user agrees to use it (either by explicit consent, or implicitly via the set of trusted keys). If the entry is trusted, it should be copied to the local store.
- The metadata client MAY be configurable to limit the amount downloaded.
- The metadata client MAY accept user updates for metadata entries.

## Rationale: how does this CIP achieve its goals?

### Interfaces and progressive enhancement

The design space for a metadata server is quite large. For example, any of the following examples could work, or other combinations of these features:

- A single centralized server maintained by the Cardano Foundation, and updated by emailing the administrator (no user-facing front end website)
- An open-source federation of servers with key-based authentication.
- A commercial server with an account-based system, a web-frontend, and a payments system for storage.

This design aims to be agnostic about the details of the implementation, by specifying only a very simple interface between the client, server, and system-component users (wallet, etc.). This allows:
- Progressive enhancement of servers over time. Initially servers may provide a very bare-bones implementation, but this can be improved later.
- Competition between multiple implementations, including from third parties.

### Decentralization

For much of the metadata we are concerned with there is no “right” answer. The metadata server is thus playing a key trusted role - even if that trust is partial because users can rely on attestation signatures. We therefore believe that it is critical that we allow users to choose which metadata server (or servers) they refer to.

An analogy is with DNS nameservers: these are a trusted piece of infrastructure for any particular user, but users have a choice of which nameservers to use, and can use multiple.

This also makes it possible for these servers to be a true piece of community infrastructure, rather than something wedded to a major player (although we hope that the Cardano Foundation and IOHK will produce a competitive offering).

### Verifiable vs non-verifiable metadata

A key distinction is between metadata that is verifiably correct and that which is non-verifiable.

The key example of verifiable metadata is hashe pre-images. Where the metadata subject is a hash, the preimage of that hash is verifiable metadata, since we can simply hash it and check that it matches. This covers several cases that we are interested in:

- Mapping script hashes to the script
- Mapping public key hashes to the public key

However, most metadata is non-verifiable:

- Human-readable names for scripts are not verifiable: there is no “right” answer to what the name of a script is.
- Many scripts do not have an obvious “owner” (certainly this is true for Plutus scripts), but even for those which do (e.g. phase-1 monetary scripts) this does not mean that the owner is trusted! See the “Security” section below for more discussion on trust.
- Any associations with authors, websites, icons, etc are similarly non-verifiable as there is no basis for establishing trust.

### Security

Most of the security considerations relate to non-verifiable metadata. Verifiable metadata can generally always be accepted as is (provided that it is verified). Our threat model is that non-verifiable metadata may always have been provided by an attacker.

Accepting non-verifiable metadata blindly can lead to attacks. For example, a malicious server or user might attempt to name their currency “Ada”. If we blindly accept this and overwrite the existing mapping for “Ada”, this would lead to easy phishing attacks. The approach we take is heavily inspired by petname systems, GPG keyservers, and local address books. The user always has a local mapping which is trusted, and then adding to or updating that mapping requires explicit user consent, unless we can prove that this is trustworthy (i.e. the metadata is verifiable).

How can users decide whether to trust an update? This is where the attestation signatures come into play. If a user trusts the entity which signs the metadata record, that may be sufficient for them to accept it as a legitimate update.

Clients could also be tricked into downloading large amounts of metadata that the user does not want. For this reason clients should expose some kind of configurable download limiting, and we suggest that the server set the Content-Length header to support this. However, this problem is no worse than that faced by the average web browser, so we do not think it will be a problem in practice.

Clients also need to worry about replay attacks, where they are sent old (correctly signed) records in an attempt to “roll back” a legitimate update. The easiest way to avoid this is to correctly implement sequence numbers, in which case old updates will be rejected.

### Data storage location

This design needs to store a fair amount of data in a shared location. We might wonder whether we should use the blockchain for this: we could store metadata updates in transaction metadata.

However, storing this information on-chain does not actually help us:

- The trust model for metadata is different than that of the ledger and transactions. The only trust we have (and can expect to have) in the metadata is that it is signed by a particular key, regardless of the purpose or nature of the data. E.g. when posting a script, there is no explicit association between the script and the signing key other than the owner of the key choosing to post it
- The metadata is precisely that: metadata. While it is about the chain, it does not directly affect ledger state transitions and therefore we should not require it to be associated with a specific transaction.

Besides, on-chain storage comes with considerable downsides:

- Higher cost to users for modifications and storage
- Increases in the UTXO size
- Awkwardness of querying the data
- Size limits on transaction metadata

For this reason, we think that a traditional database is a much better fit. However, it would be perfectly possible for someone to produce an implementation that was backed by the chain if they believed that that could be competitive.

### Storage cost

Metadata may potentially be sizable. For example, preimages of hashes can in principle be any size!

Servers will want some way to manage this to avoid abuse. However, this is a typical problem faced by web services and can be solved in the usual ways: size limits per account, charging for storage, etc.

### Backwards compatibility

See [Special Case: Phase-1 Monetary Policies](#special-case--phase--1-monetary-policies) which covers existing implementations.

## Path to Active

### Acceptance Criteria

- [x] Document commercial or community implementations of any of the use cases described above.
- [cardano-token-registry](https://github.com/cardano-foundation/cardano-token-registry)

### Implementation Plan

- [x] Provide a reference implementation: [Off-chain metadata tools](https://github.com/input-output-hk/offchain-metadata-tools)

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

[schema.json]: https://raw.githubusercontent.com/cardano-foundation/CIPs/master/CIP-0026/schema.json
[CIP-0010]: https://github.com/cardano-foundation/CIPs/blob/master/CIP-0010

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0027/README.md
---

- --
CIP: 27
Title: CNFT Community Royalties Standard
Status: Active
Category: Tokens
Authors:
- Huth S0lo <john@digitalsyndicate.io>
- TheRealAdamDean <adam@crypto2099.io>
Implementors: N/A
Discussions:
- https://forum.cardano.org/t/cip-royalties/68599
- https://github.com/cardano-foundation/CIPs/pull/116
Created: 2021-08-29
License: Apache-2.0
- --

## Abstract

This proposed standard will allow for uniform royalties' distributions across the secondary market space. It is easy to implement using metadata only, and does not require a smart contract.  However, it is scalable to allow for the usage of a downstream smart contract, as needed by the asset creator.

## Motivation: why is this CIP necessary?

There is a significant interest within the Cardano Community for an implementation of royalties distribution when a Cardano Asset is resold on the secondary market. It has become a common theme to see and hear statements that the only thing stopping artists from adopting Cardano, is that they are waiting for an implementation of royalties.

At the present time, smart contracts do not create a simple mechanism to implement royalties.  By developing a community standard, we can resolve the immediate need for royalties, and create a path forward for a potential future iteration of smart contracts.

## Specification

A new tag of 777 is proposed for this implementation.  The community guidelines have been agreed as follows:
1) A brand new unused policy for implementation is required.
2) The royalties tags are to be written to an unnamed token, using the policy to be used for the intended Cardano Assets.
3) Only the first minted set of instructions will be honored.  Any future updates or rewrites will be ignored.  This prevents a Cardano Asset maker from changing the royalties at a future date.
4) Within this created asset will be the metadata for royalties distributions.  It will use a tag of 777, and then have two tags to identify the percentage of future sales requested as a royalty, and the payment address to forward those royalties to.  Those tags will be "rate" and "addr" respectively.
5) The "rate" key tag can be any floating point value from 0.0 to 1.0, to represent between 0 and 100 percent.  For example, a 12.5 percent royalty would be represented with "rate": "0.125".  Previous version 1.0 of this proposal used pct instead of rate.  Marketplaces to continue to honor legacy pct tag.
6) The "addr" key tag can be a string value, or an array.  It is to include a single payment address.  By allowing for an array, the payment address can exceed the per line 64 character limitation.  This payment address could be part of a smart contract, which should allow for greater flexibility of royalties distributions, controlled by the asset creator.
7) The royalty token must be minted prior to creating any assets off the policy.  All markets will be instructed to look only for the first minted asset on a policy, which would need to be the unnamed 777 token.

### Example JSON with string

{
	"777": {
		"rate": "0.2",
		"addr": "addr1v9nevxg9wunfck0gt7hpxuy0elnqygglme3u6l3nn5q5gnq5dc9un"
	}
}

### Example JSON with array

{
	"777": {
		"rate": "0.2",
		"addr": [
			"addr1q8g3dv6ptkgsafh7k5muggrvfde2szzmc2mqkcxpxn7c63l9znc9e3xa82h",
			"pf39scc37tcu9ggy0l89gy2f9r2lf7husfvu8wh"
		]
	}
}

### Process Flow

1) Create policy for planned assets.
2) Mint no name token with community standard royalties metadata.
3) Burn no name token to free up UTxO (recommended, but not required).
4) Mint planned assets using this same policy.

## Rationale: how does this CIP achieve its goals?

By creating a new tag for the distinct purpose of royalties distributions, Cardano Asset makers, and Marketplaces can uniformly apply royalties to assets with predictable results.

By creating the instructions on a single, no name token, all marketplaces will know the correct location of the royalties asset, without having to further locate it.

By enforcing the requirement of honoring only the first mint, cardano asset buyers and owners can predict the future resale value of the assets in their possession.

The solution is scalable to any desired royalty percentage.  It is easy to work with this new standard, and does not require an in depth understanding of smart contracts.

## Path to Active

### Acceptance Criteria

- [x] Support of royalty distribution according to this standard by multiple significant NFT related platforms.
- [x] Implementation in libraries supporting NFT minting, including:
- [x] Mesh ([Minting Royalty Token](https://meshjs.dev/apis/transaction/minting#mintingRoyaltyToken))

### Implementation Plan

- [x] Incorporate input from many Cardano NFT related entities, including:
- [x] Artano
- [x] BuffyBot
- [x] CNFT.io
- [x] Digital Syndicate
- [x] Fencemaker
- [x] MADinArt
- [x] NFT-Maker.io
- [x] Hydrun
- [x] Tokhun

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0028/README.md
---

- --
CIP: 28
Title: Protocol Parameters (Alonzo Era)
Status: Active
Category: Ledger
Authors:
- Kevin Hammond <kevin.hammond@iohk.io>
Implementors:
- IOG
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/140
Created: 2021-10-14
License: Apache-2.0
- --

## Abstract

This CIP extends CIP-0009 to include the new protocol parameters that have been introduced for Alonzo, specifically those relating to the costing of Plutus scripts.  It describes the initial settings for those parameters.

## Motivation: why is this CIP necessary?

We need to document the chain of changes to the protocol parameters.  This document describes precisely the changes that have been made from CIP-0009, allowing the differences to be determined.  It thus supplements rather than replaces CIP-0009.

## Specification

### New Updatable Protocol Parameters

The new **updatable** protocol parameter values for Alonzo are given below (in JSON format).  Any of these parameters may be changed by submitting
a parameter update proposal to the chain, and without triggering a "hard fork".  Note that these parameters are given using the names used
in the genesis file.  Be aware that some parameters are shown differently using ``cardano-cli query protocol-parameters`` -- this has been raised
as an issue with the development team.

```
{
    "lovelacePerUTxOWord": 34482,
    "executionPrices": {
        "prSteps":
	{
	    "numerator" :   721,
	    "denominator" : 10000000
		},
        "prMem":
	{
	    "numerator" :   577,
	    "denominator" : 10000
	}
    },
    "maxTxExUnits": {
        "exUnitsMem":   10000000,
        "exUnitsSteps": 10000000000
    },
    "maxBlockExUnits": {
        "exUnitsMem":   50000000,
        "exUnitsSteps": 40000000000
    },
    "maxValueSize": 5000,
    "collateralPercentage": 150,
    "maxCollateralInputs": 3,
    "costModels": {
        "PlutusV1": {
            "sha2_256-memory-arguments": 4,
            "equalsString-cpu-arguments-constant": 1000,
            "cekDelayCost-exBudgetMemory": 100,
            "lessThanEqualsByteString-cpu-arguments-intercept": 103599,
            "divideInteger-memory-arguments-minimum": 1,
            "appendByteString-cpu-arguments-slope": 621,
            "blake2b-cpu-arguments-slope": 29175,
            "iData-cpu-arguments": 150000,
            "encodeUtf8-cpu-arguments-slope": 1000,
            "unBData-cpu-arguments": 150000,
            "multiplyInteger-cpu-arguments-intercept": 61516,
            "cekConstCost-exBudgetMemory": 100,
            "nullList-cpu-arguments": 150000,
            "equalsString-cpu-arguments-intercept": 150000,
            "trace-cpu-arguments": 150000,
            "mkNilData-memory-arguments": 32,
            "lengthOfByteString-cpu-arguments": 150000,
            "cekBuiltinCost-exBudgetCPU": 29773,
            "bData-cpu-arguments": 150000,
            "subtractInteger-cpu-arguments-slope": 0,
            "unIData-cpu-arguments": 150000,
            "consByteString-memory-arguments-intercept": 0,
            "divideInteger-memory-arguments-slope": 1,
            "divideInteger-cpu-arguments-model-arguments-slope": 118,
            "listData-cpu-arguments": 150000,
            "headList-cpu-arguments": 150000,
            "chooseData-memory-arguments": 32,
            "equalsInteger-cpu-arguments-intercept": 136542,
            "sha3_256-cpu-arguments-slope": 82363,
            "sliceByteString-cpu-arguments-slope": 5000,
            "unMapData-cpu-arguments": 150000,
            "lessThanInteger-cpu-arguments-intercept": 179690,
            "mkCons-cpu-arguments": 150000,
            "appendString-memory-arguments-intercept": 0,
            "modInteger-cpu-arguments-model-arguments-slope": 118,
            "ifThenElse-cpu-arguments": 1,
            "mkNilPairData-cpu-arguments": 150000,
            "lessThanEqualsInteger-cpu-arguments-intercept": 145276,
            "addInteger-memory-arguments-slope": 1,
            "chooseList-memory-arguments": 32,
            "constrData-memory-arguments": 32,
            "decodeUtf8-cpu-arguments-intercept": 150000,
            "equalsData-memory-arguments": 1,
            "subtractInteger-memory-arguments-slope": 1,
            "appendByteString-memory-arguments-intercept": 0,
            "lengthOfByteString-memory-arguments": 4,
            "headList-memory-arguments": 32,
            "listData-memory-arguments": 32,
            "consByteString-cpu-arguments-intercept": 150000,
            "unIData-memory-arguments": 32,
            "remainderInteger-memory-arguments-minimum": 1,
            "bData-memory-arguments": 32,
            "lessThanByteString-cpu-arguments-slope": 248,
            "encodeUtf8-memory-arguments-intercept": 0,
            "cekStartupCost-exBudgetCPU": 100,
            "multiplyInteger-memory-arguments-intercept": 0,
            "unListData-memory-arguments": 32,
            "remainderInteger-cpu-arguments-model-arguments-slope": 118,
            "cekVarCost-exBudgetCPU": 29773,
            "remainderInteger-memory-arguments-slope": 1,
            "cekForceCost-exBudgetCPU": 29773,
            "sha2_256-cpu-arguments-slope": 29175,
            "equalsInteger-memory-arguments": 1,
            "indexByteString-memory-arguments": 1,
            "addInteger-memory-arguments-intercept": 1,
            "chooseUnit-cpu-arguments": 150000,
            "sndPair-cpu-arguments": 150000,
            "cekLamCost-exBudgetCPU": 29773,
            "fstPair-cpu-arguments": 150000,
            "quotientInteger-memory-arguments-minimum": 1,
            "decodeUtf8-cpu-arguments-slope": 1000,
            "lessThanInteger-memory-arguments": 1,
            "lessThanEqualsInteger-cpu-arguments-slope": 1366,
            "fstPair-memory-arguments": 32,
            "modInteger-memory-arguments-intercept": 0,
            "unConstrData-cpu-arguments": 150000,
            "lessThanEqualsInteger-memory-arguments": 1,
            "chooseUnit-memory-arguments": 32,
            "sndPair-memory-arguments": 32,
            "addInteger-cpu-arguments-intercept": 197209,
            "decodeUtf8-memory-arguments-slope": 8,
            "equalsData-cpu-arguments-intercept": 150000,
            "mapData-cpu-arguments": 150000,
            "mkPairData-cpu-arguments": 150000,
            "quotientInteger-cpu-arguments-constant": 148000,
            "consByteString-memory-arguments-slope": 1,
            "cekVarCost-exBudgetMemory": 100,
            "indexByteString-cpu-arguments": 150000,
            "unListData-cpu-arguments": 150000,
            "equalsInteger-cpu-arguments-slope": 1326,
            "cekStartupCost-exBudgetMemory": 100,
            "subtractInteger-cpu-arguments-intercept": 197209,
            "divideInteger-cpu-arguments-model-arguments-intercept": 425507,
            "divideInteger-memory-arguments-intercept": 0,
            "cekForceCost-exBudgetMemory": 100,
            "blake2b-cpu-arguments-intercept": 2477736,
            "remainderInteger-cpu-arguments-constant": 148000,
            "tailList-cpu-arguments": 150000,
            "encodeUtf8-cpu-arguments-intercept": 150000,
            "equalsString-cpu-arguments-slope": 1000,
            "lessThanByteString-memory-arguments": 1,
            "multiplyInteger-cpu-arguments-slope": 11218,
            "appendByteString-cpu-arguments-intercept": 396231,
            "lessThanEqualsByteString-cpu-arguments-slope": 248,
            "modInteger-memory-arguments-slope": 1,
            "addInteger-cpu-arguments-slope": 0,
            "equalsData-cpu-arguments-slope": 10000,
            "decodeUtf8-memory-arguments-intercept": 0,
            "chooseList-cpu-arguments": 150000,
            "constrData-cpu-arguments": 150000,
            "equalsByteString-memory-arguments": 1,
            "cekApplyCost-exBudgetCPU": 29773,
            "quotientInteger-memory-arguments-slope": 1,
            "verifySignature-cpu-arguments-intercept": 3345831,
            "unMapData-memory-arguments": 32,
            "mkCons-memory-arguments": 32,
            "sliceByteString-memory-arguments-slope": 1,
            "sha3_256-memory-arguments": 4,
            "ifThenElse-memory-arguments": 1,
            "mkNilPairData-memory-arguments": 32,
            "equalsByteString-cpu-arguments-slope": 247,
            "appendString-cpu-arguments-intercept": 150000,
            "quotientInteger-cpu-arguments-model-arguments-slope": 118,
            "cekApplyCost-exBudgetMemory": 100,
            "equalsString-memory-arguments": 1,
            "multiplyInteger-memory-arguments-slope": 1,
            "cekBuiltinCost-exBudgetMemory": 100,
            "remainderInteger-memory-arguments-intercept": 0,
            "sha2_256-cpu-arguments-intercept": 2477736,
            "remainderInteger-cpu-arguments-model-arguments-intercept": 425507,
            "lessThanEqualsByteString-memory-arguments": 1,
            "tailList-memory-arguments": 32,
            "mkNilData-cpu-arguments": 150000,
            "chooseData-cpu-arguments": 150000,
            "unBData-memory-arguments": 32,
            "blake2b-memory-arguments": 4,
            "iData-memory-arguments": 32,
            "nullList-memory-arguments": 32,
            "cekDelayCost-exBudgetCPU": 29773,
            "subtractInteger-memory-arguments-intercept": 1,
            "lessThanByteString-cpu-arguments-intercept": 103599,
            "consByteString-cpu-arguments-slope": 1000,
            "appendByteString-memory-arguments-slope": 1,
            "trace-memory-arguments": 32,
            "divideInteger-cpu-arguments-constant": 148000,
            "cekConstCost-exBudgetCPU": 29773,
            "encodeUtf8-memory-arguments-slope": 8,
            "quotientInteger-cpu-arguments-model-arguments-intercept": 425507,
            "mapData-memory-arguments": 32,
            "appendString-cpu-arguments-slope": 1000,
            "modInteger-cpu-arguments-constant": 148000,
            "verifySignature-cpu-arguments-slope": 1,
            "unConstrData-memory-arguments": 32,
            "quotientInteger-memory-arguments-intercept": 0,
            "equalsByteString-cpu-arguments-constant": 150000,
            "sliceByteString-memory-arguments-intercept": 0,
            "mkPairData-memory-arguments": 32,
            "equalsByteString-cpu-arguments-intercept": 112536,
            "appendString-memory-arguments-slope": 1,
            "lessThanInteger-cpu-arguments-slope": 497,
            "modInteger-cpu-arguments-model-arguments-intercept": 425507,
            "modInteger-memory-arguments-minimum": 1,
            "sha3_256-cpu-arguments-intercept": 0,
            "verifySignature-memory-arguments": 1,
            "cekLamCost-exBudgetMemory": 100,
            "sliceByteString-cpu-arguments-intercept": 150000
        }
    }
}
```

The meaning of the fields is:

| Field                 	| Initial Value                                                          	| Description                                                                                                                                                                                                                      	|
|-----------------------	|------------------------------------------------------------------------	|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------	|
| lovelacePerUTxOWord       	| 34482 	| Deposit charged per word of UTxO storage.  |
| executionPrices | `{ "prSteps": { "numerator" : 721, "denominator" : 10000000}, "prMem": { "numerator" :   577, "denominator" : 10000 } }` | Fee per Plutus execution step and per memory unit |
| maxTxExUnits | `{ "exUnitsMem":   10000000,        "exUnitsSteps": 10000000000 }` | Maximum number of memory units and steps in a single transaction. |
| maxBlockExUnits | `{ "exUnitsMem":   50000000, "exUnitsSteps": 40000000000 }` | Maximum number of memory units and steps in a single block. |
| maxValueSize | 5000 | The limit on the serialized size of the Value in each output. |
| collateralPercentage | 150 | Percentage of fee that is used as collateral for a failed transaction. |
| maxCollateralInputs | 3 | Maximum number of collateral inputs in a transaction. |
| costModels | `{  "PlutusV1": { ... } }` | Detailed cost models for each Plutus version. |

Each version of the Plutus interpreter may use different cost model parameters and settings.  Although the parameters are updatable,
they are likely to be changed only when introducing new Plutus interpreter versions at a "hard fork".
For simplicity, the details of the parameter settings is omitted here.

### Obsoleted Updatable Protocol Parameters

``minUTxOValue`` is no longer used.  It is replaced by ``lovelacePerUTxOWord``.

### Non-Updatable Parameters

There are no changes to the non-updatable protocol parameters.

## Rationale: how does this CIP achieve its goals?

The majority of the parameters are needed to enable the use of Plutus scripts on-chain.  They relate to the fees calculations for
transactions that include Plutus scripts.

``executionPrices`` are specified in fractions of lovelace per Plutus CPU execution step or memory unit.
These have been set to be consistent with the cost for a full transaction.

``lovelacePerUTxOWord`` replaces ``minUTxOValue``.
Rather than determining a fixed minimum deposit, the new value scales each word that is used.
The value is set to give a very similar result for an ada-only UTxO entry (previously, 1,000,000 lovelace; now 999,978 lovelace, since each ada-only
UTxO entry is 29 words).

``collateralPercentage``has been chosen to be higher than the transaction fee.  Collateral should only be used to pay fees if a user has deliberately
submitted a transaction that is known to fail.  Setting the percentage high acts to discourage the submission of rogue transactions, which
maliciously consume chain resources.

``maxCollateralInputs`` has been set to allow the option of multiple inputs to be used to pay collateral, if needed (e.g. so that
multiple instance of a transaction can be submitted without sharing a single collateral, that might restrict concurrency or cause
script failure if the collateral was not available).

``maxValueSize`` has been set based on benchmarking.

``costModels`` has been set for ``PlutusV1`` based on benchmarking inputs.  Each Plutus Core primitive has associated costs.

## Path to Active

### Acceptance Criteria

- [x] The Alonzo ledger era is activated.
- [x] Documented parameters have been in operational use by Cardano Node and Ledger as of the Alonzo ledger era.

### Implementation Plan

- [x] Alonzo ledger era parameters are deemed correct by working groups at IOG.

## Copyright

This CIP is licensed under [Apache-2.0](https://www.apache.org/licenses/LICENSE-2.0).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0029/README.md
---

- --
CIP: 29
Title: Phase-1 Monetary Scripts Serialization Formats
Status: Active
Category: Tools
Authors:
- Matthias Benkort <matthias.benkort@iohk.io>
Implementors:
- IOG
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/117
Created: 2020-08-17
License: CC-BY-4.0
- --

## Abstract

This specification describes how to serialize Phase-1 monetary scripts (a.k.a. _"native scripts"_) to various formats (JSON, CBOR) to facilitate inter-operability between applications.

## Motivation: why is this CIP necessary?

While the existence of scripts is well-known, and have an unambiguous on-chain representation. There's no agreed upon format for off-chain or higher-level interfaces for which a binary string is a poor fit. This CIP regroups both the on-chain binary format and other, more verbose, formats like JSON.

## Specification

This specification covers at present two serialization formats: JSON and CBOR. The CBOR matches exactly the on-chain representation and is extracted from the cardano-ledger-specs source code and put here as a convenient place to lookup while the source can change location over time.

### CBOR

The CBOR serialization format is given as a [CDDL specification in annexe](./phase-1-monetary-scripts.cddl). When a hash of the phase-1 monetary script is needed, it usually refers to a Blake2b-192 digest of the corresponding serialized script byte-string prefixed with a null byte: `\x00`.

### JSON

The JSON format is given as a [JSON schema in annexe](./phase-1-monetary-scripts.json). It is preferred in user interfaces such as command-lines or APIs, where some level of human inspection may be required.

### Notes

- Scripts may contain unbounded integers! Implementation parsing them, either from CBOR or JSON shall be prepared to handle possible big integers (>= 2^64).

### Test Vectors

```yaml
- json:
    { "type": "sig"
    , "keyHash": "00000000000000000000000000000000000000000000000000000000"
    }
  cbor:
    "8200581c00000000000000000000000000000000000000000000000000000000"

- json:
    { "type": "all"
    , "scripts":
      [ { "type": "sig"
        , "keyHash": "00000000000000000000000000000000000000000000000000000000"
        }
      , { "type": "any"
        , "scripts":
          [ { "type": "after"
            , "slot": 42
            }
          , { "type": "sig"
            , "keyHash": "00000000000000000000000000000000000000000000000000000001"
            }
          ]
        }
      ]
    }
  cbor:
    "8201828200581c000000000000000000000000000000000000000000000000000000008202828204182a8200581c00000000000000000000000000000000000000000000000000000001"

- json:
    { "type": "before"
    , "slot": 42
    }
  cbor:
    "8205182a"

- json:
    { "type": "atLeast"
    , "required": 2
    , "scripts":
      [ { "type": "sig", "keyHash": "00000000000000000000000000000000000000000000000000000000" }
      , { "type": "sig", "keyHash": "00000000000000000000000000000000000000000000000000000001" }
      , { "type": "sig", "keyHash": "00000000000000000000000000000000000000000000000000000002" }
      ]
    }
  cbor:
    "00830302838200581c000000000000000000000000000000000000000000000000000000008200581c000000000000000000000000000000000000000000000000000000018200581c00000000000000000000000000000000000000000000000000000002"
```

## Rationale: how does this CIP achieve its goals?

- The preimage for computing script hashes is prefixed with `\x00` to distinguish them from phase-2 monetary scripts (a.k.a Plutus Script) which are then prefixed with `\x01`. This is merely a discriminator tag.

- The current JSON format is based off the cardano-cli's format which has been used widely for minting tokens and is likely the most widely accepted format at the moment.

## Path to Active

### Acceptance Criteria

- [x] There exist official software releases supporting this serialization format:
- [x] [cardano-cli](https://github.com/IntersectMBO/cardano-cli)
- [x] [cardano-api](https://github.com/IntersectMBO/cardano-api)

### Implementation Plan

- [x] Incorporating this serialization format into Cardano software libraries and command line tools.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0030/README.md
---

- --
CIP: 30
Title: Cardano dApp-Wallet Web Bridge
Status: Active
Category: Wallets
Authors:
- rooooooooob
Implementors:
- Begin <https://begin.is/>
- Eternl <https://eternl.io/>
- Flint <https://flint-wallet.com/>
- GeroWallet <https://www.gerowallet.io/>
- Lace <https://www.lace.io/>
- Nami <https://namiwallet.io/>
- NuFi <https://nu.fi/>
- RayWallet <https://raywallet.io/>
- Yoroi <https://yoroi-wallet.com/>
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/88
- https://github.com/cardano-foundation/CIPs/pull/148
- https://github.com/cardano-foundation/CIPs/pull/151
- https://github.com/cardano-foundation/CIPs/pull/183
- https://github.com/cardano-foundation/CIPs/pull/208
- https://github.com/cardano-foundation/CIPs/pull/323
- https://github.com/cardano-foundation/CIPs/issues/169
- https://github.com/cardano-foundation/CIPs/issues/178
- https://github.com/cardano-foundation/CIPs/issues/204
- https://github.com/cardano-foundation/CIPs/issues/253
- https://github.com/cardano-foundation/CIPs/issues/386
- https://github.com/cardano-foundation/CIPs/issues/404
- https://github.com/cardano-foundation/CIPs/issues/419
- https://github.com/cardano-foundation/CIPs/pull/446
Created: 2021-04-29
License: CC-BY-4.0
- --

## Abstract

This documents describes a webpage-based communication bridge allowing webpages (i.e. dApps) to interface with Cardano wallets. This is done via injected javascript code into webpages. This specification defines the manner that such code is to be accessed by the webpage/dApp, as well as defining the API for dApps to communicate with the user's wallet. This document currently concerns the Shelley-Mary era but will have a second version once Plutus is supported. This specification is intended to cover similar use cases as web3 for Ethereum or [EIP-0012](https://github.com/ergoplatform/eips/pull/23) for Ergo. The design of this spec was based on the latter.

## Motivation: why is this CIP necessary?

In order to facilitate future dApp development, we will need a way for dApps to communicate with the user's wallet. While Cardano does not yet support smart contracts, there are still various use cases for this, such as NFT management. This will also lay the groundwork for an updated version of the spec once the Alonzo hardfork is released which can extend it to allow for Plutus support.

## Specification

### Data Types

#### Address

A string representing an address in either bech32 format, or hex-encoded bytes. All return types containing `Address` must return the hex-encoded bytes format, but must accept either format for inputs.

#### Bytes

A hex-encoded string of the corresponding bytes.

#### `cbor<T>`

A hex-encoded string representing [CBOR](https://tools.ietf.org/html/rfc7049) corresponding to `T` defined via [CDDL](https://tools.ietf.org/html/rfc8610) either inside of the [Shelley Multi-asset binary spec](https://github.com/input-output-hk/cardano-ledger-specs/blob/0738804155245062f05e2f355fadd1d16f04cd56/shelley-ma/shelley-ma-test/cddl-files/shelley-ma.cddl) or, if not present there, from the [CIP-0008 signing spec](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0008/README.md).
This representation was chosen when possible as it is consistent across the Cardano ecosystem and widely used by other tools, such as [cardano-serialization-lib](https://github.com/Emurgo/cardano-serialization-lib), which has support to encode every type in the binary spec as CBOR bytes.

#### DataSignature

```ts
type DataSignature = {|
  signature: cbor<COSE_Sign1>,
  key: cbor<COSE_Key>,
|};
```

#### TransactionUnspentOutput

If we have CBOR specified by the following CDDL referencing the Shelley-MA CDDL:

```cddl
transaction_unspent_output = [
  input: transaction_input,
  output: transaction_output,
]
```

then we define

```ts
type TransactionUnspentOutput = cbor<transaction_unspent_output>
```

This allows us to use the output for constructing new transactions using it as an output as the `transaction_output` in the Shelley Multi-asset CDDL does not contain enough information on its own to spend it.

#### Paginate

```ts
type Paginate = {|
  page: number,
  limit: number,
|};
```
Used to specify optional pagination for some API calls. Limits results to {limit} each page, and uses a 0-indexing {page} to refer to which of those pages of {limit} items each. dApps should be aware that if a wallet is modified between paginated calls that this will change the pagination, e.g. some results skipped or showing up multiple times but otherwise the wallet must respect the pagination order.

#### Extension

An extension is an object with a single field `"cip"` that describes a CIP number extending the API (as a plain integer, without padding). For example:

```ts
{ "cip": 30 }
```

### Error Types

#### APIError

```ts
APIErrorCode {
	InvalidRequest: -1,
	InternalError: -2,
	Refused: -3,
	AccountChange: -4,
}
APIError {
	code: APIErrorCode,
	info: string
}
```

- InvalidRequest - Inputs do not conform to this spec or are otherwise invalid.
- InternalError - An error occurred during execution of this API call.
- Refused - The request was refused due to lack of access - e.g. wallet disconnects.
- AccountChange - The account has changed. The dApp should call `wallet.enable()` to reestablish connection to the new account. The wallet should not ask for confirmation as the user was the one who initiated the account change in the first place.

#### DataSignError

```ts
DataSignErrorCode {
	ProofGeneration: 1,
	AddressNotPK: 2,
	UserDeclined: 3,
}
type DataSignError = {
	code: DataSignErrorCode,
	info: String
}
```

- ProofGeneration - Wallet could not sign the data (e.g. does not have the secret key associated with the address)
- AddressNotPK - Address was not a P2PK address and thus had no SK associated with it.
- UserDeclined - User declined to sign the data

#### PaginateError

```ts
type PaginateError = {|
    maxSize: number,
|};
```
{maxSize} is the maximum size for pagination and if the dApp tries to request pages outside of this boundary this error is thrown.

#### TxSendError

```ts
TxSendErrorCode = {
	Refused: 1,
	Failure: 2,
}
type TxSendError = {
	code: TxSendErrorCode,
	info: String
}
```

- Refused - Wallet refuses to send the tx (could be rate limiting)
- Failure - Wallet could not send the tx

#### TxSignError

```ts
TxSignErrorCode = {
	ProofGeneration: 1,
	UserDeclined: 2,
}
type TxSignError = {
	code: TxSignErrorCode,
	info: String
}
```

- ProofGeneration - User has accepted the transaction sign, but the wallet was unable to sign the transaction (e.g. not having some of the private keys)
- UserDeclined - User declined to sign the transaction

### Initial API

In order to initiate communication from webpages to a user's Cardano wallet, the wallet must provide the following javascript API to the webpage. A shared, namespaced `cardano` object must be injected into the page if it did not exist already. Each wallet implementing this standard must then create a field in this object with a name unique to each wallet containing a `wallet` object with the following methods. The API is split into two stages to maintain the user's privacy, as the user will have to consent to `cardano.{walletName}.enable()` in order for the dApp to read any information pertaining to the user's wallet with `{walletName}` corresponding to the wallet's namespaced name of its choice.

#### `cardano.{walletName}.enable({ extensions: Extension[] } = {}): Promise<API>`

Errors: `APIError`

This is the entrypoint to start communication with the user's wallet. The wallet should request the user's permission to connect the web page to the user's wallet, and if permission has been granted, the full API will be returned to the dApp to use. The wallet can choose to maintain a whitelist to not necessarily ask the user's permission every time access is requested, but this behavior is up to the wallet and should be transparent to web pages using this API. If a wallet is already connected this function should not request access a second time, and instead just return the `API` object.

Upon start, dApp can explicitly request a list of additional functionalities they expect as a list of CIP numbers capturing those extensions. This is used as an extensibility mechanism to document what functionalities can be provided by the wallet interface. CIP-0030 provides a set of base interfaces that every wallet must support. Then, new functionalities are introduced via additional CIPs and may be all or partially supported by wallets.

DApps are expected to use this endpoint to perform an initial handshake and ensure that the wallet supports all their required functionalities. Note that it's possible for two extensions to be mutually incompatible (because they provide two conflicting features). While we may try to avoid this as much as possible while designing CIPs, it is also the responsability of wallet providers to assess whether they can support a given combination of extensions, or not. Hence wallets aren't expected to fail should they not recognize or not support a particular combination of extensions. Instead, they should decide what they enable and reflect their choice in the response to `api.getExtensions()` in the Full API. As a result, dApps may fail and inform their users or may use a different, less-efficient, strategy to cope with a lack of functionality.

It is at the extension author's discretion if they wish to separate their endpoints from the base API via namespacing. Although, it is highly recommend that authors do namespace all of their extensions. If namespaced, endpoints must be preceded by `.cipXXXX.` from the `API` object, without any leading zeros.

For example; CIP-0123's endpoints should be accessed by:
```ts
api.cip123.endpoint1()
api.cip123.endpoint2()
```

For list of accepted CIP-30 extensions please see [CIP-30 Accepted Extensions Register](./extensions-register.md).

Authors should be careful when omitting namespacing. Omission should only be considered when creating endpoints to override those defined in this specification or other extensions. Even so when overriding; the new functionality should not prevent dApps from accessing past functionality thus overriding must ensure backwards compatibility.

Any namespace omission needs to be fully justified via the proposal's Rationale section, with explanation to why it is necessary. Any potential backwards compatibility considerations should be noted to give wallets and dApps a clear unambiguous direction.

##### Draft or Experimental Extensions

Extensions that are draft, in development, or prototyped should not use extension naming nor should they use official namspacing until assigned a CIP number. Draft extension authors are free to test their implementation endpoints by using the [Experimental API](#experimental-api). Once a CIP number is assigned implementors should move functionality out of the experimental API.

##### Can extensions depend on other extensions?

Yes. Extensions may have other extensions as pre-requisite. Some newer extensions may also invalidate functionality introduced by earlier extensions. There's no particular rule or constraints in that regards. Extensions are specified as CIP, and will define what it entails to enable them.

##### Should extensions follow a specific format?

Yes. They all are CIPs.

##### Can extensions add their own endpoints and/or error codes?

Yes. Extensions may introduce new endpoints or error codes, and modify existing ones. Although, it is recommended that endpoints are namespaced. Extensions may even change the rules outlined in this very proposal. The idea being that wallet providers should start off implementing this CIP, and then walk their way to implementing their chosen extensions.

##### Are wallet expected to implement all extensions?

No. It's up to wallet providers to decide which extensions they ought to support.


#### `cardano.{walletName}.isEnabled(): Promise<bool>`

Errors: `APIError`

Returns true if the dApp is already connected to the user's wallet, or if requesting access would return true without user confirmation (e.g. the dApp is whitelisted), and false otherwise. If this function returns true, then any subsequent calls to `wallet.enable()` during the current session should succeed and return the `API` object.

#### `cardano.{walletName}.apiVersion: String`

The version number of the API that the wallet supports. Set to `1`.

#### `cardano.{walletName}.supportedExtensions: Extension[]`

A list of extensions supported by the wallet. Extensions may be requested by dApps on initialization. Some extensions may be mutually conflicting and this list does not thereby reflect what extensions will be enabled by the wallet. Yet it informs on what extensions are known and can be requested by dApps if needed.

#### `cardano.{walletName}.name: String`

A name for the wallet which can be used inside of the dApp for the purpose of asking the user which wallet they would like to connect with.

#### `cardano.{walletName}.icon: String`

A URI image (e.g. data URI base64 or other) for img src for the wallet which can be used inside of the dApp for the purpose of asking the user which wallet they would like to connect with.

### Full API

Upon successful connection via `cardano.{walletName}.enable()`, a javascript object we will refer to as `API` (type) / `api` (instance) is returned to the dApp with the following methods. All read-only methods (all but the signing functionality) should not require any user interaction as the user has already consented to the dApp reading information about the wallet's state when they agreed to `cardano.{walletName}.enable()`. The remaining methods `api.signTx()` and `api.signData()` must request the user's consent in an informative way for each and every API call in order to maintain security.

The API chosen here is for the minimum API necessary for dApp <-> Wallet interactions without convenience functions that don't strictly need the wallet's state to work. The API here is for now also only designed for Shelley's Mary hardfork and thus has NFT support. When Alonzo is released with Plutus support this API will have to be extended.

#### `api.getExtensions(): Promise<Extension[]>`

Errors: `APIError`

Retrieves the list of extensions enabled by the wallet. This may be influenced by the set of extensions requested in the initial `enable` request.

#### `api.getNetworkId(): Promise<number>`

Errors: `APIError`

Returns the network id of the currently connected account. 0 is testnet and 1 is mainnet but other networks can possibly be returned by wallets. Those other network ID values are not governed by this document. This result will stay the same unless the connected account has changed.

#### `api.getUtxos(amount: cbor<value> = undefined, paginate: Paginate = undefined): Promise<TransactionUnspentOutput[] | null>`

Errors: `APIError`, `PaginateError`

If `amount` is `undefined`, this shall return a list of all UTXOs (unspent transaction outputs) controlled by the wallet. If `amount` is not `undefined`, this request shall be limited to just the UTXOs that are required to reach the combined ADA/multiasset value target specified in `amount`, and if this cannot be attained, `null` shall be returned. The results can be further paginated by `paginate` if it is not `undefined`.

#### (DEPRECATED) `api.getCollateral(params: { amount: cbor<Coin> }): Promise<TransactionUnspentOutput[] | null>`

Errors: `APIError`

##### DEPRECATION NOTICE

Since [CIP-0040 | Collateral Output](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0040/README.md) and Babbage era "Collateral Return" and "Total Collateral" fields on a transaction body are supported, a transaction can now use up to three **any** collateral inputs and specify the collateral return outputs to receive extra ada and tokens back as change. Transaction-building SDKs and libraries now all support this functionality as well (see [Blaze](https://blaze.butane.dev/), [Mesh](https://meshjs.dev/), [Lucid Evolution](https://anastasia-labs.github.io/lucid-evolution/), [CSL](https://github.com/Emurgo/cardano-serialization-lib)). The dedicated transaction fields and strong tooling support removes the **need** for dedicated cumbersome API for providing *special* collateral inputs to exist.

DApps should make effort to build transactions using the latest available best methods and practices. In this case it means always utilising collateral return and not risking burning UTxOs with no return. Wallets looking to implement this standard are free to **not** support this deprecated API function, which does require some extra special management of UTxOs within the wallet, beyond normal logic.

Deprecating this method aims to motivate DApp implementors not to use this method, but rather to use newer better techniques.

Any future versions of "wallet-to-dapp" APIs seeking to replace CIP30 should **not ever** include any methods of this kind. This mistake of history should be forgotten forever.

##### Description

The function takes a required object with parameters. With a single **required** parameter for now: `amount`. (**NOTE:** some wallets may be ignoring the amount parameter, in which case it might be possible to call the function without it, but this behavior is not recommended!). Reasons why the `amount` parameter is required:
1. Dapps must be motivated to understand what they are doing with the collateral, in case they decide to handle it manually.
2. Depending on the specific wallet implementation, requesting more collateral than necessarily might worsen the user experience with that dapp, requiring the wallet to make explicit wallet reorganisation when it is not necessary and can be avoided.
3. If dapps don't understand how much collateral they actually need to make their transactions work - they are placing more user funds than necessary in risk.

So requiring the `amount` parameter would be a by-spec behavior for a wallet. Not requiring it is possible, but not specified, so dapps should not rely on that and the behavior is not recommended.

This shall return a list of one or more UTXOs (unspent transaction outputs) controlled by the wallet that are required to reach **AT LEAST** the combined ADA value target specified in `amount` **AND** the best suitable to be used as collateral inputs for transactions with plutus script inputs (pure ADA-only utxos). If this cannot be attained, an error message with an explanation of the blocking problem shall be returned. **NOTE:** wallets are free to return utxos that add up to a **greater** total ADA value than requested in the `amount` parameter, but wallets must never return any result where utxos would sum up to a smaller total ADA value, instead in a case like that an error message must be returned.

The main point is to allow the wallet to encapsulate all the logic required to handle, maintain, and create (possibly on-demand) the UTXOs suitable for collateral inputs. For example, whenever attempting to create a plutus-input transaction the dapp might encounter a case when the set of all user UTXOs don't have any pure entries at all, which are required for the collateral, in which case the dapp itself is forced to try and handle the creation of the suitable entries by itself. If a wallet implements this function it allows the dapp to not care whether the suitable utxos exist among all utxos, or whether they have been stored in a separate address chain (see https://github.com/cardano-foundation/CIPs/pull/104), or whether they have to be created at the moment on-demand - the wallet guarantees that the dapp will receive enough utxos to cover the requested amount, or get an error in case it is technically impossible to get collateral in the wallet (e.g. user does not have enough ADA at all).

The `amount` parameter is required, specified as a `string` (BigNumber) or a `number`, and the maximum allowed value must be agreed to be something like 5 ADA. Not limiting the maximum possible value might force the wallet to attempt to purify an unreasonable amount of ADA just because the dapp is doing something weird. Since by protocol the required collateral amount is always a percentage of the transaction fee, it seems that the 5 ADA limit should be enough for the foreseeable future.

#### `api.getBalance(): Promise<cbor<value>>`

Errors: `APIError`

Returns the total balance available of the wallet. This is the same as summing the results of `api.getUtxos()`, but it is both useful to dApps and likely already maintained by the implementing wallet in a more efficient manner so it has been included in the API as well.

#### `api.getUsedAddresses(paginate: Paginate = undefined): Promise<Address[]>`

Errors: `APIError`

Returns a list of all used (included in some on-chain transaction) addresses controlled by the wallet. The results can be further paginated by `paginate` if it is not `undefined`.

#### `api.getUnusedAddresses(): Promise<Address[]>`

Errors: `APIError`

Returns a list of unused addresses controlled by the wallet.

#### `api.getChangeAddress(): Promise<Address>`

Errors: `APIError`

Returns an address owned by the wallet that should be used as a change address to return leftover assets during transaction creation back to the connected wallet. This can be used as a generic receive address as well.

#### `api.getRewardAddresses(): Promise<Address[]>`

Errors: `APIError`

Returns the reward addresses owned by the wallet. This can return multiple addresses e.g. CIP-0018.

#### `api.signTx(tx: cbor<transaction>, partialSign: bool = false): Promise<cbor<transaction_witness_set>>`

Errors: `APIError`, `TxSignError`

Requests that a user sign the unsigned portions of the supplied transaction. The wallet should ask the user for permission, and if given, try to sign the supplied body and return a signed transaction. If `partialSign` is true, the wallet only tries to sign what it can. If `partialSign` is false and the wallet could not sign the entire transaction, `TxSignError` shall be returned with the `ProofGeneration` code. Likewise if the user declined in either case it shall return the `UserDeclined` code. Only the portions of the witness set that were signed as a result of this call are returned to encourage dApps to verify the contents returned by this endpoint while building the final transaction.

#### `api.signData(addr: Address, payload: Bytes): Promise<DataSignature>`

Errors: `APIError`, `DataSignError`

This endpoint utilizes the [CIP-0008 signing spec](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0008/README.md) for standardization/safety reasons. It allows the dApp to request the user to sign a payload conforming to said spec. The user's consent should be requested and the message to sign shown to the user. The payment key from `addr` will be used for base, enterprise and pointer addresses to determine the EdDSA25519 key used. The staking key will be used for reward addresses. This key will be used to sign the `COSE_Sign1`'s `Sig_structure` with the following headers set:

- `alg` (1) - must be set to `EdDSA` (-8)
- `kid` (4) - Optional, if present must be set to the same value as in the `COSE_key` specified below. It is recommended to be set to the same value as in the `"address"` header.
- `"address"` - must be set to the raw binary bytes of the address as per the binary spec, without the CBOR binary wrapper tag

The payload is not hashed and no `external_aad` is used.

If the payment key for `addr` is not a P2Pk address then `DataSignError` will be returned with code `AddressNotPK`. `ProofGeneration` shall be returned if the wallet cannot generate a signature (i.e. the wallet does not own the requested payment private key), and `UserDeclined` will be returned if the user refuses the request. The return shall be a `DataSignature` with `signature` set to the hex-encoded CBOR bytes of the `COSE_Sign1` object specified above and `key` shall be the hex-encoded CBOR bytes of a `COSE_Key` structure with the following headers set:

- `kty` (1) - must be set to `OKP` (1)
- `kid` (2) - Optional, if present must be set to the same value as in the `COSE_Sign1` specified above.
- `alg` (3) - must be set to `EdDSA` (-8)
- `crv` (-1) - must be set to `Ed25519` (6)
- `x` (-2) - must be set to the public key bytes of the key used to sign the `Sig_structure`

#### `api.submitTx(tx: cbor<transaction>): Promise<hash32>`

Errors: `APIError`, `TxSendError`

As wallets should already have this ability, we allow dApps to request that a transaction be sent through it. If the wallet accepts the transaction and tries to send it, it shall return the transaction id for the dApp to track. The wallet is free to return the `TxSendError` with code `Refused` if they do not wish to send it, or `Failure` if there was an error in sending it (e.g. preliminary checks failed on signatures).

### Experimental API

Multiple experimental namespaces are used:
- under `api` (ex: `api.experimental.myFunctionality`).
- under `cardano.{walletName}` (ex: `window.cardano.{walletName}.experimental.myFunctionality`)

The benefits of this are:
1. Wallets can add non-standardized features while still following the CIP30 structure
1. dApp developers can use these functions explicitly knowing they are experimental (not stable or standardized)
1. New features can be added to CIP30 as experimental features and only moved to non-experimental once multiple wallets implement it
1. It provides a clear path to updating the CIP version number (when functions move from experimental -> stable)

## Rationale: how does this CIP achieve its goals?

See justification and explanations provided with each API endpoint.

### Extensions

Extensions provide an extensibility mechanism and a way to negotiate (possibly conflicting) functionality between a DApp and a wallet provider. There's rules enforced as for what extensions a wallet decide to support or enable. The current mechanism only gives a way for wallets to communicate their choice back to a DApp.

We use object as extensions for now to leave room for adding fields in the future without breaking all existing interfaces. At this point in time however, objects are expected to be singleton.

Extensions can be seen as a smart versioning scheme. Except that, instead of being a monotonically increasing sequence of numbers, they are multi-dimensional feature set that can be toggled on and off at will. This is a versioning "à-la-carte" which is useful in a context where:

1. There are multiple concurrent standardization efforts on different fronts to accommodate a rapidly evolving ecosystem;
2. Not everyone agrees and has desired to support every existing standard;
3. There's a need from an API consumer standpoint to clearly identify what features are supported by providers.

#### Namespacing Extensions

By encouraging the explicit namespacing of each extension we aim to improve the usability of extensions for dApps. By allowing special cases where namespacing can be dropped we maintain good flexibility in extension design.

## Path to Active

### Acceptance Criteria

- [x] The interface is implemented and supported by various wallet providers. See also: [cardano-caniuse](https://www.cardano-caniuse.io/).
- [x] The interface is used by DApps to interact with wallet providers. Few examples:
- https://www.jpg.store/
- https://app.minswap.org/
- https://muesliswap.com/
- https://exchange.sundaeswap.finance/
- https://app.indigoprotocol.io/

### Implementation Plan

- [x] Provide some reference implementation of wallet providers
- [Berry-Pool/nami-wallet](https://github.com/berry-pool/nami/blob/4d7539b2768464480a9cff53a2d66af9879f8534/src/pages/Content/injected.js)
- [Emurgo/yoroi-wallet](https://github.com/Emurgo/yoroi-frontend/blob/f4eabb25eedd564821514059479835601f8073ab/packages/yoroi-connector/example-cardano/index.js)

- [x] Provide some reference implementation of the dapp connector
- [cardano-foundation/connect-with-wallet](https://github.com/cardano-foundation/cardano-connect-with-wallet)

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0031/README.md
---

- --
CIP: 31
Title: Reference inputs
Status: Active
Category: Plutus
Authors:
- Michael Peyton Jones <michael.peyton-jones@iohk.io>
Implementors:
- Michael Peyton Jones <michael.peyton-jones@iohk.io>
- Jared Corduan <jared.corduan@iohk.io>
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/159
Created: 2021-11-29
License: CC-BY-4.0
- --

## Abstract

We introduce a new kind of input, a _reference_ input, which allows looking at an output without spending it.
This will facilitate access to information stored on the blockchain without the churn associated with spending and recreating UTXOs.

## Motivation: why is this CIP necessary?

Datums in transaction outputs provide a way to store and access information on the blockchain.
However, they are quite constrained in a number of ways.
Most notably, in order to access the information which is contained in them, you have to _spend_ the output that the datum is attached to.

This has a number of undesirable features:
1. Even if the output is recreated after being spent, it is still a _new_ output. Any other user who wishes to look at the data cannot spend the old output (which is gone), but must rather spend the new output (which they will not know about until the next block). In practice this throttles some applications to one "operation" per block.
2. Looking at the _information_ in the datum requires _spending_ the output, which means that care must be taken over the distribution of any funds in the output (possibly requiring scripts), and spending conditions must be met. This is overly stringent, inconvenient, and expensive.

We would like to have a mechanism for accessing the information in datums that avoids these issues.

### Use cases

Here are some cases where we expect this to be helpful.

1. Inspecting the state (datum, or locked value) of an on-chain application without having to consume the output, e.g. checking the current state of a stablecoin state machine.
2. On-chain data providers that store data in outputs, to be referenced by other scripts.
3. (With inline datums) Creating a single UTXO with data to be used in multiple subsequent transactions, but only paying the cost for submitting it once.

### Enabling further improvements

This proposal was designed in tandem with two further proposals which make use of reference scripts, CIP-32 (Inline datums) and CIP-33 (Reference scripts).
A full assessment of the worth of this proposal should take into account the fact that it is an enabler for these other proposals.

## Specification

### Reference inputs

We introduce a new kind of transaction input, a reference input.
Transaction authors specify inputs as either normal (spending) inputs or reference inputs.

A reference input is a transaction input, which is linked to a particular transaction output as normal, except that it _references_ the output rather than _spending_ it. That is:

- A referenced output must exist in the UTXO set.
- Any value on a referenced output is _not_ considered when balancing the transaction.
- The spending conditions on referenced outputs are _not_ checked, nor are the witnesses required to be present.
- i.e. validators are not required to pass (nor are the scripts themselves or redeemers required to be present at all), and signatures are not required for pubkey outputs.
- Referenced outputs are _not_ removed from the UTXO set if the transaction validates.
- Reference inputs _are_ visible to scripts.

For clarity, the following two behaviours which are present today are unchanged by this proposal:

1. Transactions must _spend_ at least one output.[^1]
2. Spending an output _does_ require the spending conditions to be checked.[^2]

[^1]: This restriction already exists, and is important. It seems unnecessary, since transactions must always pay fees and fees must come from somewhere, but fees could in principle be paid via reward withdrawals, so the requirement to spend a UTXO is relevant.
[^2]: That is, this proposal does not change outputs or the spending of outputs, it instead adds a new way of _referring_ to outputs.

### Script context

Scripts are passed information about transactions via the script context.
The script context therefore needs to be augmented to contain information about reference inputs.

Changing the script context will require a new Plutus language version in the ledger to support the new interface.
The change in the new interface is: a _new_ field is added to the structure which contains the list of reference inputs.

The interface for old versions of the language will not be changed.
Scripts with old versions cannot be spent in transactions that include reference inputs, attempting to do so will be a phase 1 transaction validation failure.

### Extra datums

Today, transactions can contain extra datums that are not required for validation.
Currently the extra datums must all be pre-images of datum hashes that appear in outputs.
We change this so that the extra datums can also be pre-images of datum hashes that appear in reference inputs.[^3]

Note that the existing mechanism includes the hash of the extra datum structure in the transaction body, so that additional datums cannot be stripped by an attacker without voiding transaction signatures.

[^3]: Pre-images of datum hashes that appear in _spent_ inputs are of course mandatory.

### CDDL

The CDDL for transaction bodies will change to the following to reflect the new field.
```
transaction_body =
 { 0 : set<transaction_input>    ; inputs
 ...
 , ? 16 : set<transaction_input> ; reference inputs
 }
```

## Rationale: how does this CIP achieve its goals?

The key idea of this proposal is to use UTXOs to carry information.
But UTXOs are currently a bad fit for distributing information.
Because of locality, we have to include outputs that we use in the transaction, and the only way we have of doing that is to _spend_ them - and a spent output cannot then be referenced by anything else.
To put it another way: outputs are resource-like, but information is not resource-like.

The solution is to add a way to _inspect_ ("reference") outputs without spending them.
This allows outputs to play double duty as resource containers (for the value they carry) and information containers (for the data they carry).

### Requirements

We have a number of requirements that we need to fulfil.
- Determinism
- It must be possible to predict the execution of scripts precisely, given the transaction.
- Locality
- All data involved in transaction validation should be included in the transaction or the outputs which it spends (or references).
- Non-interference
- As far as possible, transactions should not interfere with others. The key exception is when transactions consume resources that other transactions want (usually by consuming UTXO entries).
- Replay protection
- The system should not be attackable (e.g. allow unexpected data reads) by replaying old traffic.
- Storage control and garbage-collection incentives
- The amount of storage required by the system should have controls that prevent it from overloading nodes, and ideally should have incentives to shrink the amount of storage that is used over time.
- Optimized storage
- The system should be amenable to optimized storage solutions.
- Data transfer into scripts
- Scripts must have a way to observe the data.
- Tool and library support
- It should not be too difficult for tools and libraries to work with the new system.
- Hydra
- The system should be usable without compromising the needs of Hydra, such as parallel processing of independent parts of the transaction graph.

Reference inputs satisfy these requirements, mostly by inheriting them from the corresponding properties of UTXOs.

- Referencing an output still requires the output to be presented as part of the transaction and be unspent, so determinism is preserved.
- Referenced outputs appear as transaction inputs, so locality is preserved.
- Outputs can be referenced multiple times, so we have non-interference even under heavy usage of referencing. Conflicts can occur only if the output is actually spent.
- Replay protection is inherited because a transaction must always spend at least one UTXO, and UTXOs can only be spent once.
- Storage control is inherited from control of the UTXO set, via the min UTXO value incentives. Data can be "retired" by spending the input (rather than referencing it), reclaiming the min UTXO value.
- Optimized storage is inherited directly from work on improving UTXO storage.
- Data transfer into scripts works automatically since scripts can see transaction inputs.
- Tools and libraries will only have to deal with a new kind of transaction input, and unless they care about scripts they can likely ignore them entirely.
- Hydra is able to work with reference inputs (see below).

### Why UTXOs?

There are various approaches to augmenting Cardano with the ability to store and retrieve data more easily.
All of these require some kind of resource-like system for the _storage_ that is used for data.
The main argument in favour of the reference input approach is that it reuses our existing resource-like system: UTXOs.

As we've seen above, this allows us to satisfy most of our requirements for free.
Any other approach would need new solutions to these problems.

As a brief example, suppose we wanted to instead implement data storage as an on-chain, hash-indexed data store (a proposal we considered).

Then we would need to answer a lot of questions:
- How is the data accessed and retrieved? Are these stateful operations? Does this make transaction ordering more significant, threatening determinism?
- How is the storage usage controlled? Do people pay for storing data? Do they pay for a fixed period, or perpetually? How is data retired?
- How is the storage going to be implemented? How will this affect node memory usage?
- How are scripts going to access the data in the store?
- How will tools interact with and visualize the store and changes to it?

### How should we present the information to scripts?

Scripts definitely need to see reference inputs.
But we have at least two options for how we represent this in the script context: put the reference inputs in their own field; or include them with the other inputs, but tag them appropriately.

Keeping them separate seems wise given the potential for confusing reference inputs for normal inputs.
That would be quite a dangerous programming error, as it might lead a script to believe that e.g. an output had been spent when in fact it had only been referenced.

We also have the question of what to do about old scripts.
We can't really present the information about reference inputs to them in a faithful way: representing them as spending inputs would be wildly misleading, and there is nowhere else to put them.
We could omit the information entirely, but this is dangerous in a different way.
Omitting information may lead scripts to make assumptions about the transaction that are untrue; for this reason we prefer not to silently omit information as a general principle.
That leaves us only one option: reject transactions where we would have to present information about reference inputs to old scripts.

### Accessing the datums of reference inputs

Currently this proposal has somewhat limited utility, because datums in outputs are just hashes, and the spending party is required to find and provide the preimage themselves.
Therefore the CIP-32 proposal for inline datums is complementary, as it allow UTXOs to carry the data itself rather than a hash.

In the mean time (or if CIP-32 is not implemented), we still want users to be _able_ to provide the datums corresponding to datum hashes in reference outputs, so that at least it is possible to reference the data, albeit clumsily.
The easiest solution here is to reuse the existing mechanism for providing pre-images of datum hashes which appear in outputs.

Providing datum hash pre-images remains optional, since there are reasons to use reference inputs even without looking at the datum, e.g. to look at the value in an output.

### Accessing the value locked in outputs

The motivation of this proposal mainly requires looking at the _datums_ of outputs.
But reference inputs allow us to do more: they let us look at the _value_ locked in an output (i.e. how much Ada or other tokens it contains).

This is actually a very important feature.
Since anyone can lock an output with any address, addresses are not that useful for identifying _particular_ outputs on chain, and instead we usually rely on looking for particular tokens in the value locked by the output.
Hence, if a script is interested in referring to the data attached to a _particular_ output, it will likely want to look at the value that is locked in the output.

For example, an oracle provider would need to distinguish the outputs that they create (with good data) from outputs created by adversaries (with bad data).
They can do this with a token, so long as scripts can then see the token!

### Hydra

We want reference inputs to be usable inside Hydra heads.
This raises a worry: since Hydra processes independent parts of the transaction graph in parallel, what happens if it accepts one transaction that references an output, and one that spends the output, but then the first transaction gets put into a checkpoint before the second?

Fortunately, Hydra already has to deal with double-spend conflicts of this kind (although in the naive protocol this results in a decommit).
This proposal simply introduces a new kind of conflict, a reference-spend conflict.
Reference-spend conflicts should be handled by the same mechanism that is used to handle double-spend conflicts.

### Controlling referencing

One thing that a user might want to do is to control who can reference an output.
For example, an oracle provider might want to only allow a transaction to reference a particular output if the transaction also pays them some money.

Reference inputs alone do _not_ provide any way to do this.
Another mechanism would be required, but there is no consensus on what the design should be, so it is currently out of scope for this proposal.
A brief summary of a few options and reasons why they are not obvious choices is included below.

A key issue is that the choice to control referencing must lie with the _creator_ of the output, not the _spender_.
Therefore we _must_ include some kind of change to outputs so that the creator can record their requirements.

#### Check inputs

A "check input" is like a reference input except that the spending conditions _are_ checked.
That is, it acts as proof that you _could_ spend the input, but does not in fact spend it.

Since check inputs cause validator scripts to be run, it seems like they could allow us to control referencing.
There are two wrinkles:

- The same script would be used for both referencing and spending, overloading the meaning of the validator script. This is still _usable_, however, since the redeemer could be used to indicate which action is being taken.
- We would need a flag on outputs to say "this output cannot be referenced, but only checked". Exactly what this should look like is an open question, perhaps it should be generic enough to control all the possible ways in which an output might be used (of which there would be three).

#### Referencing conditions

"Referencing conditions" would mean adding a new field to outputs to indicate under what conditions the output may be referenced.
This could potentially be an entire additional address, since the conditions might be any of the normal spending conditions (public key or script witnessing).

However, this would make outputs substantially bigger and more complicated.

### Related work

Reference inputs are very similar to Ergo's "data inputs".
We chose to name them differently since "data" is already a widely used term with risk for confusion.
We might also want to introduce other "verb" inputs in future.

## Path to Active

### Acceptance Criteria

- [x] Fully implemented in Cardano as of the Vasil protocol upgrade.

### Implementation Plan

- [x] Passes all requirements of both Plutus and Ledger teams as agreed to improve Plutus script efficiency and usability.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0032/README.md
---

- --
CIP: 32
Title: Inline datums
Status: Active
Category: Plutus
Authors:
- Michael Peyton Jones <michael.peyton-jones@iohk.io>
Implementors:
- Michael Peyton Jones <michael.peyton-jones@iohk.io>
- Jared Corduan <jared.corduan@iohk.io>
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/160
Created: 2021-11-29
License: CC-BY-4.0
- --

## Abstract

We propose to allow datums themselves to be attached to outputs instead of datum hashes.
This will allow much simpler communication of datum values between users.

## Motivation: why is this CIP necessary?

Conceptually, datums are pieces of data that are attached to outputs.
However, in practice datums are implemented by attaching _hashes_ of datums to outputs, and requiring that the spending transaction provides the actual datum.

This is quite inconvenient for users.
Datums tend to represent the result of computation done by the party who creates the output, and as such there is almost no chance that the spending party will know the datum without communicating with the creating party.
That means that either the datum must be communicated between parties off-chain, or communicated on-chain by including it in the witness map of the transaction that creates the output ("extra datums").
This is also inconvenient for the spending party, who must watch the whole chain to spot it.

It would be much more convenient to just put the _datum itself_ in an output, which is what we propose.

### Use cases

We expect that, provided we are able to bring the cost low enough, a large proportion of dapp developers will make use of this feature, as it will simplify their systems substantially.

## Specification

Transaction outputs are changed so that the datum field can contain either a hash or a datum (an "inline datum").

The min UTXO value for an output with an inline datum depends on the size of the datum, following the `coinsPerUTxOWord` protocol parameter.

When an output with an inline datum is spent, the spending transaction does not need to provide the datum itself.

### Script context

Scripts are passed information about transactions via the script context.
The script context therefore needs to be augmented to contain information about inline datums.

Changing the script context will require a new Plutus language version in the ledger to support the new interface.

There are two changes in the new version of the interface:
- The datum field on transaction outputs can either be a hash or the actual datum.
- The datum field on transaction inputs can either be a hash or the actual datum.

The interface for old versions of the language will not be changed.
Scripts with old versions cannot be spent in transactions that include inline datums, attempting to do so will be a phase 1 transaction validation failure.

### CDDL

The CDDL for transaction outputs will change as follows to reflect the new alternative.
```
transaction_output =
  [ address
  , amount : value
  , ? datum : $hash32 / plutus_data
  ]
```
TODO: should there be a dedicated production for datum-hash-or-datum? Does it need to be tagged?

## Rationale: how does this CIP achieve its goals?

The key idea of this proposal is simply to restore the conceptually straightforward situation where datums are attached to outputs.
Historically, this was the way that the EUTXO model was designed, and switching to datum hashes on outputs was done to avoid bloating UTXO entries, which at that time (pre-multiasset) were constant-size (see [^1] page 7).

Now that we have variable-sized UTXO entries and the accounting to support them, we can restore inline datums.

Since inline datums change very little about the model apart from where data is stored, we don't need to worry about violating any of the other requirements of the ledger, but we do need to worry about the effect on the size of the UTXO set.

### UTXO set size

This proposal gives users a way to put much larger amounts of data into the UTXO set.
Won’t this lead to much worse UTXO set bloat?

The answer is that we already have a mechanism to discourage this, namely the minimum UTXO value.
If inline datums turns out to drive significantly increased space usage, then we may need to increase `coinsPerUTxOWord` in order to keep the UTXO size down.
That will be costly and inconvenient for users, but will still allow them to use inline datums where they are most useful and the cost is bearable.
Furthermore, we hope that we will in fact be able to _reduce_ `coinsPerUTxOWord` when the upcoming work on moving the UTXO mostly to on-disk storage is complete.

Another guard rail would be to enforce upper limits on the size of inline datums.
At the extreme, we could bound them to the size of a hash, which would guarantee no more space usage than today.
However, this is much worse for users, since it introduces a sharp discontinuity where an inline datum is entirely acceptable, until it crosses the size threshold at which point it is unacceptable, and there is no way to avoid this.
Generally we prefer to avoid such discontinuities in favour of gradually increasing costs.

In practice, what is implemented here may depend on whether the UTXO-on-disk work is completed by the time that this proposal is implemented.

### Other modes of specifying datums

We could deprecate the other methods of specifying datums (datum hashes or datum hashes+extra datums).
However, the other approaches also have some advantages.

- Transmission costs: creator pays versus consumer pays
- Inline datums: creator pays
- Datum hashes: consumer pays
- Datum hashes+extra datums: both pay
- Min UTXO value costs
- Inline datums: depends on data size
- Datum hashes: fixed cost
- Datum hashes+extra datums: fixed cost
- Privacy
- Inline datums: datum is immediately public
- Datum hashes: datum is not public until consumed
- Datum hashes+extra datums: datum is immediately public, but only to chain-followers
- Communication of datums
- Inline datums: easy on-chain communication
- Datum hashes: off-chain communication necessary
- Datum hashes+extra datums: complicated on-chain communication

Any one of these factors could be important to particular use cases, so it is good to retain the other options.

### The script context

#### Including information about inline datums

In principle we do not need to let scripts see whether a datum is inline or not.
We could pretend that inline datums are non-inline and insert them into the datum witness map.

The underlying question is: do we want scripts to be able to make assertions about whether datums are inline or not?
There are reasons to want to do this: _not_ using inline datums causes communication issues for users, and so it is quite reasonable that an application developer may want to be able to enforce their use.

Furthermore, as a general principle we try to keep the script context as faithful to real transactions as possible.
Even if the use for the information is not immediately obvious, we try to err on the side of providing it and letting users decide.

Hence we _do_ include information about inline datums in the script context.

#### Representation and backwards compatibility

There are a couple of options for how to change the representation of the script context to include the new information, and whether to make a backwards compatibility effort for old language versions.

For the new script context:

1. Match the ledger representation as much as possible: change the fields on inputs and outputs to be either a datum hash or a datum.
2. Try to only have one way to look up datums: put inline datums in the datum witness map and insert their hashes into the corresponding inputs and outputs; optionally add a boolean to inputs and outputs to indicate whether the datum was originally inline.

For backwards compatibility:

1. Don't try to represent inline datums for scripts using old language versions: old scripts simply can't be run in transactions that use inline datums (since we can't represent the information).
2. Rewrite inline datums as non-inline datums: put inline datums in the datum witness map and insert their hashes into the corresponding inputs and outputs.

For the new script context, option 1 has the significant advantage of matching the ledger representation of transactions.
This makes it easier to implement, and also avoids conceptual overhead for users who would have to distinguish the two ways of representing transactions.
While the conceptual distance here may be small, if we let it grow over time then it may become quite confusing.

We then have the choice of what to do about backwards compatibility.
Option 2 would work, but is more complicated for the ledger and is inconsistent in representation with our choice for the new script context (inline datums are sometimes represented faithfully, and sometimes put in the witness map).
Option 1 is simple, but doesn't allow old scripts to work with inline datums.
This would not be so bad if it just meant that old scripts could not be spent in transactions that include inline datums, but it also introduces a new way to make an unspendable output.
If a user creates an output with an old script and an inline datum, then any transaction spending that output will include an inline datum, which we would not allow.

Unfortunately, we cannot prevent users from creating such outputs in general, since script addresses do not include the script language, so the ledger cannot tell whether the inline datum is permissible or not.
Client code typically _will_ be able to do this, since it will usually know the script.

The mitigating factor is that we expect this to be uncommon in practice, particularly since we expect that most users will move to the new version relatively quickly, since we expect support for inline datums to be desirable, and released alongside other desirable features.

Hence we choose both option 1s and do _not_ provide backwards compatibility for old language versions.

## Path to Active

### Acceptance Criteria

- [x] Fully implemented in Cardano as of the Vasil protocol upgrade.

### Implementation Plan

- [x] Passes all requirements of both Plutus and Ledger teams as agreed to improve Plutus script efficiency and usability.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

[^1]: Chakravarty, Manuel M. T. et al., "The extended UTXO model"

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0033/README.md
---

- --
CIP: 33
Title: Reference scripts
Status: Active
Category: Plutus
Authors:
- Michael Peyton Jones <michael.peyton-jones@iohk.io>
Implementors:
- Michael Peyton Jones <michael.peyton-jones@iohk.io>
- Jared Corduan <jared.corduan@iohk.io>
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/161
- https://github.com/cardano-foundation/CIPs/pull/213
Created: 2021-11-29
License: CC-BY-4.0
- --

## Abstract

We propose to allow scripts ("reference scripts") to be attached to outputs, and to allow reference scripts to be used to satisfy script requirements during validation, rather than requiring the spending transaction to do so.
This will allow transactions using common scripts to be much smaller.

## Motivation: why is this CIP necessary?

Script sizes pose a significant problem. This manifests itself in two ways:
1. Every time a script is used, the transaction which caused the usage must supply the whole script as part of the transaction. This bloats the chain, and passes on the cost of that bloat to users in the form of transaction size fees.
2. Transaction size limits are problematic for users. Even if individual scripts do not hit the limits, a transaction which uses multiple scripts has a proportionally greater risk of hitting the limit.

We would like to alleviate these problems.

The key idea is to use reference inputs and modified outputs which carry actual scripts ("reference scripts"), and allow such reference scripts to satisfy the script witnessing requirement for a transaction.
This means that the transaction which _uses_ the script will not need to provide it at all, so long as it referenced an output which contained the script.

## Specification

We extend transaction outputs with a new optional field, which contains a script (a "reference script").

The min UTXO value for an output with an additional script field depends on the size of the script, following the `coinsPerUTxOWord` protocol parameter.

When we are validating a transaction and we look for the script corresponding to a script hash, in addition to the scripts provided in the transaction witnesses, we also consider any reference scripts from the outputs referred to by the inputs of the transaction.

### Script context

Scripts are passed information about transactions via the script context.
We propose to augment the script context to include some information about reference scripts.

Changing the script context will require a new Plutus language version in the ledger to support the new interface.
The change is: a new optional field is added to outputs and inputs to represent reference scripts.
Reference scripts are represented by their hash in the script context.

The interface for old versions of the language will not be changed.
Scripts with old versions cannot be spent in transactions that include reference scripts, attempting to do so will be a phase 1 transaction validation failure.

### CDDL

The CDDL for transaction outputs will change as follows to reflect the new field.
```
transaction_output =
  [ address
  , amount : value
  , ? datum : $hash32
  , ? ref_script : plutus_script
  ]
```
TODO: can we use a more generic type that allows _any_ script in a forwards-compatible way?

## Rationale: how does this CIP achieve its goals?

The key idea of this proposal is stop sending frequently-used scripts to the chain every time they are used, but rather make them available in a persistent way on-chain.

The implementation approach follows in the wake of CIP-31 (Reference inputs) and CIP-32 (Inline datums).
The former considers how to do data sharing on chain, and concludes that referencing UTXOs is a good solution.
The latter shows how we can safely store substantial data in UTXOs by taking advantage of existing mechanisms for size control.

It is therefore natural to use the same approach for scripts: put them in UTXOs, and reference them using reference inputs.

### Storing scripts in outputs

There are a few possible alternatives for where to store reference scripts in outputs.

#### 1: The address field

In principle, we could add an "inline scripts" extension that allowed scripts themselves to be used in the address field instead of script hashes.
We could then use such scripts as reference scripts.

However, this approach suffers from a major confusion about the functional role of the script.
You would only be able to provide a reference script that _also_ controlled the spending of the output.
This is clearly not what you want: the reference script could be anything, perhaps a script only designed for use in quite specific circumstances; whereas in many cases the user will likely want to retain control over the output with a simple public key.

#### 2: The datum field

With inline datums, we could put reference scripts in the datum field of outputs.

This approach has two problems.
First, there is a representation confusion: we would need some way to know that a particular datum contained a reference script.
We could do this implicitly, but it would be better to have an explicit marker.

Secondly, this prevents having an output which is locked by a script that needs a datum _and_ has a reference script in it.
While this is a more unusual situation, it's not out of the question.
For example, a group of users might want to use a Plutus-based multisig script to control the UTXO with a reference script in it.

#### 3: A new field

A new field is the simplest solution: it avoids these problems because the new field clearly has one specific purpose, and we do not overload the meanings of the other fields.

### UTXO set size

This proposal gives people a clear incentive to put large amounts (i.e. kilobytes) of data in outputs as reference scripts.

This is essentially the same problem which is faced in CIP-32, and we can take the same stance.
We don't want to bloat the UTXO set unnecessarily, but we already have mechanisms for limiting that (in the form of the min UTXO value), and these should work transparently for reference scripts as they will for inline datums.

### Changing the script context

We don't strictly need to change the script context.
We could simply omit any information about reference scripts carried by outputs.
This would mean that we don't need to change the interface.

We don't have obvious use cases for the information about reference scripts, but the community may come up with use cases, and our general policy is to try and include as much information about the transaction as we can, unless there is a good reason not to.

We also have the question of what to do about old scripts.
We can't really present the information about reference scripts to them in a faithful way, there is nowhere to put the information.
We could omit the information entirely, but this is dangerous in a different way.
Omitting information may lead scripts to make assumptions about the transaction that are untrue; for this reason we prefer not to silently omit information as a general principle.
That leaves us only one option: reject transactions where we would have to present information about reference scripts to old scripts.

## Path to Active

### Acceptance Criteria

- [x] Fully implemented in Cardano as of the Vasil protocol upgrade.

### Implementation Plan

- [x] Passes all requirements of both Plutus and Ledger teams as agreed to improve Plutus script efficiency and usability.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0034/README.md
---

- --
CIP: 34
Title: Chain ID Registry
Status: Proposed
Category: Tools
Authors:
- Sebastien Guillemot <seba@dcspark.io>
Implementors: []
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/158
- https://github.com/cardano-foundation/CIPs/pull/332
- https://github.com/cardano-foundation/CIPs/pull/412
Created: 2021-11-24
License: CC-BY-4.0
- --

## Abstract

Currently Cardano has two easily-usable networks: "mainnet" and "testnet". However, in the future, we expect more networks to exist and so we need some way to refer to these networks to be able to write better multi-network applications and systems.

## Motivation: why is this CIP necessary?

Cardano currently has three ways to refer to a network:
- The "network ID" included in every address and also optionally present in the transaction body. This only stores 16 possibilities (4 bits)
- The "network magic" used for Byron addresses and in the handshake with other nodes in the network layer. This is a random 32-bit number
- The genesis block hash (a 28-byte number)

Blockchains can have multiple deployments of the same codebase. For example:

1. Test networks where the base asset has no value (so devs can test at no cost)
1. Test networks where the protocol is simplified for ease of testing (ex: Cardano but with a Proof of Authority consensus for stable block production in testing)
1. Test networks for new features
1. Plutus Application Backend (PAB) testnet
2. Shelley incentivized testnet
1. Forks that diverge in feature set (the many forks of Bitcoin and Ethereum)

dApps may be deployed on specific testnets that match their criteria and wallets need to know about these networks to know how to behave.

Additionally, having a standardized registry for networks allows easy integration into the broader crypto ecosystem via standards like [CAIP-2](https://github.com/ChainAgnostic/CAIPs/blob/master/CAIPs/caip-2.md)

## Specification

We create a machine-readable registry of networks

All entries in this registry should have the following entries:

- User-friendly name
- Network ID
- Network magic
- Genesis hash

When representing these networks in a human-readable string, the following format shall be used:

```
cip34:NetworkId-NetworkMagic
```

# Rationale: how does this CIP achieve its goals?

We pick this format for the following reason:
- The network ID is too small to be used by itself. You can see from [chainlist](https://chainlist.org/) that 16 possibilities is too few
- The genesis hash is too long and user-unfriendly to be used.

## Path to Active

### Acceptance Criteria

- [ ] There are at least two (from different providers) wallets, libraries, CLI packages, or other tools which use this standard for network identification.

### Implementation Plan

- [x] Develop and publish reference implementation: [CIP34-JS](https://www.npmjs.com/package/@dcspark/cip34-js)

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode)

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0035/README.md
---

- --
CIP: 35
Title: Changes to Plutus Core
Status: Active
Category: Meta
Authors:
- Michael Peyton Jones <michael.peyton-jones@iohk.io>
Implementors: N/A
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/215
- https://github.com/cardano-foundation/CIPs/pull/428
- https://github.com/cardano-foundation/CIPs/pull/437
- https://github.com/cardano-foundation/CIPs/pull/484
Created: 2022-02-09
License: CC-BY-4.0
- --

# Changes to Plutus Core

## Abstract

This CIP specifies a process for proposing changes to Plutus Core, its builtins, and its interface to the Cardano ledger.
It gives a taxonomy of typical changes, and explains how these changes may be made, which in some cases requires a CIP.
It introduces the 'Plutus' CIP category for tracking these.

## Motivation: why is this CIP necessary?

The Plutus Core language, its builtins, and its interface to the ledger are all likely to evolve significantly over time. There are many reasons for this:
- We may be able to increase performance, improve safety, or reduce script sizes by changing the language.
- We may be able to improve performance by providing builtin versions of expensive functions.
- We may need to provide builtin versions of sensitive functions (e.g. cryptography) in order to ensure access to high-quality implementations.
- We may find bugs in the implementation that need to be fixed.
- We may need to change the interface to the ledger in order to represent changes in transaction formats.
- We may wish to remove elements which have been deprecated due to the addition of improved versions.
- … and more

This CIP gives a taxonomy of changes, explains how such changes might be implemented in Cardano, and prescribes processes for proposing such changes.

## Background

This CIP assumes general familiarity with Plutus Core and the Cardano ledger, but we give some brief background here.

### Plutus Core

_Plutus Core_ is a script language used in the Cardano ledger.
For the purposes of this document, Plutus Core consists of various _language constructs_, and also _builtin types and functions_.

Plutus Core has a number of builtin types, such as integers, and builtin functions, such as integer addition.
Builtin functions provide access to functionality that would be difficult or expensive to implement using the basic constructs of Plutus Core, which is otherwise little more than the untyped lambda calculus.
Builtin functions can operate only over builtin types or arbitrary Plutus Core terms treated opaquely.
Builtin types come with a _size metric_ which is used by costing functions.
For example, the size metric for integers returns the bit-size of the integer.

The performance of Plutus Core scripts has two components: how expensive the script actually is to run (_real performance_) and how expensive we say it is to run in the ledger (_model performance_).

Model performance is calculated by costing _evaluation_ in abstract resource units ("exunits") of CPU and memory.
Individual steps of evaluation are costed, and builtin functions must also come with a _costing function_ that provides costing information for them.
The costing function for a builtin function is a mathematical function which takes the sizes of the arguments (as computed by their size metrics), and returns an estimate of the budget that will be used to perform that function.
For example, the costing function for addition says that the CPU and memory costs are both proportional to the maximum of the sizes of the input integers (with some appropriate constants).

Determining costing functions is done empirically, by running the function in question against a large number of inputs and choosing a costing function that fits the data well.

Plutus Core has a _language version_ (LV).
This is the version of the Plutus Core programming language itself, and it controls e.g. which constructs are available in the language.
Changing any of the features which are guarded by the language version requires a new language version to be supported by the chain.
Note that changing the set of builtin types or functions does _not_ require a new language version; any individual Plutus Core language version is compatible with any set of builtin types and functions.

Depending on the type of change, a major or minor bump to the language version may be required.
The following table shows typical examples.

| Change                                                | Type  | Notes                     |
|-------------------------------------------------------|-------|---------------------------|
| Adding a construct to the language                    | Minor | Backwards-compatible.     |
| Removing a construct from the language                | Major | Not backwards-compatible. |
| Changing the behaviour of a construct in the language | Major | Not backwards-compatible. |
| Changing the binary format of the language in a backwards-compatible way | Minor | Safe even if it makes previously non-deserializable scripts deserializable.[^backwards-safe] |
| Changing the binary format of the language            | Major | Not backwards-compatible. |

[^backwards-safe]: See "Are backwards-compatible binary format changes really safe?".

Since we always need a new Plutus Core langauge version for any change to the language, in the rest of this document we will focus on the introduction of a new langauge version as the proxy for changes to the language.

### Scripts in the Cardano ledger

The Cardano ledger recognizes various kinds of _scripts_ identified by _language_.
This language tag is the only way that the ledger has to distinguish between different types of script.
Hence if we require different behaviour, we need a new language.
We refer to these languages as _ledger languages_ (LLs).

Part of the specification of a language in the ledger is how scripts of that language are run, what arguments they are passed, how those arguments are structured, etc.
We refer to this as the _ledger-script interface_.

Because we want to occasionally change e.g. the ledger-script interface for Plutus Core scripts, this means we need several ledger languages which all run scripts written in Plutus Core.[^ledger-language-versions]
All Plutus Core ledger languages which are used in the ledger must be supported forever, in order to be able to validate the history of the chain.

[^ledger-language-versions]: The Plutus Core family of ledger languages are sometimes referred to as the Plutus Core _ledger language versions_, and named as such ("PlutusV1", "PlutusV2" etc.) although they actually entirely distinct _languages_ from the perspective of the ledger. In this document we will use the more precise language and refer to them just as distinct ledger languages.

Ledger languages also have an associated subset of the _protocol parameters_.
These parameters provide the ability to control some aspects of evaluation without a software update.
The most notable example is a set of parameters which parameterize the costing of program execution.
Hence, a different Plutus Core ledger language can have a different set of costing protocol parameters.

We can change the behaviour of ledger languages in a backwards compatible way with new protocol versions.
This ensures that the new behaviour only becomes available at a particular time in the history of the chain.

Overall the combination of ledger language and protocol version controls:
- The protocol parameters which are available
- The ledger-script interface
- The Plutus Core language versions that are available
- The set of builtin types and values that are available

### Types of change

This document considers the following types of change:

1. The Plutus Core language
1. Adding a new Plutus Core language version
2. The Plutus Core builtin functions and types
1. Adding a new builtin function or type
2. Removing a builtin function or type
3. Changing the behaviour of a builtin function or type
3. The ledger-script interface
1. Changing the interface between the ledger and the interpreter
4. Protocol parameters
1. Improving model performance (i.e. changing the costing parameters so that scripts use less budget)
2. Regressing model performance (i.e. changing the costing parameters so that scripts use more budget)
3. Adding costing parameters
4. Removing costing  parameters
5. Performance of the Plutus Core interpreter
1. Improving real performance
2. Regressing real performance

### Types of release

Changes to Plutus Core can be released onto Cardano in four ways, with ascending levels of difficulty:

1. A _protocol parameter_ change (PP), taking effect as soon as the new parameters are accepted (in a new epoch).
2. A _software update_ (SU) to the node, taking effect when nodes upgrade.
3. A _hard fork_ (HF) (accompanied by a software update), requiring a software update for the new protocol version, and taking effect after the hard fork.
4. A new Plutus Core _ledger language_ (LL), introduced in a hard fork, and taking effect for scripts that use the new language, but not for those that use the old language.

Intuitively, these correspond to how _compatible_ the change is.
- A backwards- and forwards-compatible change can be deployed with a software update, as nobody can perceive the difference.
- A backwards-compatible (but not forwards-compatible) change must be deployed in a hard fork, since it makes more blocks acceptable than before.
- A backwards-incompatible change requires a new Plutus Core ledger language, so that the ledger can distinguish them, and maintain the old behaviour for old scripts.

The following table lists, for each type of change in "Types of change", what kind of release it requires.

| Change                                                                     | Release            | Notes                                                                                               |
|----------------------------------------------------------------------------|--------------------|-----------------------------------------------------------------------------------------------------|
| Adding a new Plutus Core language version                                 | HF                 | Backwards-compatible since the new behaviour is guarded by the new LV. |
| Adding a new builtin function or type                                      | HF (rarely LL[^binary-backwards]) | Backwards-compatible. Requires a binary format change.                                              |
| Removing a builtin function or type                                        | LL                 | This will cause scripts which use this builtin to be rejected, so is not backwards compatible.      |
| Changing the behaviour of a builtin function or type                       | LL                 | This changes the behaviour of existing scripts, so is not backwards compatible.                     |
| Changing the interface between the ledger and the interpreter              | LL                 | The ledger must provide scripts with exactly the right interface. New interface means new language. |
| Improving model performance                                                | PP                 | _Must_ strictly follow an improvement in real performance.[^why-perf-1]                                      |
| Regressing model performance                                               | PP                 |                                                                                                     |
| Adding cost model parameters                                               | HF                 | All nodes must recognize the new parameter.                                                         |
| Removing cost model parameters                                             | LL                 | Old scripts will require all the old parameters.                                                    |
| Improving real performance                                                 | SU                 |                                                                                                     |
| Regressing real performance                                                | SU                 | _Must_ strictly follow a regression in model performance.[^why-perf-2]                                       |

[^binary-backwards]: The binary format change is backwards compatible unless it breaches the limit of how many builtin functions or types can be encoded, in which case that must be changed, forcing a new LL.
[^why-perf-1]: See "Why do performance changes require extra steps?".
[^why-perf-2]: See "Why do performance changes require extra steps?".

## Specification

### Scope

This CIP deals with the types of change listed in "Types of change".
That list aims to cover the most typical changes to Plutus Core, but it is not exhaustive.
CIPs which do not propose changes in the list but whose authors believe they significantly affect Plutus Core should nonetheless be assigned to the Plutus category.

Additionally, there is significant overlap with the Ledger category around the ledger-script interface and the protocol parameters.
CIPs which change these parts of Cardano should generally use the Plutus category and not the Ledger category, although the Editors may ask the Ledger reviewers to comment.

### The Plutus reviewers

The following table gives the current set of reviewers for Plutus CIPs.

| Name                 | Email                        | GitHub username |
|----------------------|------------------------------|-----------------|
| Ziyang Liu           | ziyang.liu@iohk.io           | zliu41          |

### Changes that require a CIP

This proposal requires that some of the changes listed in "Types of change" (specified below) should:

1. Be proposed in a CIP.
2. Go through additional process in addition to the [usual CIP process](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0001/README.md).

The additional process mostly takes the form of additional information that should be present in the CIP before it moves to particular stages.

The requirement to propose a change via a CIP is, as all CIPs are, advisory.
In exceptional circumstances or where swift action is required, we expect that changes may still be made without following this process.
In such circumstances, a retrospective CIP SHOULD be made after the fact to record the changes and the rationale for them.

Changes that require a CIP do not have to each be in an individual CIP, they can be included in batches or in other CIPs.
So, for example, a single CIP could propose multiple new builtin functions, or a CIP proposing a change to the ledger could also propose a change to the ledger-script interface.

### Processes

All changes that require a CIP SHOULD adhere to the following generic process.

In order to move to Proposed status:
- The Specification MUST include:
- The type of change that is being proposed.
- For changes to Plutus Core itself, a formal specification of the changes which is precise enough to update the Plutus Core specification from.
- The Acceptance Criteria MUST include:
- The external implementations are available.
- The `plutus` repository is updated with the specification of the proposal.
- The `plutus` repository is updated with an implementation of the proposal.
- The Implementation Plan MUST include:
- The type of release that the change requires.

#### Additions to the Plutus Core Builtins

Proposals for additions to the set of Plutus Core builtins SHOULD be proposed in a CIP and SHOULD adhere to the following additional process.

In order to move to Proposed status:
- The Specification MUST include:
- Names and types/kinds for the new functions or types.
- A source for the implementation (e.g. a library which can be linked against); or a generic description of the functionality which is implementable in any programming language.
- For new builtin types: a rough description of how the size of a value of that type can be measured.
- For new builtin functions: a description of their time and space complexity.
- The Rationale MUST include:
- If an external implementation is provided: an argument that it satisfies the following non-exhaustive list of criteria:
- It is trustworthy
- It always terminates
- It (or its Haskell bindings) never throw any exceptions
- Its behaviour is predictable (e.g. does not have worst-case behaviour with much worse performance)
- Discussion of how any measures and costing functions were determined.
- The Acceptance Criteria MUST include:
- The ledger is updated to include new protocol parameters to control costing of the new builtins.

The Rationale of a CIP should always be a clear argument for why the CIP should be adopted.
In this case we recommend including:
- An argument for the utility of the new builtins.
- Examples of real-world use cases where the new additions would be useful.
- If feasible, a comparison with an implementation using the existing features, and an argument why the builtin is preferable (e.g. better performance).

#### Protocol parameter updates

Protocol parameter updates that affect Plutus Core should be proposed in the Ledger category and following its processes.
The only additional process required is review by the Plutus reviewers.

#### Performance changes

This CIP does not require any process for proposing performance changes.

#### Bug fixes

A "bug fix" is a change to behaviour where:
- The implemented behaviour does not match the specification; or
- The specified behaviour is clearly wrong (in the judgement of relevant experts)

In this case the fix may be submitted directly to the `plutus` repository and is NOT required to go through the CIP process.
It must still be released as appropriate.
For example, if a bug fix changes behaviour, it will have to wait for a new Plutus Core ledger language.

#### Other changes

Proposals for other additions, removals, or changes to behaviour of any part of Plutus Core or its builtins SHOULD be proposed in a CIP.

## Rationale: how does this CIP achieve its goals?

### Do removals and changes really need a new ledger language?

Not being able to make removals or changes to behaviour without a LL is quite painful.
For example, it means that we cannot just fix bugs in the semantics: we must remain bug-for-bug compatible with any given LL.

It is tempting to think that if we can show that a particular behaviour has never been used in the history of the chain, then changing it is backwards-compatible, since it won’t change the validation of any of the actual chain.
However, this is sadly untrue.

1. The behaviour could be triggered (potentially deliberately) in the interval between the update proposal being accepted and it being implemented. This is extremely dangerous and could lead to an un-managed hard fork.
2. The behaviour could be triggered in a script which has not yet been executed on the chain, but whose hash is used to lock an output. This could lead to that output being unexpectedly un-spendable, or some other change in behaviour. Moreover, since we only have the hash of the script, we have no way of telling whether this is the case.

So even if a behaviour is obscure, we cannot just change it.

### Are backwards-compatible binary format changes really safe?

Changing the binary format in a backwards compatible way may mean that binary scripts which previously would have been invalid might now deserialize correctly into a program.

There is a worry here: scripts which fail execution (a phase 2 validation failure) actually get posted on the chain as failures.
We must be careful not to turn any such failures into successes, otherwise we could break history.

However, we do not need to worry in this case, since checking that a script deserializes properly is a phase 1 validation check, so no scripts will be posted as failures due to failing to deserialize, so we cannot break any such postings by making deserialization more lenient.

### Why do performance changes require extra steps?

Performance changes must be carefully managed in order to avoid the possibility of an accidental hard fork.

Consider an example of improving performance.
The interpreter gets 50% faster (real performance), which is released as a software update.
Now, we want to lower the cost model parameters (model performance) so that users will be charged fewer resources for their scripts.

However, the parameter change means that scripts which previously would have exceeded the transaction/block limit become acceptable.
The parameter change is not itself a hard fork, because all the nodes accept the new parameters by consensus.
But if the model performance tracks real performance well, then nodes which have not adopted the software update may have issues with the real performance of these newly allowed scripts!
In the worst case, they might suffer resource exhaustion, preventing them from following the new chain, which is tantamount to a hard fork.

This is a relatively unlikely scenario, since it requires a situation where nodes are close enough to their resource limits that an (effective) regression in real performance of scripts can push them over the edge.
Nonetheless, the cautious approach is to not perform such parameter updates until we are sure that all nodes are using a version with the required software update, i.e. after the protocol version has been bumped in a hard fork.

Conversely, if (for some reason) we needed to regress the real performance of the interpreter, we should only do this after all the nodes have accepted a regression in model performance (increasing the cost model parameters).

### Why are we concerned about the implementations of builtins being trustworthy?

Builtin functions in Plutus Core are implemented via Haskell functions. Often these implementations come from somewhere else, e.g. a cryptography library written in C.

It is vitally important that these libraries are trustworthy.
The Plutus Core package (and hence its dependencies) are linked into the Cardano node.
A buffer overrun vulnerability in the implementation of a builtin function could therefore become an attack on a node.

### Why is the process for new builtins so much more structured?

We expect additions to builtins to be particularly common, and to have lots of interest from the community.

However, the process of adding new builtins is not totally straightforward, due in particular to the need to find a good implementation and to cost it.
Surfacing these difficulties quickly is a key goal of this process.

Finally, builtins are a comparatively structured extension point for the language.
In comparison, proposals for changes to Plutus Core itself are likely to be much more heterogeneous.

### Why are we reluctant to release new ledger language?

Ledger languages incur a large maintenance cost.
Each one must continue to work, perfectly, in perpetuity. Furthermore, they may need their own, independent set of cost model protocol parameters, etc.

So it is very desirable to keep the number of ledger languages down.
The simplest way to do this is to batch changes, and only release a new ledger language occasionally.

## Path to Active

### Acceptance Criteria

This CIP requires the acceptance of the Plutus team, which it has in virtue of its authorship.

### Implementation Plan

No implementation is required.

## Copyright

This CIP is licensed under [CC-BY-4.0][].

[CC-BY-4.0]: https://creativecommons.org/licenses/by/4.0/legalcode

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0036/README.md
---

- --
CIP: 36
Title: Catalyst Registration Transaction Metadata Format (Updated)
Category: Tools
Status: Proposed
Authors:
- Giacomo Pasini <giacomo.pasini@iohk.io>
- Kevin Hammond <kevin.hammond@iohk.io>
- Mark Stopka <mark.stopka@perlur.cloud>
Implementors:
- cardano-signer <https://github.com/gitmachtl/cardano-signer>
Discussions:
- https://forum.cardano.org/t/cip-catalyst-registration-metadata-format/44038
- https://github.com/cardano-foundation/CIPs/pull/188
- https://github.com/cardano-foundation/CIPs/pull/349
- https://github.com/cardano-foundation/CIPs/pull/373
Created: 2021-12-06
License: CC-BY-4.0
- --

## Abstract

Cardano uses a sidechain for its treasury system. One needs to "register" to participate on this sidechain by submitting a registration transaction on the mainnet chain. This CIP details the registration transaction format.
This is a revised version of the original [CIP-15 | Registration Transaction Metadata Format](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0015).

## Motivation: why is this CIP necessary?

Cardano uses a sidechain for its treasury system ("Catalyst") and for other voting purposes. One of the desirable properties of this sidechain is that even if its safety is compromised, it doesn't cause loss of funds on the main Cardano chain. To achieve this, instead of using your wallet's recovery phrase on the sidechain, we need to use a brand new "voting key".

However, since 1 ADA = 1 vote, a user needs to associate their mainnet ADA to their new voting key. This can be achieved through a registration transaction.

In addition, to encourage participation by a broader range of ADA holders, it should be possible to delegate one's rights to vote to (possibly multiple) representatives and/or expert voters. Such delegations will still be able to receive Catalyst rewards.

We therefore need a registration transaction that serves three purposes:

1. Registers a "voting key" to be included in the sidechain and/or delegates to existing "voting key"s
2. Associates mainnet ADA to this voting key(s)
3. Declares a payment address to receive Catalyst rewards

- *Note**: This schema does not attempt to differentiate delegations from direct registrations, as the two options have exactly the same format. It also does not distinguish between delegations that are made as "private" arrangements (proxy votes)
from those that are made by delegating to representatives who promote themselves publicly.
Distinguishing these possibilities is left to upper layers or future revisions of this standard, if required.
In this document, we will use the term 'delegations' to refer to all these possibilities.

## Specification

### Registration metadata format

A registration transaction is a regular Cardano transaction with a specific transaction metadata associated with it.

Notably, there should be five entries inside the metadata map:
- A non-empty array of delegations, as described below;
- A stake address for the network that this transaction is submitted to (to point to the Ada that is being delegated);
- A Shelley payment address (see [CIP-0019](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0019)) discriminated for the same network this transaction is submitted to, to receive rewards.
- A nonce that identifies that most recent delegation
- A non-negative integer that indicates the purpose of the vote. This is an optional field to allow for compatibility with CIP-15. For now, we define 0 as the value to use for Catalyst, and leave others for future use. A new registration should not invalidate a previous one with a different voting purpose value.

### Delegation format

A delegation assigns (a portion of) the ADA controlled by one or more UTxOs on mainnet to the voting key in the sidechain as voting power.  The UTxOs can be identified via the stake address at some designated point in time.

Each delegation therefore contains:
- a voting key: simply an ED25519 public key. This is the spending credential in the sidechain that will receive voting power from this delegation. For direct voting it's necessary to have the corresponding private key to cast votes in the sidechain. How this key is created is up to the wallet.
- the weight that is associated with this key: this is a 4-byte unsigned integer (CBOR major type 0, The weight may range from 0 to 2^32-1) that represents the relative weight of this delegation over the total weight of all delegations in the same registration transaction.

### Voting key

The terms "(CIP-36) voting keys" and "(CIP-36) vote keys" should be used interchangeably to indicate the keys described in this specification. But it should be made clear that implementations should favour the term "(CIP-36) vote key" and that the association of both terms aims to reduce the possibility of confusion.

The term governance should not be associated with these keys nor should these keys be associated with other vote or voting keys used in the ecosystem. When discussing these keys in a wider context they should be specified by such terminology as "CIP-36 vote keys" or "CIP-36 style vote keys".

#### Derivation path

To avoid linking voting keys directly with Cardano spending keys, the voting key derivation path must start with a specific segment:

`m / 1694' / 1815' / account' / chain / address_index`

We recommend that implementors only use `address_index=0` to avoid the need for voting key discovery.

#### Tooling

Supporting tooling should clearly define and differentiate this as a unique key type, describing such keys as "CIP-36 vote keys". When utilizing Bech32 encoding the appropriate `cvote_sk` and `cvote_vk` prefixes should be used as described in [CIP-05](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0005).

Examples of acceptable `keyType`s for supporting tools:

| `keyType` | Description |
| --------- | ----------- |
| `CIP36VoteSigningKey_ed25519` | Catalyst Vote Signing Key |
| `CIP36VoteExtendedSigningKey_ed25519` | Catalyst Vote Signing Key |
| `CIP36VoteVerificationKey_ed25519` | Catalyst Vote Verification Key |
| `CIP36VoteExtendedVerificationKey_ed25519` | Catalyst Vote Verification Key |

For hardware implementations:
| `keyType` | Description |
| --------- | ----------- |
| `CIP36VoteVerificationKey_ed25519` | Hardware Catalyst Vote Verification Key |
| `CIP36VoteHWSigningFile_ed25519` | Hardware Catalyst Vote Signing File |

The intention with this design is that if projects beyond Catalyst implement this specification they are able to add to themselves `keyType` **Description**s.

### Example - Registration

#### Voting registration example:
```
61284: {
  // delegations - CBOR byte array
  1: [["0xa6a3c0447aeb9cc54cf6422ba32b294e5e1c3ef6d782f2acff4a70694c4d1663", 1], ["0x00588e8e1d18cba576a4d35758069fe94e53f638b6faf7c07b8abd2bc5c5cdee", 3]],
  // stake_pub - CBOR byte array
  2: "0xad4b948699193634a39dd56f779a2951a24779ad52aa7916f6912b8ec4702cee",
  // payment_address - CBOR byte array
  3: "0x00588e8e1d18cba576a4d35758069fe94e53f638b6faf7c07b8abd2bc5c5cdee47b60edc7772855324c85033c638364214cbfc6627889f81c4",
  // nonce
  4: 5479467
  // voting_purpose: 0 = Catalyst
  5: 0
}
```
The entries under keys 1, 2, 3, 4 and 5 represent the Catalyst delegation array,
the staking credential on the Cardano network, the payment address to receive rewards,
a nonce, and a voting purpose, respectively. A registration with these metadata will be
considered valid if the following conditions hold:

- The nonce is an unsigned integer (of CBOR major type 0) that should be
  monotonically rising across all transactions with the same staking key.
  The advised way to construct a nonce is to use the current slot number.
  This is a simple way to keep the nonce increasing without having to access
  the previous transaction data.
- The payment address is a Shelley payment address discriminated for the same network
  this transaction is submitted to.
- The delegation array is not empty
- The weights in the delegation array are not all zero

Delegation to the voting key `0xa6a3c0447aeb9cc54cf6422ba32b294e5e1c3ef6d782f2acff4a70694c4d1663` will have relative weight 1 and delegation to the voting key `0x00588e8e1d18cba576a4d35758069fe94e53f638b6faf7c07b8abd2bc5c5cdee` relative weight 3 (for a total weight of 4).
Such a registration will assign 1/4 and 3/4 of the value in ADA to those keys respectively, with any remainder assigned to the second key.

The registration witness depends on the type of stake credential used.
To produce the witness field in case of a staking public key, the CBOR representation of a map containing
a single entry with key 61284 and the registration metadata map in the
format above is formed, designated here as `sign_data`.
This data is signed with the staking key as follows: first, the
blake2b-256 hash of `sign_data` is obtained. This hash is then signed
using the Ed25519 signature algorithm. The witness metadata entry is
added to the transaction under key 61285 as a CBOR map with a single entry
that consists of the integer key 1 and signature as obtained above as the byte array value.

#### Witness example:
```
61285: {
  // witness - ED25119 signature CBOR byte array
  1: "0x8b508822ac89bacb1f9c3a3ef0dc62fd72a0bd3849e2381b17272b68a8f52ea8240dcc855f2264db29a8512bfcd522ab69b982cb011e5f43d0154e72f505f007"
}
```

### Deregistration metadata format (Catalyst)

This deregistration format is currently only specified for Catalyst (vote_purpose=0), other voting chain purposes may handle a deregistration in a different way.
There was a discussion before if an empty delegation array could also be used to fulfil a deregistration. This idea was cancelled, because it would currently require additional resources in the Hardware-Wallets state machine to do additional checks about an empty array. So the decision was made to leave the registration part untouched and only add the deregistration via the unused key 61286. Wallets/Tools are not forced to support this deregistration method.

Definition:
- A deregistration removes all the voting power (associated stake amount) for the provided stake credential from the delegated vote-public-keys.
- A deregistration resets the state of the stake credential on the voting chain like they were never registered before.
- A deregistration transaction is a regular Cardano transaction with a specific transaction metadata associated with it.

Notably, there should be three entries inside the metadata map (key 61286):
- The public key of the stake signing key
- A nonce that identifies that most recent deregistration.
- A non-negative integer that indicates the purpose of the vote. For now, we define 0 as the value to use for Catalyst, and leave others for future use.

Be aware, the deregistration metadata key is 61286, and not 61284 like it is used for a registration! The registraton metadata format and specification is independent from the deregistration one, and may not be supported by all wallets/tools.

#### Example - Deregistration (Catalyst)

```
{
  61286: {
    // stake_pub - CBOR byte array
    1: "0x57758911253f6b31df2a87c10eb08a2c9b8450768cb8dd0d378d93f7c2e220f0",
    // nonce
    2: 74412400,
    // voting_purpose: 0 = Catalyst
    3: 0
  },
  61285: {
    // witness - ED25119 signature CBOR byte array
    1: "0xadb7c90955c348e432545276798478f02ee7c2be61fd44d22f9de22131d9bcf0b23eb413766b74b9e7ba740e71266467a5d35363411346972db9e7b710b00603"
  }
}
```
CBOR-Hex:
`A219EF66A301582057758911253F6B31DF2A87C10EB08A2C9B8450768CB8DD0D378D93F7C2E220F0021A046F7170030019EF65A1015840ADB7C90955C348E432545276798478F02EE7C2BE61FD44D22F9DE22131D9BCF0B23EB413766B74B9E7BA740E71266467A5D35363411346972DB9E7B710B00603`

The entries under keys 1, 2 and 3 represent the staking credential on the Cardano network, a nonce, and a voting purpose, respectively.
A deregistration with these metadata will be considered valid if the following conditions hold:

- The stake credentials is a stake public-key byte array (of CBOR major type 2)
- The nonce is an unsigned integer (of CBOR major type 0) that should be
  monotonically rising across all transactions with the same staking key.
  The advised way to construct a nonce is to use the current slot number.
  This is a simple way to keep the nonce increasing without having to access
  the previous transaction data.
- The voting_purpose is an unsigned integer (of CBOR major type 0)

To produce the witness field in case of a staking public key, the CBOR representation of a map containing
a single entry with key 61286 and the deregistration metadata map in the
format above is formed, designated here as `sign_data`.
This data is signed with the staking key as follows: first, the
blake2b-256 hash of `sign_data` is obtained. This hash is then signed
using the Ed25519 signature algorithm. The witness metadata entry is
added to the transaction under key 61285 as a CBOR map with a single entry
that consists of the integer key 1 and signature as obtained above as the byte array value.

### Metadata schema

See the [schema file](./schema.cddl).

### Test vector

See [test vector file](./test-vector.md).

## Rationale: how does this CIP achieve its goals?

### Associating voting power with a voting key

This method has been used since Fund 2.
For future fund iterations, a new method making use of time-lock scripts may
be introduced as described [below][future-development].

Recall: Cardano uses the UTXO model so to completely associate a wallet's balance with a voting key (i.e. including enterprise addresses), we would need to associate every payment key to a voting key individually. Although there are attempts at this (see [CIP-0008]), the resulting data structure is a little excessive for on-chain metadata (which we want to keep small)

Given the above, we choose to associate staking credentials with voting keys. At the moment, the only supported staking credential is a staking key. Since most Cardano wallets only use base addresses for Shelley wallet types, in most cases this should perfectly match the user's wallet.

The voting power that is associated with each delegated voting key is derived from the user's total voting power
as follows.

1. The total weight is calculated as a sum of all the weights;
2. The user's total voting power is calculated as a whole number of ADA (rounded down);
3. The voting power associated with each voting key in the delegation array is calculated as the weighted fraction of the
   total voting power (rounded down);
4. Any remaining voting power is assigned to the last voting key in the delegation array.

This ensures that the voter's total voting power is never accidentally reduced through poor choices of weights,
and that all voting powers are exact ADA.

### Future development

[future-development]: #future-development

A future change of the Catalyst system may make use of a time-lock script to commit ADA on the mainnet for the duration of a voting period. The voter registration metadata in this method will not need an association
with a staking credential. Therefore, the `staking_credential` map entry
and the `registration_witness` payload with key 61285 will no longer
be required.

### Changelog

Fund 3 added the `reward_address` inside the `key_registration` field.

Fund 4:
- added the `nonce` field to prevent replay attacks;
- changed the signature algorithm from one that signed `sign_data` directly
  to signing the Blake2b hash of `sign_data` to accommodate memory-constrained hardware wallet devices.

It was planned that since Fund 4, `registration_signature` and the `staking_pub_key` entry inside the `key_registration` field will be deprecated.
This has been deferred to a future revision of the protocol.

Fund 8:
- renamed the `voting_key` field to `delegations` and add support for splitting voting power across multiple vote keys.
- added the `voting_purpose` field to limit the scope of the delegations.
- rename the `staking_pub_key` field to `stake_credential` and `registration_signature` to `registration_witness` to allow for future credentials additions.

Fund 10:
- Replaced the `reward_address` field with `payment_address` field, keeping it at index 3. Stipulating that `payment_address` must be a Shelley payment address, otherwise voting reward payments will not be received.
- **Note:** up to Catalyst's Fund 9, voting rewards were paid via MIR transfer to a stake address provided within the `reward_address` field. From Fund 10 onwards, a regular payment address must be provided in the `payment_address` field to receive voting rewards. This allows Catalyst to avoid MIR transfers and instead pay voting rewards via regular transactions.

Fund 11:
- added the `deregistration` metadata format.

## Path to Active

### Acceptance Criteria

- [ ] Have this registration format supported in Catalyst infrastructure for two Catalyst funds.
- [ ] Have this registration format supported by three wallets/tools.
- [cardano-signer](https://github.com/gitmachtl/cardano-signer)

### Implementation Plan

- [x] Authors to provide test vector file.
- See the [schema file](./schema.cddl).
- [x] Authors to provide CDDL schema file.
- See [test vector file](./test-vector.md).
- [x] Authors to test this format for security and compatibility with existing Catalyst infrastructure.


## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0037/README.md
---

- --
CIP: 37
Title: Dynamic Saturation Based on Pledge
Status: Proposed
Category: Ledger
Authors:
- Casey Gibson <caseygibson@protonmail.ch>
Implementors: []
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/163
Created: 2021-12-03
License: CC-BY-4.0
- --

## Abstract

The pledge should be used to calculate the saturation point of a pool: setting a maximum delegation proportional to pledge.

## Motivation: why is this CIP necessary?

Currently Cardano has been plagued with an ever increasing amount of single entity Stake Pool Operators (SPO) creating multiple pools. The pools that are known to be operated by single entity SPOs account for just 18.72% of the total stake and 50% of the total stake can be attributed to at least 23 single entities (as of 3rd Dec 2021).

The vision of Cardano is that it everyone should be able to have the opportunity to be able to run a stake pool regardless of their financial capabilities and this is even more important for developing countries.

The issue we're currently facing is that many SPOs have been able to exploit a loophole that has allowed them to use their influence to create many sub-pools without any restrictions. As the size of their pools grow, instead of honouring the saturation limits already in place, they are bypassing them by creating more sub-pools. From a technical point, it is theoretically possible for a single entity to run enough stake pools to control more then 50% of the total stake.

While unlikely, the system does not take into account external influences (eg. political) that could harm the system. Shelly has been active for less then 2 years and we are already seeing the risks on a small scale. As real world adoption continues, there could be situations in the next decade that could jeopardise the decentralisation of Cardano. One example could be that a political party could run a ISPO (Initial Stake Pool Offering) on a mass scale. Seeing as Cardano has the potential to be used for voting (even at a political level), the integrity of Cardano could be questioned if political parties controlled stake pools with large shares of total stake.

The pledge used in a pool was meant to show how serious a SPO is and how much "skin in the game" they had. The idea was that if they have more pledge, they have more to lose if something goes wrong. This seems to have lost it's meaning as SPOs that already have a dominating position have created many sub-pools. Some have split up their pledge evenly, some have next to no pledge and have gained high amounts of stake through influential means. This has not only reduced the security of Cardano, but has also lost the meaning of having pledge in the pool.

For example, a SPO might have a pledge of 30,000 ADA across 10 pools, while another SPO might have 1,000,000 pledge but only has 1 pool with a small amount of active stake. Seeing as the SPO with 1M pledge has more overall, they have more to lose and should be trusted more. Since there is no technical advantage to having a high pledge, the meaning and purpose of a pledge is redundant.

To make the pledge a meaningful metric that is fair to all SPOs and aligns with the core values of Cardano, the pledge should be used to calculate the saturation point of a pool. This will mean that SPOs, no matter how many pools they operate, will have a maximum saturation based on their total pledge. For example, if a pool operates a single pool and wanted to open up another pool with the same amount of stake, they will have to assign the equal amount of pledge against that new pool. If a pool wishes to up their saturation point, they will need to assign a higher amount of pledge.

This proposal has had active discussions for over 6 months and is now waiting for a review from IOG to provide feedback on the feasibility and soundness of the approach.

1. This CIP should be considered a medium priority as it directly impacts the health and growth of the Cardano ecosystem. Large Cardano pools have had several years to take advantage of the lack of oversight and have control of a large portion of mining operations. This continued lack of restrictions will further damage the trust and reliability of the framework.

2. Timelines around the implementation of the CIP will depend on urgency, however its implementation should be trivial as there are no new parameters required or risks involved. Further this CIP from a high-level aspect only requires an update to the existing algorithm. From an external context, the CIP will require trivial updates as it should be self-contained in cardano-node.

## Specification

For this to be able to work, there firstly needs to be an upper limit and a lower limit. The K parameter can still be used as the upper saturation limit of a single pool. As in, if a SPO has enough pledge assigned to a single pool, that pool will be able to run at the maximum saturation point of K. The lower limit is in place to safe guard small SPOs and allow them to grow.

An example of how Dynamic Saturation would be calculated:

500,000 ADA Pledged = Saturation point at 100% of K

250,000 ADA Pledged = Saturation point at 50% of K

125,000 ADA Pledged = Saturation point at 25% of K

62,500 ADA Pledged = Saturation point at 12.5% of K

To not penalise small pools, there should be a lower limit saturation point, such as 10% of K. Based on this, a pool with a pledge of 50,000 has the same saturation point as a pool with 25,000 pledge, both being 10% of K. This will allow smaller pools to still have some growth potential.

The only way a pool can receive more stake across all their pools without impacting their rewards is to increase their total pledge.

Example with SPO of 1,000,000 Pledge with current k implementation:

| Pools | Pledge    | Saturation Per Pool | Total Stake | Fees                 |
|-------|-----------|---------------------|-------------|----------------------|
| 1     | 1,000,000 | 100% of K           | ~65M        | 340 fee + margin     |
| 2     | 500,000   | 100% of K           | ~130M       | 340 fee + margin x2  |
| 4     | 250,000   | 100% of K           | ~260M       | 340 fee + margin x4  |
| 8     | 125,000   | 100% of K           | ~520M       | 340 fee + margin x8  |
| 16    | 62,500    | 100% of K           | ~1040M      | 340 fee + margin x16 |

Example with SPO of 1,000,000 Pledge with Dynamic Saturation:

| Pools | Pledge    | Saturation Per Pool | Total Stake | Fees                 |
|-------|-----------|---------------------|-------------|----------------------|
| 1     | 1,000,000 | 100% of K           | ~65M        | 340 fee + margin     |
| 2     | 500,000   | 100% of K           | ~130M       | 340 fee + margin x2  |
| 4     | 250,000   | 50% of K            | ~130M       | 340 fee + margin x4  |
| 8     | 125,000   | 25% of K            | ~130M       | 340 fee + margin x8  |
| 16    | 62,500    | 12.5% of K          | ~130M       | 340 fee + margin x16 |

As we can see above with the **current** implementation of K, a pool owner can split the pool and double their delegators active stake (or however many times they split).

The Dynamic Saturation method caps the pools so that the amount of total stake across all pools is the same no matter how much they split up their pledge and the only benefit will be the extra fee collected (340 ADA) per pool.

This will mean that the saturation metric will have a direct corelation to an SPOs total pledge.

### Proposals Based On Feedback

After some discussions among the community and some help from https://github.com/cffls, the below code example has been proposed for how the dynamic saturation could work.

```
let lovelace = 1000000;

function calc_sat(pledge){
    k = 500;
    e = 0.2;
    l = 125;
    total_supply = 33719282563 * lovelace;
    orig_sat = total_supply / k;
    new_sat = orig_sat * Math.max(e, min(1 / k, pledge / orig_sat * l));
    final_sat = max(new_sat, orig_sat);
    console.log(`pledge: ${pledge / lovelace}, sat: ${final_sat / lovelace}`);
}

function max(val1, val2){
    if(val1 < val2){
        return val1;
    }
    return val2;
}

function min(val1, val2){
    if(val1 > val2){
        return val1;
    }
    return val2;
}

calc_sat(50000 * lovelace);
calc_sat(100000 * lovelace);
calc_sat(150000 * lovelace);
calc_sat(250000 * lovelace);
calc_sat(500000 * lovelace);
calc_sat(750000 * lovelace);
calc_sat(1000000 * lovelace);
calc_sat(2000000 * lovelace);
```

Results:

```
[Log] pledge: 50000, sat: 13487713.0252
[Log] pledge: 100000, sat: 13487713.0252
[Log] pledge: 150000, sat: 18750000
[Log] pledge: 250000, sat: 31250000
[Log] pledge: 500000, sat: 62500000
[Log] pledge: 750000, sat: 67438565.126
[Log] pledge: 1000000, sat: 67438565.126
[Log] pledge: 2000000, sat: 67438565.126
```

## Rationale: how does this CIP achieve its goals?

Since a single entity SPO only has a certain amount of ADA they can pledge, they will eventually hit their saturation point no matter how many pools they create. The only way they can add more delegators is to increase their pledge. Once they run out of pledge and reach their saturation point, the delegators will have no choice but to move to another SPO and increase decentralisation.

In the above example, the base pledge of 500,000 ADA should be set as a parameter that can be adjusted in the future. E.g, if it is found that it is too low or too high to gain 100% saturation, it can be adjusted in the same way k can be adjusted.

One of the questions raised by the community was, will the lower limit stop the growth of small pools if it is set at a level where they can't reach the expected annual 5% return on ADA. This case could be handled a few ways, but the main aim would be to keep it at a sustainable amount for small pools.

1. The value is a percentage of k, such as 10%. This percentage could increase as needed, such as to 15% of k.
2. The value could be calculated based on the average of active stake compared to active pools. E.g, active stake = 23837 M / 3000 = saturation point of 7.94 M ADA

## Path To Active

### Acceptance Criteria

- [ ] The new relationship between pledge and saturation defined here is implemented in the Ledger and enacted through a hard-fork.

### Implementation Plan

- [ ] Agreement by the Ledger team as defined in [CIP-0084](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0084) under _Expectations for ledger CIPs_ including "expert opinion" on changes to rewards & incentives.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode)

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0040/README.md
---

- --
CIP: 40
Title: Collateral Output
Status: Active
Category: Ledger
Authors:
- Sebastien Guillemot <seba@dcspark.io>
- Jared Corduan <jared.corduan@iohk.io>
- Andre Knispel <andre.knispel@iohk.io>
Implementors: N/A
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/216
Created: 2022-02-10
License: CC-BY-4.0
- --

## Abstract

This document describes adding a new output type to transactions called Collateral Outputs.

## Motivation: why is this CIP necessary?

As of Alonzo, transactions that call Plutus smart contracts are required to put up collateral to cover the potential cost of smart contract execution failure. Inputs used as collateral have the following properties:

1. Cannot contain any tokens (only ADA)
2. Cannot be a script address
3. Must be a UTXO input
4. Must be at least some percentage of the fee in the tx (concrete percentage decided by a protocol parameter)
5. Can be the same UTXO entry as used in non-collateral tx input
6. Is consumed entirely (no change) if the contract execution fails during phase 2 validation
7. Is not consumed if phase phase 2 validation succeeds

Additionally, there cannot be more than *maxColInputs* (protocol parameter) inputs and the inputs have to cover a percentage of the fee defined by *collateralPercent* (protocol parameter)

However,

- Restriction #1 is problematic because hardcore dApp users rarely have UTXO entries that do not contain any tokens. To combat this, wallets have created a special wallet-dependent "collateral" UTXO to reserve for usage of collateral for dApps which is not a great UX.
- Restriction #6 is problematic because wallets want to protect users from signing transactions with large collateral as they cannot verify whether or not the transaction will fail when submitted (especially true for hardware wallets)

## Specification

If phrase-2 verification fails, we can send outputs to a special output marked as the collateral output.

There are two ways to create collateral outputs

1. Add collateral outputs as a new field inside the transaction. This change is similar to how collateral inputs were created a new field
2. Change the definition of outputs as `TxOut = Addr × Value × DataHash? × Source?` where source (optional for backwards compatibility) is an enum `0 = regular output, 1 = collateral output`.

Option #1 provides the best backwards compatibility because we don't expect phase-2 validation to be a common occurrence and so wallets that (due to not being updated) never check collateral outputs will still in the overwhelming majority of cases return the correct result.

Additionally, this requires updating the collateral requirement.

If no collateral output is specified (and therefore no tokens are in the collateral input), then we keep the old definition

```
ubalance (collateral txb ◁ utxo) ≥ quot (txfee txb * (collateralPercent pp)) 100
```

However, if collateral output is specified, then
1. Each collateral output needs to satisfy the same minimum ADA requirement as regular outputs
2. Collateral output needs to be balanced according to `sum(collateral_input) = sum(collateral_output) + collateral_consumed`
Where `collateral_consumed` is equal to the old formula (`quot (txfee txb * (collateralPercent pp)) 100`). Note that when collateral is consumed, any certificate, etc. in the transaction is ignored so they have no impact on the change calculation.

## Rationale: how does this CIP achieve its goals?

### Self-contained balancing

Some use-cases like hardware wallets, who do not have access to the content of the collateral inputs, cannot easily check if the collateral is balanced. Similar to how we specify an explicit fee as part of the transaction body to tackle this problem, the transaction body also needs a new field that explicitly specified how much collateral will be consumed in the case of phase-2 validation failure.

## Path to Active

### Acceptance Criteria

- [x] Fully implemented in Cardano as of the Vasil protocol upgrade.

### Implementation Plan

- [x] Passes all Ledger team requirements for desirability and feasibility.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0042/README.md
---

- --
CIP: 42
Title: New Plutus Builtin serialiseData
Status: Active
Category: Plutus
Authors:
- Matthias Benkort <matthias.benkort@iohk.io>
- Sebastian Nagel <sebastian.nagel@iohk.io>
Implementors:
- Michael Peyton Jones <michael.peyton-jones@iohk.io>
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/218
Created: 2022-02-09
License: Apache-2.0
## 


## Abstract

This document describes the addition of a new Plutus builtin for serialising `BuiltinData` to `BuiltinByteString`.

## Motivation: why is this CIP necessary?

As part of developing on-chain script validators for [the Hydra Head protocol](https://eprint.iacr.org/2020/299), we stumble across a peculiar need for on-chain scripts: we need to verify and compare digests obtained from hashing elements of the script's surrounding transaction.

In this particular context, those elements are transaction outputs (a.k.a. `TxOut`). While Plutus already provides built-in for hashing data-structure (e.g. `sha2_256 :: BuiltinByteString -> BuiltinByteString`), it does not provide generic ways of serialising some data type to `BuiltinByteString`.

In an attempt to pursue our work, we have implemented [an on-chain library (plutus-cbor)][plutus-cbor] for encoding data-types as structured [CBOR / RFC 8949][CBOR] in a _relatively efficient_ way (although still quadratic, it is as efficient as it can be with Plutus' available built-ins) and measured the memory and CPU cost of encoding `TxOut` **in a script validator on-chain**.

![](https://i.imgur.com/AtHE0p4.png)

The above graph shows the memory and CPU costs **relative against a baseline**, of encoding a `TxOut` using `plutus-cbor` in function of the number of assets present in that `TxOut`. The costs on the y-axis are relative to the maximum execution budgets (as per mainnet's parameters, December 2021) allowed for a single script execution. As can be seen, this is of linear complexity, i.e. O(n) in terms of the number of assets. These results can be reproduced using the [encoding-cost][] executable in our repository.

> Note that we have also calculated similar costs for ada-only `TxOut`, in function of the number of `TxOut` which is about twice as worse but of similar linear shape.

We we can see on the graph, the cost is manageable for a small number of assets (or equivalently, a small number of outputs) but rapidly becomes limiting. Ideally, we would prefer the transaction size to be the limiting factor when it comes to the number of outputs we can handle in a single validation.

Besides, in our discussions with the Marlowe team, we also discovered that they shared a similar problem when it came to serialising merkleized ASTs.

Underneath it all, it seems that it would be beneficial to have a new built-in at our disposal to serialise any Plutus `BuiltinData` to `BuiltinByteString` such that validators could leverage more optimized implementations and bytestring builders via built-ins than what's available on-chain, hopefully reducing the overall memory and CPU costs.

## Specification

### Function definition

We define a new Plutus built-in function with the following type signature:

```hs
serialiseData :: BuiltinData -> BuiltinByteString
```

### Binary data format

Behind the scene, we expect this function to use a well-known encoding format to ease construction of such serialisation off-chain (in particular, for non-Haskell off-chain contract codes). A natural choice of binary data format in this case is [CBOR][] which is:

1. Efficient;
2. Relatively simple;
3. Use pervasively across the Cardano ecosystem

Furthermore, the Plutus' ecosystem already provides [a _quite opinionated_ implementation of a CBOR encoder][encodeData] for built-in `Data`. For the sake of documenting it as part of this proposal, we provide here-below the CDDL specification of that existing implementation:

```cddl
plutus_data =
    constr<plutus_data>
  / { * plutus_data => plutus_data }
  / [ * plutus_data ]
  / big_int
  / bounded_bytes

constr<a> =
    #6.121([])
  / #6.122([a])
  / #6.123([a, a])
  / #6.124([a, a, a])
  / #6.125([a, a, a, a])
  / #6.126([a, a, a, a, a])
  / #6.127([a, a, a, a, a, a])
  ; similarly for tag range: #6.1280 .. #6.1400 inclusive
  / #6.102([uint, [* a]])

big_int = int / big_uint / big_nint
big_uint = #6.2(bounded_bytes)
big_nint = #6.3(bounded_bytes)

bounded_bytes = bytes .size (0..64)
```

> NOTE: The CDDL specification is extracted from the wider [alonzo_cddl specification][] of the Cardano ledger.

### Cost Model

The `Data` type is a recursive data-type, so costing it properly is a little tricky. The Plutus source code defines an instance of `ExMemoryUsage` for `Data` with [the following interesting note](https://github.com/input-output-hk/plutus/blob/37b28ae0dc702e3a66883bb33eaa5e1156ba4922/plutus-core/plutus-core/src/PlutusCore/Evaluation/Machine/ExMemory.hs#L205-L225):

> This accounts for the number of nodes in a `Data` object, and also the sizes of the contents of the nodes.  This is not ideal, but it seems to be the best we can do. At present this only comes into play for 'equalsData', which is implemented using the derived implementation of '==' [...].

We propose to re-use this instance to define a cost model linear in the size of data defined by this instance. What remains is to find a proper coefficient and offset for that linear model. To do so, we can benchmark the execution costs of encoding arbitrarily generated `Data` of various sizes, and retro-fit the cost into a linear model (provided that the results are still attesting for that type of model).

Benchmarking and costing `serialiseData` was done in [this PR](https://github.com/input-output-hk/plutus/pull/4480) according to this strategy. As the benchmark is not very uniform, because some cases of `Data` "structures" differ in CPU time taken to process, the linear model is used as an **upper bound** and thus conservatively overestimating actual costs.

## Rationale: how does this CIP achieve its goals?

- Easy to implement as it reuses existing code of the Plutus codebase;
- Such built-in is generic enough to also cover a wider set of use-cases, while nicely fitting ours;
- Favoring manipulation of structured `Data` is an appealing alternative to many `ByteString` manipulation use-cases;
- CBOR as encoding is a well-known and widely used standard in Cardano, existing tools can be used;
- The hypothesis on the cost model here is that serialisation cost would be proportional to the `ExMemoryUsage` for `Data`; which means, given the current implementation, proportional to the number and total memory usage of nodes in the `Data` tree-like structure.
- Benchmarking the costs of serialising `TxOut` values between [plutus-cbor][] and [cborg][] confirms [cborg][] and the existing [encodeData][]'s implementation in Plutus as a great candidate for implementing the built-in:

  ![](https://i.imgur.com/6GWrIHb.png)

  Results can be reproduced with the [plutus-cbor benchmark][].

### Alternatives

- We have identified that the cost mainly stems from concatenating bytestrings; so possibly, an alternative to this proposal could be a better way to concatenate (or to cost) bytestrings (Builders in Plutus?)

- If costing for `BuiltinData` is unsatisfactory, maybe we want have only well-known input types, e.g. `TxIn`, `TxOut`, `Value` and so on.. `WellKnown t => t -> BuiltinByteString`

### Backward Compatibility

- Additional built-in: so can be added to PlutusV1 and PlutusV2 without breaking any existing script validators. A hard-fork is however required as it would makes more blocks validate.

## Path To Active

### Acceptance Criteria

- [x] Release it as a backward-compatible change within the Vasil protocol upgrade

### Implementation Plan

- [x] Using the existing _sizing metric_ for `Data`, determine a costing function (using existing tooling / benchmarks? TBD)
- [x] The Plutus team updates plutus to add the built-in to PlutusV1 and PlutusV2 and uses a suitable cost function
- [x] The binary format of `Data` is documented and embraced as an interface within `plutus`. (see [cardano-ledger's CDDL specification](https://github.com/IntersectMBO/cardano-ledger/blob/faa40b812511bfb6592cdfbdd85fe560cbcaed43/eras/babbage/impl/cddl-files/babbage.cddl#L306-L311))

## Copyright

This CIP is licensed under [Apache-2.0](https://www.apache.org/licenses/LICENSE-2.0).

[CBOR]: https://www.rfc-editor.org/rfc/rfc8949
[plutus-cbor]: https://github.com/input-output-hk/hydra-poc/tree/a4b843a040897e45120cb63b666d965759091651/plutus-cbor
[cborg]: https://hackage.haskell.org/package/cborg-0.2.4.0
[encoding-cost]: https://github.com/input-output-hk/hydra-poc/tree/759fee84475f951aaf2f35acdb8ab82094ec5fbf/plutus-cbor/exe/encoding-cost/Main.hs
[alonzo_cddl specification]: https://github.com/input-output-hk/cardano-ledger/blob/aebd64e015ec0825776c256faed9d8632712beb0/eras/alonzo/test-suite/cddl-files/alonzo.cddl#L276-L296
[encodeData]: https://github.com/input-output-hk/plutus/blob/1f31e640e8a258185db01fa899da63f9018c0e85/plutus-core/plutus-core/src/PlutusCore/Data.hs#L108
[plutus-cbor benchmark]: https://github.com/input-output-hk/hydra-poc/tree/759fee84475f951aaf2f35acdb8ab82094ec5fbf/plutus-cbor/bench/Main.hs

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0045/README.md
---

- --
CIP: 45
Title: Decentralized WebRTC dApp-Wallet Communication
Status: Active
Category: Wallets
Authors:
- Fabian Bormann <fabian.bormann@cardanofoundation.org>
- Jaime Caso <jaime.caso@cardanofoundation.org>
Implementors:
- Fabian Bormann <fabian.bormann@cardanofoundation.org>
- Jaime Caso <jaime.caso@cardanofoundation.org>
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/395
Created: 2022-11-29
License: CC-BY-4.0
- --

## Abstract

We want to introduce a decentralized communication method between dApps and wallets based on WebTorrent trackers and WebRTC. This CIP also contains a proof of concept implementation injecting the wallet rpc methods into the dApps global window object similar to [CIP-0030](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0030).

## Motivation

In a decentralized ecosystem a communication between wallet-apps and dApps is still challanging. The inter-app communication on mobile devices does not directly allow remote procedure calls and is mostly restricted to Universal Links (iOS) or Deeplinks (Android). State-of-the-art solutions like WalletConnect tackle these problems using WebRTC communication which also works across devices, but requires a central signaling server to estalblish a WebRTC connection. In this CIP we want to introduce an architecture which uses WebTorrent trackers for the peer discovery to remove the need of this central component.

## Specification

### Establish a WebRTC Connection Using a Signaling Server (State-of-the-art approach)

```mermaid
sequenceDiagram
    dApp-->>+Signaling Server: Who am I?
    Signaling Server-->>dApp: You are <ip:port>
    Wallet-->>+Signaling Server: Who am I?
    Signaling Server-->>Wallet: You are <ip:port>
    dApp->>Wallet: messages
    Wallet->>dApp: messages
```

The data will be send peer to peer via a WebRTC data channel once the ip discovery has been finished. E.g. WalletConnect expects/provides a relay server URL to initialize the connection. While this method allows dApps to communitcate peer-to-peer with wallets it also leads to a [possible SPOF](https://twitter.com/walletconnect/status/1407279853943001088?lang=en).

### Establish a WebRTC Connection Using WebTorrent Tracker (Our approach)

```mermaid
flowchart LR
    subgraph dApp
        subgraph .torrent server
        end
    end

    subgraph Wallet[Wallet App]
    end

    dApp-->|.torrent| TrackerA[Tracker 1]
    dApp-->|.torrent| TrackerB[Tracker 2]
    dApp-->|.torrent| TrackerC[...]
    dApp-->|.torrent| TrackerD[Tracker n]

    dApp-->|.torrent| TrackerA[Tracker 1]
    dApp-->|.torrent| TrackerB[Tracker 2]
    dApp-->|.torrent| TrackerC[...]
    dApp-->|.torrent| TrackerD[Tracker n]

    dApp-->|Share public key| Wallet
```

Deep links, Universal Links, or even the clipboard could be utilized to share the identifier (public key) on the same device (in cases of a wallet based on web technology like Ionic). For sharing the identifier across different devices, QR codes would come into play. This method could be applied, for example, between a wallet mobile app and a dapp running on a PC, or vice versa. The wallet application would then initiate a query to a list of trackers using this distinct identifier in order to establish the WebRTC connection. After this process is completed, the data is transmitted peer-to-peer following the WebRTC standard, for instance, to invoke RPC calls.

```mermaid
flowchart LR
    Wallet-->|queries| TrackerA[Tracker 1]
    Wallet-->|queries| TrackerB[Tracker 2]
    Wallet-->|queries| TrackerC[...]
    Wallet-->|queries| TrackerD[Tracker n]

    Wallet-->|queries| TrackerA[Tracker 1]
    Wallet-->|queries| TrackerB[Tracker 2]
    Wallet-->|queries| TrackerC[...]
    Wallet-->|queries| TrackerD[Tracker n]

    subgraph dApp
        subgraph .torrent server
        end
    end

    Wallet<--Establish WebRTC data channel\n for peer to peer communication-->dApp
```

#### CIP-0013 Compliant Identifiers

The keys (public key and corresponding 64-byte secret key) are generated using a function that implements Ed25519. This function requires a (random) seed, which can also be stored and re-used to ensure that whenever a client employs a dApp or a Wallet, the same key pair is generated consistently, even if the browser or mobile app is restarted. The public key will be used as an identifier for the torrent-based peer discovery. When this identifier is shared through methods like QR codes or links, it needs to be compliant to the following Cardano Uri Scheme [(CIP-0013)](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0013):

| [scheme]	    | [authority] |	[version]  |	[data]    |
|---------------|-------------|------------|--------------|
| web+cardano:  |	//connect |	/v1	       | ?identifier= |

```
web+cardano://connect/v1?identifier=<public_key>
```
### Proof of Concept

The idea of using WebTorrent trackers instead of signaling servers for peer discovery was already mentioned in [Aug 2018 by Chris McCormick](https://mccormick.cx/news/entries/on-self-hosting-and-decentralized-software):
 "I've also been tinkering with WebTorrent. [...]
Working with this technology made me realise something the other day: it's now possible to host back-end services, or "servers" inside browser tabs. [...] So anyway, I've made this weird thing to enable developers to build "backend" services which run in browser tabs"

McCormick's idea has been developed and open sourced as a library called [bugout](https://github.com/chr15m/bugout/) (MIT).

For this proof of concept we wrote two small pieces of software:

- A html page aka the dApp
- An ionic react app (to target mutliple devices) aka the wallet app

The whole code is provided within this [demo implementation](https://github.com/fabianbormann/WebRTC-WebTorrent-tracker-communication-demo) that also contains a step-by-step guide.

#### dApp

The dApp consists of a standard HTML5 template including the following lines of code:

```html
<script src="https://chr15m.github.io/bugout/bugout.min.js"></script>
<script>
    var bugout = new Bugout({
        seed: localStorage["poc-server-seed"],
        announce: [
            'udp://tracker.opentrackr.org:1337/announce',
            'udp://open.tracker.cl:1337/announce',
            'udp://opentracker.i2p.rocks:6969/announce',
            'https://opentracker.i2p.rocks:443/announce',
            'wss://tracker.files.fm:7073/announce',
            'wss://spacetradersapi-chatbox.herokuapp.com:443/announce',
            'ws://tracker.files.fm:7072/announce'
        ]
        });
    localStorage["poc-server-seed"] = bugout.seed;

    var connected = false;
    bugout.on("connections", function (clients) {
        if (clients == 0 && connected == false) {
            connected = true;
            console.log("[info]: server ready");
            console.log(`[info]: share this address with your wallet app -> ${bugout.address()}`);
        }
        console.log(`[info]: ${clients} clients connected`);
    });

    bugout.register("api", function (address, args, callback) {
        const api = { version: args.api.version, address: address }

        for (method of args.api.methods) {
            api[method] = () => new Promise((resolve, reject) => {
                bugout.rpc(address, method, {}, (result) => resolve(result));
            });
        }

        window.cardano = window.cardano || {};
        window.cardano[args.api.name] = api;
        console.log(`[info]: injected api of ${args.api.name} into window.cardano`);
    });
</script>
```

#### Wallet App

The wallet app is a standard ionic react app built by the ionic cli:

```zsh
ionic start WalletApp blank --type=react
cd WalletApp
npm i bugout
```

The following lines of code were added to the index.tsx file:

```js
const bugout = new Bugout(
  "<HASH provided by the dAPP>", {
    announce: [
      'udp://tracker.opentrackr.org:1337/announce',
      'udp://open.tracker.cl:1337/announce',
      'udp://opentracker.i2p.rocks:6969/announce',
      'https://opentracker.i2p.rocks:443/announce',
      'wss://tracker.files.fm:7073/announce',
      'wss://spacetradersapi-chatbox.herokuapp.com:443/announce',
      'ws://tracker.files.fm:7072/announce'
    ]
  });

bugout.on("server", function() {
  console.log("[info]: connected to server")
  bugout.rpc("<HASH provided by the dAPP>", "api", {"api": {
    version: "1.0.3",
    name: 'boostwallet',
    methods: ["getRewardAddresses"]
  }});
});

bugout.register("getRewardAddresses", (address:string, args:any, callback:Function) => {
    callback(["e1820506cb0ce54ae755b2512b6cf31856d7265e8792cb86afc94e0872"]);
});
```

This example has a few restrictions:

1. bugout is currently not compatible with Webpack 5, so polyfills are not automatically included and a react-scripts eject is needed to add them to the webpack.config.js file

2. bugout does not directly provide type declarations. There are some declarations within a [PR](https://github.com/chr15m/bugout/pull/45), but they need to be adjusted (a few parameters are not mandatory) and added to a bugout.d.ts file.

### User Flow

```mermaid
sequenceDiagram
    dApp-->>Wallet: Share address using Deeplink /Universal Link / QR
    Wallet-->>Tracker List: Accept connection and start quering using the address
    Wallet-->>dApp: Once connected register RPC functions and share the API
    dApp-->>dApp: Inject the API to window.cardano[walletName]
    dApp-->>Wallet: Call RPC functions (e.g. getRewardAddresses)
    Wallet-->>Wallet: If needed bring app to foregrund (e.g. request signing)
    Wallet-->>dApp: Send response
```

### Security Aspects

We decided to spawn the server within the dApp to force the user to manually scan a QR code (using a wallet app) or accept an "Open with `<WalletAppName>`" ui dialog (in case of Universal Links or Deeplinks). This prevents the user from connecting the wallet to an unwanted dApp. Additionally we need to add  a few security checks to prevent a misusage of this method.

- The wallet app needs to verifiy the origin (address) of the RPC call
- dApps should ask the user for permission to inject the wallet names into the window.cardano object to prevent XSS attack (Maybe using a graphical representation of the wallet app address e.g. blockies)

## Rationale

The purpose of this CIP mainly consists of two parts. It addresses the current lack of dApp mobile support, but at the same time provides an even more decentralized alternative to state-of-the-art communication methods. To achieve this goal we have introduced a WebTorrent and WebRTC based architecture. To demonstrate a viable implementation, we have implemented a proof of concept which also shows how a rpc method injection like CIP-0030 might look like.

## Path to Active

### Acceptance Criteria

- [x] A library should be build to make it easy from dAPP and wallet side to implement the proposed communication method
- [x] The library target should be browser to avoid the need of manual polyfills
- [x] Mobile testing on different devices and operating systems needs to be done with a special focus to the wallet app running in background mode
- [x] Potential security issues and attack vectors need to be discussed in detail
1. We discussed potential security issues in the [CIP discussion](https://github.com/cardano-foundation/CIPs/pull/395#issuecomment-1669460822) and we
      implement an identicon solutin, but it is obviously an never ending task
- [x] A full reference implementation is needed to test if the entire user flow and at the same time provide this as a how-to for developers
1. Has been implemented here https://github.com/fabianbormann/cip-0045-demo-implementation

### Implementation Plan

- [x] Fork/Extend bugout to add webpack 5 and typescript support
- [x] Povide a general intermediate cardano-connect typescript library to provide
1. A check for mobile/desktop environment
2. Depending on the environment provide interfaces for CIP-0030 / and / or CIP-?
3. Add a full implementation of the server/client side code above to define a communication standard similar to CIP-0030 (getRewardAddresses, signData, signTx, ...)
- [x] Start discussions about security gaps within the proposed method with various developers and also look for research papers
- [x] Check if the wallet app also reacts to rpc calls in background mode on Android
- [x] Check if the wallet app also reacts to rpc calls in background mode on iOS
- [x] Implement the library within an example dApp:
1. Implemented in Cardano Ballot for the [summit voting 2023](https://voting.summit.cardano.org/)
2. Implemented by (SundeaSwap)[https://www.youtube.com/watch?v=mRpXIh-DyYM]
3. Implemented into the [cardano-connect-with-wallet core and react library](https://github.com/cardano-foundation/cardano-connect-with-wallet/tree/main)
4. Implemented in [walkinwallet](https://walkinwallet.com/)


### Updates

- The re-implementation of bugout that matches the expectations below is now available as [meerkat](https://github.com/fabianbormann/meerkat)

- A general [cardano-connect typescript library](https://github.com/fabianbormann/cardano-peer-connect) with 100% CIP-30 support has been provided

- The copy & paste [demo implementation](https://github.com/fabianbormann/cip-0045-demo-implementation) is ready to use

- Cardano Foundation's [connect-with-wallet](https://github.com/cardano-foundation/cardano-connect-with-wallet) component does include the dApp part of CIP-45 (via feature flag), so that dApp developers don't need to write a single line of code if they rely on this component

- The wording of the CIP-45 has been changed. Many thanks to [@jehrhardt](https://github.com/jehrhardt) for his valuable explanation and suggestions

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode)

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0049/README.md
---

- --
CIP: 49
Title: ECDSA and Schnorr signatures in Plutus Core
Status: Active
Category: Plutus
Authors:
- Koz Ross <koz@mlabs.city>
- Michael Peyton-Jones <michael.peyton-jones@iohk.io>
- Iñigo Querejeta Azurmendi <querejeta.azurmendi@iohk.io>
Implementors:
- MLabs
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/250
Created: 2022-04-27
License: Apache-2.0
- --

## Abstract

Support ECDSA and Schnorr signatures over the SECP256k1 curve in Plutus Core;
specifically, allow validation of such signatures as builtins.
These builtins work over ``BuiltinByteString``s.

## Motivation: why is this CIP necessary?

Signature schemes based on the SECP256k1 curve are common in the blockchain
industry; a notable user of these is Bitcoin. Supporting signature schemes which
are used in other parts of the industry provides an interoperability benefit: we
can verify signatures produced by other systems as they are today, without
requiring other people to produce signatures specifically for us. This not only
provides us with improved interoperability with systems based on Bitcoin, but
also compatibility with other interoperability systems, such as Wanchain and
Renbridge, which use SECP256k1 signatures for verification. Lastly, if we can
verify Schnorr signatures, we can also verify Schnorr-compatible multi or
threshold signatures, such as [MuSig2](https://eprint.iacr.org/2020/1261.pdf) or
[Frost](https://eprint.iacr.org/2020/852).

## Specification

Two new builtin functions would be provided:

- A verification function for ECDSA signatures using the SECP256k1 curve; and
- A verification function for Schnorr signatures using the SECP256k1 curve.

These would be based on [`secp256k1`](https://github.com/bitcoin-core/secp256k1),
a reference implementation of both kinds of signature scheme in C. This
implementation would be called from Haskell using direct bindings to C. These
bindings would be defined in `cardano-base`, using its existing `DSIGN`
interface, with new builtins in Plutus Core on the basis of the `DSIGN`
interface for both schemes.

The builtins would be costed as follows: ECDSA signature verification has
constant cost, as the message, verification key and signature are all
fixed-width; Schnorr signature verification is instead linear in the message
length, as this can be arbitrary, but as the length of the verification key and
signature are constant, the costing will be constant in both.

More specifically, Plutus would gain the following primitive operations:

- `verifyEcdsaSecp256k1Signature :: BuiltinByteString -> BuiltinByteString ->
  BuiltinByteString -> BuiltinBool`, for verifying 32-byte message hashes signed
  using the ECDSA signature scheme on the SECP256k1 curve; and
- `verifySchnorrSecp256k1Signature :: BuiltinByteString -> BuiltinByteString
- > BuiltinByteString -> BuiltinBool`, for verifying arbitrary binary messages
  signed using the Schnorr signature scheme on the SECP256k1 curve.

Both functions take parameters of a specific part of the signature scheme, even
though they are all encoded as `BuiltinByteString`s. In order, for both
functions, these are:

1. A verification key;
2. An input to verify (either the message itself, or a hash);
3. A signature.

The two different schemes handle deserialization internally: specifically, there
is a distinction made between 'external' representations, which are expected as
arguments, and 'internal' representations, used only by the implementations
themselves. This creates different expecations for each argument for both of
these schemes; we describe these below.

For the [ECDSA signature
scheme](https://en.bitcoin.it/wiki/Elliptic_Curve_Digital_Signature_Algorithm),
the requirements are as follows. Note that these differ from the serialization
used by Bitcoin, as the serialisation of signatures uses DER-encoding, which
result in variable size signatures up to 72 bytes (instead of the 64 byte encoding
we describe in this document).

- The verification key must correspond to the _(x, y)_ coordinates of a point
  on the SECP256k1 curve, where _x, y_ are unsigned integers in big-endian form.
- The verification key must correspond to a result produced by
  [``secp256k1_ec_pubkey_serialize``](https://github.com/bitcoin-core/secp256k1/blob/master/include/secp256k1.h#L394),
  when given a length argument of 33, and the ``SECP256K1_EC_COMPRESSED`` flag.
  This implies all of the following:
- The verification key is 33 bytes long.
- The first byte corresponds to the parity of the _y_ coordinate; this is
      `0x02` if _y_ is even, and `0x03` otherwise.
- The remaining 32 bytes are the bytes of the _x_ coordinate.
- The input to verify must be a 32-byte hash of the message to be checked. We
  assume that the caller of `verifyEcdsaSecp256k1Signature` receives the
  message and hashes it, rather than accepting a hash directly: doing so
  [can be dangerous](https://bitcoin.stackexchange.com/a/81116/35586).
  Typically, the hashing function used would be SHA256; however, this is not
  required, as only the length is checked.
- The signature must correspond to two unsigned integers in big-endian form;
  henceforth _r_ and _s_.
- The signature must correspond to a result produced by
  [``secp256k1_ecdsa_serialize_compact``](https://github.com/bitcoin-core/secp256k1/blob/master/include/secp256k1.h#L487).
  This implies all of the following:
- The signature is 64 bytes long.
- The first 32 bytes are the bytes of _r_.
- The last 32 bytes are the bytes of _s_.
  ```
      ┏━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━┓
      ┃ r <32 bytes> │ s <32 bytes>  ┃
      ┗━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━┛
      <--------- signature ---------->
  ```
For the Schnorr signature scheme, we have the following requirements, as
described in the requirements for [BIP-340](https://github.com/bitcoin/bips/blob/master/bip-0340.mediawiki):

- The verification key must correspond to the _(x, y)_ coordinates of a point
  on the SECP256k1 curve, where _x, y_ are unsigned integers in big-endian form.
- The verification key must correspond to a result produced by
  [``secp256k1_xonly_pubkey_serialize``](https://github.com/bitcoin-core/secp256k1/blob/master/include/secp256k1_extrakeys.h#L61).
  This implies all of the following:
- The verification key is 32 bytes long.
- The bytes of the signature correspond to the _x_ coordinate.
- The input to verify is the message to be checked; this can be of any length,
  and can contain any bytes in any position.
- The signature must correspond to a point _R_ on the SECP256k1 curve, and an
  unsigned integer _s_ in big-endian form.
- The signature must follow the BIP-340 standard for encoding. This implies all
  of the following:
- The signature is 64 bytes long.
- The first 32 bytes are the bytes of the _x_ coordinate of _R_, as a
      big-endian unsigned integer.
- The last 32 bytes are the bytes of `s`.
  ```
      ┏━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━┓
      ┃ R <32 bytes> │ s <32 bytes>  ┃
      ┗━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━┛
      <--------- signature ---------->
  ```

The builtin operations will error with a descriptive message if given inputs
that don't correspond to the constraints above, return `False` if the signature
fails to verify the input given the key, and `True` otherwise.

## Rationale: how does this CIP achieve its goals?

We consider the implementation trustworthy: `secp256k1` is the reference
implementation for both signature schemes, and is already being used in
production by Bitcoin. Specifically, ECDSA signatures over the SECP256k1 curve
were used by Bitcoin before Taproot, while Schnorr signatures over the same
curve have been used since Taproot.

An alternative approach could be to provide low-level primitives, which would
allow any signature scheme (not just the ones under consideration here) to be
implemented by whoever needs them. While this approach is certainly more
flexible, it has two significant drawbacks:

- It requires 'rolling your own crypto', rather than re-using existing
  implementations. This has been shown historically to be a bad idea;
  furthermore, if existing implementations have undergone review and audit, any
  such re-implementations would give us the same assurances as those that have
  been reviewed and audited.
- It would be significantly costlier, as the computation would happen in Plutus
  Core. Given the significant on-chain size restrictions, this would likely be
  too costly for general use: many such schemes rely on large precomputed
  tables, for example, which are totally unviable on-chain.

It may be possible that some set of primitive can avoid both of these issues
(for example, the suggestions in [this
CIP](https://github.com/cardano-foundation/CIPs/pull/220)); in the meantime,
providing direct support for commonly-used schemes such as these is worthwhile.

### Backward Compatibility

At the Plutus Core level, implementing this proposal induces no
backwards-incompatibility: the proposed new primitives do not break any existing
functionality or affect any other builtins. Likewise, at levels above Plutus
Core (such as `PlutusTx`), no existing functionality should be affected.

On-chain, this requires a hard fork.

## Path to Active

### Acceptance Criteria

- [x] Include tests of functionality with implementation.
- [x] Satisfaction of CIP-0035 requirements ([Additions to the Plutus Core Builtins](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0035#additions-to-the-plutus-core-builtins)) including costing.
- [x] Inclusion of SECP in Plutus core ([as of Valentine hard fork](https://docs.cardano.org/cardano-testnet/about/secp/)).

### Implementation Plan

- [x] Provide an implementation: by MLabs, [merged into Plutus](https://github.com/input-output-hk/plutus/pull/4368).

## Copyright

This CIP is licensed under [Apache-2.0](https://www.apache.org/licenses/LICENSE-2.0).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0050/README.md
---

- --
CIP: 50
Title: Pledge Leverage-Based Staking Rewards
Category: Ledger
Status: Proposed
Authors:
- Michael Liesenfelt <michael.liesenfelt@gmail.com>
- Ryan Wiley
- Rich Manderino [ECP]
- Stef M [RABIT]
- Wayne [OTG]
- Homer J (AAA)
- Chad [BBHMM]
Implementors: []
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/242
- https://forum.cardano.org/t/cip-shelley-s-basho-voltaire-decentralization-update/97685
- https://forum.cardano.org/t/minimum-pool-fees-with-a-brief-mention-of-k-changes/97002/82
- https://github.com/cardano-foundation/CIPs/pull/1042
Created: 4 April 2022 (original), 20 May 2025 (updated)
License: CC-BY-4.0
- --

## Abstract

Improving decentralization is critical for Cardano’s long-term health and growth. The current reward-sharing scheme (RSS) has yielded a stable but suboptimal level of decentralization. In hindsight, the original parameters *k* (the desired number of pools) and *a₀* (pledge influence factor) have not achieved their intended goals. Many stake pools with zero or minimal pledge manage to attract large delegations, undermining the Sybil-resistance that pledge was meant to provide (Liesenfelt, 2022). This proposal introduces a new pledge leverage parameter, *L*, into the RSS to more directly and fairly constrain such under-pledged pools. By capping rewards for pools with excessive stake relative to pledge, *L* penalizes severely under-pledged pools while having minimal effect on well-pledged or small pools. The adjusted scheme aligns economic incentives with decentralization: it redistributes stake toward well-pledged pools (increasing their rewards) and makes it more difficult for single entities to dominate via multiple pools. We present the motivation, specification, and rationale for this change, including simulations illustrating its impact. The goal is to significantly improve effective decentralization (approaching the theoretical *k* target) without unfairly harming small pool operators.

## Motivation: Why is this CIP necessary?

### Stagnating Decentralization and Sybil Concerns

Cardano’s block production is less decentralized in practice than the protocol parameter *k* = 500 implies. Although *k* intends to cap pool sizes and guide the network toward saturated equal-sized pools, in reality many of the top pools are operated by the same entities, which defeats the purpose (Fancee, 2022). For example, certain large exchanges run dozens of pools (e.g. 91 by Binance and 31 by Coinbase), collectively controlling roughly 12.59% of stake. This means the Nakamoto Coefficient – the minimum number of distinct entities required to control greater than 50% of block production is only about 25 as of epoch 559 (Balance Analytics, 2025). In other words, the *effective* decentralization is on the order of only a few dozen independent actors, far below the target of 500 pools (Liesenfelt, 2022).

As of epoch 560, **473 pools have zero pledge**, yet they still command **2,740,223,943 ADA** in delegated stake (Koios, 2025).  This concentration is fueled by Sybil behavior: many pools are essentially split nodes run by one operator. Pool-splitting hurts the network as a whole: operators running multiple low-pledge pools collect multiple fixed fees and margins, reducing rewards for delegators and squeezing out smaller, independent operators (Kiayias, 2020).

Under the current Reward Sharing Scheme (RSS) there is no explicit mechanism to limit leverage (stake-to-pledge ratio). An operator can split stake across many pools with zero or tiny amounts of pledge and still earn nearly full rewards on each pool. The saturation parameter *k* limits stake per pool but cannot limit how many pools a single entity operates. The pledge-influence factor a₀ was intended to discourage Sybil attacks, yet at its present value (0.3) it provides only a modest boost to heavily pledged pools. A private pool with high pledge might earn ~3.33 % annualized, while a zero-pledge public pool still earns ~2.6% which is only ~22% less. That small gap is *statistically unnoticed* by most delegators and is overwhelmed by normal epoch-to-epoch luck variance.

Crucially, it now takes a pledge of only about **53,870,468 ADA** spread across enough low-pledge pools to control more than 50% of total stake (Koios, 2025). This minimal capital requirement highlights how the current design enables highly leveraged Sybil attacks that erode decentralization and threaten network resilience.

### Limits of the Current Parameters (*k* and *a₀*)

Cardano’s decentralization parameters face a dilemma. The parameter *k* governs the target number of pools (decentralization threshold), and *a₀* is the traditional lever for Sybil resistance via pledge. In theory, setting a higher *a₀* would strongly reward pledged stake and penalize pools with little pledge, thereby discouraging Sybil attacks (multiple zero-pledge pools). However, increasing *a₀* would also widen the reward gap between large pledged pools and smaller community pools, harming small pool operators. As of February 2022, with *a₀*=0.3, roughly **1.25 billion ADA** concentrated in **19 fully pledged pools** earns maximal rewards (Fancee, 2022), giving wealthy operators a significant yield advantage. Raising *a₀* further would make this disparity even worse – effectively creating *two classes of stakeholders*, where large custodial actors can offer materially higher yields than individual operators. This is antithetical to Cardano’s egalitarian ethos and has proven politically difficult: the community not made any significant efforts to adjust *a₀* upward, likely knowing it would push many small pools out of the ecosystem.

Cardano's previous increase of k from 150 to 500 did result in a modest increase in the number of unique pools making blocks, but it also presented a tradeoff. Some multi-pool operators responded by creating additional low-pledge pools to capture more stake. A concern that has been shared about raising *k* is that it carries the risk of a proliferation of minimally-pledged pools run by the same entities, increasing total pool count with only a minor improvement in the actual diversity of operators. This dilution weakens Sybil resistance and keeps effective decentralization, as measured by k-effective, significantly below the target *k*.

To investigate these relationships, the Rewards Sharing Simulation (RSS) engine (University of Edinburgh: Blockchain Technology Lab, 2025) was used with minor modifications to model CIP-50 behavior. When sweeping *k* from 250 to 2,000 while holding *a₀* at a very low value (0.1), the headline “target pool” count grows, but the Nakamoto coefficient actually falls from about 142 at *k*=1,000 to just 116 at *k*=2,000. The hypothesis here is that pledge is too insignificant which leads big operators to simply split their stake into even more zero-pledge pools.  The on-chain diversity looks larger, yet real control collapses into fewer hands.

![Current Cardano Rewards Sharing Scheme](images/RSS1.png "Current Cardano Rewards Sharing Scheme")

(University of Edinburgh: Blockchain Technology Lab, 2025)

Moving a₀ to the opposite extreme tells demonstrates a different issue. At *a₀*=10, the simulation does push the Nakamoto coefficient upward to around 173 at *k*=1,000.  But there is a small drop to 169 at *k*=2,000.  The hypothesis here is that in this case, pledge has actually been made *too* significant such that only whales with large amounts of pledge are able to compete. This would presumably reshape the ecosystem into an elite upper tier and a struggling lower tier. In effect, we swap Sybil risk for wealth concentration which indirectly harms decentralization.

In summary, Cardano’s current RSS parameters are insufficient to prevent Sybil attacks and pool splitting: *k* sets an ideal pool count but doesn’t control for unique identities, and *a₀* is too weak (for political and economic reasons) to enforce meaningful pledge contributions. As a result, many pools with zero or tiny pledge and high delegated stake thrive – which deviates from the original decentralization goals of the Shelley design. An adjustment is needed to strengthen Sybil resistance without unfairly disadvantaging small pools.

### Objectives for an Improved Scheme

Any improvement to the reward scheme should adhere to the following high-level goals:

- **Fairness**: All stakeholders, from small delegators to whales and exchanges, should have the opportunity to earn *on average* the same *yield* (rewards per unit of stake). The system should avoid creating “VIP” pools with inherently better returns. In other words, no two classes of stakeholders.

- **Sybil Protection:** Running multiple pools *must* come at a cost. The scheme should *require* a meaningful pledge to support each pool’s delegated stake – not just mildly incentivize it. Creating many low-pledge pools should result in diminishing returns, removing the current advantage of pool splitting (Kiayias, 2020).

- **Decentralization:** The effective number of independent block-producing entities (k-effective) should converge towards the target *k*. The **Minimum Attack Vector (MAV)** (a.k.a. Nakamoto coefficient – number of entities to control 51%) should remain stable or increase.

- **Predictability and Simplicity:** The new reward formula should be as simple and transparent as possible, avoiding unnecessary complexity (e.g. no exotic curves or cryptic parameters). Operators and delegators should easily understand the cause-and-effect of the parameters on rewards.

This CIP addresses these goals by introducing a new pledge leverage parameter *L* and corresponding modifications to the reward formula. The motivation is to curb extreme leverage (high stake with low pledge) in a targeted way that primarily affects Sybil or multi-pool scenarios, while ordinary single-pool operators with reasonable pledge are largely unaffected (and may even benefit from a fairer playing field). In short, we seek to increase decentralization and security without sacrificing the openness and egalitarian nature of Cardano’s stake pool system.

## Specification

### Introducing the Pledge Leverage Parameter (*L*)

For reference we will share the relevant elements of the existing Cardano rewards formula here:

![Cardano Rewards Formula](images/RewardsFormula.png "Current Cardano Rewards Formula")

(Pledging and Rewards | Cardano Docs, n.d.)

- *R* - Epoch Rewards Pot
- *a₀* - Pledge Influence Factor
- *k*  - Target Number of Stake Pools
- *𝜎'* - Pool stake eligible for rewards
- *𝜎* = Pool’s total stake (pledge + delegated stake)
- *p'* = Pool pledge eligible for rewards
- *p* = Pool’s pledge
- *z0*  - Circulating ADA divided by k

We propose adding a new protocol parameter L (maximum pledge leverage) to the staking reward formula that fits in the third equation for calculating 𝜎'. The parameter L is defined as the maximum ratio of total stake to pledge that a pool can have before its rewards begin to plateau. In other words, L imposes a pledge-based saturation point for each pool.

- **Range of *L*:** 1 ≤ *L* ≤ 10,000 (dimensionless ratio). An *L* of 10,000 represents an extremely high allowed leverage (i.e. pledge need only be 0.01% of the stake), effectively similar to the status quo with a very weak pledge influence. An *L* of 1 represents a very strict requirement where a pool’s stake cannot exceed its pledge (100% pledge) if it is to earn full rewards.

- **Reward Formula Changes:** The reward calculation (pool reward R) is modified to incorporate a pledge leverage cap as a new parameter L. Each pool’s stake is effectively capped by its pledge according to L such that the eligible pool stake used in reward computations is:

![CIP-50 Formula](images/CIP50Formula.png "CIP-50 Formula")

 This means a pool is subject to *two* soft caps: one based on the global saturation (*k*) and one based on its own pledge.

- **Stake cap (k):** If a pool’s total stake 𝜎 exceeds *1/k* (the normal saturation point), it gets no extra rewards beyond that point (as in the current formula).

- **Pledge cap (L):** If 𝜎 exceeds *L·p* (i.e. the pool’s stake is more than *L* times its pledge), then any stake above that is not counted for rewards either. In effect, the pool is “leverage-saturated” if it tries to grow too large without sufficient pledge backing.

### Implications:
- A pool with **very low pledge** relative to its delegation will hit the pledge cap long before the *k* cap. For example, a pool pledging 1k ADA with *L*=100 can fully utilize at most 100k ADA of stake; beyond that, additional delegations won’t increase its rewards. If that pool has, say, 1 million ADA delegated, it will still only earn rewards as if it had 100k (plus its 1k pledge) – the rest of its stake is effectively providing zero rewards to its delegators. This creates a strong incentive for delegators to move to better-pledged pools if their pool is over-leveraged.

- A pool with **adequate pledge** will not be affected by the *L* cap until it reaches the normal saturation point. For instance, with *L*=100 and *k*=500, a pool needs about 1% pledge to utilize full stake up to *1/k*. In absolute terms, given the current saturation point around 75M ADA. At *L*=100, a pledge of ~750k ADA would be required to support 75M stake. Pools that meet this pledge-to-stake ratio can grow to saturation normally and will earn the same rewards as under the current formula.

- Pools with a **substantial pledge** relative to their stake will not be constrained by the pledge leverage parameter, *L*. These high-pledge pools will instead only reach the global network saturation point, which is determined by *1/k*. Furthermore, these well-pledged pools will also receive a small additional reward boost based on the pledge influence factor, *a₀*, in the existing reward formula, providing them with a modest advantage over pools that are approaching the leverage cap.

- If a pool’s stake is **below saturation**, it can still earn rewards on the full amount of stake as long as its pledge is at least *𝜎·L*. Smaller pools typically have smaller 𝜎, so the pledge needed is modest. For example, at *L*=100 and *k*=500, a pool with 500k ADA total stake would only need a pledge of ~5k ADA (1% of 500k) to not be limited by leverage, and it would offer delegators a competitive APY. This is a reasonable pledge threshold, demonstrating that **honest small pools are not penalized** by *L*. They can achieve comparable yields as large pools on a proportional basis.

- Crucially, a pool with **zero pledge will earn zero rewards** if it has any delegated stake. This is a clear break from the current scheme, where a pool with 0 pledge can still generate rewards for delegators (just slightly less). Under the new formula, some amount of pledge becomes absolutely mandatory.  This eliminates the “free rider” problem of profit-seeking pool operators undermining the network’s security.

## Rationale: How Does This CIP Achieve Its Goals?

### Penalizing Under-Pledged Pools (Sybil Deterrence)

The pledge leverage parameter *L* directly targets the worst-offending pools from a Sybil perspective – those with lots of stake but little skin in the game. By establishing an upper limit on stake relative to pledge, *L* introduces a hard economic penalty for over-leveraged pools. An operator can no longer create unlimited pools with minimal pledge and expect nearly full rewards on all of them. Any pool that is* severely under-pledged *will quickly hit the leverage cap and see its additional delegated stake yield zero incremental rewards.

This has an immediate discouraging effect on Sybil behaviors:

- A malicious actor attempting to control a large portion of stake by spinning up many pools would now need to proportionally increase pledge for each pool to make them effective. Otherwise, delegators will not join those pools (due to poor returns) or will abandon them once they realize additional stake is wasted. The economic cost of a Sybil attack thus becomes *linearly proportional* to the stake under control, thanks to the pledge requirement, rather than merely the fixed cost of running many nodes.

- Importantly, *L* removes the “free ride” incentive that previously existed where splitting one’s stake into multiple pools could circumvent the saturation limit. Under the new scheme, splitting stake across N pools without adding more pledge might split one’s effective rewards as well. For example, if an entity with a fixed total pledge *p* runs two pools, each pool will have at most *p·L* worth of effective stake. If they had combined everything in one pool, they’d have *p·L* effective stake (assuming that was the limiting factor). By splitting into two pools with pledge p/2 each, each pool caps at (p/2·L), for a combined effective stake of *p·L* – the same as one pool. Thus, there is no gain in total rewards by operating multiple pools under a fixed pledge budget. Any attempt to gain more by adding a new pool will just spread the pledge thinner and impose a lower cap per pool, keeping the total unchanged. This is a fundamental change: in the current scheme, splitting stake *can* increase an entity’s total rewards (because each pool can almost fully saturate and earn fees), but with *L* enforced, splitting yields *diminishing returns* unless accompanied by more pledge.

- Furthermore, *L* ensures pledge is absolutely mandatory for participation, aligning every operator’s incentives with the network’s security. Unlike today where many pools operate with almost zero pledge, under the new formula a pool with 0 pledge simply cannot earn rewards. Even a minimal pledge pool will severely limit its delegators’ returns if it grows, making it unappealing. This policy embodies the principle that if you want to benefit from community delegation, you must put up a bond to secure the network. It converts pledge from a weak nudge into a strict requirement, dramatically strengthening Sybil resistance.

In short, *L* provides an enforceable limit on Sybil actors and their maximum return on invested capital. It closes the loophole that allows large holders to game the system by simply multiplying pools. By tying rewards to pledge, the economic playing field is leveled: operators must either commit capital or accept a cap on their pool’s influence. This directly realigns incentives with decentralization and security.

### Minimal Impact on Honest Small Pools

A key advantage of using *L* (a leverage cap) instead of raising *a₀* (linear pledge influence) is that it minimizes negative effects on small or medium operators. The leverage cap kicks in when a pool tries to scale beyond what its pledge supports. Small pools, by definition, are not huge in stake, so most will not even reach the cap if *L* is tuned reasonably. For example, a pool with 1M ADA stake and 10k ADA pledge at *L*=100 is right at the cap (since 10k*100 = 1M); if it grows above 1M, then yes, it would need more pledge to maintain max rewards. But if that pool stays small or gradually grows alongside its pledge, its delegators experience no reduction in yield. In practice, many community pools already maintain a healthy pledge relative to their current delegation. These pools will see no change in their rewards from the new formula, except that they might actually become more attractive to delegators compared to under-pledged competitors.

By contrast, increasing *a₀* would hurt small pools immediately by lowering their rewards relative to large pools (regardless of the small pool’s size). The *L* parameter avoids that blunt harm by acting as a soft cut-off rather than a across-the-board tax. Pools earn normally up to a point, then flatline. Thus, a small pool that isn’t anywhere near saturation or leverage limits will function just as before (or better, since delegators may redistribute in its favor). The reward system aims to remain largely egalitarian: most pools that meet the pledge ratio can reach a similar maximum reward per stake. While increasing a₀ can create a slightly steeper reward gradient favoring high-pledge pools, introducing *L* creates a flat plateau for all pools that meet the leverage requirement while still maintaining a minor pledge benefit through *a₀* up to the global saturation point. This leverages the new *L* parameter to be more egalitarian while still incentivizing pledge, aiming to minimize the creation of disparate classes of participants.


### Discouraging Multi-Pool Splitting and Easing *k* Increases

One of the strongest arguments for the leverage-based approach is its effect on multi-pool operators (MPOs). In the current scheme, an MPO (like an exchange or a pool group) can slice their stake into *N* pools and, aside from the minor dilution of pledge, face little downside. They often benefit by collecting multiple fixed fees and saturating more pools. This has led directly to the situation of single entities running tens of pools and dominating the top ranks.

With *L*, the advantage of operating multiple pools is blunted for operators with too little pledge to support them. Since the rewards of each pool are limited by pledge, an MPO with a fixed total pledge won’t gain by spreading it thin. In fact, operating more pools could reduce their overall efficiency if their pledge per pool drops. The rationale is simple: if splitting your pledge into two pools halves the effective cap of each, you end up with the same total effective stake across your pools (and thus the same total rewards) as if you kept one pool fully pledged. Therefore, a rational operator under the new scheme would have no incentive to run more pools than necessary to accommodate their stake and pledge. We anticipate that many MPOs will consolidate or reduce the number of pools they run once *L* is in effect, because having many half-pledged pools would just advertise their leverage weakness and fail to increase their earnings.

This property is especially important when considering future increases in *k*. If Cardano wants to raise *k* (say to 1000 or beyond) to encourage further decentralization, *L* will ensure that those additional pool “slots” actually translate to new operators, not just existing MPOs multiplying again. As noted earlier, raising *k* alone without leverage control might invite big players to split stake into more pools. But if those players are constrained by pledge, they cannot effectively occupy all the new slots at full rewards. For instance, an exchange with limited pledge might fill some pools to the cap, but then either stop or create more pools that are underperforming. This leaves room (and incentive) for other operators to step in and operate the remaining pools, because delegators will seek out pools that *can* give full rewards. Thus, *L* works in synergy with raising *k*: it amplifies the decentralization impact of a larger *k* by preventing single entities from monopolizing the new capacity. In effect, *L* helps align *k* (the protocol’s target) with *k-effective* (the real outcome). We can safely pursue higher *k* values knowing that effective decentralization will follow more closely, rather than being undermined by Sybil pool clusters.

### Validating Behavior with Reward Sharing Simulation Engine

To test whether the leverage cap (*L*) proposed in CIP-50 delivers the intended balance between Sybil resistance and fairness, we re-ran the University of Edinburgh Reward-Sharing Simulation Engine with two configurations:

1. **Baseline** – the current Cardano rewards formula exploring *k = 250 → 2,000* and *a₀ ∈ {0.1, 0.3, 0.5, 1, 10}*.

2. **CIP-50** – identical *k* sweep but with *a₀ locked at 0.3* and *L ∈ {10, 100, 1 000, 10,000}*.

![Current RSS vs CIP-50](images/RSS2.png "Current RSS vs CIP-50")

(University of Edinburgh: Blockchain Technology Lab, 2025)

- *Decentralization is roughly the same or slightly improved.**
Where the baseline delivers 159 independent entities for the current parameter set (*k=500, a₀=0.3*), CIP-50 achieves **~160** at almost all settings of *L*. Thus the new rule neither harms decentralization nor relies on an unpalatable increase to *a₀*.

- *Network pledge rises slightly.**
Total pledged stake in the baseline hovers between **0.73 and 0.93** of the maximum possible, depending on *a₀* and *k*. CIP-50 nudges this figure upward at *a₀*=0.3, showing that a leverage ceiling likely would entice operators to increase their pledges rather than spawn extra pools.

- *Lower *L* values are preferred.**
While any finite *L* blocks the worst Sybil behavior, tighter caps yield the best Sybil protection without penalizing small-to-medium operators the way a large *a₀* increase would. The sweet spot lies at the lower end of the tested range (~10 to ~100), where Sybil protection is strongest yet the number of viable pools remains healthy.

- **L* primarily strengthens Sybil protection.**
In the baseline, lowering *a₀* to 0.1 caused a notable drop in Nakamoto coefficient at *k=2000*. To investigate this particular issue, further simulations were run with extremely low values of *a₀* with the current rewards formula and with CIP-50 and a flat *L* value of 10.

![Current RSS with low a0 values](images/RSS3.png "Current RSS with low a0 values")

![CIP-50 with low a0 values and L=10](images/RSS4.png "CIP-50 with low a0 values and L=10")

(University of Edinburgh: Blockchain Technology Lab, 2025)

The addition of *L* resulted in a notable improvement in the Nakamoto coefficient over the baseline rewards formula.  Taken together, the simulations support the hypothesis that CIP-50’s leverage cap will accomplish its primary mission of curbing Sybil pool proliferation.

## Path to Active

### Acceptance Criteria

- *Consensus on an Initial *L*** – An initial value of *L* must be agreed upon before hard-fork combinator (HFC) activation. The choice should balance Sybil protection against operational viability, drawing on empirical analyses (e.g., RSS results) and community feedback.

- *Endorsement by Technical Bodies** – The Cardano Parameter-Change Proposals (PCP) Committee and the Intersect Technical Steering Committee (TSC) should both recommend the proposal as technically sound and aligned with the protocol’s long-term roadmap.

- *CIP Editorial Approval** – Cardano CIP Editors must confirm that the specification is complete, unambiguous, and internally consistent with existing CIPs.

- *Stakeholder Concurrence** – A majority of stake pool operators (SPOs), ecosystem tooling maintainers, dReps, and other infrastructure providers must signal readiness to upgrade.

- *Governance Ratification** – The on-chain Hard-Fork Governance Action must pass the requisite dRep and Constitutional Committee thresholds, establishing legal-constitutional legitimacy and stakeholder support for the change.

### Implementation Plan

- *Community Deliberation (Preparation Phase)**

- Publish the finalized CIP-50 revision and present it to the PCP committee , TSC, CIP Editors, and wider community channels (Discord, X, Cardano Forum, etc.).

- Collect structured feedback—particularly on candidate values for *L*—and iterate until broad technical consensus emerges.

- *Specification & Code Integration (Development Phase)**

- Once an initial *L* is determined, integrate the leverage-cap logic into cardano-node and related libraries (ledger, CLI, wallet APIs).

- Submit pull requests to the canonical repositories; obtain code reviews from IOG, CF, and community contributors.

- Release a new protocol version that includes the changes made in this CIP.

- Use a dedicated pre-production testnet that mirrors main-net parameters but enforces the new *L* rule, allowing SPOs and exchanges to test end-to-end flows.

- *Readiness Sign-off (Testing Phase)**

- Require at least two weeks of uninterrupted testnet stability plus green results from regression and property-based tests.

- Monitor ecosystem dApps and tooling to confirm that major node implementations, explorers, wallets, and exchange integrations support the new rule set.

- *On-chain Governance (Ratification Phase)**

- File the Hard-Fork Governance Action on-chain with the agreed *L*, tagged for the next hard fork event.

- Mobilize dRep outreach to ensure quorum and super-majority passage; concurrently, the Constitutional Committee validates procedural compliance.

- *Hard-Fork Activation (Deployment Phase)**

- Upon successful vote, the hard fork event is automatically triggered upon epoch turnover.

- Monitor main-net metrics during the changeover epoch; provide real-time support for any late-upgrading SPOs.

## References

  AdaPulse. (2023, February 23). *MAV: The Safety Metric In Block Production Decentralization*. AdaPulse. Retrieved May 20, 2025, from https://adapulse.io/mav-the-safety-metric-in-block-production-decentralization

- Balance Analytics*. (2025, May 20). Average Resulting Decentralization. Retrieved May 20, 2025, from https://www.balanceanalytics.io/chartboards/decentralization

  Balance Analytics. (2025, May 20). *Group Stake Donut Chart*. Balance Analytics. Retrieved May 20, 2025, from https://www.balanceanalytics.io/chartboards/donut_shop

  Fancee, T. (2022, February 1). *CIP - Leverage-based Saturation and Pledge Benefit*. Cardano Forum. Retrieved May 20, 2025, from https://forum.cardano.org/t/cip-leverage-based-saturation-and-pledge-benefit/95632

  Kiayias, A. (2020, November 12). *The general perspective on staking in Cardano*. Input | Output. Retrieved May 20, 2025, from https://iohk.io/en/blog/posts/2020/11/13/the-general-perspective-on-staking-in-cardano/

  Kiayias, A. (2020, November 29). *Blockchain reward sharing - a comparative systematization from first principles*. Input | Output. https://iohk.io/en/blog/posts/2020/11/30/blockchain-reward-sharing-a-comparative-systematization-from-first-principles/

  Koios. (2025, May 28). *Pool List API*. https://api.koios.rest/

  Liesenfelt, M. (2022, April 4). *Pledge Leverage-Based Staking Rewards*. Cardano Improvement Proposals. Retrieved May 20, 2025, from https://cips.cardano.org/cip/CIP-0050

  Pledging and rewards | Cardano Docs. (n.d.). Cardano Docs. Retrieved June 4, 2025, from https://docs.cardano.org/about-cardano/learn/pledging-rewards

  University of Edinburgh: Blockchain Technology Lab. (2025, May 28). *Rewards Sharing Simulation Engine*. https://github.com/Blockchain-Technology-Lab/Rewards-Sharing-Simulation-Engine

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0052/README.md
---

- --
CIP: 52
Title: Cardano audit best practice guidelines
Status: Proposed
Category: Meta
Authors:
- Simon Thompson <simon.thompson@iohk.io>
Implementors: []
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/252
- https://github.com/cardano-foundation/CIPs/pull/344
- https://github.com/cardano-foundation/CIPs/pull/406
- https://github.com/cardano-foundation/CIPs/pull/560
Created: 2022-04-25
License: CC-BY-4.0
- --

## Abstract

These guidelines describe the audit process in general before setting out for DApp developers what information they will need to supply to auditors as part of the process. These are guidelines rather than requirements, and different auditors may engage differently, providing complementary services. The guidelines aim to establish a common baseline, including alternative ways of satisfying high-level requirements. Appendices provide (1) a glossary, (2) an audit FAQ, (3) a list of auditors for Cardano, and (4) a sample audit report.

## Motivation: why is this CIP necessary?

This CIP aims to promote the process of audit for DApps on Cardano, to improve the overall standard of assurance of DApps built for Cardano, and thus to contribute the improvement of the wider Cardano ecosystem.

## Specification

### Introduction

DApp users seek assurance about DApps that they wish to use. This comes from running automated tools on DApps and their components, as well as by audit of complete DApps. Secure evidence of these tool (level 1) and audit (level 2) results is provided by a certification service, and made available to users through a service such as a DApp store or wallet. In the longer term, results of formal verification (level 3) will also form part of the certification process.

DApp developers seek to drive adoption through their DApps being certified. Wallets and DApp stores are enhanced by providing certification services, and the wider Cardano ecosystem is strengthened through certification becoming widespread. Best practice standards can be developed by the audit and tooling communities, and systematised by the Cardano Foundation. This document is a first step in that direction.

- *Assurance can only ever be partial:** a DApp can be shown to have some good features and to avoid some bad ones, but this is not a guarantee that using a DApp will not have negative consequences for a user. This is not simply because tools and audits cannot give complete coverage, but also because attacks may come at lower (e.g. network, browser) or higher (e.g. crypto-economic) levels than addressed here.

These are guidelines rather than requirements, and different auditors may engage differently, providing complementary services. The guidelines aim to establish a common baseline, including alternative ways of satisfying high-level requirements. Appendices provide (1) a glossary, (2) an audit FAQ, (3) a list of auditors for Cardano, and (4) a sample audit report.

### Prerequisites

#### Key terms

- **(Smart) contract***: a program that runs on blockchain. In the case of Cardano, this will be a Plutus program that will contain Plutus Core components that run on blockchain as well as other code that runs off chain. All auditors will scrutinise on-chain code, some will examine off-chain code too.Some auditors might also provide orthogonal services eg. auditing a zero-knowledge protocol or an economic model.

- **DApp***: a complete “distributed” application that runs on blockchain. This will include off-chain code written in other languages, e.g. running in a browser, and will integrate or link with other services such as wallets and oracles.

- **Assurance***: the process of establishing various properties of systems, both positive (it does this) and negative (it doesn’t do this), with different degrees of certainty.

- **Audit***: the process of establishing assurance by means of manual examination of artefacts, systems, processes etc. Can involve some tooling, but it is a human-led process.

- **Tooling***: using automated processes to establish degrees of assurance of systems. Tools may be run on target DApps by parties providing such a service.

- **Evidence***: artefacts coming out of tooling and audit that can support assertions of assurance of systems. Examples include formal proofs, test suites, prior counterexamples and so on, as well as audit reports.

- **Certification***: the process and technology of giving to DApp end-users and developers secure evidence of forms of assurance about DApps and their component smart contracts. In the case of Cardano our approach is to provide information on  chain as transaction metadata.

- Note*: “certification” has also been used informally to cover the combined process of testing, audit, verification and providing evidence of these, as in “level 1 certification”. If it is felt that it is too confusing to use the term in both senses, then another term should be found, e.g. *evidencing* or *witnessing*.

- **Component***: a discrete part of the complete system supplied by a third party, such as a wallet, (part of) which will be run on the user’s system.

- **Service***: a part of the system, such as Blockfrost, that is provided by an online server accessed through a well-defined protocol.

- **Scope***: the parts of the DApp that are subject to audit. A repository may contain much more than Plutus code. Some audits will look at the complete app, some will just concentrate on on-chain transactions, others on the web interface around it. The possible scopes are divided in three main categories:
- On-chain
- Off-chain
- Context: the business and economic models for the DApp

- **Deployment***: once a DApp is developed it is deployed by submitting the relevant on-chain scripts onto the Cardano blockchain as transaction validator scripts. The scripts being deployed  might be the scripts that have been audited, or be instances of them in the case that the audited scripts are *parametric*.

#### Audit FAQ

##### *What is an audit?*

An audit is a comprehensive investigation of a DApp that provides an in-depth analysis on bugs, vulnerabilities, code quality and correctness of implementation. An audit does not necessarily analyse completed DApps and often will instead analyse a fragment of a DApp. For example many auditors will only analyse the on-chain code of a DApp.

##### *Who provides an audit?*

Audits are provided by companies that specialise in the area of the developed DApp. In this case it will be experts on the Cardano blockchain and Plutus smart contracts.

##### *How does audit work? What is the process?*

This first part of the process is tendering, where developers will need to provide preliminary information about their DApp to candidate auditors, as described below.

Once a contract is agreed, the next step in the process is to provide the auditors with the necessary information to perform the audit as set out in the guidelines below. Auditors may work with developers to ensure that the documentation and other materials for audit are prepared to the standard that is required for the audit to take place. Different auditors will have different requirements, but the guidelines below establish the minimum requirements.

Auditors will typically produce a first version of a report, which the developer can use as a guide to improving their DApp, before submitting changes to produce the final version of the code on which the final, published report is based. Once a DApp is audited, it will be deployed on the Cardano blockchain.

##### *When should a developer contact an auditor?*

A developer should contact an auditor when they have a final working version of a DApp or fragment of a DApp that they want to have audited. Once the contract is agreed, the developer will need to provide the auditor with a final version of the DApp for audit.

However, it is recommended to contact a potential auditor as early as possible because many auditors also provide consultation services for the design and development of the DApps. A developer is encouraged to contact the auditor as early as possible so as to mitigate any design issues which may be very hard if not impossible to fix. Early contact is also encouraged because securing a time slot with an auditor in advance shortens the DApp’s overall time to market, finally, early contact allows for scheduling and potentially avoids long delays between starting and completing an audit.

##### *What is audited when an audit takes place?*

All audits will examine the on-chain code that is used to validate transactions submitted to advance the smart contracts that constitute part of a DApp. Audit may also cover more of the code in a DApp, including on- and off-chain code written in Plutus, as well as other languages, e.g. JavaScript running in a browser.

##### *What guarantees can and cannot be given by an audit?*

An audit ***is not*** a guarantee of unbreakable security nor a way to offset trust or responsibility. An audit will provide an in-depth review of the source code of a DApp. The audit will provide a comprehensive code review detailing any found vulnerabilities, comments on the quality of code as well as an analysis of the implementation in regards to the supplied specification. An audit cannot guarantee that all possible vulnerabilities will be found or that the deployed DApp will perform as intended. This is especially true in the cases where an audit only looks at a fragment of a DApp or when a DApp has been updated.

##### *Who are the stakeholders involved in the audit process?*

DApps are used by *DApp users*, and built by *DApp developers*. Audit is performed by *audit companies*, using tools developed by themselves and other *tool developers*. Tooling can be run by *tool service providers*, and evidence of those and other results produced by *certification providers*. Audit is also impacted by components (e.g. light wallets) and services (e.g. blockfrost) provided by *ecosystem members*. Standards can be developed by *industry consortia* or *governance organisations* (e.g. the Cardano Foundation). In the widest sense, all *holders of Ada* stand to benefit from Cardano building an expectation that DApps are certified.

### Cardano auditors

| Audit company   | URL                                     | Contact email      | Public key      |
| -----------     | -----------                             | -----------        | -----------        |
| FYEO Inc.       | https://gofyeo.com/blockchain-security  | sales@gofyeo.com  | |
| Hachi           | https://hachi.one                       | team@hachi.one        | |
| MLabs           | https://mlabs.city                      | info@mlabs.city      | 64BC640B5454215D12165EEAEEFF303D2643ABA2 (PGP, ed25519) |
| Runtime Verification           | https://www.runtimeverification.com | contact@runtimeverification.com  | |
| Tweag                | https://www.tweag.io                  | sales@tweag.io                  | |
| Vacuumlabs (audits → Invariant0) | https://vacuumlabs.com/   | info@invariant0.com  | 16541FD112978F3C6D49E79881E6B1F9C0BC6BF9 (PGP, ed25519) |
| CertiK      | https://www.certik.com/products/smart-contract-audit/   | sales@certik.com  | |
| Invariant0  | https://invariant0.com/                     | info@invariant0.com  | 3C010EA5654D57D0AEF0E50B75D3AA3D42D52499 (PGP, ed25519) |


### Tendering
In order to provide a quote for audit, developers will need to supply
- A specification of the DApp to be audited (more details below).
- The scope of the audit.
- An estimate of the scale of the audit work, e.g. the number of lines in the on-chain code to be audited, or the code itself, in its current state of development.

### Submission
In order to be audited, developers will need to supply the following documentation.

#### *Specification / design documents*

Submitters shall provide specification and design documents that describe in a precise and unambiguous way the architecture, interfaces and requirements characterising the DApp.

The documentation shall identify the expected behaviour of the code, given without direct reference to the code itself. The description should also include high-level examples demonstrating use cases of the DApp. All assumptions about how the DApp will be used will be described. The documentation shall identify and document all the interfaces with other components and services.

Submitters might also wish to explain mitigating actions that they have taken to protect against potential failures and attacks to the DApp.

#### *On-Chain Specification*

The format of transactions accepted by the smart contracts should be specified using the template provided in the auxiliary document `Tx-spec.md`.

The document should clearly specify the properties to be satisfied by the smart contract.
- Properties shall be as extensive as possible and ideally would cover functionality, robustness, safety, liveness and efficiency, e.g. cost of execution, of the smart contract.
- Discussion should describe whether any of the properties addresses common vulnerabilities pertaining to Cardano blockchain or the smart contract domain in general.

A formal specification is recommended but not mandatory.

#### *Off-Chain Specification*

For off-chain analysis additional information should be provided for the components and services interfaced:
- For all interfacing components, a specification shall be given detailing their expected behaviour in relation to the DApp, including any assumptions, constraints and requirements of the component that are expected to hold by the DApp developers.
- It also shall be stated whether any of the interfacing components have been certified.

#### *Testing*

Ideally, submitters should submit a description of how the DApp has been tested, the results of the tests, and details of how those test results can be replicated.In particular:
- The test cases and their results shall be recorded or reproducible in a machine-readable format to facilitate subsequent analysis.
- Tests are to be performed for each targeted platform (browser, wallet etc).
- The identity, configuration and version of all test components involved shall be documented.
- The checksum and version of the DApp submitted for certification shall correspond to the same version making the subject of the test report.
- An evaluation of the test coverage and test completion should be provided.

In the case that off-chain code is included in the scope of the audit, testing should be able to assess the performance and robustness of the DApp against significant throughput, under substantial workload, and in the scenario of a DoS attack.

#### *Source code and version*

A final version of the source code should be provided that works with the use cases specified in the documentation. Information needs to be provided to allow the DApp to be built in an unambiguous and reproducible way, including any external components and services that the DApp uses.  This could be in the form of


- The URL for a commit to a repository.
- Build information for the DApp: a pure nix build is particularly suitable, since this will identify versions of  libraries, compilers, OS, etc.
- For the on-chain code for a DApp, the specific contracts to be audited.

#### *Versioning*

Versioning information needs to be given in a way that allows end users of a DApp to determine whether or not the version of the DApp that they are using is covered by certification information held on blockchain.


This can be done in a number of different ways, depending on the type of audit. These include:
1. The hash of a URL for a commit to a publicly-available repository.
2. A hash that identifies the files that contain the on-chain code that has been audited, e,g computing, from the root of the repository, listed in lexicographic order.

#### *Registration*

It is planned that DApps will be registered on the Cardano blockchain. This is currently under discussion. Once that discussion has been settled, it will also be possible to provide on-chain evidence of audit, linked to a registered entity. The mechanism for this is described in a separate document which it is intended to make into another CIP. A current draft of that document is here: [Proof of Audit document](https://docs.google.com/document/d/1FvgX8QiGKVPv4c7HanZ92zwstD9U1amOf8eHvyIb1dI).

### Requirements for Auditors

#### *Responsibilities*
Auditors shall be able to carry out the following activities:
- Review the requirement specification document against the intended environment and use so as to:
- Identify any inconsistencies, security flaws or incomplete requirements
- Identify any implicit assumptions and whether they are justifiable or not
- Evaluate the adequacy of strategies applied by the submitter to guarantee the consistency, correctness and completeness of the requirements
- Identify a threat model to guarantee that any identified mitigations are indeed appropriate against a list of possible vulnerabilities for Cardano smart contracts, and which is currently being finalised.
- The source code shall be audited by manual and/or automated means. In particular,
- The source code shall be reviewed against the requirements to ensure that all of these are properly taken into account and completely fulfilled.
- The adequacy of the source code documentation and traceability with the requirements shall be assessed.
- The source code shall be free from coding patterns/programming mistakes that may introduce exploitable vulnerabilities/failures leading to security issues.
- Produce a detailed audit report describing scope, methodology, and results categorised by severity. In particular,
- Any discrepancies, deviations or spotted vulnerabilities shall be described and classified with an appropriate severity level. Recommendations to rectify the identified deficiencies shall also be provided whenever appropriate.
- When automated tools are used as a replacement for manual review/code inspection, they shall be documented or referenced. Note that it’s the responsibility of the auditor to ensure that such tooling may not exhibit potential failures that can adversely affect the review outcome.
- Any strategies/methodologies used to assess the consistency, correctness and completeness of the requirements shall also be documented or referenced.

#### *Key competencies*

Auditors shall provide credentials for the following competencies:
- They shall have an in-depth knowledge of the syntax and semantics of the smart contract language to be audited, the underlying blockchain technology and associated computation and cost models.
- They shall be competent in the strategies and methods used to elaborate threat models.
- They shall be competent in assessing the suitability of methods (or combination of methods) used to justify the consistency, correctness and completeness of requirements against the list of common vulnerabilities pertinent to the smart contract domain and to guarantee (as far as possible) the absence of security flaws in the design.
- They shall be competent in various test and verification methods and have solid background in the various test coverage criteria (i.e., statement, data flow, branching, compound condition, MC/DC and Path).
- They shall also be able to assess whether the set of test cases produced for each specific test objective/property are sufficient enough to cover all the possible functional cases.
- They shall have analytical and critical thinking ability pertaining to the:
- deployment and execution of smart contracts on the underlying blockchain technology;
- Potential attacks or sequence of events relative to the smart contract’s logic that may lead to an unsafe state or invalidate some of the fundamental properties of the contract.
- They shall be able to judge the adequacy of the justifications provided by submitters w.r.t., development processes (e.g., requirement elicitation techniques, threat models, test objectives and test cases, coding standard, quality management, etc) for Level 2 certification.

#### *Disclosure*
Disclosure
It is common – but not universal – practice for disclosure/publication of audit report, for example as a part of a responsible disclosure policy. A typical policy would be to publish a report after a certain period (e.g. 30-90 day) or at the point that a DApp goes live, whichever is earlier.

## Rationale: how does this CIP achieve its goals?

These guidelines are the result of a process of discussion between IOG staff and members of the audit and academic communities over a series of online meetings in February and March 2022. Audit organisations involved include Tweag, WellTyped, Certik, Runtime Verification, BT Block, MLabs, Quviq and Hachi/Meld, all of which supported the guidelines outlined here.

## Path to active

### Acceptance Criteria

- [ ] Evidence that Cardano audits are being performed according to this proposed standard, by reference to specific audit(s) citing CIP-0052 and containing these audit elements.

### Implementation Plan

- [x] Initial set of Cardano auditors provided with CIP, with others added afterward along with contact information and verification keys.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode)


---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0054/README.md
---

- --
CIP: 54
Title: Cardano Smart NFTs
Status: Proposed
Category: Tokens
Authors:
- Kieran Simkin <hi@clg.wtf>
Implementors:
- Kieran Simkin
Discussions:
- https://forum.cardano.org/t/cip-draft-cardano-smart-nfts/100470
- https://github.com/cardano-foundation/CIPs/pull/263
Created: 2022-05-18
License: CC-BY-4.0
- --

## Abstract

This CIP specifies a standard for an API which should be provided to Javascript NFTs, it also defines some additions to the 721 metadata standard defined in [CIP-0025](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0025) which allow an NFT to specify it would like to receive certain current information from the blockchain.

## Motivation: why is this CIP necessary?

Currently if an NFT creator wishes to change or otherwise “evolve” their NFT after minting, they must burn the token and re-mint. It would be very nice if the user were able to modify their NFT simply by sending it to themselves with some extra data contained in the new transaction metadata. This would allow implementation of something like a ROM+RAM concept, where you have the original immutable part of the NFT (in Cardano’s case represented by the original 721 key from the mint transaction), and you also have a mutable part – represented by any subsequent transaction metadata.

It would also be nice to be able to retrieve data that has been previously committed to the blockchain, separately to the NFT which wishes to access it. This would be useful for retrieving oracle data such as current Ada price quotes – as well as for allowing an NFT to import another NFT’s data.

Further to this - for on-chain programatically generated NFTs, it makes sense to mint the code to render the NFT as one token, and then have the individual NFTs contain only the input variables for that code. This CIP specifies an additional metadata option which specifies that an NFT should be rendered by another token - this will massively reduce code duplication in on-chain NFTs.

This combination of functionality enables many exciting new possibilities with on-chain NFTs.

## Specification

### The Metadata

Minting metadata for Smart NFTs – based on the existing [CIP-0025](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0025) standard:

```
{
	"721": {
		"<policy_id>": {
			"<asset_name>": {
				"name": <string>,

				"image": <uri | array>,
				"mediaType": "image/<mime_sub_type>",

				"description": <string | array>,

				"files": [{
					"id": <string>
					"name": <string>,
					"mediaType": <mime_type>,
					"src": <uri | array>,
					<other_properties>
				}],

				"uses": {
					"transactions": <string | array>,
					"tokens": <string | array>,
					"renderer": <string>
				}
			}
		},
		"version": "1.0"
	}
}
```

Here we have added the “uses” key – any future additions to the Smart NFT API can be implemented by adding additional keys here.
We've also added an additional field to the files array - this is to specify a unique identifier to enable the files to be referenced by the Javascript API below.

#### The transactions key

To enable evolving NFTs where the NFT monitors a transaction history and changes in response to new transaction metadata, we define a sub-key called “transactions”, which can contain either a string or an array, specifying the tokens or addresses the NFT wishes to receive the transaction history for. These transaction histories will be provided to the NFT via the Javascript API detailed below.

We also define a special keyword “own” which can be used to monitor the NFT’s own transaction history. So if we wish to create an “evolvable” NFT that can respond to additions to its own transaction history, the “uses” key within the metadata would look like:

```
	"uses": {
		"transactions": "own"
	}
```

If you wanted to create an evolving NFT which monitors its own transaction history, as well as that of an external smart contract address, the metadata would look like this:

```
 	"uses": {
		"transactions": [
			"own",
			"addr1wywukn5q6lxsa5uymffh2esuk8s8fel7a0tna63rdntgrysv0f3ms"
		]
	}
```
Finally, we also provide the option to receive the transaction history for a specific token other than the NFT itself (generally this is intended to enable import of Oracle data or control data from an external source – although monitoring an address transaction history could also be used for that).

When specifying an external token to monitor, you should do so via the token’s fingerprint as in this example:

```
 	"uses": {
		"transactions": [
			"own",
			"asset1frc5wn889lcmj5y943mcn7tl8psk98mc480v3j"
		]
	}
```

#### The tokens key

To enable modifier tokens - (that is, a token which you can hold alongside a Smart NFT which changes the Smart NFT's appearance or behaviour in some way); we provide a way for the Smart NFT to monitor the tokens held by a specific address. Similarly to the “transactions” key, the “tokens” key will also accept either a string or array.

In this case we define the “own” keyword to mean the tokens held by the same stake key that currently holds the Smart NFT itself.

For example, to create an evolvable NFT which also supports modifier tokens, the “uses” block would look like this:

```
	"uses": {
		"transactions": "own",
		"tokens": "own"
	}
```

We could also monitor a particular smart contract address, for example if we wanted to see how many tokens were listed for sale on a marketplace. The following example creates an NFT that supports modifier tokens and also monitors the tokens held by a script address:

```
	"uses": {
		"tokens": [
			"own",
			"addr1wywukn5q6lxsa5uymffh2esuk8s8fel7a0tna63rdntgrysv0f3ms"
		]
	}
```

#### The renderer key

The idea behind the renderer key is to reduce code duplication in on-chain Javascript by moving the generative code part of the project into a single asset which is minted once in the policy. Each individual NFT within a project is then just a set of input parameters to the generative script - this totally removes the need to fill the metadata of every mint transaction with encoded HTML and Javascript, as is the case with many on-chain Javascript NFTs now.

When a Smart NFT is encountered which specifies another asset as the renderer, the site rendering the NFT should look-up the referenced asset and render that - the rendering token will then be responsible for reading the appropriate information via the Javascript API below and changing its appearance and/or behaviour based on that. In its simplest form, the rendering token could simply read the current token fingerprint and use that to seed a random number generator - this would enable a generative project to mint NFTs without even changing anything in the metadata and still have the renderer change its appearance for each one. In practice though, it's probably cooler to put actual traits like "colour scheme" or "movement speed" into the metadata and then have the renderer change its behaviour based on that.

Via the Javascript API, the rendering token will always receive the properties of the child token which specified it as its renderer. This means if you wish to use a renderer with a token which also evolves based on its own transaction history, you will need to specify both "renderer" and "transactions" keys within the child token, and within the renderer token you do not need to specify these keys.

For example, to create a Smart NFT which is rendered by another token, and is also evolvable based on its own transaction history, the "uses" key would look like this:

```
	"uses": {
		"transactions": "own",
		"renderer": "asset1frc5wn889lcmj5y943mcn7tl8psk98mc480v3j"
	}
```

### The Javascript API

When an on-chain Javascript NFT is rendered which specifies any of the metadata options above, the website / dApp / wallet which creates the `<iframe>` sandbox, should inject the API defined here into that `<iframe>` sandbox. It is worth saying that the wallet dApp integration API from [CIP-0030](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0030) should probably not be exposed inside the sandbox, to prevent cross-site-scripting attacks.

The Paginate data type along with APIError and PaginateError are copied directly from [CIP-0030](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0030) and these functions should operate in a similar manner to that API.

It is recommended that the Smart NFT API not be injected for every NFT – only the ones which specify the relevant metadata - this is an important step so that it’s clear which NFTs require this additional API, and also to enable pre-loading and caching of the required data. We are aiming to expose only the specific data requested by the NFT in its metadata – in this CIP we are not providing a more general API for querying arbitrary data from the blockchain.

- There is potentially a desire to provide a more open-ended interface to query arbitrary data from the blockchain – perhaps in the form of direct access to GraphQL – but that may follow in a later CIP – additional fields which could be added to the `uses: {}` metadata to enable the NFT to perform more complex queries on the blockchain.*

Although an asynchronous API is specified – so the data could be retrieved at the time when the NFT actually requests it – it is expected that in most instances the site which renders the NFT would gather the relevant transaction logs in advance, and inject them into the `<iframe>` sandbox at the point where the sandbox is created, so that the data is immediately available to the NFT without having to
perform an HTTP request.

### `cardano.nft.fingerprint: String`

The fingerprint of the current token - in the case where we're rendering a child token, this will be the fingerprint of the child token.

### `cardano.nft.metadata : Array`

The content of the 721 key from the metadata json from the mint transaction of the current NFT - if we are rendering on behalf of a child NFT, this will be the metadata from the child NFT.

### `cardano.nft.getTransactions( string which,  paginate: Paginate = undefined ) : Promise<Object>`

Errors: `APIError`, `PaginateError`

The argument to this function should be either an address, token fingerprint or the keyword “own”. It must match one of the ones specified via the `transactions` key in the new metadata mechanism detailed above.

This function will return a list of transaction hashes and metadata relating to the specified address or token. The list will be ordered by date with the newest transaction first, and will match the following format:

```
 {
	"transactions": [
		{
			"txHash":  "1507d1b15e5bd3c7827f1f0575eb0fdc3b26d69af0296a260c12d5c0c78239e0",
			"metadata": <raw metadata from blockchain>,
			"datum": <the datum from the UTXO holding the token, if set>
		},
		<more transactions here>
	],
	"fetched": "2022-05-01T22:39:03.369Z"
}
```
For simplicity, we do not include anything other than the txHash and the metadata – since any other relevant details about the transaction can always be encoded into the metadata, there is no need to over-complicate by including other transaction data like inputs, outputs or the date of the transaction etc. That is left for a potential future extension of the API to include more full GraphQL support.

### `cardano.nft.getTokens( string address, paginate: Paginate = undefined ) : Promise<Object>`

Errors: `APIError`, `PaginateError`

This function accepts either an address or the keyword “own” as its argument - it must match one of the ones specified via the the `tokens` key in the new metadata mechanism detailed above.

This function will return a list of the tokens held by the address specified in the argument, or held by the same stake key as the current token in the case of the “own” keyword.

```
 {
	"tokens": [
		{
			"policyID": "781ab7667c5a53956faa09ca2614c4220350f361841a0424448f2f30",
			"assetName": "Life150",
			"fingerprint": "asset1frc5wn889lcmj5y943mcn7tl8psk98mc480v3j",
			"quantity": 1,
			"datum": <the datum from the UTXO holding the token, if set>
		},
		<more tokens here>
	],
	"fetched": "2022-05-01T22:39:03.369Z"
}
```

### `cardano.nft.getFileURL( string id = null, string fingerprint = null ) : Promise<String>`

Errors: `APIError`

This function provides access to the contents of files listed in the `files[]` array for this NFT - if the NFT is rendering on behalf of another NFT, the files arrays from both should be merged, with the child NFT items overwriting the rendering NFT, in the case of ID conflicts.

The first argument specifies which entry from the files array should be retreived - if this argument is null, then the NFT's default image should be returned, which will typically come from the NFT's `image` metadata field rather than the files array.
The second argument allows you to specify which token's files to search - it should either be the token itself (either the child token or the rendering token, in the case of tokens with a separate renderer). In the case where an NFT also uses the `tokens` part of this API, then the getFileURL() function will also allow you to specify any one of the fingerprints returned by the getTokens() query.

The URL returned by this function should be in a format that is accessible from within the `<iframe>` sandbox - perhaps using `window.URL.createObjectURL()` to generate a static URL from raw data if necessary.

## Rationale: how does this CIP achieve its goals?

Currently the NFT sites which support on-chain Javascript NFTs do so by creating a sandboxed `<iframe>` into which they inject the HTML from the NFT’s metadata. From within this sandbox it is not possible to bring-in arbitrary data from external sources – everything must be contained within the NFT, or explicitly bought into the sandbox via an API.

This proposal suggests an addition to the 721 metadata key from [CIP-0025](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0025), to enable an NFT to specify that it would like to receive a particular transaction history accessible to it from within the sandbox – thus defining it as a “Smart NFT”.

In tandem with the additional metadata, we also define a standard for the Javascript API which is provided to the NFT within the sandbox.

## The Smart NFT toolchain

This CIP now has a [reference implementation](https://clg.wtf/policy/smart-life) which consists of a [front-end React control](https://github.com/kieransimkin/SmartNFTPortal) which takes care of rendering an NFT - it creates the sandbox and exposes the CIP54 Javascript API to it. This works in tandem with a [backend library](https://github.com/kieransimkin/libcip54) which takes care of reading the necessary data from a dbsync instance and making it available for the front end control to render.

There is also an [integrated development environment](https://nft-playground.dev/) made available to enable realtime experimentation and debugging of Smart NFTs without having to repeatedly mint new tokens.

Furthermore, [a complete visual blockchain explorer](https://clg.wtf/) has been made available which utilises libcip54 and SmartNFTPortal and fully supports the reference implementation of this standard.

The first CIP54 collection has been minted on mainnet under the policy ID `1eaf3b3ffb75ff27c43c512c23c6450b307f138281efb1d690b84652` and is [available to see here](https://clg.wtf/policy/smart-life). [A number of other instructive example NFTs](https://nft-playground.dev/examples) have also been provided as part of the NFT Playground website.

[Libcip54](https://github.com/kieransimkin/libcip54), [SmartNFTPortal](https://github.com/kieransimkin/SmartNFTPortal), [Cardano Looking Glass](https://github.com/kieransimkin/looking-glass) and the [NFT Playground](https://github.com/kieransimkin/cip54-playground) are all opensource - pull requests are welcome!

## Path to Active

### Acceptance Criteria

- [ ] Identify at least 1 pair of wallets, minting services, CLIs, or software utilities from separate providers which do at least 1 each of:
- [ ] creating NFTs according to this specification
- [ ] rendering NFTs according to this specification

### Implementation Plan

- [X] Provide a [reference](https://github.com/kieransimkin/libcip54) [implementation](https://github.com/kieransimkin/smartnftportal) of this scheme, which illustrates both:
- [X] [a means of creating a "Smart NFT"](https://nft-playground.dev/)
- [X] [a means of rendering it](https://clg.wtf/)
- [ ] Update this specification to match the new features added in the reference implementation.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0055/README.md
---

- --
CIP: 55
Title: Protocol Parameters (Babbage Era)
Status: Active
Category: Ledger
Authors:
- Jared Corduan <jared.corduan@iohk.io>
Implementors:
- IOG
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/265
Created: 2022-05-19
License: Apache-2.0
- --

## Abstract

This CIP extends CIP-0028 to introduce a change to one of the Alonzo protocol parameters in the Babbage era, namely `lovelacePerUTxOWord`.
We propose to have this updateable parameter be based on bytes instead of words (eight bytes).
Additionally, two Alonzo era protocol parameters were removed, namely the decentralization parameter and the extra entropy parameter.

## Motivation: why is this CIP necessary?

### Lovelace Per UTxO Byte

Since the Shelley era, there has been an minimum number of lovelace requirement for every unspent transaction output.
This requirement acts like a deposit, guarding the network from dust (the proliferation of small-valued unspent transaction outputs).
Initially it was a constant value, since the Shelley era UTxO were simple and quite uniform.
Starting in the Mary era, however, the constant value was replace with a
[formula](https://cardano-ledger.readthedocs.io/en/latest/explanations/min-utxo-mary.html)
to account for the variability in outputs that contained multi-assets.
The formula was [changed again](https://cardano-ledger.readthedocs.io/en/latest/explanations/min-utxo-alonzo.html)
in the Alonzo era.
Both the Mary and the Alonzo era formulas provide an upper bound on the size in memory of an unspent transaction output in the Haskell implementation.
We would like to simplify the formula to instead count the number of bytes in the CBOR serialization.

### Transitional Praos

Two Alonzo era protocol parameters need to be removed for the Babbage era, since they relate to `TPraos`.
Transitional Praos (named `TPraos` in the code base) is the addition of two features to
[Praos](https://iohk.io/en/research/library/papers/ouroboros-praosan-adaptively-securesemi-synchronous-proof-of-stake-protocol/),
which were added to provide a smooth transition from
[Ouroboros-BFT](https://iohk.io/en/research/library/papers/ouroboros-bfta-simple-byzantine-fault-tolerant-consensus-protocol).
In particular, Transitional Praos included an overlay schedule which could be tuned by the `d` parameter
(`d == 1` means that all the blocks are produced by the BFT nodes, `d == 0` means that none of them are).
It also included a way of injecting extra entropy into the epoch nonce.
The extra entropy feature was used precisely once, and was
[explained wonderfully](https://iohk.io/en/blog/posts/2021/03/29/the-secure-transition-to-decentralization)
by one of the original authors of the Praos paper.

The Babbage era removes both of the "transitional" features of TPraos, rendering the decentralization parameter
and the extra entropy parameter useless.

## Specification

The removal of the decentralization parameter and the extra entropy parameter is self explanatory.
We now describe the specification of the `coinsPerUTxOByte` parameter.

### Rename

The name of the protocol parameter is actually `coinsPerUTxOWord` in the Haskell implementation.
It should be renamed to `coinsPerUTxOByte`.

### Translation from the Alonzo era to the Babbage era

At the moment that the hard fork combinator translates the Alonzo era ledger state to the Babbage era,
the current value of `coinsPerUTxOWord` will be converted to

```
⌊ coinsPerUTxOWord / 8 ⌋
```

### The new minimum lovelace calculation

In the Babbage era, unspent transaction outputs will be required to contain _at least_

```
(160 + |serialized_output|) * coinsPerUTxOByte
```

many lovelace. The constant overhead of 160 bytes accounts for the transaction input
and the entry in the UTxO map data structure (20 words * 8 bytes).

## Rationale: how does this CIP achieve its goals?

We would like the formula for the minimum lovelace in a unspent transaction output
be simpler and easier to reason about by all users of the Cardano network, while at
the same time accounting for the size of the output.

### Backwards compatibility

The [translation](#translation-from-the-alonzo-era-to-the-babbage-era) section
explains how we will transition from the `coinsPerUTxOWord` parameter to the `coinsPerUTxOByte` parameter.
Starting in the Babbage era, update proposals that want to modify `coinsPerUTxOByte` must bear in mind
that the measurement is in bytes, not words.

The two protocol parameters that have been removed, `d` and `extraEntropy`, can no longer be used
in protocol parameter updates.

## Path to Active

### Acceptance Criteria

- [x] The Babbage ledger era is activated.
- [x] Documented parameters have been in operational use by Cardano Node and Ledger as of the Babbage ledger era.

### Implementation Plan

- [x] Babbage ledger era parameters are deemed correct by working groups at IOG.

## Copyright

This CIP is licensed under [Apache-2.0](https://www.apache.org/licenses/LICENSE-2.0).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0057/README.md
---

- --
CIP: 57
Title: Plutus Contract Blueprint
Status: Active
Category: Tools
Authors:
- KtorZ <matthias.benkort@cardanofoundation.org>
- scarmuega <santiago@carmuega.me>
Implementors:
- Aiken <https://aiken-lang.org>
- Plu-Ts <https://github.com/HarmonicPool/plu-ts>
- OpShin <https://github.com/OpShin>
- Lucid <https://lucid.spacebudz.io/>
- Mesh.js <https://martify.io/>
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/258
- https://discord.gg/yUkkhqBnyV
- https://github.com/aiken-lang/aiken/issues/972
Created: 2022-05-15
License: CC-BY-4.0
- --

## Abstract

This document specifies a language for documenting Plutus contracts in a machine-readable manner. This is akin to what [OpenAPI](https://swagger.io/specification) or [AsyncAPI](https://www.asyncapi.com/docs/specifications/v2.4.0) are for, documenting HTTP services and asynchronous services respectively. In a similar fashion, A Plutus contract has a binary interface which is mostly defined by its datum and redeemer.

This document is therefore a meta-specification defining the vocabulary and validation rules with which one can specify a Plutus contract interface, a.k.a **Plutus contract blueprint**.

## Motivation: why is this CIP necessary?

While publicly accessible, on-chain contracts are currently inscrutable. Ideally, one would want to get an understanding of transactions revolving around script executions. This is both useful to visualize and to control the evolution of a contract life-cycle; but also, as a user interacting with a contract, to ensure that one is authorizing a transaction to do what it's intended to. Having a machine-readable specification in the form of a JSON-schema makes it easier (or even, possible) to enable a wide variety of use cases from a single concise document, such as:

- Code generators for serialization/deserialization of Contract's elements
- Contract API Reference / Documentation, also automatically generated
- Extra automated transaction validation layers
- Better wallet UI / integration with DApps
- Automated Plutus-code scaffolding

Moreover, by making the effort to write a clear specification of their contracts, DApps developers make their contracts easier to audit (as they're able to specify the expected behavior).

## Specification

### Overview

This specification introduces the notion of a _Plutus contract blueprint_, as a JSON document which it itself a _JSON-schema_ as per the definition of given in [JSON Schema: A Media Type for Describing JSON Documents: Draft 2020-12](https://json-schema.org/draft/2020-12/json-schema-core.html).

Said differently, Plutus blueprints are first and foremost, valid JSON schemas (according to the specification linked above). This specification defines a core vocabulary and additional keywords which are tailored to the specification of Plutus contracts. Tools supporting this specification must implement the semantic and validation rules specified in this document.

Meta-schemas for Plutus blueprints (i.e. schemas used for validating Plutus blueprints themselves) are given [in annexe](./schemas/README.md).

A _Plutus contract blueprint_ is made of a single document describing one or more on-chain validators. By convention, the document is named `plutus.json` and should be located at the root of a project's repository to facilitate its discoverability.

### Document Structure

The document itself is a JSON object with the following fields:

| Fields                       | Description                                               |
| ---                          | ---                                                       |
| [preamble](#preamble)        | An object with meta-information about the contract        |
| [validators](#validators)    | An object of named validators                             |
| ?[definitions](#definitions) | A registry of definition re-used across the specification |

Note that examples of specifications are given later in the document to keep the specification succinct enough and not bloated with examples.

#### preamble

The `preamble` fields stores meta-information about the contract such as version numbers or a short description. This field is mainly meant for humans as a mean to contextualize a specification.

| Fields         | Description                                                                  |
| ---            | ---                                                                          |
| title          | A short and descriptive title of the application                             |
| ?description   | A more elaborate description                                                 |
| ?version       | A version number for the project.                                            |
| ?compiler      | Information about the compiler or framework used to produce the validator(s) |
| ?plutusVersion | The Plutus version assumed for all validators                                |
| ?license       | A license under which the specification and contract code is distributed     |

#### compiler

The `compiler` field is optional, but allows specifying metadata about the toolkit that produced the validator and blueprint.

| Fields   | Description                                                      |
| ---      | ---                                                              |
| name     | The name of the compiler/framework/tool that generated the file. |
| ?version | An optional version number in any format.                        |

#### validators

Validators are the essence of the blueprint. This section describes each validator involved in the contract (simple applications will likely have only a single validator). A validator is mainly defined by three things: a title, arguments (i.e parameters, redeemer and/or datum) and some compiled code. Parameters refer to compile-time arguments that can be applied to a validator template. They must be instantiated to produce a final compiled code as they are embedded in the code of the validator itself. This is often the case for validators that must hold on unique external nonce to produce unique hashes.

| Fields        | Description                                                                                                                     |
| ---           | ---                                                                                                                             |
| title         | A short and descriptive name for the validator                                                                                  |
| ?description  | An informative description of the validator                                                                                     |
| redeemer      | A description of the redeemer format expected by this validator                                                                 |
| ?datum        | A description of the datum format expected by this validator                                                                    |
| ?parameters   | A list of parameters required by the script in addition of the datum and redeemer                                               |
| ?compiledCode | The full compiled and cbor-encoded serialized flat script                                                                       |
| ?hash         | A blake2b-224 hash digest of the validator script, as found in addresses. Optional, but mandatory if `compiledCode` is provided |

##### redeemer, datum and parameters

`redeemer`, `datum` and `parameters` items all share the same schema structure. They must define a `schema` that describes how to construct valid on-chain values for each of these fields, and they also specify a purpose (`spend`, `mint`, `withdraw` or `publish`) that indicates in which context it can figure. The purpose is either a string, or an applicator `oneOf` that specifies multiple (distinct) purposes. Similarly, an argument is either an object as described below, or an applicator `oneOf` of such objects. In case where it is defined as an applicator, `purpose` values between objects must be strictly non-overlapping as they are used as discriminant in chosing a schema. This allows, for example, to define different redeemer schemas for different purposes.

| Fields       | Description                                                                                                      |
| ---          | ---                                                                                                              |
| ?title       | A short and descriptive name for the redeemer, datum or parameter                                                |
| ?description | An informative description of the redeemer, datum or parameter                                                   |
| ?purpose     | One of `"spend"`, `"mint"`, `"withdraw"` or `"publish"`, or a `oneOf` applicator of those                        |
| schema       | A _Plutus Data Schema_ using the core vocabulary defined below, or a `oneOf` applicator of _Plutus Data Schemas_ |

#### definitions

A set of extra schemas to be re-used as references across the specification.

### Core vocabulary

Plutus blueprints ultimately describes on-chain data value that can be found at the validator's interface boundaries. This means that while we would generally operate at the level of _Plutus Data_, the vocabulary covers in practice any of the possible Untyped Plutus Core (abbrev. UPLC) primitives that can appear at a validator's boundary (e.g. compile-time parameters). Any UPLC primitive is therefore represented as a schema with a `dataType` keyword. The possible values for `dataType` are detailed just below. In addition, and depending on the value of `dataType`, we may find additional keywords in the vocabulary.

| dataType      | UPLC Type  | Description                                                               |
| ---           | ---        | ---                                                                       |
| `integer`     | Data       | A signed integer at an arbitrary precision, wrapped as `iData`.           |
| `bytes`       | Data       | A bytes string of an arbitrary length, wrapped as `bData`.                |
| `list`        | Data       | An ordered list of Plutus data, wrapped as `listData`                     |
| `map`         | Data       | An associative list of Plutus data keys and values, wrapped as `mapData`. |
| `constructor` | Data       | A constructor with zero, one or many fields, wrapped as `constrData`.     |
| `#unit`       | Unit       | A builtin unit value (the unary constructor).                             |
| `#boolean`    | Boolean    | A builtin boolean value.                                                  |
| `#integer`    | Integer    | A builtin signed integer at an arbitrary precision.                       |
| `#bytes`      | ByteString | A builtin bytes string of an arbitrary length.                            |
| `#string`     | String     | A builtin UTF-8 text string.                                              |
| `#pair`       | ProtoPair  | A builtin pair of `Data` elements.                                        |
| `#list`       | ProtoList  | A builtin list of `Data` elements.                                        |

> **Warning**
>
> While they exist for completeness, frameworks are strongly discouraged to use any of the constructs starting with a `#` as they refer to Plutus Core builtins types used by the Plutus virtual machines but aren't meant to figure in outward-facing interfaces. Validators should, as much as possible, stick to `integer`, `bytes`, `list`, `map` and `constructor` (and any composition of those) for their binary interface.

Using these primitives, it becomes possible to represent the entire domain (i.e. possible values) which can be manipulated by Plutus contracts.

### Additional keywords

Similarly to JSON schemas, we provide extra validation keywords and keywords for applying subschemas with logic to further refine the definition of core primitives. Keywords allow to combine core data-types into bigger types and we'll later give some pre-defined definitions which we assume to be part of the core vocabulary and therefore, recognized by any tool supporting this standard.

When presented with a validation keyword with a malformed value (e.g. `"maxLength": "foo"`), programs are expected to return an appropriate error.

Beside, we define a _Plutus Data Schema_ as a JSON object with a set of fields depending on its corresponding data-type. When we refer to a _Plutus Data Schema_, we refer to the entire schema definition, with its validations and with the semantic of each keywords applied.

Unless otherwise specified, keywords are all considered optional.

Here below are detailed all the accepted keywords for each data-type.

#### For any data-type

> **Note** Keywords in this section applies to any instance data-type described above.

##### `dataType`

The value of this keyword must be a string, with one of the following value listed in the first column of the table above. This keyword is **optional**. When missing, the instance is implicitly typed as an opaque Plutus Data. When set, it defines the realm of other applicable keywords for that instance.

##### `title`

This keyword's value must be a string. This keyword can be used to decorate a user interface and qualify an instance with some short title.

##### `description`

This keyword's value must be a string. This keyword can be used to decorate a user interface and provide explanation about the purpose of the instance described by this schema.

##### `$comment`

This keyword's value must be a string. It is meant mainly for programmers and humans reading the specification. This keyword should be ignored by programs.

##### `allOf`

This keyword's value must be a non-empty array.  Each item of the array MUST be a valid _Plutus Data Schema_. An instance validates successfully against this keyword if it validates successfully against all schemas defined by this keyword's value.

##### `anyOf`

This keyword's value must be a non-empty array. Each item of the array must be a valid _Plutus Data Schema_. An instance validates successfully against this keyword if it validates successfully against at least one schema defined by this keyword's value.

##### `oneOf`

This keyword's value must be a non-empty array. Each item of the array must be a valid _Plutus Data Schema_. An instance validates successfully against this keyword if it validates successfully against exactly one schema defined by this keyword's value.

##### `not`

This keyword's value must be a valid _Plutus Data Schema_. An instance is valid against this keyword if it fails to validate successfully against the schema defined by this keyword.

#### For `{ "dataType": "bytes" }`

> **Note** Keywords in this section only applies to `bytes`. Using them in conjunction with an invalid data-type should result in an error.

##### `enum`

The value of this keyword must be an array of hex-encoded string literals. An instance validates successfully against this keyword if once hex-encoded, its value matches one of the elements of the keyword's values.

##### `maxLength`

The value of this keyword must be a non-negative integer. A bytes instance is valid against this keyword if its length is less than, or equal to, the value of this keyword.

##### `minLength`

The value of this keyword must be a non-negative integer. A bytes instance is valid against this keyword if its length is greater than, or equal to, the value of this keyword.

#### For `{ "dataType": "integer" }`

> **Note** Keywords in this section only applies to `integer`. Using them in conjunction with an invalid type should result in an error.

##### `multipleOf`

The value of "multipleOf" must be a integer, strictly greater than 0. The instance is valid if division by this keyword's value results in an integer.

##### `maximum`

The value of "maximum" must be a integer, representing an inclusive upper limit. This keyword validates only if the instance is less than or exactly equal to "maximum".

##### `exclusiveMaximum`

The value of "exclusiveMaximum" must be an integer, representing an exclusive upper limit. The instance is valid only if it has a value strictly less than (not equal to) "exclusiveMaximum".

##### `minimum`

The value of "minimum" must be an integer, representing an inclusive lower limit. This keyword validates only if the instance is greater than or exactly equal to "minimum".

##### `exclusiveMinimum`

The value of "exclusiveMinimum" must be a integer, representing an exclusive lower limit. The instance is valid only if it has a value strictly greater than (not equal to) "exclusiveMinimum".

#### For `{ "dataType": "list" }`

> **Note** Keywords in this section only applies to `list`. Using them in conjunction with an invalid data-type should result in an error.

##### `items`

The value of this keyword must be either another _Plutus Data Schema_ or a list of _Plutus Data Schema_. When this keyword is a single schema, it applies its subschema to all child instances of the list. When it is a list, then the list is expected to have exactly the same number of elements as specified by the keyword and each element must match against the schema corresponding to its position. The list variation is useful to represent product types such as tuples.

##### `maxItems`

The value of this keyword must be a non-negative integer. An array instance is valid against "maxItems" if its size is less than, or equal to, the value of this keyword.

##### `minItems`

The value of this keyword must be a non-negative integer. A list instance is valid against "minItems" if its size is greater than, or equal to, the value of this keyword. Omitting this keyword has the same behavior as a value of 0.

##### `uniqueItems`

The value of this keyword must be a boolean. If this keyword has boolean value false, the instance validates successfully. If it has boolean value true, the instance validates successfully if all of its elements are unique.

#### For `{ "dataType": "map" }`

> **Note** Keywords in this section only applies to `map`. Using them in conjunction with an invalid data-type should result in an error.

##### `keys`

The value of this keyword must be another _Plutus Data Schema_. This keyword applies its subschema to all keys of the map.

##### `values`

The value of this keyword must be another _Plutus Data Schema_. This keyword applies its subschema to all values of the map.

##### `maxItems`

The value of this keyword must be a non-negative integer. An object instance is valid against "maxItems" if its number of key-value pair elements is less than, or equal to, the value of this keyword.

##### `minItems`

The value of this keyword must be a non-negative integer. An object instance is valid against "minItems" if its number of key-value pair elements is greater than, or equal to, the value of this keyword.

#### For `{ "dataType": "constructor" }`

> **Note** Keywords in this section only applies to `constructor`. Using them in conjunction with an invalid data-type should result in an error.

##### `index`

This keyword's value must be a non-negative integer. An instance is valid against this keyword if it represents a Plutus constructor whose index is the same as this keyword's value. This keyword is mandatory.

##### `fields`

This keyword's value must be an array of valid _Plutus Data Schema_; possibly empty. An instance is valid against this keyword if it represents a Plutus constructor for which each field is valid under each subschema given by this keyword's value. Fields are compared positionally. This keyword is mandatory.

## Example(s)

<details>
  <summary>Aiken's Hello World</summary>

```json
{
  "$schema": "https://cips.cardano.org/cips/cip57/schemas/plutus-blueprint.json",

  "$id": "https://github.com/aiken-lang/aiken/blob/main/examples/hello_world/plutus.json",

  "$vocabulary": {
    "https://json-schema.org/draft/2020-12/vocab/core": true,
    "https://json-schema.org/draft/2020-12/vocab/applicator": true,
    "https://json-schema.org/draft/2020-12/vocab/validation": true,
    "https://cips.cardano.org/cips/cip57": true
  },

  "preamble": {
    "title": "aiken-lang/hello_world",
    "description": "Aiken contracts for project 'aiken-lang/hello_world'",
    "version": "1.0.0",
    "plutusVersion": "v2"
  },

  "validators": [
    {
      "title": "hello_world",
      "datum": {
        "title": "Datum",
        "purpose": "spend",
        "schema": {
          "anyOf": [
            {
              "title": "Datum",
              "dataType": "constructor",
              "index": 0,
              "fields": [
                {
                  "title": "owner",
                  "dataType": "bytes"
                }
              ]
            }
          ]
        }
      },
      "redeemer": {
        "title": "Redeemer",
        "schema": {
          "anyOf": [
            {
              "title": "Redeemer",
              "dataType": "constructor",
              "index": 0,
              "fields": [
                {
                  "title": "msg",
                  "dataType": "bytes"
                }
              ]
            }
          ]
        }
      },
      "compiledCode": "58ad0100003232322225333004323253330063372e646e64004dd7198009801002240009210d48656c6c6f2c20576f726c64210013233300100137586600460066600460060089000240206eb8cc008c00c019200022253335573e004294054ccc024cdc79bae300a00200114a226660060066016004002294088c8ccc0040052000003222333300a3370e008004016466600800866e0000d2002300d001001235573c6ea8004526165734ae855d11",
      "hash": "5e1e8fa84f2b557ddc362329413caa3fd89a1be26bfd24be05ce0a02"
    }
  ]
}
```
</details>

## Rationale: how does this CIP achieve its goals?

### Documenting binary interfaces

THe primary goal of this CIP is to offer a mean of interoperability between tools of the ecosystem. In a world where every step of a contract development happens within a single framework -- like it's been the case with PlutusTx, this may not be seen as particularly useful. However, as soon as we start having an ecosystem of tools that operate a different levels (e.g. a language compiler, a transaction building library, an chain explorer, ...) we need some level of interoperability between them. Because the on-chain binary interface is the ultimate source of truth, it only makes sense to find an adequate way to capture it.

### Choice of JSON-Schemas as a foundation

JSON schemas are pervasively used in the industry for describing all sort of data models. Over the years, they have matured enough to be well understood by and familiar to a large portion of developers. Plus, tooling now exists in pretty much any major language to parse and process JSON schemas. Thus, using it as a foundation for the blueprint only makes sense.

### Divergence from JSON-Schemas primitives

This specification defines a new set of primitives types such as `integer`, `bytes`, `list`, `map` and `constructor` instead of the classic `integer`, `number`, `string`, `bool`, `array`, `object`, `null`. This is not only to reflect better the underlying structure of Plutus data which differs from JSON by many aspects, but also to allow defining or re-defining logic and validation keywords for each of those primitives.

Note however that apart from the keyword `type`, the terminology (and semantic) used for JSON schemas has been preserved to not "reinvent the wheel" and makes it easier to build tools on top by leveraging what already exists. Plutus schemas do not use `type` but use `dataType` instead to avoid possible confusion with JSON-schemas. A Plutus data schema is almost a JSON-schemas, but only supports a subset of the available keywords and has subtle differences for some of them (e.g. keywords for the `bytes` data-type operate mostly on hex-encoded strings).

### UPLC builtins

In the original design specification of CIP-0057, we did not include UPLC builtins. But, there are a few legitimate cases where they might be found in the binary interface of validators. In particular, the `ProtoPair` builtin for constructing 2-tuples. This poses a problem of exhaustiveness (why only include ProtoPair and not the others where similar arguments could probably be made anyway). This is solved by being exhaustive in the capabilities of the blueprint specification, while discouraging their usage.

Another point of designs here is to make `Data` rather transparent and promote Data's constructor variants as first-class data-types even though it's not faithfully representing what is really happening on-chain. An alternative, more faithful, representation to what's proposed would have been to have `data` as one of the data-type, and then keywords that identifies which of the data variant we are dealing with. So for example, instead of writing:

```
{ "dataType": "integer" }
```

One would have written:

```
{ "dataType": "data", "variant": "iData" }
```

Yet, because we do want `Data` to be the primary binary interface medium, we keep the former notation as it's more succinct and is a unambiguous shorthand. This also allows to segregate all builtins behind a common notation -- that is, prefixed with a `#`.

### Purpose

Originally, blueprints did not include any notion of _purpose_. However, as one of the end goal is to utilize blueprint as an input source for user interfaces, it becomes useful to:

1. indicates under what circumstances is a certain validator expected to be used.
2. provides different schemas based on the purpose

Yet, whereas there's a notion of purpose on-chain that is tightly coupled to the script context, different on-chain framework may handle the purpose differently. Some may chose, for example, to abstract that concern away from their users. Which is why we only make the purpose an _optional_ field (except for `datum`) to leave a bit of flexibility for blueprint producers. For consumers, we recommend to treat purposes as discriminants to refine interfaces, but assume that a validator without purpose simply apply to _any_ purpose.

### Additional Resources

- https://json-schema.org/draft/2020-12/json-schema-core.html
- https://json-schema.org/draft/2020-12/json-schema-validation.html

## Path to Active

### Acceptance criteria

- [x] Blueprints are produced by one or (ideally) more smart-contract frameworks on Cardano.
- [x] Aiken (implemented)
- [ ] Plu-ts (under way)
- [x] OpShin (implemented)
- [ ] Helios (under consideration)
- [ ] PlutusTx (?)
- [ ] Plutarch (?)
- [ ] Scalus (?)

- [x] There exist one or (ideally) more tools leveraging the blueprints
- [x] [Aiken](https://aiken-lang.org/)
- [x] [Mesh.js](https://meshjs.dev/)
- [x] [Lucid](https://lucid.spacebudz.io/)
- [x] [Bloxbean/cardano-client-lib](https://github.com/bloxbean/cardano-client-lib)
- [ ] [PyCardano](https://pycardano.readthedocs.io)
- [ ] [Demeter](https://demeter.run/)

### Implementation Plan

- [x] Write specifications for a few real-world contracts, identify and fix gaps
- [x] PoC of a toolkit generating blueprint definitions for a validator
- [x] Parse and interpret blueprints to produce smart-constructors for datums and redeemers in various languages
- [x] JavaScript
- [x] TypeScript
- [ ] Python
- [x] (optional) develop a tool for rendering Plutus blueprint specifications as documentation
- [paima/aiken-mdx](https://www.npmjs.com/package/@paima/aiken-mdx)

## Copyright

CC-BY-4.0

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0058/README.md
---

- --
CIP: 58
Title: Plutus Bitwise Primitives
Category: Plutus
Authors:
- Koz Ross <koz@mlabs.city>
- Maximilian König <maximilian@mlabs.city>
Implementors:
- Las Safin <me@las.rs>
Status: Inactive (superseded by CIP-0121 and CIP-0122)
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/283
- https://github.com/input-output-hk/plutus/issues/4252
- https://github.com/input-output-hk/plutus/pull/4733
Created: 2022-05-27
License: Apache-2.0
- --

# Abstract

Add primitives for bitwise operations, based on `BuiltinByteString`, without
requiring new data types.

# Motivation

Bitwise operations are one of the most fundamental building blocks of algorithms
and data structures. They can be used for a wide variety of applications,
ranging from representing and manipulating sets of integers efficiently, to
implementations of cryptographic primitives, to fast searches. Their wide
availability, law-abiding behaviour and efficiency are the key reasons why they
are widely used, and widely depended on.

At present, Plutus lacks meaningful support for bitwise operations, which
significantly limits what can be usefully done on-chain. While it is possible to
mimic some of these capabilities with what currently exists, and it is always
possible to introduce new primitives for any task, this is extremely
unsustainable, and often leads to significant inefficiencies and duplication of
effort.

We describe a list of bitwise operations, as well as their intended semantics,
designed to address this problem.

## Example applications

We provide a range of applications that could be useful or beneficial on-chain,
but are difficult or impossible to implement without some, or all, of the
primitives we propose.

### Finite field arithmetic

[Finite field arithmetic](https://en.wikipedia.org/wiki/Finite_field_arithmetic)
is an area with many applications, ranging from [linear block
codes](https://en.wikipedia.org/wiki/Block_code) to [zero-knowledge
proofs](https://en.wikipedia.org/wiki/Zero-knowledge_proof) to scheduling and
experimental design. Having such capabilities on-chain is useful in for a wide
range of applications.

A good example is multiplication over the [Goldilocks
field](https://blog.polygon.technology/introducing-plonky2) (with characteristic
$2^64 - 2^32 + 1$). To perform this operation requires 'slicing' the
representation being worked with into 32-bit chunks. As finite field
representations are some kind of unsigned integer in every implementation, in
Plutus, this would correspond to `Integer`s, but currently, there is no way to
perform this kind of 'slicing' on an `Integer` on-chain.

Furthermore, finite field arithmetic can gain significant performance
optimizations with the use of bitwise primitive operations. Two good examples
are power-of-two division and computing inverses. The first of these (useful
even in `Integer` arithmetic) replaces a division by a power of 2 with a shift;
the second uses a count trailing zeroes operation to compute a multiplicative
finite field inverse. While some of these operations could theoretically be done
by other means, their performance is far from guaranteed. For example, GHC does
not convert a power-of-two division or multiplication to a shift, even if the
divisor or multiplier is statically-known. Given the restrictions on computation
resources on-chain, any gains are significant.

Having bitwise primitives, as well as the ability to convert `Integer`s into a
form amenable to this kind of work, would allow efficient finite field
arithmetic on-chain. This could enable a range of new uses without being
inefficient or difficult to port.

### Succinct data structures

Due to the on-chain size limit, many data structures become impractical or
impossible, as they require too much space either for their elements, or their
overheads, to allow them to fit alongside the operations we want to perform on
them. [Succinct data
structures](https://en.wikipedia.org/wiki/Succinct_data_structure) could serve
as a solution to this, as they represent data in an amount of space much
closer to the entropy limit and ensure only constant overheads. There are
several examples of these, and all rely on bitwise operations for their
implementations.

For example, consider wanting to store a set of `BuiltinInteger`s
on-chain. Given current on-chain primitives, the most viable option involves
some variant on a `BuiltinList` of `BuiltinInteger`s; however,
this is unviable in practice unless the set is small. To see why, suppose that
we have an upper limit of $k$ on the `BuiltinInteger`s we want to store;
this is realistic in practically all cases. To store $n$
`BuiltinInteger`s under the above scheme requires

$$n \cdot \left( \left\lceil \frac{\log_2(k)}{64} \right\rceil \cdot 64  + c\right)
$$

bits, where $c$ denotes the constant overhead for each cons cell of
the `BuiltinList` holding the data. If the set being represented is dense
(meaning that the number of entries is a sizeable fraction of $k$), this cost
becomes intolerable quickly, especially when taking into account the need to
also store the operations manipulating such a structure on-chain with the script
where the set is being used.

If we instead represented the same set as a
[bitmap](https://en.wikipedia.org/wiki/Bit_array) based on
`BuiltinByteString`, the amount of space required would instead be

$$\left\lceil \frac{k}{8} \right\rceil \cdot 8 + \left\lceil
\frac{\log_2(k)}{64} \right\rceil \cdot 64
$$

bits. This is significantly better unless $n$ is small. Furthermore,
this representation would likely be more efficient in terms of time in practice,
as instead of having to crawl through a cons-like structure, we can implement
set operations on a memory-contiguous byte string:

- The cardinality of the set can be computed as a population count. This
can have terrifyingly efficient implementations: the
[Muła-Kurz-Lemire](https://lemire.me/en/publication/arxiv161107612/)
algorithm (the current state of the art) can process four kilobytes per loop
iteration, which amounts to over four thousand potential stored integers.
- Insertion or removal is a bit set or bit clear respectively.
- Finding the smallest element uses a count leading zeroes.
- Finding the last element uses a count trailing zeroes.
- Testing for membership is a check to see if the bit is set.
- Set intersection is bitwise and.
- Set union is bitwise inclusive or.
- Set symmetric difference is bitwise exclusive or.

A potential implementation could use a range of techniques to make these
operations extremely efficient, by relying on
[SWAR](https://en.wikipedia.org/wiki/SWAR)
techniques if portability is desired, and
[SIMD](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data)
instructions for maximum speed. This would allow both potentially large
integer sets to be represented on-chain without breaking the size limit, and
nodes to efficiently compute with such, reducing the usage of resources by the
chain. Lastly, in practice, if compression techniques are used (which also
rely on bitwise operations!), the number of required bits can be reduced
considerably in most cases without compromising performance: the current
state-of-the-art ([Roaring Bitmaps](https://roaringbitmap.org/)) can be
used as an example of the possible gains.

In order to make such techniques viable, bitwise primitives are mandatory.
Furthermore, succinct data structures are not limited to sets of integers, but
- all* require bitwise operations to be implementable.

### Binary representations and encodings

On-chain, space comes at a premium. One way that space can be saved is with binary
representations, which can potentially represent something much closer to the
entropy limit, especially if the structure or value being represented has
significant redundant structure. While some possibilities for a more efficient
'packing' already exist in the form of `BuiltinData`, it is rather
idiosyncratic to the needs of Plutus, and its decoding is potentially quite
costly.

Bitwise primitives would allow more compact binary encodings to be defined,
where complex structures or values are represented using fixed-size
`BuiltinByteString`s. The encoders and decoders for these could also be
implemented more efficiently than currently possible, as there exist numerous
bitwise techniques for this.

### On-chain vectors

For linear structures on-chain, we are currently limited to `BuiltinList`
and `BuiltinMap`, which don't allow constant-time indexing. This is a
significant restriction, especially when many data structures and algorithms
rely on the broad availability of a constant-time-indexable linear structure,
such as a C array or Haskell `Vector`. While we could introduce a primitive
data type like this, doing so would be a significant undertaking, and would
require both implementing and costing a large API.

While for variable-length data, we don't have any alternatives if constant-time
indexing is a goal, for fixed-length (or limited-length at least) data, there is
a possibility, based on a similar approach taken by the
[`finitary`](https://hackage.haskell.org/package/finitary)
library. Essentially, given finitary data, we can transform any item into a
numerical index, which is then stored by embedding into a byte array. As the
indexes are of a fixed maximum size, this can be done efficiently, but only if
there is a way of converting indices into bitstrings, and vice versa. Such a
construction would allow using a (wrapper around) `BuiltinByteString` as
a constant-time indexable structure of any finitary type. This is not much of a
restriction in practice, as on-chain, fixed-width or size-bounded types are
preferable due to the on-chain size limit.

Currently, all the pieces to make this work already exist: the only missing
piece is the ability to convert indices (which would have to be
`BuiltinInteger`s) into bit strings (which would have to be
`BuiltinByteString`s) and back again. With this capability, it would be
possible to use these techniques to implement something like an array or vector
without new primitive data types.

## Goals

To ensure a focused and meaningful proposal, we specify our goals below.

### Useful primitives

The primitives provided should enable implementations of algorithms and data
structures that are currently impossible or impractical. Furthermore, the
primitives provided should have a high power-to-weight ratio: having them should
enable as much as possible to be implemented.

### Maintaining as many algebraic laws as possible

Bitwise operations, via [Boolean
algebras](https://en.wikipedia.org/wiki/Boolean_algebra_(structure)), have a
long and storied history of algebraic laws, dating back to important results
by the like of de Morgan, Post and many others. These algebraic laws are
useful for a range of reasons: they guide implementations, enable easier
testing (especially property testing) and in some cases much more efficient
implementations. To some extent, they also formalize our intuition about how
these operations 'should work'. Thus, maintaining as many of these laws in our
implementation as possible, and being clear about them, is important.

### Allowing efficient, portable implementations

Providing primitives alone is not enough: they should also be efficient. This is
not least of all because many would associate 'primitive operation' with a
notion of being 'close to the machine', and therefore fast. Thus, it is on us to
ensure that the implementations of the primitives we provide have to be
implementable in an efficient way, across a range of hardware.

### Clear indication of failure

While totality is desirable, in some cases, there isn't a sensible answer for us
to give. A good example is a division-by-zero: if we are asked to do such a
thing, the only choice we have is to reject it. However, we need to make it as
easy as possible for someone to realize why their program is failing, by
emitting a sensible message which can later be inspected.

## Non-goals

We also specify some specific non-goals of this proposal.

### No metaphor-mixing between numbers and bits

A widespread legacy of C is the mixing of treatment of numbers and blobs of
bits: specifically, the allowing of logical operations on representations of
numbers. This applies to Haskell as much as any other language: according to the
[Haskell
Report](https://www.haskell.org/onlinereport/haskell2010/haskellch15.html#x23-20800015),
it is in fact *required* that any type implementing
`Bits` implement `Num` first. While GHC Haskell [only mandates
`Eq`](https://hackage.haskell.org/package/base-4.16.1.0/docs/Data-Bits.html#t:Bits),
it still defines `Bits` instances for types clearly meant to
represent numbers. This is a bad choice, as it creates complex situations and
partiality in several cases, for arguably no real gain other than easier
translation of bit twiddling code originally written in C.

Even if two types share a representation, their type distinctness is meant to be
a semantic or abstraction boundary: just because a number is represented as a
blob of bits does not necessarily mean that arbitrary bit manipulations are
sensible. However, by defining such a capability, we create several semantic
problems:

- Some operations end up needing multiple definitions to take this into
account. A good example are shifts: instead of simply having left or right
shifts, we now have to distinguish *arithmetic* versus *logical*
shifts, simply to take into account that a shift can be used on something
which is meant to be a number, which could be signed. This creates
unnecessary complexity and duplication of operations.
- As Plutus `BuiltinInteger`s are of arbitrary precision, certain
bitwise operations are not well-defined on them. A good example is bitwise
complement: the bitwise complement of $0$ cannot be defined sensibly, and in
fact, is partial in its `Bits` instance.
- Certain bitwise operations on `BuiltinInteger` would have quite
undesirable semantic changes in order to be implementable. A good example
are bitwise rotations: we should be able to 'decompose' a rotation left or
right by $n$ into two rotations (by $m_1$ and $m_2$ such that $m_1 + m_2 = n$)
without changing the outcome. However, because trailing zeroes are not
tracked by the implementation, this can fail depending on the choice of
decomposition, which seems needlessly annoying for no good reason.
- Certain bitwise operations on `BuiltinInteger` would require
additional arguments and padding to define them sensibly. Consider bitwise
logical AND: in order to perform this sensibly on `BuiltinInteger`s
we would need to specify what 'length' we assume they have, and some policy
of 'padding' when the length requested is longer than one, or both,
arguments. This feels unnecessary, and it isn't even clear exactly how we
should do this: for example, how would negative numbers be padded?

These complexities, and many more besides, are poor choices, owing more to the
legacy of C than any real useful functionality. Furthermore, they feel like a
casual and senseless undermining of type safety and its guarantees for very
small and questionable gains. Therefore, defining bitwise operations on
`BuiltinInteger` is not something we wish to support.

There are legitimate cases where a conversion from `BuiltinInteger` to
`BuiltinByteString` is desirable; this conversion should be provided, and
be both explicit and specified in a way that is independent of the machine or
the implementation of `BuiltinInteger`, as well as total and
round-tripping. Arguably, it is also desirable to provide built-in support for
`BuiltinByteString` literals specified in a way convenient to their
treatment as blobs of bytes (for example, hexadecimal or binary notation), but
this is outside the scope of this proposal.

# Specification

## Proposed operations

We propose several classes of operations. Firstly, we propose two operations for
inter-conversion between  `BuiltinByteString` and `BuiltinInteger`:

```haskell
integerToByteString :: BuiltinInteger -> BuiltinByteString
```

Convert a non-negative number to its bitwise representation, erroring if given a
negative number.
## 

```haskell
byteStringToInteger :: BuiltinByteString -> BuiltinInteger
```

Reinterpret a bitwise representation to its corresponding non-negative number.
## 

We also propose several logical operations on `BuiltinByteString`s:

```haskell
andByteString :: BuiltinByteString -> BuiltinByteString -> BuiltinByteString
```
Perform a bitwise logical AND on arguments of the same
length, producing a result of the same length, erroring otherwise.
## 

```haskell
iorByteString :: BuiltinByteString -> BuiltinByteString -> BuiltinByteString
```
Perform a bitwise logical IOR on arguments of the same
length, producing a result of the same length, erroring otherwise.
## 

```haskell
xorByteString :: BuiltinByteString -> BuiltinByteString -> BuiltinByteString
```
Perform a bitwise logical XOR on arguments of the same
length, producing a result of the same length, erroring otherwise.
## 

```haskell
complementByteString :: BuiltinByteString -> BuiltinByteString
```
Complement all the bits in the argument, producing a
result of the same length.
## 

Lastly, we define the following additional operations:

```haskell
shiftByteString :: BuiltinByteString -> BuiltinInteger -> BuiltinByteString
```
Performs a bitwise shift of the first argument by a number of bit positions
equal to the absolute value of the second argument. A positive second argument
indicates a shift towards higher bit indexes; a negative second argument
indicates a shift towards lower bit indexes.
## 

```haskell
rotateByteString :: BuiltinByteString -> BuiltinInteger -> BuiltinByteString
```
Performs a bitwise rotation of the first argument by a number of bit positions
equal to the absolute value of the second argument.  A positive second argument
indicates a rotation towards higher bit indexes; a negative second argument
indicates a rotation towards lower bit indexes.
## 

```haskell
popCountByteString :: BuiltinByteString -> BuiltinInteger
```
Returns the number of $1$ bits in the argument.
## 

```haskell
testBitByteString :: BuiltinByteString -> BuiltinInteger -> BuiltinBool
```
If the position given by the second argument is not in
bounds for the first argument, error; otherwise, if the bit given by that
position is $1$, return `True`, and `False` otherwise.
## 

```haskell
writeBitByteString :: BuiltinByteString -> BuiltinInteger -> BuiltinBool -> BuiltinByteString
```
If the position given by the second argument is not in bounds for the first
argument, error; otherwise, set the bit given by that position to $1$ if the
third argument is `True`, and $0$ otherwise.
## 

```haskell
countLeadingZeroesByteString :: BuiltinByteString -> BuiltinInteger
```

Counts the initial sequence of 0 bits in the argument (that is, starting from
index 0). If the argument is empty, this returns 0.
## 

```haskell
countTrailingZeroesByteString :: BuiltinByteString -> BuiltinInteger
```

Counts the final sequence of 0 bits in the argument (that is, starting from the
1 bit with the highest index). If the argument is empty, this returns 0.

## Semantics

### Preliminaries

We define $\mathbb{N}^{+} = \\{ x \in \mathbb{N} \mid x \neq 0 \\}$. We assume
that `BuiltinInteger` is a faithful representation of $\mathbb{Z}$, and will
refer to them (and their elements) interchangeably. A *byte* is some
$x \in \\{ 0,1,\ldots,255 \\}$.

We observe that, given some *base* $b \in \mathbb{N}^{+}$, any
$n \in \mathbb{N}$ can be viewed as a sequence of values in $\\{0,1,\ldots, b - 1\\}$.
We refer to any such sequence as a *base* $b$ *sequence*. In such a 'view', given
a base $b$ sequence $S = s_0 s_1 \ldots s_k$, we can compute its corresponding
$m \in \mathbb{N}^+$ as

$$\sum_{i \in \\{0,1,\ldots,k\\}} b^{k - i} \cdot s_i$$

If $b > 1$ and $Z$ is a base $b$ sequence consisting only of zeroes, we observe
that for any other base $b$ sequence $S$, $Z \cdot S$ and $S$ correspond to the
same number, where $\cdot$ is sequence concatenation.

We use *bit sequence* to refer to a base 2 sequence, and *byte sequence* to
refer to a base 256 sequence. For a bit sequence $S = b_0 b_1 \ldots b_n$, we
refer to $\\{0,1,\ldots,n \\}$ as the *valid bit indices* of $S$; analogously,
for a byte sequence $T = y_0 y_1 \ldots y_m$, we refer to $\\{0,1,\ldots,m\\}$
as the *valid byte indices* of $T$. We observe that the length of $S$ is $n + 1$
and the length of $T$ is $m + 1$; we refer to these as the *bit length* of $S$
and the *byte length* of $T$ for clarity. We write $S[i]$ and $T[j]$ to
represent $b_i$ and $y_j$ for valid bit index $i$ and valid byte index $j$
respectively.

We describe a 'view' of bytes as bit sequences. Let $y$ be a byte; its
corresponding bit sequence is $S_y = y_0 y_1 y_2 y_3 y_4 y_5 y_6 y_7$ such that

$$\sum_{i \in \\{0,1,\ldots,7\\}} 2^{7 - i} \cdot y_i = y$$

For example, the byte $55$ has the corresponding byte sequence $00110111$. For
any byte, its corresponding byte sequence is unique. We use this to extend our
'view' to byte sequences as bit sequences. Specifically, let
$T = y_0 y_1 \ldots y_m$ be a byte sequence. Its corresponding bit sequence
$S = b_0b_1 \ldots b_m b_{m + 1} \ldots b_{8(m + 1) - 1}$ such that for any valid bit index $j$ of $S$,
$b_j = 1$ if and only if $T[j / 8][j \mod 8] = 1$, and is $0$ otherwise.

Based on the above, we observe that any `BuiltinByteString` can be a bit
sequence or a byte sequence. Furthermore, we assume that `indexByteString` and
`sliceByteString` 'agree' with valid byte indices. More precisely, suppose
`bs` represents a byte sequence $T$; then `indexByteString bs i` is seen as
equivalent to $T[\mathtt{i}]$; we extend this notion to `sliceByteString`
analogously. Throughout, we will refer to `BuiltinByteString`s and their 'views'
as bit or byte sequences interchangeably.

### Representation of `BuiltinInteger` as `BuiltinByteString` and conversions

We describe the translation of `BuiltinInteger` into `BuiltinByteString`, which
is implemented as the `integerToByteString` primitive. Let $i$ be the argument
`BuiltinInteger`; if this is negative, we produce an error, specifying at least
the following:

- The fact that specifically the `integerToByteString` operation failed;
- The reason (given a negative number); and
- What exact number was given as an argument.

Otherwise, we produce the `BuiltinByteString` corresponding to the base 256
sequence which represents $i$.

We now describe the reverse operation, implemented as the `byteStringToInteger`
primitive. This treats its argument `BuiltinByteString` as a base 256 sequence,
and produces its corresponding number as a `BuiltinInteger`. We note that this
is necessarily non-negative.

We observe that `byteStringToInteger` 'undoes' `integerToByteString`:

```haskell
byteStringToInteger . integerToByteString = id
```

The other direction does not necessarily hold: if the argument to
`byteStringToInteger` contains a prefix consisting only of zeroes, and we
convert the resulting `BuiltinInteger` `i` back to a `BuiltinByteString` using
`integerToByteString`, that prefix will be lost.

### Bitwise logical operations on `BuiltinByteString`

Throughout, let $S = s_0 s_1 \ldots s_n$ and $T = t_0 t_1 \ldots t_n$ be byte
sequences, and let $S^{\prime}$ and $T^{\prime}$ be their corresponding bit
sequences, with bit lengths $n^{\prime} + 1$ and $m^{\prime} + 1$ respectively.
Whenever we specify a *mismatched length error* result, its error message
must contain at least the following information:

- The name of the failed operation;
- The reason (mismatched lengths); and
- The byte lengths of the arguments.

For any of `andByteString`, `iorByteString` and `xorByteString`, given inputs
$S$ and $T$, if $n \neq m$, the result is an error which must contain at least
the following information:

- The name of the failed operation;
- The reason (mismatched lengths); and
- The byte lengths of the arguments.

If $n = m$, the result of each of these operations is the bit sequence
$U = u_0u_1 \ldots u_{n^{\prime}}$, such that for all $i \in \\{0, 1, \ldots, n^{\prime}\\}$,
$U[i] = 1$ under the following conditions:

- For `andByteString`, when $S^{\prime}[i] = T^{\prime}[i] = 1$;
- For `iorByteString`, when at least one of $S^{\prime}[i], T^{\prime}[i]$ is
  $1$;
- For `xorByteString`, when $S^{\prime}[i] \neq T^{\prime}[i]$.

Otherwise, $U[i] = 0$.

We observe that, for length-matched arguments, each of these operations
describes a commutative and associative operation. Furthermore, for any given
byte length $k$, each of these operations has an identity element:

- For `andByteString` and `xorByteString`, the byte sequence of length $k$ where
  each element is zero; and
- For `iorByteString`, the byte sequence of length $k$ where each element is
255.

Lastly, `andByteString` and `iorByteString` have an absorbing element for each
byte length $k$, which is the byte sequence of length $k$ where each element is
zero and 255 respectively.

We now describe the semantics of `complementByteString`. For input $S$, the
result is the bit sequence $U = u_0 u_1 \ldots u_{n^{\prime}}$ such that for all
$i \in \{0, 1, \ldots, n^{\prime}\}$, we have $U[i] = 0$ if $S^{\prime}[i] = 1$
and $1$ otherwise.

We observe that `complementByteString` is self-inverting. We also note
the following equivalences hold assuming `b` and `b'` have the
same length; these are [De Morgan's
laws](https://en.wikipedia.org/wiki/De_Morgan%27s_laws):

```haskell
complementByteString (andByteString b b') = iorByteString (complementByteString b) (complementByteString b')
```

```haskell
complementByteString (iorByteString b b') = andByteString (complementByteString b) (complementByteString b')
```

### Mixed operations

Throughout, let $S = s_0 s_1 \ldots s_n$ be a byte sequence, and let
$S^{\prime}$ be its corresponding bit sequence with bit length $n^{\prime} + 1$.

We describe the semantics of `shiftByteString` and `rotateByteString`.
Informally, both of these are 'bit index modifiers': given a positive $i$, the
index of a bit in the result 'increases' relative to the argument, and given a
negative $i$, the index of a bit in the result 'decreases' relative to the
argument. This can mean that for some bit indexes in the result, there is no
corresponding bit in the argument: we term these *missing indexes*.
Additionally, by such calculations, a bit index in the argument may be projected
to a negative index in the result: we term these *out-of-bounds indexes*. How we
handle missing and out-of-bounds indexes is what distinguishes `shiftByteString`
and `rotateByteString`:

- `shiftByteString` sets any missing index to $0$ and ignores any data at
  out-of-bounds indexes.
- `rotateByteString` uses out-of-bounds indexes as sources for missing indexes
  by 'wraparound'.

We describe the semantics of `shiftByteString` precisely. Given arguments $S$
and some $i \in \mathbb{Z}$, the result is the bit sequence
$U = u_0 u_1 \ldots u_{n^{\prime}}$ such that for all
$j \in \\{0, 1, \ldots, n^{\prime}\\}$, we have $U[j] = S^{\prime}[j - i]$ if
$j - i$ is a valid bit index for $S^{\prime}$ and $0$ otherwise.

Let $k, \ell \in \mathbb{Z}$
such that either
$k$ or $\ell$ is $0$, or
$k$ and $\ell$ have the same sign.
We observe that, for any `bs`, we have


```haskell
shiftByteString (shiftBytestring bs k) l = shiftByteString bs (k + l)
```

We now describe the semantics of `rotateByteString` precisely; we assume the
same arguments as for `shiftByteString` above. The result is the bit sequence
$U = u_0 u_1 \ldots u_{n^{\prime}}$ such that for all
$j \in \\{0, 1, \ldots, n^{\prime}\\}$, we have $U[j] = S^{\prime}[n^{\prime} + j - i \mod n^{\prime}]$.

We observe that for any $k, \ell$, and any
`bs`, we have

```haskell
rotateByteString (rotateByteString bs k) l = rotateByteString bs (k + l)
```

We also note that

```haskell
rotateByteString bs 0 = shiftByteString bs 0 = bs
```

Lastly, we note that

```haskell
rotateByteString bs k = rotateByteString bs (k `remInteger` (lengthByteString bs * 8))
```

For `popCountByteString` with argument $S$, the result is

$$\sum_{j \in \\{0, 1, \ldots, n^{\prime}\\}} S^{\prime}[j]$$

Informally, this is just the total count of $1$ bits. We observe that
for any `bs` and `bs'`, we have

```haskell
popCountByteString bs + popCountByteString bs' = popCountByteString (appendByteString bs bs')
```

We now describe the semantics of `testBitByteString` and `writeBitByteString`.
Throughout, whenever we specify an *out-of-bounds error* result, its error
message must contain at least the following information:

- The name of the failed operation;
- The reason (out of bounds access);
- What index was accessed out-of-bounds; and
- The valid range of indexes.

For `testBitByteString` with arguments $S$ and some $i \in \mathbb{Z}$, if $i$
is a valid bit index of $S^{\prime}$, the result is `True` if
$S^{\prime}[i] = 1$, and `False` if $S^{\prime}[i] = 0$. If $i$ is not a valid
bit index of $S^{\prime}$, the result is an out-of-bounds error.

For `writeBitByteString` with arguments $S$, some $i \in \mathbb{Z}$ and some
`BuiltinBool` $b$, if $i$ is not a valid bit index for $S^{\prime}$, the result
is an out-of-bounds error. Otherwise, the result is the bit sequence
$U = u_0 u_1 \ldots u_{n^{\prime}}$ such that for all $j \in \\{0, 1, \ldots, n\\}$, we
have:

- $U[j] = 1$ when $i = j$ and $b$ is `True`;
- $U[j] = 0$ when $i = j$ and $b$ is `False`;
- $U[j] = S^{\prime}[j]$ otherwise.

Lastly, we describe the semantics of `countLeadingZeroesByteString` and
`countTrailingZeroesByteString`. Given the argument $S$,
`countLeadingZeroesByteString` gives the result $k$ such that all of the
following hold:

- $0 \leq k < n^{\prime} + 1$;
- For all $0 \leq i < k$, $S^{\prime}[i] = 0$; and
- If $n^{\prime} \neq 0$, then $S^{\prime}[k] = 1$.

Given the same argument, `countTrailingZeroesByteString` instead gives the
result $k$ such that all of the following hold:

- $0 \leq k < n^{\prime} + 1$;
- For all $k \leq i < n^{\prime}$, $S^{\prime}[i] = 0$; and
- If $k /neq n^{\prime} + 1$, then $S^{\prime}[n^{prime} - k] = 1$.

Let `zeroes` be a `BuiltinByteString` consisting only of zero bytes of length
`len`. We observe that

```haskell
countTrailingZeroesByteString zeroes = countLeadingZeroesByteString zeroes = len
- 8
```

Furthermore, for two `BuiltinByteString`s `bs` and `bs'`, we have

```haskell
countLeadingZeroesByteString (iorByteString bs bs') =
  min (countLeadingZeroesByteString bs) (countLeadingZeroesByteString bs')

countTrailingZeroesByteString (iorByteString bs bs') =
  min (countTrailingZeroesByteString bs) (countTrailingZeroesByteString bs')
```

where `min` is the minimum value function.

### Costing

All of the primitives we describe are linear in one of their arguments. For a
more precise description, see the table below.

Primitive | Linear in
- -- | ---
`integerToByteString` | Argument (only one)
`byteStringToInteger` | Argument (only one)
`andByteString` | One argument (same length for both)
`iorByteString` | One argument (same length for both)
`xorByteString` | One argument (same length for both)
`complementByteString` | Argument (only one)
`shiftByteString` | `BuiltinByteString` argument
`rotateByteString` | `BuiltinByteString` argument
`popCountByteString` | Argument (only one)
`testBitByteString` | `BuiltinByteString` argument
`writeBitByteString` | `BuiltinByteString` argument
`countLeadingZeroesByteString` | Argument (only one)
`countTrailingZeroesByteString` | Argument (only one)

# Rationale

## Why these operations?

For work in finite field arithmetic (and the areas it enables), we frequently
need to move between the 'worlds' of `BuiltinInteger` and `BuiltinByteString`.
This needs to be consistent, and allow round-trips. We simplify this by only
requiring conversions work on non-negative integers: this means that the
translations can be simpler and more efficient, and also avoids representational
questions for negative numbers.

Our choice of logical AND, IOR, XOR and complement as the primary logical
operations is driven by a mixture of prior art, utility and convenience. These
are the typical bitwise logical operations provided in hardware, and in most
programming languages; for example, in the x86 instruction set, the following
bitwise operations have existed since the 8086:

- `AND`: Bitwise AND.
- `OR`: Bitwise IOR.
- `NOT`: Bitwise complement.
- `XOR`: Bitwise XOR.

Likewise, on the ARM instruction set, the following bitwise operations have
existed since ARM2:

- `AND`: Bitwise AND.
- `ORR`: Bitwise IOR.
- `EOR`: Bitwise XOR.
- `ORN`: Bitwise IOR with complement of the second argument.
- `BIC`: Bitwise AND with complement of the second argument.

Going 'up a level', the C and Forth programming languages (according to C89 and
ANS Forth respectively) define bitwise AND (denoted `&` and `AND`
respectively), bitwise IOR (denoted `|` and `OR` respectively), bitwise XOR
(denoted ` ^` and `XOR` respectively) and bitwise complement (denoted `~` and
`NOT` respectively) as primitive bitwise operations. These choices are mirrored
by basically all 'high-level' languages; for example, Haskell's `Bits` type
class defines these same four operations as `.&.`, `.|.`, `xor` and `complement`
respectively.

This ubiquity in choices leads to most algorithm descriptions that rely on
bitwise operations to assume that these specific four operations are
'primitive', implying that they are constant-time and constant-cost. While we
could reduce the number of primitive bitwise operations (and, in fact, due to
Post, we know that there exist two operations that can implement all of them),
this would be both inconvenient and inefficient. As an example, consider
implementing XOR using AND, IOR and complement: this would translate `x XOR y`
into

```
(COMPLEMENT x AND y) IOR (x AND COMPLEMENT y)
```

This is both needlessly complex, and also inefficient, as it requires copying
the arguments twice, only to then throw away both copies. This is less of a
concern if copying is 'cheap', but given that we need to operate on
variable-width data (specifically `BuiltinByteString`s), this seems needlessly
wasteful.

Like our 'baseline' bitwise operations above, shifts and rotations are widely
used, and considered as primitive. For example, x86 platforms have had the
following available since the 8086:

- `RCL`: Rotate left.
- `RCR`: Rotate right.
- `SHL`: Shift left.
- `SHR`: Shift right.

Likewise, ARM platforms have had the following available since ARM2:

- `ROR`: Rotate right.
- `LSL`: Shift left.
- `LSR`: Shift right.

While C and Forth both have shifts (denoted with `<<` and `>>` in C, and
`LSHIFT` and `RSHIFT` in Forth), they don't have rotations; however, many
higher-level languages do: Haskell's `Bits` type class has `rotate`, which
enables both left and right rotations.

While `popCountByteString` could in theory be simulated using
`testBitByteString` and a fold, this is quite inefficient: the best way to
simulate this operation would involve using something similar to the
Harley-Seal algorithm, which requires a large lookup table, making it
impractical on-chain. Furthermore, population counting is important for several
classes of succinct data structure (particularly rank-select dictionaries and
bitmaps), and is in fact provided as part of the `SSE4.2` x86 instruction set
as a primitive named `POPCNT`.

In order to usefully manipulate individual bits, both `testBitByteString`
and `writeBitByteString` are needed. They can also be used as part of
specifying, and verifying, that other bitwise operations, both primitive and
non-primitive, are behaving correctly. They are also particularly essential for
binary encodings.

`countLeadingZeroesByteString` and `countTrailingZeroesByteString` is an
essential primitive for several succinct data structures: both Roaring Bitmaps
and rank-select dictionaries rely on them for much of their usefulness. For
finite field arithmetic, these instructions are also beneficial to have
available as efficiently as possible. Furthermore, this operation is provided
in hardware by several instruction sets:
on x86, there exist (at least) `BSF`, `BSR`, `LZCNT` and `TZCNT`, while on ARM,
we have `CLZ` for counting leading zeroes. These instructions also exist in higher-level
languages: for example, GHC's `FiniteBits` type class has `countTrailingZeros`
and `countLeadingZeros`. Lastly, while they can be emulated by
`testBitByteString`, this is tedious, error-prone and extremely slow.

# Backwards compatibility

At the Plutus Core level, implementing this proposal introduces no
backwards-incompatibility: the proposed new primitives do not break any existing
functionality or affect any other builtins. Likewise, at levels above Plutus
Core (such as `PlutusTx`), no existing functionality should be affected.

On-chain, this requires a hard fork, as this introduces new primitives.

# Path to Active

MLabs will implement these primitives, as well as tests for these. Costing will
have to be done after this is complete, but must be done by the Plutus Core
team, due to limitations in how costing is performed.

# Copyright

This CIP is licensed under Apache-2.0.

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0059/README.md
---

- --
CIP: 59
Title: Terminology Surrounding Core Features
Status: Active
Category: Meta
Authors:
- Jared Corduan <jared.corduan@iohk.io>
Implementors: N/A
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/274
Created: 2022-06-09
License: CC-BY-4.0
- --

## Abstract

This CIP seeks to clarify the language around groups of features.
At the very least, it provides some history.

## Motivation: why is this CIP necessary?

When @CharlesHoskinson conceived of Cardano, he had a vision for what features the network would support.
This vision is still present on the [Cardano roadmap website](https://roadmap.cardano.org).
In particular, the features are grouped into "phases", which are mostly named after poets (Goguen is the exception).
The word "era" is used interchangeably with "phases" on the roadmap.

### History

The word "era", however, has been muddled by an implementation detail in the Cardano ledger.
The Shelley phase was implemented as an entire re-write of the code from the Byron phase.
While the consensus layer for the Shelley phase was written with an abstraction in place for the ledger,
the ledger layer was not written with any abstractions to make future phases possible.

Upon starting into the Goguen phase, the ledger team retroactively introduce a notion of "era"
into the ledger code, and deemed the Shelley features "the Shelley era".
In hindsight, however, the word "era" in unfortunate, since the Goguen phase was completed in the ledger
by what was called "the Allegra era, the Mary era, the Alonzo era, and the Babbage era".

The names Allegra and Mary were chosen for their connection to the poet Percy Shelley,
and were only intended to be used as
[variable names](https://github.com/input-output-hk/cardano-ledger/blob/1cbf1fc2bb005a8206e5b5a7cdf44d35baaca455/eras/shelley-ma/impl/src/Cardano/Ledger/Allegra.hs#L40)
for a very specific abstraction used in the ledger code.
(The story is even a bit more confusing, since the Allegra and Mary era share a lot of code
and are specified together in the "Shelley-MA
[specification](https://github.com/input-output-hk/cardano-ledger/releases/latest/download/mary-ledger.pdf).
The letters "MA" can hilariously refer to both "Mary Allegra" and "Multi-Assets".)

How did we then go from poets to Alonzo?
Recall that "Goguen" was the only non-poet named in the phases on the Cardano roadmap.
We found it fitting, therefore, to name the ledger era which introduced Plutus
after the person who invented the lambda calculus
(Plutus Core uses a variant of [system F](https://en.wikipedia.org/wiki/System_F).).

Moreover, going forward, we decided to use names in A, B, C, ... order, names coming from
other people who walk the line between mathematics and computer science.
One lack of consistency to notice is that we have used both first and last names.
The inconsistency was mostly driven by the desire to find short and memorable names.

Another complication to the story is the notion of "intra-era hard forks".
A new era _must_ be introduced with a hard fork, but the ledger can also
change semantics during a controlled hard fork with another mechanism, namely
an intra-era hard fork.
This is an implementation detail which involves bumping the major protocol version
but not creating a new ledger era.
The Alonzo era experienced an intra-era hard fork when going from major protocol version 5 to 6.

Yet another complication stems from the named releases.
We chose to honor the late Cardano community member and Bulgarian mathematician Vasil Dabov
by naming a release date after him.
The ledger era after the Alonzo era was named Babbage.
Babbage is a feature set, Vasil is a release date which ushered in the Babbage era.

Lastly, it is important to understand that not all of the semantic changes to the Cardano network involve the ledger,
though the changes to the ledger are often the most user-facing.
Changes to the consensus protocol or the networking layer may also involve a hard fork.
Moreover, there is an abstraction that sits between the consensus and ledger layers,
which we have named the "protocol" (a regrettably vague name).

The distinction between the ledger protocols and the ledger eras
correspond roughly to how block headers are validated (protocol) versus
how block bodies are validated (era).
The Shelley era used the "transitional Praos" protocol (or TPraos for short).
It consisted of Praos together with a transition system to move away from Ouroboros-BFT.
The Babbage era replaced TPraos with Praos.

## Specification

A table of all the features, as of the time this CIP was submitted, can be found [here](./feature-table.md).

Note that the protocol version mentioned above is unrelated to the node-to-node and node-to-client protocol versions.
The consensus layer maintains a versioning scheme for the node queries which does not necessarily
align with the protocol version described in this CIP.

Note also that the protocol version present inside of each block header indicates the maximum supported protocol version
that the block producer is capable of supporting (see section 13, Software Updates, of the
[Shelley ledger specification](https://github.com/input-output-hk/cardano-ledger/releases/latest/download/shelley-ledger.pdf)).

Let us use the following language:

- **Phase** - A phase in Cardano is a high level collection of features described on the Cardano roadmap.
- **Ledger Era** - A ledger era (or era for short if there is no confusion) in Cardano is a collection of ledger features introduced at a hard fork. Moreover, starting with the Alonzo era, they will be named after mathematicians and computer scientists (preferably both!) in A, B, C, ... ordering. Some letters might prove challenging.
- **Intra-era Hardfork** - An intra-era hard fork in Cardano is a small and focused semantic change to the ledger which requires a hard fork.
- **Consensus mechanism** - A consensus mechanism in Cardano is a collection of consensus features introduced at a hard fork. Historically, these have had the name "Ouroboros" in them.
- **Ledger Protocol** - A ledger protocol in Cardano is a collection of ledger features sitting between the consensus layer and the ledger layer, roughly characterized by block header validation.
- **Release Dates** - When we are confident about the release of a new features, we can chose to honor Cardano community members by naming a date after them.

## Rationale: how does this CIP achieve its goals?

If we can agree to common language, it will greatly improve communication among ourselves and also with new community members.

### Backwards compatibility

Since this is an issue of language, we will strive to use consistent language going forward, and we can correct misalignment when we find it.

## Path to Active

### Acceptance Criteria

- [x] Terminology has met with positive response from community.
- [x] Terminology has continued in use particularly in the CIP process and the Feature Table has been kept up to date.

### Implementation Plan

- [x] Ledger architects have committed to standardising their language for the community.
- [x] Table of strict definitions, with protocol versions and block heights, is produced to remove any ambiguities.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0060/README.md
---

- --
CIP: 60
Title: Music Token Metadata
Status: Active
Category: Metadata
Authors:
- Andrew Westberg <awestberg@projectnewm.io>
- Ryan Jones <rjones@projectnewm.io>
- Justin Morgan <jusemorgan@gmail.com>
- Ian Singer <tcl@fre5hmusic.com>
- Anthony Eizmendiz <aeizmendiz@icloud.com>
- Session Cruz <session@demu.pro>
- Jimmy Londo <SickCityCleveland@gmail.com>
- Gudbrand Tokerud <Gudbrand.tokerud@gmail.com>
- Kevin St.Clair <kos1777@gmail.com>
- Brandon Loyche <dsqise@gmail.com>
- Andrew Donovan <adonovan23@gmail.com>
- The Finest LLC (DBA So Litty Records) <solittyrecords@gmail.com>
- Cristhian Escobar <escobarcristhian18@gmail.com>
- Gabriel Stephan Talamantes <contact@psyencelab.media>
Implementors:
- NEWM <newm.io>
- SoundRig <soundrig.io>
- SickCityNFT <sickcity.xyz>
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/307
- https://github.com/cardano-foundation/CIPs/pull/367
- https://github.com/cardano-foundation/CIPs/pull/502
- https://github.com/cardano-foundation/CIPs/pull/868
Created: 2022-07-26
License: CC-BY-4.0
- --

## Abstract

This proposal defines an extension to CIP-25 and CIP-68 for token metadata specific to music tokens.

## Motivation: why is this CIP necessary?

Music tokens on Cardano can be either NFTs or FTs and contain links to audio files. In order for players, indexers, and wallets to be able to properly search and categorize a user's music collection, we need to define a common schema for creating music on Cardano. If all parties creating these music tokens follow similar patterns, apps can consume this information and make proper use of it. The existing CIP-25 is a good base to build upon, but for a good music experience, we need to standardize additional fields that will be required specifically for music tokens.

## Specification

This CIP divides the additional metadata parameters into two categories of `Required` and `Optional`. When minting a music token on Cardano, you are expected to include ALL of the required fields. If you choose to include one or more of the optional fields, they must be named exactly as defined in this CIP. This will properly allow indexing apps and music players to utilize as much of your token metadata as possible without issues.

[CDDL Spec Version 3 ](./cddl/version-3.cddl)<br/>
[CDDL Spec Version 2 (deprecated)](./cddl/version-2.cddl)<br/>
[CDDL Spec Version 1 (deprecated)](./cddl/version-1.cddl)

### Summary of v2 Changes ###
In version 2 of the CIP-60 spec, `album_title` has been renamed to `release_title`. `release` is a more generic name that covers all types of releases from Albums, EPs, LPs, Singles, and Compilations. At the top level, we are grouping those metadata items that relate to the release under a new key `release`. At the file for each song, there is a new `song` key that holds the metadata specific to the individual song. These changes separate the music-specific metadata from the general CIP-25/CIP-68 NFT metadata. A music player can look at just the information necessary instead of having to ignore extra NFT-related fields. CIP-68 NFTs are officially supported and an example specific to CIP-68 has been added below.

### Summary of v3  Proposed Changes ###
Version 3 reorders identifiers like IPN, ISNI, etc into objects tied with the entities they are associated with. `contributing_artists`, `artists`, and `featured_artists` fields are explicitly defined to reduce interpretation.  `ipi` array replaced with `author` array, which includes `ipi` key.  Removed the `parental_advisory` field, as it was redundant (`explicit` is all players need to look for). `lyricist` is removed and merged into `contributing_artist`, under `role`.  `copyright` adds `master` and `composition` to distinguish recording and composition copyright owners.  Certain fields may be included in `release` within "Album/EP" `release_type` if they are qualifying GRAM (Group Registration for Works on an Album of Music) publications. This is done in order to help conserve data in otherwise redundant entries.

### Required Fields ###
| Field | Type | Example(s) | Notes |
| -------- | -------- | -------- | -------- |
| artists     | Array\<Artist\>   | "artists": [{"name": "Sick City", "isni":"xxxxxxxxxxxxx", "links:{ "website":"https://sickcity.xyz"}}]  | Players should use these values to determine the song's artist, and should be kept minimal. `isni` and `links` are optional.  Included in `song` for "Single" and "Multiple" releases, and in `release` for "Album/EP" types. |
| release_title| String | "release_title": "Mr. Bad Guy" | Included in `release` |
| track_number | Integer | "track_number": 1 |  Included in `song` |
| song_title | String \| Array\<String\> | "song_title": "Let's Turn it On" |  Included in `song` |
| song_duration | String | "song_duration": "PT3M21S"  | ISO8601 Duration Format, included in `song`  https://www.iso.org/iso-8601-date-and-time-format.html |
| copyright | String | "copyright": {"master":"℗ 1985 Sony Records", "composition":"© 1985 Marvin Gaye"}  or <br/> "copyright": {"composition": "Public Domain", "master": "℗ 2024 Cool Guy"} | Included in `release` within "Album/EP" `release_type` (ONLY IF **ALL** compositions are owned by the same artist) , and in `song` within "Single" and "Multiple" releases. |
| genres | Array\<String\> | "genres": ["Rock","Classic Rock"] | Limited to 3 genres total. Players should ignore extra genres. Included in `song` within "Single" and "Multiple" releases, and in `release` for "Album/EP" releases should all songs share the same `genre` values. |
| release_type | Enum\<String\> | "release_type": "Single" | Must be "Single", "Album/EP" for GRAM (Group Registration for Works on an Album of Music- https://www.copyright.gov/rulemaking/gram/) publications, or "Multiple" (for all other cases"). "Multiple" and "Album/EP" releases need to be wary of txn size limits .  Included in `release`  |
| music_metadata_version | Integer | "music_metadata_version" : "3" | Players should look for the presence of this field to determine if the token is a Music Token.  Use integers only. |

#### Optional Fields ###
| Field | Type | Example(s) | Notes |
| -------- | -------- | -------- | -------- |
| isni | String | "artists": [{"name": "Sick City", "isni":"xxxxxxxxxxxxx", "links:{ "website":"https://sickcity.xyz"}}] |  Included in `song` with `artists` and `featured_artists` |
| links | Map | "artists": [{"name": "Sick City", "isni":"xxxxxxxxxxxxx", "links:{ "website":"https://sickcity.xyz"}}] | Included in `artists` and `featured_artist`, and in `release` where applicable, i.e. if a "Multiple" `release_type` is a single artist album and has recurring links |
| ai_generated | Boolean | "ai_generated": "true"  | Used to distinguish works that are entirely AI generated. |
| contributing_artists |  Array\<Artist\> | "contributing_artists": [{"name":"Jimmy Londo", "ipi":"158743685", "role":["guitars", "vocals"]}]  | Contributing artist are defined as any creative contributor who is not necessarily identified as an author, but will receive performance royalties when applicable.  eg, a band would place the band name in `artists`, while the band members would be listing individually here.  Should not pass to players, but readable within metadata.  May contain `ipn` or `ipi` (based on use/jurisdiction, i.e. `ipi` within the US; enables indexing of similarly named contributors) `links`, and `role`, all of which are optional |
| ipn | String | "contributing_artists": [{"name":"Jimmy Londo", "ipn":"158743685", "role":["guitars", "vocals"]}] |  Included in `song` within `contributing_artists` where used (typically outside US, though internationally recognized.)|
| role | string | "contributing_artists": [{"name":"Jimmy Londo", "ipi":"158743685", "role":["guitars", "vocals"]}] | Included in `song` within `contributing_artists` (declares a contributor's role/contribution to the work), as well as `authors` (establishing role in songwriting, following "Roles" from ASCAP)|
| series | string | "series": "That's What I call Music" | Included in `release` |
| collection | string | "collection": "Now Dance" | Included in `release` |
| set | string | "set": "86 - 20 Smash Dance Hits of the Year" | If the song is a part of a collection of songs, such as an album, EP, live performance, etc. that is separate from this release, it can be listed here.  Included in `song` |
| mood | String | "mood": "Empowered" | Included in `song` |
| lyrics | URL | "lyrics": "ipfs://QmSmadTEhB9bJQ1WHq58yN1YZaJo4jv5BwVNGaePvEj4Fy" | Included in `song` |
| special_thanks | Array\<String\> | "special_thanks": ["Your mom","Your grandma"] | Included in `song` |
| visual_artist | String | "visual_artist": "beeple" | Included in `release` |
| distributor | String | "distributor": "https://newm.io" | Included in `release` |
| release_date | String | "release_date": "2022-07-27" | ISO8601 Date Format, included in `release` |
| publication_date | String | "publication_date": "2022-07-27" | ISO8601 Date Format, included in `release` https://www.iso.org/iso-8601-date-and-time-format.html |
| catalog_number | Integer | "catalog_number": REC#4582 | Catalog numbers for digital releases should only be entered if the label or digital distributor has given a unique catalog number for the release. Included in `release` |
| bitrate | String | "bitrate": "256 kbit/s" | Included in `song` |
| bpm | String | "bpm": "120 BPM" | Included in `song`|
| mix_engineer | String | "mix_engineer": "Robert Smith II" |  Included in `song` for "Single" and "Multiple" `release_type`, and in `release` for "Album/EP" types (if shared across the entire release, otherwise, in `song`). |
| mastering_engineer | String | "mastering_engineer": "Michael Tyson" | Included in `song` |
| producer | String | "producer": "Simon Cowell" |  Included in `song` for "Single" and "Multiple" `release_type`, and in `release` for "Album/EP" types (if shared across the entire release, otherwise, in `song`). |
| co_producer | String | "co_producer": "Shavaun Dempsey" |  Included in `song` for "Single" and "Multiple" `release_type`, and in `release` for "Album/EP" types (if shared across the entire release, otherwise, in `song`). |
| featured_artists | Array\<Artist\> | "featured_artists": [{"name":"Paul McCartney", isni":"xxxxxxxxx", "links"{"website":"www.paulmccartney.com"} }] | `feautured_artists` should be passed to players along with the `artists`, and should be expected to appear as "artistName(s) ft. featuredArtist(s)".  Contains `isni` and `links` keys, included in `song`  Should be kept minimal. |
| recording_engineer | String | "recording_engineer": "Sharon Liston" |  Included in `song` for "Single" and "Multiple" `release_type`, and in `release` for "Album/EP" types (if shared across the entire release, otherwise, in `song`). |
| explicit | Boolean | "explicit": true |  Included in `song` |
| isrc | String | "isrc": "US-SKG-22-12345" |  Included in `song` |
| iswc | String | "iswc": "T-123456789-Z" |  Included in `song` |
| authors | Array\<Author\> | "authors": [{"name":"Mark Ronson", "ipi:"157896357", "share":"25%"}] | Publishers and authors will be listed here. May contain `ipi`, `role`, and `share`. Included in `song` |
| ipi | String | "authors": [{"name":"Mark Ronson", ipi:"157896357", "role":"Composer/Author", "share":"25%"}] |  Included in `song` within `authors` and `contributing_artists`|
| share | String | "authors": [{"name":"Mark Ronson", ipi:"157896357", "share":"25%"}] |  Included in `song` within `authors`.  Total percentage of all listed authors' shares MUST equal 100% |
| metadata_language | String | "metadata_language": "en-US" | https://tools.ietf.org/search/bcp47 | Included in `song` |
| country_of_origin | String | "country_of_origin": "United States" |  Included in `song` |
| language | String | "language": "en-US" | https://tools.ietf.org/search/bcp47 | Included in `song` |
| derived_from | String | "derived_from" : "Some other work" |  Included in `song`|

### Examples ##

### Single Release ###

```
{
    "721": {
        "<policyId>": {
            "<assetName>": {
                "name": "<releaseName>",
                "image": "<mediaURL>",
                "music_metadata_version": 3,
                "release": {
                        "release_type": "<Single/Multiple>",
                        "release_title": "<releaseTitle>",
                        "distributor": "<distributor>"
                          },
                "files": [
                    {
                        "name": "<fileName>",
                        "mediaType": "<mimeType>",
                        "src": "<mediaURL>",
                        "song": {
                            "song_title": "<songName>",
                            "song_duration": "PT<minutes>M<seconds>S",
                            "track_number": 1,
                            "mood": "<mood>",
                            "artists": [
                                {
                                    "name:": "<artistName>",
                                    "isni": "<isni>",
                                    "links": {
                                            "<linkName>": "<url>",
                                            "<link2Name>": "<url>",
                                            "<link3Name>": "<url>"
                                        }
                                    },
                                {
                                    "name:": "<artistName>",
                                    "isni": "<isni>",
                                    "links": {
                                            "<linkName>": "<url>",
                                            "<link2Name>": "<url>",
                                            "<link3Name>": "<url>"
                                        }
                                    }
                            ],
                            "featured_artists": [
                                {
                                    "name:": "<artistName>",
                                    "isni": "<isni>",
                                    "links": {
                                            "<linkName>": "<url>",
                                            "<link2Name>": "<url>",
                                            "<link3Name>": "<url>"
                                        }
                                    },
                               {
                                    "name:": "<artistName>",
                                    "isni": "<isni>",
                                    "links": {
                                            "<linkName>": "<url>",
                                            "<link2Name>": "<url>",
                                            "<link3Name>": "<url>"
                                        }
                                    }
                            ],
                            "authors": [
                                {
                                        "name": "<authorName>",
                                        "ipi": "<ipi>",
                                        "share": "<percentage>"
                                    },
                                    {
                                        "name": "<authorName>",
                                        "ipi": "<ipi>",
                                        "share": "<percentage>"
                                    },
                                    {
                                        "name": "<authorName>",
                                        "ipi": "<ipi>",
                                        "share": "<percentage>"
                                    }
                            ],
                            "contributing_artists": [
                                {
                                   "name": "<artistName>",
                                        "ipn": "<ipi>",
                                        "role": [
                                            "<roleDescription>",
                                            "<roleDescription>"
                                        ]

                                },
                                 {
                                   "name": "<artistName>",
                                        "ipi": "<ipi>",
                                        "role": [
                                            "<roleDescription>",
                                            "<roleDescription>"
                                        ]

                                },
                                 {
                                   "name": "<artistName>",
                                        "ipi": "<ipi>",
                                        "role": [
                                            "<roleDescription>",
                                            "<roleDescription>"
                                        ]

                                }
                            ],
                            "collection": "<collectionName>",
                            "genres": [
                                "<genre>",
                                "<genre>",
                                "<genre>"
                            ],
                            "copyright": {"master": "℗ <year, copyrightHolder>", "composition": "© <year, copyrightHolder>"}
                        }
                    }

                ]
            }
        }
    }
}
```
### Album Release ###
```
{
    "721": {
        "c00d776a22ca5db986039420b2a9b3f880d593136a9e2262fabeeb58": {
            "ZiplineFromOuterspace": {
                "name": "Refraktal - Zipline From Outerspace",
                "image": "ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2",
                "music_metadata_version": 3,
                "release": {
                    "release_type": "Album/EP",
                    "release_title": "Zipline From Outerspace",
                    "copyright": {
                        "master": "℗ 2024 Refraktal",
                        "composition": "© 2024 Refraktal"
                    },
                    "artists": [
                        {
                            "name:": "Refraktal",
                            "isni": "0000000517483974",
                            "links": {
                                "website": "https://refraktal.com",
                                "exclusive_content": "https://refraktalnft.duckdns.org"
                            }
                        }
                    ],
                    "contributing_artists": [
                        {
                            "name": "Sudo Scientist",
                            "ipi": "1251891449",
                            "role": [
                                "guitar on VOID and Lullaby for My Demons",
                                "synth",
                                "programming"
                            ]
                        },
                        {
                            "name": "RX the Pharm Tech",
                            "ipi": "1251891057",
                            "role": [
                                "guitar on Bellywub",
                                "synth",
                                "programming"
                            ]
                        }
                    ],
                    "genre": [
                        "Electronic",
                        "Experimental",
                        "Psychedelic"
                    ]
                },
                "files": [
                    {
                        "name": "Void",
                        "mediaType": "audio/wav",
                        "src": "ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2",
                        "song": {
                            "song_title": "Void",
                            "song_duration": "PT4M21S",
                            "track_number": 1,
                            "isrc": "US-SKG-22-12345",
                            "iswc": "T-123456789-Z"
                        }
                    },
                    {
                        "name": "Bellywub",
                        "mediaType": "audio/wav",
                        "src": "ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2",
                        "song": {
                            "song_title": "Bellywub",
                            "song_duration": "PT5M31S",
                            "track_number": 2,
                            "isrc": "US-SKG-22-12345",
                            "iswc": "T-123456789-Z"
                        }
                    },
                    {
                        "name": "Lullaby for my Demons",
                        "mediaType": "audio/wav",
                        "src": "ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2",
                        "song": {
                            "song_title": "Lullaby for My Demons",
                            "song_duration": "PT3M11S",
                            "track_number": 3,
                            "isrc": "US-SKG-22-12345",
                            "iswc": "T-123456789-Z"
                        }
                    },
                    {
                        "name": "Meliorism",
                        "mediaType": "audio/wav",
                        "src": "ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2",
                        "song": {
                            "song_title": "Meliorism",
                            "song_duration": "PT4M21S",
                            "track_number": 4,
                            "isrc": "US-SKG-22-12345",
                            "iswc": "T-123456789-Z"
                        }
                    },
                    {
                        "name": "Zipline From Outerspace",
                        "mediaType": "audio/wav",
                        "src": "ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2",
                        "song": {
                            "song_title": "Zipline From Outerspace",
                            "song_duration": "PT3M36S",
                            "track_number": 5,
                            "isrc": "US-SKG-22-12345",
                            "iswc": "T-123456789-Z"
                        }
                    },
                    {
                        "name": "Another Cool Song",
                        "mediaType": "audio/wav",
                        "src": "ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2",
                        "song": {
                            "song_title": "Another Cool Song",
                            "song_duration": "PT2M12S",
                            "track_number": 6,
                            "isrc": "US-SKG-22-12345",
                            "iswc": "T-123456789-Z"
                        }
                    },
                    {
                        "name": "Another Cool Song",
                        "mediaType": "audio/wav",
                        "src": "ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2",
                        "song": {
                            "song_title": "Another Cool Song",
                            "song_duration": "PT3M36S",
                            "track_number": 7,
                            "isrc": "US-SKG-22-12345",
                            "iswc": "T-123456789-Z"
                        }
                    },
                    {
                        "name": "Another Cool Song",
                        "mediaType": "audio/wav",
                        "src": "ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2",
                        "song": {
                            "song_title": "Another Cool Song",
                            "song_duration": "PT3M36S",
                            "track_number": 8,
                            "isrc": "US-SKG-22-12345",
                            "iswc": "T-123456789-Z"
                        }
                    },
                    {
                        "name": "Another Cool Song",
                        "mediaType": "audio/wav",
                        "src": "ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2",
                        "song": {
                            "song_title": "Another Cool Song",
                            "song_duration": "PT3M36S",
                            "track_number": 9,
                            "isrc": "US-SKG-22-12345",
                            "iswc": "T-123456789-Z"
                        }
                    },
                    {
                        "name": "Another Cool Song",
                        "mediaType": "audio/wav",
                        "src": "ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2",
                        "song": {
                            "song_title": "Another Cool Song",
                            "song_duration": "PT3M36S",
                            "track_number": 10,
                            "isrc": "US-SKG-22-12345",
                            "iswc": "T-123456789-Z"
                        }
                    },
                    {
                        "name": "Another Cool Song",
                        "mediaType": "audio/wav",
                        "src": "ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2",
                        "song": {
                            "song_title": "Another Cool Song",
                            "song_duration": "PT3M36S",
                            "track_number": 11,
                            "isrc": "US-SKG-22-12345",
                            "iswc": "T-123456789-Z"
                        }
                    },
                    {
                        "name": "Another Cool Song",
                        "mediaType": "audio/wav",
                        "src": "ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2",
                        "song": {
                            "song_title": "Another Cool Song",
                            "song_duration": "PT3M36S",
                            "track_number": 12,
                            "isrc": "US-SKG-22-12345",
                            "iswc": "T-123456789-Z"
                        }
                    },
                    {
                        "name": "Another Cool Song",
                        "mediaType": "audio/wav",
                        "src": "ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2",
                        "song": {
                            "song_title": "Another Cool Song",
                            "song_duration": "PT3M36S",
                            "track_number": 13,
                            "isrc": "US-SKG-22-12345",
                            "iswc": "T-123456789-Z"
                        }
                    },
                    {
                        "name": "Another Cool Song",
                        "mediaType": "audio/wav",
                        "src": "ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2",
                        "song": {
                            "song_title": "Another Cool Song",
                            "song_duration": "PT3M36S",
                            "track_number": 14,
                            "isrc": "US-SKG-22-12345",
                            "iswc": "T-123456789-Z"
                        }
                    },
                    {
                        "name": "Another Cool Song",
                        "mediaType": "audio/wav",
                        "src": "ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2",
                        "song": {
                            "song_title": "Another Cool Song",
                            "song_duration": "PT3M36S",
                            "track_number": 15,
                            "isrc": "US-SKG-22-12345",
                            "iswc": "T-123456789-Z"
                        }
                    },
                    {
                        "name": "Another Cool Song",
                        "mediaType": "audio/wav",
                        "src": "ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2",
                        "song": {
                            "song_title": "Another Cool Song",
                            "song_duration": "PT3M36S",
                            "track_number": 16,
                            "isrc": "US-SKG-22-12345",
                            "iswc": "T-123456789-Z"
                        }
                    },
                    {
                        "name": "Another Cool Song",
                        "mediaType": "audio/wav",
                        "src": "ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2",
                        "song": {
                            "song_title": "Another Cool Song",
                            "song_duration": "PT3M36S",
                            "track_number": 17,
                            "isrc": "US-SKG-22-12345",
                            "iswc": "T-123456789-Z"
                        }
                    },
                    {
                        "name": "Another Cool Song",
                        "mediaType": "audio/wav",
                        "src": "ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2",
                        "song": {
                            "song_title": "Another Cool Song",
                            "song_duration": "PT3M36S",
                            "track_number": 18,
                            "isrc": "US-SKG-22-12345",
                            "iswc": "T-123456789-Z"
                        }
                    },
                    {
                        "name": "Another Cool Song",
                        "mediaType": "audio/wav",
                        "src": "ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2",
                        "song": {
                            "song_title": "Another Cool Song",
                            "song_duration": "PT3M36S",
                            "track_number": 19,
                            "isrc": "US-SKG-22-12345",
                            "iswc": "T-123456789-Z"
                        }
                    },
                    {
                        "name": "Another Cool Song",
                        "mediaType": "audio/wav",
                        "src": "ipfs://QmeeHGqiRo8gvAfhG6MuHSTKv6rQpw2bxbnDkAPYvt9jD2",
                        "song": {
                            "song_title": "Another Cool Song",
                            "song_duration": "PT3M36S",
                            "track_number": 20,
                            "isrc": "US-SKG-22-12345",
                            "iswc": "T-123456789-Z"
                        }
                    }
                ]
            }
        }
    }
}
```
#### CIP-68 ###

```
{
    "constructor": 0,
    "fields": [
        {
            "map": [
                {"k": {"bytes": "373231"}, "v": {
                    "map": [
                        {"k": {"bytes": "<encoded policyId>"}, "v": {
                            "map": [
                                {"k": {"bytes": "<encoded assetName>"}, "v": {
                                    "map": [
                                        {"k": {"bytes": "6E616D65"}, "v": {"bytes": "<encoded releaseName>"}},
                                        {"k": {"bytes": "696D616765"}, "v": {"bytes": "<encoded mediaURL>"}},
                                        {"k": {"bytes": "6D757369635F6D657461646174615F76657273696F6E"}, "v": {"int": 3}},
                                        {"k": {"bytes": "72656C65617365"}, "v":
                                            {
                                                "map": [
                                                    {"k": {"bytes": "72656C656173655F74797065"}, "v": {"bytes": "<encoded Single/Multiple>"}},
                                                    {"k": {"bytes": "72656C656173655F7469746C65"}, "v": {"bytes": "<encoded releaseTitle>"}},
                                                    {"k": {"bytes": "6469737472696275746F72"}, "v": {"bytes": "<encoded distributor>"}}
                                                ]
                                            }
                                        },
                                        {"k": {"bytes": "66696C6573"}, "v":
                                            {
                                                "array": [
                                                    {
                                                        "map": [
                                                            {"k": {"bytes": "6E616D65"}, "v": {"bytes": "<encoded fileName>"}},
                                                            {"k": {"bytes": "6D6564696154797065"}, "v": {"bytes": "<encoded mimeType>"}},
                                                            {"k": {"bytes": "737263"}, "v": {"bytes": "<encoded mediaURL>"}},
                                                            {"k": {"bytes": "736F6E67"}, "v":
                                                                {
                                                                    "map": [
                                                                        {"k": {"bytes": "736F6E675F7469746C65"}, "v": {"bytes": "<encoded songName>"}},
                                                                        {"k": {"bytes": "736F6E675F6475726174696F6E"}, "v": {"bytes": "<encoded PT<minutes>M<seconds>S>"}},
                                                                        {"k": {"bytes": "747261636B5F6E756D626572"}, "v": {"int": track#}},
                                                                        {"k": {"bytes": "6D6F6F64"}, "v": {"bytes": "<encoded mood>"}},
                                                                        {"k": {"bytes": "61727469737473"}, "v":
                                                                            {
                                                                                "array": [
                                                                                    {
                                                                                        "map": [
                                                                                            {"k": {"bytes": "6E616D65"}, "v": {"bytes": "<encoded artistName>"}},
                                                                                            {"k": {"bytes": "69736E69"}, "v": {"bytes": "<encoded ISNI>"}},
                                                                                            {"k": {"bytes": "6C696E6B73"}, "v":
                                                                                                {
                                                                                                    "map": [
                                                                                                        {"k": {"bytes": "<encoded linkName>"}, "v": {"bytes": "<encoded url>"}},
                                                                                                        {"k": {"bytes": "<encoded link2Name>"}, "v": {"bytes": "<encoded url>"}},
                                                                                                        {"k": {"bytes": "<encoded link3Name>"}, "v": {"bytes": "<encoded url>"}}
                                                                                                    ]
                                                                                                }
                                                                                            }
                                                                                        ]
                                                                                    }
                                                                                ]
                                                                            }
                                                                        },
                                                                        {"k": {"bytes": "6665617475726564_61727469737473"}, "v":
                                                                            {
                                                                                "array": [
                                                                                    {
                                                                                        "map": [
                                                                                            {"k": {"bytes": "6E616D65"}, "v": {"bytes": "<encoded artistName>"}},
                                                                                            {"k": {"bytes": "69736E69"}, "v": {"bytes": "<encoded ISNI>"}},
                                                                                            {"k": {"bytes": "6C696E6B73"}, "v":
                                                                                                {
                                                                                                    "map": [
                                                                                                        {"k": {"bytes": "<encoded linkName>"}, "v": {"bytes": "<encoded url>"}},
                                                                                                        {"k": {"bytes": "<encoded link2Name>"}, "v": {"bytes": "<encoded url>"}},
                                                                                                        {"k": {"bytes": "<encoded link3Name>"}, "v": {"bytes": "<encoded url>"}}
                                                                                                    ]
                                                                                                }
                                                                                            }
                                                                                        ]
                                                                                    }
                                                                                ]
                                                                            }
                                                                        },
                                                                        {"k": {"bytes": "617574686F7273"}, "v":
                                                                            {
                                                                                "array": [
                                                                                    {
                                                                                        "map": [
                                                                                            {"k": {"bytes": "6E616D65"}, "v": {"bytes": "<encoded authorName>"}},
                                                                                            {"k": {"bytes": "697069"}, "v": {"bytes": "<encoded IPI>"}},
                                                                                            {"k": {"bytes": "7368617265"}, "v": {"bytes": "<encoded percentage>"}}
                                                                                        ]
                                                                                    }
                                                                                ]
                                                                            }
                                                                        },
                                                                        {"k": {"bytes": "636F6E747269627574696E675F61727469737473"}, "v":
                                                                            {
                                                                                "array": [
                                                                                    {
                                                                                        "map": [
                                                                                            {"k": {"bytes": "6E616D65"}, "v": {"bytes": "<encoded artistName>"}},
                                                                                            {"k": {"bytes": "697069"}, "v": {"bytes": "<encoded IPI>"}},
                                                                                            {"k": {"bytes": "726F6C65"}, "v":
                                                                                                {
                                                                                                    "array": [
                                                                                                        {"bytes": "<encoded roleDescription>"},
                                                                                                        {"bytes": "<encoded roleDescription>"}
                                                                                                    ]
                                                                                                }
                                                                                            }
                                                                                        ]
                                                                                    }
                                                                                ]
                                                                            }
                                                                        },
                                                                        {"k": {"bytes": "636F6C6C656374696F6E"}, "v": {"bytes": "<encoded collectionName>"}},
                                                                        {"k": {"bytes": "67656E726573"}, "v":
                                                                            {
                                                                                "array": [
                                                                                    {"bytes": "<encoded genre1>"},
                                                                                    {"bytes": "<encoded genre2>"},
                                                                                    {"bytes": "<encoded genre3>"}
                                                                                ]
                                                                            }
                                                                        },
                                                                        {"k": {"bytes": "636F707972696768"}, "v":
                                                                            {
                                                                                "map": [
                                                                                    {"k": {"bytes": "6D6173746572"}, "v": {"bytes": "<encoded ℗ <year, copyrightHolder>"}},
                                                                                    {"k": {"bytes": "636F6D706F736974696F6E"}, "v": {"bytes": "<encoded © <year, copyrightHolder>"}}
                                                                                ]
                                                                            }
                                                                        }
                                                                    ]
                                                                }
                                                            }
                                                        ]
                                                    }
                                                ]
                                            }
                                        }
                                    ]
                                }}
                            ]
                        }}
                    ]
                }}
            ]
        },
        {
            "int": 1
        }
    ]
}
```

## Rationale: how does this CIP achieve its goals?

Implementing this simplifies and commonizes the process for creating music tokens on Cardano. It greatly simplifies the work that apps have to make when consuming such tokens.

This CIP is the result of several online meetings between many different companies building music-related projects on top of Cardano. These meetings were organized as many in the community started to see fragmentation in the way music NFTs were being minted on Cardano. These meetings gave the opportunity for a bit of a reset and will allow a much brighter future for music on Cardano. As long as all projects agree on some of these basic fields, there is great flexibility in this CIP to do application-specific unique things on top of the music NFT itself. The CIP is intentionally open-ended and can be updated in future versions if there are additional fields that the wider group could benefit from.

## Path to Active

### Acceptance Criteria

- [x] Has been implemented by a number of parties, including:
- [x] SickCityNFT - sickcity.xyz
- [x] NEWM - newm.io
- [x] SoundRig - soundrig.io
- [x] The Listening Room - https://thelr.io/
- [x] Jukeboys
- [x] So Litty Records
- [x] Arp Radio - https://arpradio.media

### Implementation Plan

- [x] Consensus of companies building music-related Cardano projects to develop a mutually beneficial metadata vocabulary.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode)

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0067/README.md
---

- --
CIP: 67
Title: Asset Name Label Registry
Status: Proposed
Category: Tokens
Authors:
- Alessandro Konrad <alessandro.konrad@live.de>
- Thomas Vellekoop <thomas.vellekoop@iohk.io>
Implementors: N/A
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/298
- https://github.com/cardano-foundation/CIPs/pull/586
Created: 2022-07-13
License: CC-BY-4.0
- --

## Abstract

This proposal defines a standard to identify Cardano native assets by the asset name to put them in an asset class, as intended by their issuer.

## Motivation: why is this CIP necessary?

As more assets are minted and different standards emerge to query data for these assets, it's getting harder for 3rd parties to determine the asset class and associated extra assumptions that may arise from this identification. For example, if an asset is identified as a non-fungible token, a third party is interested in its onchain associated metadata. This standard is similar to [CIP-0010](../CIP-0010), but focuses on the asset name of a native asset.

## Specification

To give issuers the option to classify assets, the `asset_name` MUST be prefixed with 4 bytes encoding the following binary value:
```
[ 0000 | 16 bits label_num | 8 bits checksum | 0000 ]
```
- The leading and ending four 0s are brackets
- `label_num` has a fixed size of 2 bytes (`Label range in decimal: [0, 65535]`).
If `label_num` < 2 bytes, the remaining bits MUST be left-padded with 0s.
- `checksum` has a fixed size of 1 byte. The checksum is calculated by applying the [CRC-8](#CRC-8) algorithm on the `label_num (including the padded 0s)`.

### CRC-8

- Polynomial: `0x07`
- Lookup table:
```
[
  0x00, 0x07, 0x0e, 0x09, 0x1c, 0x1b, 0x12, 0x15, 0x38, 0x3f, 0x36, 0x31, 0x24,
  0x23, 0x2a, 0x2d, 0x70, 0x77, 0x7e, 0x79, 0x6c, 0x6b, 0x62, 0x65, 0x48, 0x4f,
  0x46, 0x41, 0x54, 0x53, 0x5a, 0x5d, 0xe0, 0xe7, 0xee, 0xe9, 0xfc, 0xfb, 0xf2,
  0xf5, 0xd8, 0xdf, 0xd6, 0xd1, 0xc4, 0xc3, 0xca, 0xcd, 0x90, 0x97, 0x9e, 0x99,
  0x8c, 0x8b, 0x82, 0x85, 0xa8, 0xaf, 0xa6, 0xa1, 0xb4, 0xb3, 0xba, 0xbd, 0xc7,
  0xc0, 0xc9, 0xce, 0xdb, 0xdc, 0xd5, 0xd2, 0xff, 0xf8, 0xf1, 0xf6, 0xe3, 0xe4,
  0xed, 0xea, 0xb7, 0xb0, 0xb9, 0xbe, 0xab, 0xac, 0xa5, 0xa2, 0x8f, 0x88, 0x81,
  0x86, 0x93, 0x94, 0x9d, 0x9a, 0x27, 0x20, 0x29, 0x2e, 0x3b, 0x3c, 0x35, 0x32,
  0x1f, 0x18, 0x11, 0x16, 0x03, 0x04, 0x0d, 0x0a, 0x57, 0x50, 0x59, 0x5e, 0x4b,
  0x4c, 0x45, 0x42, 0x6f, 0x68, 0x61, 0x66, 0x73, 0x74, 0x7d, 0x7a, 0x89, 0x8e,
  0x87, 0x80, 0x95, 0x92, 0x9b, 0x9c, 0xb1, 0xb6, 0xbf, 0xb8, 0xad, 0xaa, 0xa3,
  0xa4, 0xf9, 0xfe, 0xf7, 0xf0, 0xe5, 0xe2, 0xeb, 0xec, 0xc1, 0xc6, 0xcf, 0xc8,
  0xdd, 0xda, 0xd3, 0xd4, 0x69, 0x6e, 0x67, 0x60, 0x75, 0x72, 0x7b, 0x7c, 0x51,
  0x56, 0x5f, 0x58, 0x4d, 0x4a, 0x43, 0x44, 0x19, 0x1e, 0x17, 0x10, 0x05, 0x02,
  0x0b, 0x0c, 0x21, 0x26, 0x2f, 0x28, 0x3d, 0x3a, 0x33, 0x34, 0x4e, 0x49, 0x40,
  0x47, 0x52, 0x55, 0x5c, 0x5b, 0x76, 0x71, 0x78, 0x7f, 0x6a, 0x6d, 0x64, 0x63,
  0x3e, 0x39, 0x30, 0x37, 0x22, 0x25, 0x2c, 0x2b, 0x06, 0x01, 0x08, 0x0f, 0x1a,
  0x1d, 0x14, 0x13, 0xae, 0xa9, 0xa0, 0xa7, 0xb2, 0xb5, 0xbc, 0xbb, 0x96, 0x91,
  0x98, 0x9f, 0x8a, 0x8d, 0x84, 0x83, 0xde, 0xd9, 0xd0, 0xd7, 0xc2, 0xc5, 0xcc,
  0xcb, 0xe6, 0xe1, 0xe8, 0xef, 0xfa, 0xfd, 0xf4, 0xf3,
]
```

### Example:

#### Construct a label
We want to use the decimal label `222` for an asset name:

1. Convert to hex and pad with missing 0s => `0x00de`
2. Calculate CRC-8 checksum => `0x14`
3. Add brackets and combine label => `0x000de140`

#### Verify a label
We have the following asset name: `0x000de140`

1. Slice off the first 4 bytes of the asset name => `0x000de140`
2. Check if first 4 bits and last 4 bits are `0b0000` (`0x0`)
3. Slice off the 2 `label_num` bytes and apply them to the CRC-8 algorithm. If the result matches with the `checksum` byte, a `valid` label was found and it can be returned. => `0x00de`
4. Convert to decimal => `222`

### Reserved labels

These are the reserved `asset_name_label` values

| `asset_name_label` | class | description |
|--------------------|-------|-------------|
| 0 - 15             | -     | private use |

### Adding an entry to the registry

> The keywords "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL
> NOT", "SHOULD", "SHOULD NOT", "RECOMMENDED",  "MAY", and
> "OPTIONAL" in this section are to be interpreted as described in
> [RFC 2119](https://datatracker.ietf.org/doc/html/rfc2119).

Those wishing to propose an addition to this registry **MUST** draft a new CIP describing the standard for implementing
the token. Once the CIP has achieved the `Under Review` status the proposer **SHALL** make the necessary edits to
[registry.json](./registry.json). These changes **SHOULD** be submitted under a separate pull request against the CIP
repository and include a brief description of the standard and a link to the CIP Pull Request describing implementation
details.

### Test Vectors

Keys represent labels in `decimal` numbers. Values represent the entire label, including brackets and checksum in `hex`:

```yaml
0     : 00000000
1     : 00001070
23    : 00017650
99    : 000632e0
533   : 00215410
2000  : 007d0550
4567  : 011d7690
11111 : 02b670b0
49328 : 0c0b0f40
65535 : 0ffff240
```

## Rationale: how does this CIP achieve its goals?

Asset name labels make it easy to identify native assets and classify them in their asset class intended by the issuer. Since the identification of these native assets is done by third parties, the design is focused on the usability for them.

First, the label should be quickly parsable with a first check. That is, an initial check on an asset name that is easy and will exclude a big subset of the available token names that do not follow standard. This is why the label starts and ends with `0000` in bits. Additionally, in its hex notation, this is differentiable by a human in its readable form, a more common representation.

Secondly, the remaining verification on whether a certain `asset_name_label` standard is followed should be a one shot calculation. Here we mean that the calculation of the check should be straightforward, the label should not be fitted via brute force by a third party. That's why the label contains the bit representation of the integer label it tries to follow.

Another thing that is important to understand is that an oblivious token issuer might not be aware of this standard. This could lead to the unintentional misinterpretation by third parties and injection attacks. We can minimize this attack vector by making the label format obscure. That is why the label also contains a checksum derived from the `asset_name_label` to add characters that are deterministically derived but look like nonsense. Together with the above zero "brackets", and the fixed size binary encoding, it make it unlikely someone follows this standard accidentally. The CRC-8 checksum is chosen for it low-impact on resources and its readily available implementations in multiple languages.

## Path to Active

### Acceptance Criteria

- [X] Get support for this CIP by wallets, explorers, minting platforms and other 3rd parties.
- [X] Get support by tools/libraries like Lucid, PlutusTx, cardano-cli, etc. to generate/verify labels.

### Implementation Plan

- [X] Provide reference implementations:
- [X] [Lucid TypeScript implementation of toLabel/fromLabel](https://github.com/spacebudz/lucid/blob/39cd2129101bd11b03b624f80bb5fe3da2537fec/src/utils/utils.ts#L500-L522)
- [X] [Lucid TypeScript implementation of CRC-8](https://github.com/spacebudz/lucid/blob/main/src/misc/crc8.ts)

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0068/README.md
---

- --
CIP: 68
Title: Datum Metadata Standard
Status: Active
Category: Tokens
Authors:
- Alessandro Konrad <alessandro.konrad@live.de>
- Thomas Vellekoop <thomas.vellekoop@iohk.io>
Implementors:
- Alessandro Konrad (SpaceBudz)
- 5Binaries (Blockfrost)
- Smaug (Pool.pm)
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/299
- https://github.com/cardano-foundation/CIPs/pull/359
- https://github.com/cardano-foundation/CIPs/pull/458
- https://github.com/cardano-foundation/CIPs/pull/471
- https://github.com/cardano-foundation/CIPs/pull/494
- https://github.com/cardano-foundation/CIPs/issues/520
- https://github.com/cardano-foundation/CIPs/pull/586
Created: 2022-07-13
License: CC-BY-4.0
- --

## Abstract

This proposal defines a metadata standard for native assets making use of output datums not only for NFTs but any asset
class.

## Motivation: why is this CIP necessary?

This proposal addresses a few shortcomings
of [CIP-0025](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0025):

- Lack of programmability;
- Difficult metadata update / evolution;
- Non-inspectable metadata from within Plutus validators

Besides these shortcomings CIP-0025 has
some [flaws](https://github.com/cardano-foundation/CIPs/pull/85#issuecomment-1054123645) in its design.
For people unaware of CIP-0025 or want to use a different way of minting or want to use a different metadata
format/mechanism you open up a protocol to metadata spoofing, because this standard is so established and metadata in
minting transactions are interpreted by most platforms by default. Since this standard is not enforced at the protocol
level there is no guarantee everyone will be aware of it or follow the rules. At the same time you limit and constraint
the capabilities of the ledger if everyone was forced to follow the rules of CIP-0025.

This standard tackles all these problems and offers many more advantages, not only for NFTs, but also for any asset
class that may follow. Additionally, this CIP will introduce a way to classify tokens so that third parties like wallets
can easily know what the kind of token it is.

## Specification

### Considerations

The basic idea is to have two assets issued, where one references the other. We call these two a `reference NFT` and
an `user token`, where the `user token` can be an NFT, FT or any other asset class that is transferable and represents
any value. So, the `user token` is the actual asset that lives in a user's wallet.

To find the metadata for the `user token` you need to look for the output, where the `reference NFT` is locked in. How
this is done concretely will become clear below. Moreover, this output contains a datum, which holds the metadata. The
advantage of this approach is that the issuer of the assets can decide how the transaction output with
the `reference NFT` is locked and further handled. If the issuer wants complete immutable metadata, the `reference NFT`
can be locked at the address of an unspendable script. Similarly, if the issuer wants the NFTs/FTs to evolve or wants a
mechanism to update the metadata, the `reference NFT` can be locked at the address of a script with arbitrary logic that
the issuer decides.

Lastly and most importantly, with this construction, the metadata can be used by a Plutus V2 script with the use of
reference inputs [CIP-0031](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0031). This will drive further
innovation in the token space.

### Labels

Each asset name must be prefixed by a label. The intent of this label is to identify the purpose of the token. For
example, a reference NFT is identified by the label 100 and so every token considered a reference NFT should start its
asset name with the hex `000643b0`. This is
following [CIP-0067](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0067), which specifies how the label
prefix should be formatted.

Examples of asset names:

| asset_name_label | asset_name_content | resulting_label_hex | resulting_content_hex | resulting_asset_name_hex     |
|------------------|--------------------|---------------------|-----------------------|------------------------------|
| 100              | GenToken           | 000643b0            | 47656e546f6b656e      | 000643b047656e546f6b656e     |
| 100              | NeverGonna         | 000643b0            | 4e65766572476f6e6e61  | 000643b04e65766572476f6e6e61 |
| 222              | GiveYouUp          | 000de140            | 47697665596f755570    | 000de14047697665596f755570   |

For simplicity purposes, the document will use the label `(100)` or `(<label>)` in the following documentation, but
understand it should follow the CIP-0067 specification.

### Reference NFT label

This is the registered `asset_name_label` value

| asset_name_label | class | description                                           |
|------------------|-------|-------------------------------------------------------|
| 100              | NFT   | Reference NFT locked at a script containing the datum |

### Constraints and conditions

For a correct relationship between the `user token` and the `reference NFT` a few conditions MUST be met.

- The `user token` and `reference NFT` MUST be under the same policy ID.
- For a specific `user token` there MUST exist exactly **one** `reference NFT`
- The `user token` and associated `reference NFT` MUST follow the standard naming pattern. The asset name of both assets
  is prefixed with its respective `asset_name_label` followed by a pattern defined by the asset class (e.g.
  asset_name_label 222)

Some remarks about the above,

1. The `user token` and `reference NFT` do not need to be minted in the same transaction. The order of minting is also
   not important.
2. It may be the case that there can be multiple `user tokens` (multiple asset names or quantity greater than 1)
   referencing the same `reference NFT`.

The datum in the output with the `reference NFT` contains the metadata at the first field of the constructor 0. The
version number is at the second field of this constructor. The third field allows for arbitrary plutus data. This could
be useful to forward relevant data to the plutus script:

```
big_int = int / big_uint / big_nint
big_uint = #6.2(bounded_bytes)
big_nint = #6.3(bounded_bytes)

metadata =
  { * metadata => metadata }
  / [ * metadata ]
  / big_int
  / bounded_bytes

version = int

; Custom user defined plutus data.
; Setting data is optional, but the field is required
; and needs to be at least Unit/Void: #6.121([])
extra = plutus_data

datum = #6.121([metadata, version, extra])
```

#### 222 NFT Standard

> **Note** Since `version >= 1`

Besides the necessary standard for the `reference NFT` we're introducing three specific token standards in this CIP.
Note that the possibilities are endless here and more standards can be built on top of this CIP for FTs, other NFTs,
rich fungible tokens, etc. The first is the `222` NFT standard with the registered `asset_name_label` prefix value

| asset_name_label | class | description                                                          |
|------------------|-------|----------------------------------------------------------------------|
| 222              | NFT   | NFT held by the user's wallet making use of CIP-0025 inner structure |

##### Class

The `user token` represents an NFT (non-fungible token).

##### Pattern

The `user token` and `reference NFT` MUST have an identical name, preceded by the `asset_name_label` prefix.

Example:\
`user token`: `(222)Test123`\
`reference NFT`: `(100)Test123`

##### Metadata

This is a low-level representation of the metadata, following closely the structure of CIP-0025. All UTF-8 encoded keys
and values need to be converted into their respective byte's representation when creating the datum on-chain.

```
files_details =
  {
    ? name : bounded_bytes, ; UTF-8
    mediaType : bounded_bytes, ; UTF-8
    src : uri,
    ; ... Additional properties are allowed
  }

metadata =
  {
    name : bounded_bytes, ; UTF-8

    ; The image URI must point to a resource with media type (mime type) `image/*`
    ; (for example `image/png`, `image/jpeg`, `image/svg+xml`, etc.)
    image : uri,

    ? description : bounded_bytes, ; UTF-8
    ? files : [* files_details]
    ; ... Additional properties are allowed
  }

; A valid Uniform Resource Identifier (URI) as a UTF-8 encoded bytestring.
; The URI scheme must be one of `https` (HTTP), `ipfs` (IPFS), `ar` (Arweave) or `data` (on-chain).
; Data URLs (on-chain data) must comply to RFC2397.
uri = bounded_bytes / [ * bounded_bytes ] ; UTF-8

; Custom user defined plutus data.
; Setting data is optional, but the field is required
; and needs to be at least Unit/Void: #6.121([])
extra = plutus_data

datum = #6.121([metadata, version, extra])

version = 1 / 2 / 3
```

Example datum as JSON:

```json
{
  "constructor": 0,
  "fields": [
    {
      "map": [
        {
          "k": {
            "bytes": "6E616D65"
          },
          "v": {
            "bytes": "5370616365427564"
          }
        },
        {
          "k": {
            "bytes": "696D616765"
          },
          "v": {
            "bytes": "697066733A2F2F74657374"
          }
        }
      ]
    },
    {
      "int": 1
    }
  ]
}
```

##### Retrieve metadata as 3rd party

A third party has the following NFT `d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(222)TestToken` they want
to lookup. The steps are

1. Construct `reference NFT` from `user token`: `d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(100)TestToken`
2. Look up `reference NFT` and find the output it's locked in.
3. Get the datum from the output and lookup metadata by going into the first field of constructor 0.
4. Convert to JSON and encode all string entries to UTF-8 if possible, otherwise leave them in hex.

##### Retrieve metadata from a Plutus validator

We want to bring the metadata of the NFT `d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(222)TestToken` in
the Plutus validator context. To do this we

1. Construct `reference NFT`
   from `user token`: `d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(100)TestToken` (off-chain)
2. Look up `reference NFT` and find the output it's locked in. (off-chain)
3. Reference the output in the transaction. (off-chain)
4. Verify validity of datum of the referenced output by checking if policy ID of `reference NFT` and `user token` and
   their asset names without the `asset_name_label` prefix match. (on-chain)

#### 333 FT Standard

> **Note** Since `version >= 1`

The second introduced standard is the `333` FT standard with the registered `asset_name_label` prefix value

| asset_name_label | class | description                                                                                      |
|------------------|-------|--------------------------------------------------------------------------------------------------|
| 333              | FT    | FT hold by the user's wallet making use of Cardano foundation off-chain registry inner structure |

##### Class

The `user token` is an FT (fungible token).

##### Pattern

The `user token` and `reference NFT` MUST have an identical name, preceded by the `asset_name_label` prefix.

Example:\
`user token`: `(333)Test123`\
`reference NFT`: `(100)Test123`

##### Metadata

This is a low-level representation of the metadata, following closely the structure of the Cardano foundation off-chain
metadata registry. All UTF-8 encoded keys and values need to be converted into their respective byte's representation
when creating the datum on-chain.

```
; Explanation here: https://developers.cardano.org/docs/native-tokens/token-registry/cardano-token-registry/

metadata =
  {
    name : bounded_bytes, ; UTF-8
    description : bounded_bytes, ; UTF-8
    ? ticker: bounded_bytes, ; UTF-8
    ? url: bounded_bytes, ; UTF-8
    ? decimals: int

    ; 'logo' does not follow the explanation of the token-registry, it needs to be a valid URI and not a plain bytestring.
    ; The logo URI must point to a resource with media type (mime type) `image/png`, `image/jpeg` or `image/svg+xml`.
    ? logo: uri,

    ; ... Additional properties are allowed
  }

; A valid Uniform Resource Identifier (URI) as a UTF-8 encoded bytestring.
; The URI scheme must be one of `https` (HTTP), `ipfs` (IPFS), `ar` (Arweave) or `data` (on-chain).
; Data URLs (on-chain data) must comply to RFC2397.
uri = bounded_bytes / [ * bounded_bytes ] ; UTF-8

; Custom user defined plutus data.
; Setting data is optional, but the field is required
; and needs to be at least Unit/Void: #6.121([])
extra = plutus_data

datum = #6.121([metadata, version, extra])

version = 1 / 2 / 3
```

Example datum as JSON:

```json
{
  "constructor": 0,
  "fields": [
    {
      "map": [
        {
          "k": {
            "bytes": "6E616D65"
          },
          "v": {
            "bytes": "5370616365427564"
          }
        },
        {
          "k": {
            "bytes": "6465736372697074696F6E"
          },
          "v": {
            "bytes": "54686973206973206D79207465737420746F6B656E"
          }
        }
      ]
    },
    {
      "int": 1
    }
  ]
}
```

##### Retrieve metadata as 3rd party

A third party has the following FT `d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(333)TestToken` they want
to lookup. The steps are

1. Construct `reference NFT` from `user token`: `d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(100)TestToken`
2. Look up `reference NFT` and find the output it's locked in.
3. Get the datum from the output and lookup metadata by going into the first field of constructor 0.
4. Convert to JSON and encode all string entries to UTF-8 if possible, otherwise leave them in hex.

##### Retrieve metadata from a Plutus validator

We want to bring the metadata of the FT `d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(333)TestToken` in the
Plutus validator context. To do this we

1. Construct `reference NFT`
   from `user token`: `d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(100)TestToken` (off-chain)
2. Look up `reference NFT` and find the output it's locked in. (off-chain)
3. Reference the output in the transaction. (off-chain)
4. Verify validity of datum of the referenced output by checking if policy ID of `reference NFT` and `user token` and
   their asset names without the `asset_name_label` prefix match. (on-chain)

#### 444 RFT Standard

> **Warning** Since `version >= 2`

The third introduced standard is the `444` Rich-FT standard with the registered `asset_name_label` prefix value

| asset_name_label | class | description                                                                                                                                     |
|------------------|-------|-------------------------------------------------------------------------------------------------------------------------------------------------|
| 444              | RFT   | RFT hold by the user's wallet making use of the union of CIP-0025 inner structure AND the Cardano foundation off-chain registry inner structure |

Rich-Fungible tokens don't fit cleanly into the other two FT/NFT classes of tokens and thus need their own standard. An
example of an RFT would be a fractionalized NFT. The single reference NFT `(100)` represents the NFT itself, and the
many `(444)` tokens represent the fractionalized shares. Minting 100 tokens and setting decimals to 2 would represent a
single NFT that is split into 100 fractions.

##### Class

The `user token` is an RFT (rich-fungible token).

##### Pattern

The `user token` and `reference NFT` MUST have an identical name, preceded by the `asset_name_label` prefix.

Example:\
`user token`: `(444)Test123`\
`reference NFT`: `(100)Test123`

##### Metadata

This is a low-level representation of the metadata, following closely the structure of CIP-0025 with the optional
decimals field added. All UTF-8 encoded keys and values need to be converted into their respective byte's representation
when creating the datum on-chain.

```
files_details =
  {
    ? name : bounded_bytes, ; UTF-8
    mediaType : bounded_bytes, ; UTF-8
    src : uri,
    ; ... Additional properties are allowed
  }

metadata =
  {
    name : bounded_bytes, ; UTF-8

    ; The image URI must point to a resource with media type (mime type) `image/*`
    ; (for example `image/png`, `image/jpeg`, `image/svg+xml`, etc.)
    image : uri,

    ? description : bounded_bytes, ; UTF-8
    ? decimals: int,
    ? files : [* files_details]
    ; ... Additional properties are allowed
  }

; A valid Uniform Resource Identifier (URI) as a UTF-8 encoded bytestring.
; The URI scheme must be one of `https` (HTTP), `ipfs` (IPFS), `ar` (Arweave) or `data` (on-chain).
; Data URLs (on-chain data) must comply to RFC2397.
uri = bounded_bytes ; UTF-8

; Custom user defined plutus data.
; Setting data is optional, but the field is required
; and needs to be at least Unit/Void: #6.121([])
extra = plutus_data

datum = #6.121([metadata, version, extra])

version = 3
```

Example datum as JSON:

```json
{
  "constructor": 0,
  "fields": [
    {
      "map": [
        {
          "k": {
            "bytes": "6E616D65"
          },
          "v": {
            "bytes": "5370616365427564"
          }
        },
        {
          "k": {
            "bytes": "6465736372697074696F6E"
          },
          "v": {
            "bytes": "54686973206973206D79207465737420746F6B656E"
          }
        },
        {
          "k": {
            "bytes": "696D616765"
          },
          "v": {
            "bytes": "697066733A2F2F74657374"
          }
        },
        {
          "k": {
            "bytes": "646563696D616C73"
          },
          "v": {
            "int": 2
          }
        }
      ]
    },
    {
      "int": 1
    }
  ]
}
```

##### Retrieve metadata as 3rd party

A third party has the following RFT `d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(444)TestToken` they want
to lookup. The steps are

1. Construct `reference NFT` from `user token`: `d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(100)TestToken`
2. Look up `reference NFT` and find the output it's locked in.
3. Get the datum from the output and lookup metadata by going into the first field of constructor 0.
4. Convert to JSON and encode all string entries to UTF-8 if possible, otherwise leave them in hex.

##### Retrieve metadata from a Plutus validator

We want to bring the metadata of the RFT `d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(444)TestToken` in
the Plutus validator context. To do this we

1. Construct `reference NFT`
   from `user token`: `d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(100)TestToken` (off-chain)
2. Look up `reference NFT` and find the output it's locked in. (off-chain)
3. Reference the output in the transaction. (off-chain)
4. Verify validity of datum of the referenced output by checking if policy ID of `reference NFT` and `user token` and
   their asset names without the `asset_name_label` prefix match. (on-chain)

### Extending & Modifying this CIP

> The keywords "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL
> NOT", "SHOULD", "SHOULD NOT", "RECOMMENDED",  "MAY", and
> "OPTIONAL" in this section are to be interpreted as described in
> [RFC 2119](https://datatracker.ietf.org/doc/html/rfc2119).

All CIPs proposing to modify or extend this standard **MUST** include the language or a reference link to the extension
and modification language found in the
[Extension Boilerplate](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0068/extension_boilerplate.md).

In order to prevent conflicting updates in the future; the addition of new asset classes following, or as part of, this
standard **MUST** be submitted as a new CIP providing their own justification, implementation, rationale, and community
review prior to official acceptance. Newly proposed `asset_name_labels` **SHOULD NOT** be added to
[CIP-0067](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0067) until the accompanying CIP has matured
through the community review and feedback stage to a point that it is considered in the `Under Review` status and is
assigned a tentative CIP number by the CIP Editors panel.

A brief reference to new asset classes **MAY** be added to this document after the accompanying CIP achieves
the `accepted` status. Documentation describing these token asset classes **MUST** be fully encapsulated within their
individual CIPs and a link **MUST** be provided to that CIP within this document.

If a modification or change is deemed necessary to one of the asset classes contained within this document: namely Asset
Name Labels: 100, 222, 333, or 444; which do not fundamentally change the nature, use, or reference of the tokens; it
- *MAY** be made as a modification of this document. However, any change proposed that presents a non-backwards
compatible change **MUST** include an accompanying `version` field iteration and both specifications for the proposed,
current, and historical versions of the format **MUST** be maintained to assist future implementors who may encounter a
version of these tokens from any point in time with the following format:

```
#### Versions

1. [6d897eb](https://github.com/cardano-foundation/CIPs/tree/6d897eb60805a58a3e54821fe61284d5c5903764/CIP-XXXX)
2. [45fa23b](https://github.com/cardano-foundation/CIPs/tree/45fa23b60806367a3e52231e552c4d7654237678/CIP-XXXX)
3. [bfc6fde](https://github.com/cardano-foundation/CIPs/tree/bfc6fde340280d8b51f5a7131b57f4cc6cc5f260/CIP-XXXX)
4. **Current**
```

Each time a new version is introduced the previous version's link MUST be updated to match the last commit corresponding
to the previous version.

If a change is proposed that would fundamentally alter the nature of one or more of the `asset_name_labels` and their
associated tokens contained within this document, namely Asset Name Labels: 100, 222, 333, or 444; these changes
- *MUST** be submitted via a new, separate CIP with its own justification, implementation, rationale, and community
review prior to official acceptance. These separate CIPs **MUST** include a plan for the obsolescence of any previous
versions of the affected tokens. `asset_name_labels` **MUST** only be marked obsolete once a modifying CIP achieves the
`accepted` status.

### Changelog

#### version 1

- NFT (222) & FT (333) asset classes

#### version 2

- Added new RFT asset class (444)

#### version 3

- Added [* bounded_bytes] support to the image and src tags on the metadata

## Rationale: how does this CIP achieve its goals?

Without separation of `reference NFT` and `user token` you lose all flexibility and moving the `user token` would be
quite cumbersome as you would need to add the metadata everytime to the new output where the `user token` is sent to.
Hence, you separate metadata and `user token` and lock the metadata inside another UTxO, so you can freely move
the `user token` around.

In order to reference the correct UTxO containing the metadata, it needs to be authenticated, otherwise metadata
spoofing attacks become possible. One way to achieve that is by adding an NFT (`reference NFT`) to the UTxO. This NFT
needs to under the same Policy ID as the `user token`, followed by an asset name pattern defined in the standard. This
way you create a secure link between `reference NFT` and `user token` without the need for any extra data, and you can
make use of this off-chain and on-chain.

The security for the link is derived from the minting policy itself, so it's important to write the validator with the
right constraints and rules since this CIP solely defines the interface to keep flexibility as high as possible.

### Backward Compatibility

To keep metadata compatibility with changes coming in the future, we introduce a `version` field in the datum.

## Path to Active

### Acceptance Criteria

- [X] Open-source more practical implementations/projects which make use of this CIP.
- [X] Introduce a `version` integer datum field to increment for new asset classes or
changes to the on-chain format.

### Implementation Plan

- [X] Agree on a binary encoding for asset name labels
  in [CIP-0067](https://github.com/cardano-foundation/CIPs/pull/298).
- [X] Get support for this CIP by wallets, explorers, tools, minting platforms and other 3rd parties.
- [X] Minimal reference implementation making use of [Lucid](https://github.com/spacebudz/lucid) (
  off-chain), [PlutusTx](https://github.com/input-output-hk/plutus) (on-chain): [Implementation](./ref_impl)

## References

- [CIP 25 - Media NFT Metadata Standard](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0025)
- [CIP 31 - Reference inputs](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0031)
- [CIP 67 - Asset Name Label Registry](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0067)
- [RFC 3986 - Uniform Resource Identifier (URI)](https://www.rfc-editor.org/rfc/rfc3986)
- [RFC 2397 - The "data" URL scheme](https://datatracker.ietf.org/doc/html/rfc2397)

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0069/README.md
---

- --
CIP: 69
Title: Script Signature Unification
Category: Plutus
Authors:
- Maksymilian 'zygomeb' Brodowicz <zygomeb@gmail.com>
Implementors: N/A
Status: Active
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/321
Created: 2022-08-23
License: CC-BY-4.0
- --

## Abstract

This CIP unifies the arguments given to all types of Plutus scripts currently available (spending, certifying, rewarding, minting) by removing the argument of a datum.

For a while now every builder, myself included have struggled with the mutual dependency issue (two validators that need to know each other's hash) when designing dapps and it is widely considered a substantial barrier to safe protocols and one that limits our design space considerably.

The exact change would be to have every validator take as argument the redeemer and then the script context. Datums, only relevant to locking validators would be able to be provided by either looking them up in the ScriptContext or by extending the `Spending` constructor of `TxInfo` to carry `(TxOutRef, Datum)`.

## Motivation: why is this CIP necessary?

### Multi-purpose Scripts

As it stands the scripts being made on cardano often suffer this problem, and the tokens usually are made to be able to be minted at any time. This leads to further checks being made on the frontend and further fragilitiy of the systems we create. When a mutual dependency arises we are forced to choose which script gets to statically know what's the hash of the other, and which has to be provided 'during runtime'.

- Use Case 1: Minting validator checks datum given to spending validator. The spending validator requires the token be present as witness of the datum's correctness.

- Use Case 2 (taken from Optim's Liquidity Bonds): Unique NFT is minted to give a unique identifier to a loan, that then gets reused by Bond Tokens. The spending validators require that NFT be present.

- Use Case 3 (taken from Minswap's Dex V1): NFT is minted for the same reason as above. It allows a minting policy to later mint LP tokens with that unique id token name.

We see a similar pattern repeating over and over again as witnessed by dapp developers and auditors alike. By allowing the multi-purpose scripts (spending and any other) we increase the security of Cardano by giving us more confidence and allowing to design protocols that have their architecture driven by Cardano's features, not limited by Cardano's language.

This primarily manifests in the ability to use a single validator for both minting and spending but the proposed solution makes it possible to use one validator for any and all purposes at once.

### No Datum Spend Purpose

One of the major footguns of Plutus scripts is if a user sends to the script with a wrong or missing datum. This has happened in the case of the Nami wallet having a bug that caused the wrong address to be chosen. There are other instances of user error where they send from a CEX to a script address. A wrong datum can be handled by the Plutus scripts themselves by having an alternative execution branch if type does not match the expected datum type. But in the case of no datum the script is not run and fails in phase 1 validation. The other motivation of this CIP is to be able to create spend scripts that can handle the no datum case.

I see three major use cases when it comes to running spend scripts without datums:

- Use Case 1: A script that acts as a wallet for users. By having no datum spending the user can send directly from exchanges or have friends send to their smart contract wallet with no datum needed.

- Use Case 2: As a DAO treasury. The funds in this script would be controlled by a DAO and anyone can donate/contribute to the DAO without a datum.

- Use Case 3: Allow dApp protocols to have a claim or withdraw mechanism (similar to Ethereum for tokens sent to a contract without call) for claiming tokens sent without a datum.

I'd be remiss if I didn't mention CIP-0112 which has been expanded to improve native script capabilities to provide an alternative solution for use case 1 and 2. But for use case 3, CIP-0112 does not enable a "claim or withdraw mechanism" for contracts.

## Specification

### Removing the datum argument

All the script purposes have a form of ```Redeemer -> ScriptContext -> a``` except the Spending one. It has the following form: ```Datum -> Redeemer -> ScriptContext -> a```. This is enforced by the Cardano ledger.

We propose to make the following modification:

The signature of all scripts will be ```ScriptContext -> a```.
The `ScriptInfo` type is a union type with a variant for each script purpose.
It is the same as `ScriptPurpose`, except for the additional optional datum in spending scripts.

```haskell
- - | The context that the currently-executing script can access.
data ScriptContext = ScriptContext
  { scriptContextTxInfo     :: TxInfo
- - ^ information about the transaction the currently-executing script is included in
  , scriptContextRedeemer   :: V2.Redeemer
- - ^ Redeemer for the currently-executing script
  , scriptContextScriptInfo :: ScriptInfo
- - ^ the purpose of the currently-executing script, along with information associated
- - with the purpose
  }

- - | Like `ScriptPurpose` but with an optional datum for spending scripts.
data ScriptInfo
  = MintingScript V2.CurrencySymbol
  | SpendingScript V3.TxOutRef (Haskell.Maybe V2.Datum)
  | RewardingScript V2.Credential
  | CertifyingScript
      Haskell.Integer
- - ^ 0-based index of the given `TxCert` in `txInfoTxCerts`
      TxCert
  | VotingScript Voter
  | ProposingScript
      Haskell.Integer
- - ^ 0-based index of the given `ProposalProcedure` in `txInfoProposalProcedures`
      ProposalProcedure
```

The datum in `SpendingScript` is optional, which will allow the execution of spending scripts without a datum.
One more change will be needed on the ledger side in order to make the Datum optional for spending scripts.
The ledger UTXOW rule needs to be relaxed, this ledger rule checks if a utxo has an existing datum if the address's payment credential is a phase 2 validation script.

The ScriptPurpose type used in the Redeemers Map is left the same.
It is used to uniquely identify a Plutus script within a transaction.


## Rationale: how does this CIP achieve its goals?

Unifying of the script signature is a very elegant solution to the problem, streamlining the experience of developing on cardano.
It begs the question if it should be added as an argument to all validators, to further emphasize that fact.


This CIP turns all scripts into 1 arg scripts with a Script Context union type for each purpose.

## Backwards compatibility

This change is not backwards compatible; it must be introduced in a new Plutus language version.
Node code must be modified.

## Path to Active

### Acceptance Criteria

- [x] The change has been implemented in the Plutus codebase, integrated in the ledger and released through a hard-fork.
- Included within the Chang #1 hardfork

### Implementation Plan

The Cardano Ledger and Cardano Plutus teams would need to implement this in following repositories:
  IntersectMBO/plutus
  IntersectMBO/cardano-ledger

The following languages that compile to uplc would need to update to support the new ScriptContext argument that
is passed in for the next Plutus Version:
Aiken
Helios
Opshin
Plu-ts
Plutarch
Scalus

## Copyright

This CIP is licensed under Apache-2.0

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0071/README.md
---

- --
CIP: 71
Title: Non-Fungible Token (NFT) Proxy Voting Standard
Status: Proposed
Category: Tools
Authors:
- Thaddeus Diamond <support@wildtangz.com>
Implementors: []
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/351
Created: 2022-10-11
License: CC-BY-4.0
- --

## Abstract

This proposal uses plutus minting policies to create valid "ballots" that are sent alongside datum "votes" to a centralized smart contract "ballot box" in order to perform verifiable on-chain voting in NFT projects that do not have a governance token.

## Motivation: why is this CIP necessary?

This proposal is intended to provide a standard mechanism for non-fungible token (NFT) projects to perform on-chain verifiable votes using only their NFT assets. There are several proposed solutions for governance that involve using either a service provider (e.g., Summon) with native assets or the issuance of proprietary native assets.  However, there are several issues with these approaches:
- Airdrops of governance tokens require minUTxO ada attached, costing the NFT project ada out of pocket
- Fungible tokens do not have a 1:1 mechanism for tracking votes against specific assets with voting power
- Sale of the underlying NFT is not tied to sale of the governance token, creating separate asset classes and leaving voting power potentially with those who are no longer holders of the NFT

This standard provides a simple solution for this by using the underlying NFT to mint a one-time "ballot" token that can be used only for a specific voting topic. Projects that adopt this standard will be able to integrate with web viewers that track projects' governance votes and will also receive the benefits of verifiable on-chain voting without requiring issuance of a new native token.

We anticipate some potential use cases:
- Enforcing an exact 1:1 vote based on a user's existing NFT project holdings
- Enforcing vote validity by rejecting invalid vote options (e.g., disallowing write-ins)
- Creating "super-votes" based on an NFT serial number (e.g., rare NFTs in the 9,000-10,000 serial range get 2x votes)

> **Warning**
> This specification is not intended for for governance against fungible tokens that cannot be labeled individually.

## Specification

### A Simple Analogy

The basic analogy here is that of a traditional state or federal vote.  Imagine a citizen who has a state ID (e.g., Driver's License) and wants to vote, as well as a central voting authority that counts all the ballots.
1. Citizens go to to a precinct and show their ID to the appropriate authority
2. Citizens receive a ballot with choices for the current vote
3. Citizens mark their selections on the ballot
4. Citizens sign their ballot with their name
5. Citizens submit their ballot into a single "ballot box"
6. Central voting authorities process the vote after polls close
7. Citizens await the election results through a trusted news outlet

This specification follows the same process, but using tokens:
1. A holder of a project validates their NFT by sending it to self
2. The holder signs a Plutus minting policy to create a "ballot" NFT linked to their unique NFT
3. The holder marks their desired vote selections on the ballot
4. The holder signs a tx that sends the "ballot" NFT to a "ballot box" (smart contract) with their "vote" (datum)
5. Authorized vote counting wallets process UTxOs and their datums in the "ballot box" smart contract after polls close
6. Authorized vote counters report the results in a human-readable off-chain format to holders

> **Note**
> Because of the efficient UTxO model Cardano employs, steps #1 through #4 occur in a single transaction.

### The Vote Casting Process

#### "Ballot" -> Plutus Minting Policy

Every holder that participates in the vote will have their project NFT in a wallet that can be spent from (either hardware or software, typically accessed via [CIP-30](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0030)).  To create a ballot, the voting authority will create a Plutus minting policy with a specific combination of:

```ts
type BallotMintingPolicy = {
  referencePolicyId: MintingPolicyHash,             // Reference policy ID of the original NFT project
  pollsClose: Time,                                 // Polls close (as a Unix timestamp in milliseconds)
  assetNameMapping: func(ByteArray) -> ByteArray    // Some function (potentially identity) to map reference NFT name 1-for-1 to ballot NFT name
};
```

This Plutus minting policy will perform the following checks:
1. Polls are still open during the Tx validFrom/validTo interval
2. The ballot NFTs were validly minted (at the least, the user sent-to-self the reference assets and the vote weight/choices are correct)
3. The minted assets are sent directly to the ballot box smart contract in the minting transaction (see [the potential attack below](#Creation-of-Ballot-Without-Casting-a-Vote))

For the voter, each vote they wish to cast will require creating a separate "ballot" NFT.  In the process, their reference NFT never leaves the original wallet.  Sample [Helios language](https://github.com/Hyperion-BT/Helios) pseudocode (functions elided for space) is as follows:

```ts
func main(redeemer: Redeemer, ctx: ScriptContext) -> Bool {
  tx: Tx = ctx.tx;
  minted_policy: MintingPolicyHash = ctx.get_current_minting_policy_hash();
  redeemer.switch {
    Mint => {
      polls_are_still_open(tx.time_range)
        && ballots_are_validly_minted(tx.minted, minted_policy, tx.outputs)
        && assets_locked_in_ballot_box(tx, tx.minted)
    },
    // Burn code elided for space...
  }
}
```

> **Note**
> `ballots_are_validly_minted()` includes all required and custom checks (e.g., the holder has sent the reference NFT to themselves in `tx.outputs`) to validate newly minted ballots

#### "Vote" -> eUTxO Datum

To cast the vote, the user sends the ballot NFT just created to a "ballot box".  Note that for reasons specified in [the "attacks" section below](#Creation-of-Ballot-Without-Casting-a-Vote) this needs to occur during the same transaction that the ballot was minted in.

The datum is a simple object representing the voter who cast the vote and the vote itself:

```ts
type VoteDatum = {
  voter: PubKeyHash,
  vote: object
};
```

The `voter` element is extremely important in this datum so that we know who minted the ballot NFT and who we should return it to.  At the end of the ballot counting process, this user will receive their ballot NFT back.

Note that we are trying to avoid being overly prescriptive here with the specific vote type as we want the only limitations on the vote type to be those imposed by Cardano.  Further iterations of this standard should discuss the potential for how to implement ranked-choice voting (RCV) inside of this `vote` object, support multiple-choice vote selection, and more.

#### "Ballot Box" -> Smart Contract

Essentially, the "ballot box" is a smart contract with arbitrary logic decided upon by the authorized vote counter.  Some examples include:

1. A ballot box that can be redeemed at any time by a tx signed by the authorized vote counter
2. A ballot box that can be redeemed only after polls close
3. A ballot box that can be redeemed once a majority of voters have sent in a ballot
4. A ballot box that can be redeemed only by the specific wallet specified in the `voter` datum of each UTxO
5. A ballot box that can be redeemed only after polls close, has to burn the ballots it redeems, and has to send the minUTxO back to the voter address

Because the ballot creation and vote casting process has already occurred on-chain we want to provide the maximum flexibility in the protocol here so that each project can decide what is best for their own community.  Helios code for the simple case enumerated as #1 above would be:

```ts
const EXPECTED_SIGNER: PubKeyHash = PubKeyHash::new(#0123456789abcdef)

func main(ctx: ScriptContext) -> Bool {
  ctx.tx.is_signed_by(EXPECTED_SIGNER)
}
```

### The Vote Counting Process

#### "Ballot Counter" -> Authorized Wallet

Given the flexible nature of the ["ballot box" smart contract](#Ballot-Box---Smart-Contract) enumerated above, we propose a simple algorithm for counting votes and returning the ballots to the user:

1. Ensure the polls are closed (can be either on or off-chain)
2. Iterate through all UTxOs in forward-time-order locked in the "ballot box" and for each:
- Determine which assets are inside of that UTxO
- Mark their most recent vote to match the `vote` object in the UTxOs datum
3. Ensure any required quorums or thresholds were reached
4. Report on the final ballot outcome

Javascript-like pseudocode using the [Lucid library](https://github.com/spacebudz/lucid) for the above algorithm would be as follows:

```js
function countVotes(ballotPolicyId, ballotBox) {
  var votesByAsset = {};
  const votes = await lucid.utxosAt(ballotBox);
  for (const vote of votes) {
    const voteResult = Data.toJson(Data.from(vote.datum));
    for (const unit in vote.assets) {
      if (!unit.startsWith(ballotPolicyId)) {
        continue;
      }
      const voteCount = Number(vote.assets[unit]);         // Should always be 1
      votesByAsset[unit] = {
        voter: voteResult.voter,
        vote: voteResult.vote,
        count: voteCount
      }
    }
  }
  return votesByAsset;
}
```

There is no requirement that the "ballot counter" redeem all "ballots" from the "ballot box" and send them back to the respective voters, but we anticipate that this is what will happen in practice.  We encourage further open-sourced code versions that enforce this requirement at the smart contract level.

### Reclaiming Ada Locked by the Ballot NFTs

Even if the ballot NFT is returned to the user, this will leave users with ada locked alongside these newly created assets, which can impose a financial hardship for certain project users.

We can add burn-specific code to our Plutus minting policy so that ballot creation does not impose a major financial burden on users:

```ts
func main(redeemer: Redeemer, ctx: ScriptContext) -> Bool {
  tx: Tx = ctx.tx;
  minted_policy: MintingPolicyHash = ctx.get_current_minting_policy_hash();
  redeemer.switch {
    // Minting code elided for space...
    Burn => {
      tx.minted.get_policy(minted_policy).all((asset_id: ByteArray, amount: Int) -> Bool {
        if (amount > 0) {
          print(asset_id.show() + " asset ID was minted not burned (quantity " + amount.show() + ")");
          false
        } else {
          true
        }
      })
    }
  }
}
```

The Helios code above simply checks that during a burn (as indicated by the Plutus minting policy's `redeemer`), the user is not attempting to mint a positive number of any assets.  With this code, *any Cardano wallet* can burn *any ballot* minted as part of this protocol.  Why so permissive? We want to ensure that each vote is bringing the minimal costs possible to the user.  In providing this native burning mechanism we can free up the minUTxO that had been locked with the ballot, and enable the user to potentially participate in more votes they might not have otherwise.  In addition, users who really do not like the specific commemorative NFTs or projects that choose to skip the "commemorative" aspect of ballot creation now have an easy way to dispose of "junk" assets.

## Rationale: how does this CIP achieve its goals?

### Using Inline Datums (On-Chain) Instead of Metadata (Off-Chain)

There are several existing open-source protocols (e.g., [VoteAire](https://github.com/voteaire/voteaire-onchain-spec)) that use metadata to record votes in Cardano transactions without requiring any additional minting or smart contracts.  However, since the vote counting occurs off-chain by validating metadata the vote counter is the ultimate arbiter of what is a "valid" vote.  In this specification, the validity of the vote is ensured in the Ballot creation process, so that any vote in the ballot box is guaranteed to be valid.  We strongly believe that moving the entire process into flexible on-chain logic will improve the transparency of the voting process and meet the needs of the use cases discussed in ["Motivation"](#Motivation) and ["Ballot Box"](#Ballot-Box---Smart-Contract).

### Commemorative NFTs with Optional Token Burning

There is a question as to whether we should enforce the requirement that votes be burned when they are counted by the vote counter.  However, we do not want that to be a standard as many users of NFT communities have expressed an interest in receiving commemorative NFTs (similar to an "I Voted" sticker).  Instead, we propose that the ballot Plutus minting policy be burn-able by anyone who holds the NFT in their wallet.  This way, locked ada can be reclaimed if the user has no further use for the commemorative NFT (see an example of this in the [Implementation](./example/)).

### Potential Attacks and Mitigations

#### Creation of Ballot Without Casting a Vote

Imagine a user who decides to create ballots for the current vote, but not actually cast the vote by sending it to the ballot box.  According to checks #1 and #2 in [the Plutus Minting Policy](#Ballot---Plutus-Minting-Policy), this would be possible.  After the ballot was created, the user could sell the reference NFT and wait until just before the polls close to surreptitiously cast a vote over the wishes of the new project owner.  Check #3 in the minting policy during the mint transaction itself prevents this attack.

#### Double Voting in Multiple Transactions

A user could wait until their first vote casting transaction completes, then create more ballots and resubmit to the ballot box.  The result would be the creation of more assets that count toward the ultimate vote.  However, Cardano helps us here by identifying tokens based on the concatenation of policy ID and asset identifier.  So long as the mapping function in the [Plutus minting policy for ballots](#Ballot---Plutus-Minting-Policy) is idempotent, each subsequent time the user votes the policy will create an additional fungible token with the same asset identifier.  Then, the ballot counter can ignore any prior votes based on each unique asset identifier to avoid duplicate votes (see ["'Ballot Counter' -> Authorized Wallet"](#Ballot-Counter---Authorized-Wallet)).

#### Double Creation of the Same Ballot

A user could attempt to create multiple ballots of the same name for a given reference NFT.  If the reference NFT is actually a fungible token and not an NFT, then our assumptions will have been broken and this is an unsupported use case.  But if our assumption that this is an NFT project are correct, then simply checking that the quantity minted is equal to the quantity spent inside of the Plutus minting policy will prevent this.  The [example code](./example/voting.js) attached does just that.

#### Returning the "Ballot" NFTs to the Wrong User

During the construction of the ballot NFTs we allow the user to specify their vote alongside a `voter` field indicating where their "ballot" NFT should be returned to once the vote is fully counted.  Unfortunately, this is not strictly checked inside the Plutus minting policy's code (largely due to CPU/memory constraints).  So, we rely on the user to provide an accurate return address, which means that there is the potential for someone who has not actually voted to receive a commemorative NFT.  This does not impact the protocol though, as the "ballot" NFT was legally minted, just returned to the incorrect location.  That user actually received a gift, as they can now burn the ballot and receive some small amount of dust.

### Potential Disadvantages

There are several potential disadvantages to this proposal that may be avoided by the use of a native token or other voting mechanism.  We enumerate some here explicitly so projects can understand where this protocol may or may not be appropriate to use:

- Projects concerned with token proliferation and confusing their user base with the creation of multiple new assets might want to avoid this standard as it requires one new token policy per vote/initiative
- Projects wishing to create a "secret ballot" that will not be revealed until after polls close should not use this because the datum votes appear on-chain (and typically inline)
- Performing an encrypted vote on-chain with verifiable post-vote results is an exercise left to the standard's implementer
- Projects wishing for anonymity in their votes should not use this standard as each vote can be traced to a reference asset

### Optional Recommendations

In no particular order, we recommend the following implementation details that do not impact the protocol, but may impact user experience:

- The mapping function described in the [Plutus minting policy for ballots](#Ballot---Plutus-Minting-Policy) should likely be some sort of prefixing or suffixing (e.g., "Ballot #1 - <REFERENCE NFT>"), and NOT the identity function.  Although the asset will be different than the reference NFT due to its differing policy ID, users are likely to be confused when viewing these assets in a token viewer.
- The "ballot" NFT should have some sort of unique metadata (if using [CIP-25](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0025)), datum (if using [CIP-68](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0068)) or other identification so that the users can engage with the ballot in a fun, exciting way and to ensure there is no confusion with the reference NFT.
- The "vote" represented by a datum will be easier to debug and analyze in real-time if it uses the new "inline datum" feature from Vasil, but the protocol will still work on Alonzo era transactions.
- The "ballot box" smart contract should likely enforce that the datum's "voter" field is respected when returning the ballots to users after voting has ended to provide greater transparency and trust for project participants.

### Backward Compatibility

Due to the nature of Plutus minting policies and smart contracts, which derive policy identifiers and payment addresses from the actual source code, once a vote has been started it cannot change versions or code implementations. However, because the mechanism we propose here is just a reference architecture, between votes projects are free to change either the "ballot" Plutus minting policy or the "ballot box" smart contract as they see fit.  There are no prior CIPs with which to conform with for backward interoperability.

### References

- [CIP-0025](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0025): NFT Metadata Standard
- [CIP-0030](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0030): Cardano dApp-Wallet Web Bridge
- [CIP-0068](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0068): Datum Metadata Standard
- [Helios Language](https://github.com/Hyperion-BT/Helios): On-Chain Cardano Smart Contract language used in example code
- [Lucid](https://github.com/spacebudz/lucid): Transaction construction library used in code samples and pseudocode
- [VoteAire Specification](https://github.com/voteaire/voteaire-onchain-spec): Open-source voting specification using metadata off-chain

## Path to Active

### Acceptance Criteria

- [ ] Presentation to, and adoption by, projects that may benefit from ranked-choice voting
- [ ] Open-source implementations from other NFT projects that make use of this CIP

### Implementation Plan

- [x] Minimal reference implementation making use of [Lucid](https://github.com/spacebudz/lucid) (off-chain), [Plutus Core](https://github.com/input-output-hk/plutus) [using Helios](https://github.com/Hyperion-BT/Helios) (on-chain): [Implementation](./example/)

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0072/README.md
---

- --
CIP: 72
Title: Cardano dApp Registration & Discovery
Status: Proposed
Category: Metadata
Authors:
- Bruno Martins <bruno.martins@iohk.io>
- Mateusz Czeladka <mateusz.czeladka@cardanofoundation.org>
- Daniel Main <daniel.main@iohk.io>
Implementors: ["Lace Wallet dApp Store", "DappsOnCardano dApp Store"]
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/355
Created: 2022-10-18
License: CC-BY-4.0
- --

## Abstract

dApp developers do not have a standardised method to record immutable, persistent claims about their dApp(s) that their users can verify. A dApp developer needs to "register" their dApp by providing a set of claims about their dApp(s) that can be verified by the user. This CIP describes a standardised method for dApp developers to register their dApp(s) on-chain and for users to verify the claims made by dApp developers.

This proposal aims to standardise the process of dApp registration and verification, and to provide a set of claims that dApp developers can use to register their dApp(s).

## Motivation: why is this CIP necessary?

dApps can express a plethora of information. Some of this information could be claims about who the developer is, what the dApp's associated metadata is, and more. This data lacks standardisation, persistence, and immutability. Data without these features, means that dApp users cannot verify if the dApp's expressed information is consistent across time. The goal of this CIP is to formalise how dApps register their information with a new transaction metadata format that can record the dApp's metadata, ownership, and potentially developer's identity. This formalisation means dApp users can verify if the data expressed by a dApp is consistent with what was registered on-chain.

Also, having this formalisation facilitates any actor in the ecosystem to index and query this data, and provide a better user experience when it comes to dApp discovery and usage.

## Specification

### **Definitions**

- **anchor** - A hash written on-chain (rootHash) that can be used to verify the integrity (by way of a cryptographic hash) of the data that is found off-chain.
- **dApp** - A decentralised application that is described by the combination of metadata, certificate and a set of used scripts.
- **metadata claim** - Generically any attempt to map off-chain metadata to an on-chain subject. This specification looks at dApp specific metadata claims. Caution: it is highly recommended that dApp developers provide links to a specific snapshot (version) without removing all previous snapshots / version links. Some stores may choose not to show a dApp if all off-chain historical versions are not available but instead only latest snapshot.
- **client** - Any ecosystem participant which follows on-chain data to consume metadata claims (i.e. dApp stores, wallets, auditors, block explorers, etc.).
- **dApp Store** - A dApp aggregator application which follows on-chain data looking for and verifying dApp metadata claims, serving their users linked dApp metadata.
- **publishers** - Entities which publish metadata claims on-chain, in the case of dApps the publishers are likely the dApp developer(s).

### **Developers / Publishers**

Developers and publishers of dApps can register their dApps by submitting a transaction on-chain that can be indexed and verified by stores, auditors and other ecosystem actors.

### **Stores / Auditors**

Stores and auditors should be able to follow the chain and find when a new dApp registration is **anchored** on-chain. They should then perform *integrity* and *trust* validations on the dApp's certificate and metadata.

#### **Suggested Validations**

- **`integrity`**: The dApp's off-chain metadata should match the metadata **anchored** on-chain.
- **`trust`**: The dApp's certificate should be signed by a trusted entity. It's up to the store/auditor to decide which entities are trusted and they should maintain and publish their own list of trusted entities. Although this entities might be well known, it's not the responsibility of this CIP to maintain this list. These entities could be directly associated with developer/publisher or not.

### **On-chain dApp Registration**

```json
{
  "subject": "b37aabd81024c043f53a069c91e51a5b52e4ea399ae17ee1fe3cb9c44db707eb",
  "rootHash": "7abcda7de6d883e7570118c1ccc8ee2e911f2e628a41ab0685ffee15f39bba96",
  "metadata": [
    "https://foundation.app/my_dapp_7abcda/tart/",
    "7abcda7de6d883e7570118c1ccc8ee2e91",
    "e4ea399ae17ee1fe3cb9c44db707eb/",
    "offchain.json"
  ],
  "type": {
    "action": "REGISTER",
    "comment": "New release adding zapping support."
  }
}
```

### Properties

- `subject`*: Identifier of the claim subject (dApp). A UTF-8 encoded string, max 64 chars. The uniqueness of this property cannot be guaranteed by the protocol and multiple claims for the same subject may exist, therefore it is required to exist some mechanism to assert trust in the *veracity* of this property.

- `type`*: The type of the claim. This is a JSON object that contains the following properties:

- *`action`*: The action that the certificate is asserting. It can take the following values:
- *`REGISTER`*: The certificate is asserting that the dApp is registered for the first time or is providing an update.
- *`DE_REGISTER`*: The certificate is asserting that the dApp is deprecated / archived. So, no further dApp's on-chain update is expected.

- `rootHash`*: The blake2b-256 hash of the entire offchain metadata tree object. This hash is used by clients to verify the integrity of the metadata tree object. When reading a metadata tree object, the client should calculate the hash of the object and compare it with the `rootHash` property. If the two hashes don't match, the client should discard the object. The metadata tree object is a JSON object that contains the dApp's metadata. The metadata tree object is described in the next section. Please note that off-chain JSON must be converted into RFC 8765 canonical form before taking the hash!

- `metadata`*: Chunks of URLs that make up the dApp's metadata are arranged in an array to accommodate the 64-character limit per chunk, allowing for the support of longer URLs. The metadata itself is a JSON object compatible with RFC 8785, containing detailed information about the dApp

### On-chain Schemas

[On-chain CDDL for registration / de-registration (Version 2.0.0)](./version_2.0.0_onchain.cddl)

which also can be expressed using JSON schema:

[dApp on-chain certificate JSON Schema (Version 2.0.0)](./version_2.0.0_onchain.json)

### Metadata Label

When submitting the transaction metadata pick the following value for `transaction_metadatum_label`:

- `1667`: dApp Registration

### Off-chain Metadata Format

The dApp Registration certificate itself doesn't enforce a particular structure to the metadata you might fetch off-chain. However, we recommend that you use the following structure:

[Off-chain dApp Registration certificate schema (Version 2)](./version_2.0.0_offchain.json)

This schema describes the minimum required fields for a store to be able to display and validate your dApp.

### Example

```json
{
  "version": "1.0.0",
  "subject": "abcdef1234567890",
  "projectName": "My dApp",
  "link": "https://www.exampledapp.com",
  "companyName": "Amazing dApp Inc.",
  "companyEmail": "contact@myamazingdapp.com",
  "companyWebsite": "https://www.myamazingdapp.com",
  "logo": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAUAAA...",
  "categories": ["DeFi", "Games"],
  "screenshots": [
    "data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD...",
    "data:image/jpeg;base64,/9j/4AAQSkZJRgABAQEASABIAAD..."
  ],
  "social": [
    {
      "name": "GitHub",
      "link": "https://github.com/exampledapp"
    },
    {
      "name": "Twitter",
      "link": "https://twitter.com/exampledapp"
    }
  ],
  "description": {
    "short": "This is a short description of the dApp, giving an overview of its features and capabilities."
  },
  "releases": [
    {
      "releaseNumber": "1.0.0",
      "releaseName": "Initial Release",
      "securityVulnerability": false,
      "comment": "First major release with all core features.",
      "scripts": [
        {
          "id": "script1",
          "version": "1.0.0"
        }
      ]
    }
  ],
  "scripts": [
    {
      "id": "script1",
      "name": "Example Script",
      "purposes": ["SPEND", "MINT"],
      "type": "PLUTUS",
      "versions": [
        {
          "version": "1.0.0",
          "plutusVersion": 1,
          "scriptHash": "abc123"
        }
      ]
    }
  ]
}
```

### **Stores Custom fields**

Each store might have their own requirements for the metadata. For example, some stores might require a field for logo, or screenshots links. The store's should adviertise what fields they require in their documentation so that developers are aware and they can include them in the metadata.

### **Offchain Metadata Storage**

There are multiple options to store metadata offchain. The most common options are:

- [CIP-26](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0026) compliant servers
- [IPFS](https://ipfs.tech/)
- [Bitbucket](https://bitbucket.org/)
- Any REST JSON API

## Rationale: how does this CIP achieve its goals?

### Decoupling of dApp Registration From Certifications / Audits

We quickly reached a conclusion that it is better to separate them and keep scope of CIP smaller. During discussions it became clear that while there is
some overlap of certifications / audits with dApp registration, this overlap is small and can be even removed. At one point we wanted to couple
certifications CIP to this CIP (e.g. via some link or dApp version) but we analyzed how dApp developers are currently following the process and we noticed
that in many cases certification / audit comes before an official dApp release on main-net. Having established it, we removed this link and not only
that dApp registration and certifications are different CIPs but they are very loosely coupled. Loose coupling has also disadvantages as it leads to a situation that in order to attest that a dApp is certified / audited, implementators will have to scan for all script hashes belonging to a dApp and check
whether those have been certified / explicitly mentioned in the audit.

### Small Metadata Anchor On Chain

This one is rather obvious but for the sake of completeness worth documenting. We analyzed how much we should put on-chain vs off-chain and we quickly reached the conclusion that it is better to keep small amount of data on-chain and larger chunk off-chain for which is what exactly CIP-26 is meant for.

### CIP-26 as *ONE* of Storage Layers

We believe that CIP-26 is geared towards storing this type of off-chain metadata format but we don't want by any means to stipulate / police this form of storage. In fact it is possible to use offchain metadata storage alternatives such as CIP-26 compatible server / direct http(s) hosting / IPFS, etc.

### How to Find Off-Chain Data?

We went back and forth whether we should actually store link (links) to off-chain metadata, eventually we settled on a solution that this is required
because there could be a situation that a dApp registration may have off-chain metadata stored somewhere but some stores have it, others don't have it. Now it is required that a dApp developer points to at least one store that has off-chain metadata (as a reference metadata).

### Optional Release Name?

Release Name is a field, which dApp developers can use on top of Release Version, it has been debated whether field should be mandatory or optional but eventually it has been agreed that we do not want to enforce this field, Release Name is an optional field, Release Version, however, needs to follow semver and is a mandatory field.

### Canonical JSON

At the begining neither on-chain, nor off-chain JSON has been following RFC 8785 (canonical JSON) but we reached a point that, due to consistency checks, we need to take hash of both on-chain and off-chain and this forced us to stipulate that both on-chain and off-chain metadata documents need to be converted
according to RFC 8785 before taking a blake2b-256 hash of them.

### Transaction Signature Scope

As any transaction in cardano network has a signature, which has a role to verify that a certain dApp owner made changes.

### Who Is The Owner?

Smart contracts are ownerless, it has been debated that there could be multiple claims to the same dApps from different parties.

The standard doesn't prevent anyone from making a claim, so it's up to the different operator to their diligence work and make their own choices of whom they trust. The signature should give the most confidence as anyone can collect known public keys from known development companies. Future CIP revisions can include DID's and Verifiable Credentials to tackle this problem in a more elegant way.

### DIDs

Since DIDs / Verifiable Credetials are not yet widely used in Cardano ecosystem, usage of them in this initial CIP version has been de-scoped.

### Categories

`Categories` is a predefined enum with values defined in the CIP / schema, it is *NOT* a free text field, rationale for this is that dApp developers will have no idea what ontology / classification to use, which will likely result in many duplicates of the same thing.

### Purpose Field As an Array or as a Single Item?

It may have been visible that we have a `purpose` field, which can be: "SPEND" or "MINT", those fields directly map to what is allowed by a Cardano Smart Contract. As of the time of writing CIP - PlutusTx does not allow a script to be both of type: "SPEND" and "MINT", however, there are new
languages on Cardano being worked on where they already allow one validator to be both spending UTxOs and minting tokens - all with the same script hash. To be open for the future it has been agreed to turn `purpose` field into `purposes` and make it a JSON array.

### Parametrised Scripts

On Cardano, there are parametrised scripts, meaning that before compilation takes place, it is possible to pass certain parameters instead of using `Datum`.
The consequence of this will be that as we pass different parameters, script hash will be changing. This is especially troublesome for things like certifications / audits but also dApp registration. This topic is being debated as part of CIP: https://github.com/cardano-foundation/CIPs/pull/385, however, it doesn't seem that there has been conclusion how to tackle this problem. For the moment, a new script hash (despite changing only a parameter) requires a re REGISTRATION to the existing dApp with a requirement to add new version(s) in the dApp's off-chain metadata.

### Often Changing Scripts

There are cases on Cardano main-net that script hashes are changing every day, most probably due to parameterised scripts. It is responsibility of the developers to issue an `REGISTRATION` command and provide on-chain and off-chain metadata following the change, for scripts that are changing daily / hourly it is envisaged that this process be automated by a dApp developer.

### Beacon Tokens Instead of Metadata

It has been argued that since anybody can make claims to dApps, this CIP instead of using metadata should use tokens. dApp developers
would mint token, which would ensure they are the owners of a given dApp. It is a certainly an interesting approach but shortcomings
of the current solution can also be lifted by moving to DID based system and benefit of metadata is that it is easily queriable off chain
and currently stores can attest / validate multiple claims for the same dApp. Forcing dApp developers to MINT tokens would complicate this CIP
and potentially hinder it's adoption.

### Datums Instead of Metadata

It has been suggested that we do not use metadata but rather Datums. Metadata cannot enforce format and Datums could. It has been rejected as
using Datums requires a smart contract and we want to keep this solution as accessible as possible. It is a GUI concern since if there is a
some app that can attest validity and conformance to JSON schema - dApp Registration / Update MUST never be done that does not conform to the schema.

### Scripts / Releases Fields Are Not Required

We made a decision to change the schema so that scripts and releases are no longer required. This could help to get initial registration from dApp developers faster and some stores simply do not require dApps to add their scripts in order to be listed.

### Tags

We briefly discussed tags and we will likely introduce tags in the future. An array of tags to help stores / dApp developers categories where their dApp should show. This will complement `categories` field.

### DE_REGISTER

We added DE_REGISTER in additon to already existing `REGISTER`. The idea is that once dApp devs want to deprecate dApp they can now issue DE_REGISTER request.

### Type Field

`Type` field can be `PLUTUS` or `NATIVE`, we made it optional and there are already two dApps at least on Cardano at the time of writing, which are only using NATIVE scripts. This optional field helps to differentiante between NATIVE script based and NON_NATIVE dApps.

### Version Deprecation

We discussed scenario what to do in case a dApp team wants to deprecate a particular version. Upon a few iteration we settled on doing this in off-chain section.

### Version Security Vulnerability Flagging

It is not uncommon to see a dApp release a version and then release a fix in the new version and flag the previous version
as having security vulnerability. We are intoducing an optional field in the offchain json on the release level: `securityVulnerability": true.

### Comment Field (on-chain JSON)

We are introducing a field in the on-chain JSON only, which allows dApp development teams to provide a free text field
comment about changes they are making in a given (re-)registration request.

## Path to Active

We will evaluate for a few months if we have not missed any details, collect feedback from dApp developers, stores. We reserve right to make small changes in this time, while the proposal is in the `PROPOSED` status / state.
Once `Acceptance Criteria` are met and all comments / feedback from dApp developers is addressed, we will update the proposal to be in `ACTIVE` state.

### Acceptance Criteria

- At least 3 non trivial dApps from 3 different teams register on-chain / off-chain via following this CIP
- At least one Implementator (main-net) implements the store indexing this CIP metadata from on-chain

### Implementation Plan

- DappsOnCardano dApp Store: https://github.com/Cardano-Fans/crfa-dapp-registration-and-certification-service for DappsOnCardano.com
- Lace's Wallet dApp Store: https://github.com/input-output-hk/lace

## Copyright

[CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode)

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0074/README.md
---

- --
CIP: 74
Title: Set minPoolCost to 0
Authors:
- Robin of Loxley <adarobinhood@tutanota.com>
Category: Ledger
Status: Proposed
Created: 2022-10-17
Discussions:
- https://forum.cardano.org/t/cip-69-set-minpoolcost-to-0/109309
- https://github.com/cardano-foundation/CIPs/pull/358
Implementors: []
License: CC-BY-4.0
- --

## Abstract

A minPoolCost of 340 ADA/epoch makes popularity the basis for pool desirability, causing preferred traits like pledge and performance to be overshadowed. This has promoted stake to centralize with operators who are effective at campaigning, but do not necessarily have any stake in the system of their own. We want to create a fair marketplace for stake pools that allows the network to decentralize with time; minPoolCost is averse to that goal.

## Motivation: why is this CIP necessary?

Popularity is currently the basis for desirability and defines which pools will receive high rewards and which won't. A pool with high popularity, low pledge, and low performance will offer significantly higher rewards than a pool with low popularity, high pledge, and high performance. With equal fee structures, a pool with 19.5MM pledge that is just starting out will be less desirable than a saturated pool with no pledge; a pool with 6MM pledge and perfect performance will be less desirable than a pool with 0 pledge and 90% performance. This makes it apparent we are not incentivizing a secure stable network and the network cannot self-correct if stake ends up in the wrong place.

The entirety of IOG's research is without a minPoolCost or any other minimum fee. The game theory design relied on stake pool operators (SPOs) having the ability to compete in a fair marketplace of pool desirability by modifying their pledge, cost, and margin in relation to other pools. Adding a minimum value to cost has removed the opportunity for operators to control their desirability outside of first gaining popularity. As a pool gains popularity it disproportionately gains desirability, allowing the operator to lower pledge and raise margin, while still maintaining higher desirability than a less popular pool with better fee structure and higher pledge. Operators can lie about their costs while still gaining utility. As long as the operator can maintain popularity they can exceed K indefinitely by opening more and more pools with a reward bonus instead of penalty. This significantly lowers the cost of a sybille attack, while making sybille behavior highly desirable and profitable. A sybille attacker, or anybody for that matter, should need stake in the system to compete for delegation, not just a rigorous marketing campaign.

The network is incentivizing the wrong behavior from SPOs which has made the network highly leveraged.

"The higher the leverage of the system, the worse its security [...] The lower the leverage of a blockchain system, the higher its degree of decentralization."

High security should always be the priority.

### Desired effects

Stake pool desirability will become a function of pledge, performance, cost, and margin instead of only popularity. Operators will need to consolidate and grow pledge while moderating fees to remain competitive with other pools on the market.

SPOs define their cost as an absolute value when submitting their registration.cert. They do not reference minPoolCost. Changing minPoolCost will not change any predefined costs. Pools that wish to leave their cost as-is can do so without any input. Pools that wish to lower their cost below 340 ADA/epoch will have to submit an updated registration.cert.

## Specification

"minPoolCost": 0

## Rationale: how does this CIP achieve its goals?

98% of all pools have their cost at 340 ADA/epoch or within + 10. As the price of ADA went from $0.3 to $3, almost no operators modified their cost. As the price of ADA dropped from $3 to $0.3, almost no operators modified their cost. This shows that the minPoolCost is not related to the real cost of operating a pool and the cost of a pool is no longer related to its utility.

We have a large body of research accompanied by simulations showing that removing minPoolCost will increase decentralization of the network and begin incentivizing the right behavior from SPOs and the delegating community.

A minPoolCost is not in Cardano's design specification. ALL published research is in favor of setting minPoolCost to 0.

### Research and References

Design Specification for Delegation and Incentives in Cardano
https://github.com/input-output-hk/cardano-ledger/releases/latest/download/shelley-delegation.pdf

Reward Sharing Schemes for Stake Pools
https://arxiv.org/pdf/1807.11218.pdf

Preventing Sybil attacks
https://iohk.io/en/blog/posts/2018/10/29/preventing-sybil-attacks/

Stake Pools in Cardano
https://iohk.io/en/blog/posts/2018/10/23/stake-pools-in-cardano/

Incentive Paper Lecture Series (Parts 1-7)
https://www.youtube.com/playlist?list=PLFLTrdAG7xRbAqhF3Tg8BeAea7Ard-ttn

The general perspective on staking in Cardano

## Path to Active

### Acceptance Criteria

- [ ] A protocol parameter update assigning `minPoolCost` to `0`.

### Implementation Plan

- [ ] Agreement by the Ledger team as defined in [CIP-0084](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0084) under _Expectations for ledger CIPs_ including "expert opinion" on changes to rewards & incentives.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0075/README.md
---

- --
CIP: 75
Title: Fair Stake Pool Rewards
Status: Proposed
Category: Ledger
Authors:
- Tobias Fancee <tobiasfancee@gmail.com>
Implementors: []
Discussions:
- https://forum.cardano.org/t/cip-fair-stakepool-rewards/109368
Created: 2022-10-21
License: CC-BY-4.0
- --

## Abstract
The current reward sharing scheme of Cardano is unfair and anticompetitive. As a result, Cardano has become more centralized over time. The high minimum fixed fee and current pledge benefit favor large stakepools and leave small stakepools at a significant disadvantage. The current scheme allows large pools with low pledge to be more attractive than smaller pools with higher pledge leading to centralization and potential Goldfinger attacks. Furthermore, k, the parameter representing the optimal number of stakepools, is set too low resulting in an ineffective pledge benefit, the formation of multipools, and a low incentive for stakepools to increase pledge over time. Finally, the current setting of a0, the pledge influence parameter, gives an unnecessarily large boost in rewards to fully pledged private pools resulting in significantly less rewards for public pools and their delegators.

This proposal retains most of the original reward sharing scheme, but makes changes to ensure fairness, increase decentralization, and reduce the viability of Goldfinger attacks. By removing the minimum fixed fee, adjusting the pledge benefit, increasing k, and reducing a0, a more egalitarian network can be achieved.

## Motivation: why is this CIP necessary?

### Definitions

- *Minimum Fixed Fee**
- Protocol parameter minPoolCost.

- *Margin**
- Stakepool parameter PoolRate (percentage fee).

- *Public Stakepool**
- A stakepool with a margin that is less than 100%.

- *Private Stakepool**
- A stakepool that is not seeking delegation with a margin of 100%.

- *Stakepool Operator (SPO)**
- The operator of a stakepool, can be a single person or an entity.

- *Multipool**
- A group or brand of stakepools operated by a single entity or stakepool operator.

- *ROS**
- Return on staking (often annualized and represented as a percentage of the initial investment).

- *Leverage**
- The ratio between a stakepool’s total stake and pledge.

- *Stakepool Viability Point**
- The amount of pledge required for a stakepool with zero delegations to distribute nonzero rewards to delegators (assuming minimum stakepool fees and ignoring luck in VRF block production, i.e., rewards are exactly proportional to stake).

- *Stakepool Competitive Point**
- The amount of pledge required for a stakepool with zero delegations to offer the same ROS as a fully-saturated stakepool with zero pledge (assuming minimum stakepool fees).

- *Stakepool Saturation Point**
- The maximum amount of total stake in a stakepool before total stakepool rewards are capped and ROS diminishes.

- *Unsaturated Pledge Benefit Penalty**
- The amount of potential pledge benefit (in ADA or represented as a percentage) a stakepool loses for being unsaturated. The penalty is larger the smaller the stakepool.

- *Minimum Attack Vector (MAV)**
- Also known as the Nakamoto Coefficient, the MAV is the minimum number of entities required to capture more than 50% of a network. In the context of Cardano, this refers to the minimum number of SPOs required to capture more than 50% of active stake.

- *Goldfinger Attack**
- An attack on a cryptocurrency protocol where the objective is to attack the protocol or network in order to make profit by shorting the native cryptocurrency.

- *Sybil Attack**
- An attack on an online system where an entity tries to take over the network by creating many identities or nodes.

### The Current Rewards Equation

In section 10.8 Rewards Distribution Calculation of “A Formal Specification of the Cardano Ledger” (git revision 1.1-486-g301fede) the current rewards calculation equation is described by the function $maxPool$:

$$maxPool = \frac{R}{1 + a0} \cdot (o + p \cdot a0 \cdot \frac{o - p \frac{z0 - o}{z0}}{z0})$$

where: $maxPool$ = maximum rewards for a stake pool, $R = ((reserve \cdot rho) + fees) \cdot (1 - tau), o = min(poolstake / totalstake, z0) = z0$ for fully saturated pool, $p = min(pledge / totalstake, z0)$, and $z0 = 1 / k$

Current protocol parameters: $k = 500, rho = 0.003, a0 = 0.3$, and $tau = 0.2$

The current reward sharing scheme which includes the rewards calculation equation and the minimum fixed fee are inadequate in promoting decentralization as evident by Cardano’s currently low MAV relative to k. This is due to its anticompetitive features which are discussed in this section.

### The Minimum Fixed Fee

The minimum fixed fee or minPoolCost was apparently included for additional sybil attack protection. However, it’s effect has been the opposite allowing stakepools with low pledge to offer greater rewards than pools with higher pledge in some cases. Moreover, the minimum fixed fee is certainly problematic as it places an unfair burden on small pools by enforcing a disproportionally larger fee than that of larger stakepools, reducing the ROS of small pools and incentivizing delegation to larger stakepools. This leads to centralization of the network around established stakepools, leaving less opportunity for smaller stakepools that may have greater pledge or community presence.

### The Current Pledge Benefit

The current pledge benefit in the rewards equation is a function of the total stake in a pool that is significantly biased towards large stakepools that are close to saturation. Specifically, the current equation penalizes the pledge benefit of small pools. The smaller the pool, the larger the penalty. This unsaturated pledge benefit penalty combined with the minimum fixed fee leads to illogical rewards where large pools with low pledges can offer delegators higher ROS than smaller pools with significantly higher pledges. As a result, delegators are incentivized to delegate to larger stakepools even if they have lower pledges leading to network centralization.

The unsaturated pledge benefit penalty is part of the current rewards calculation equation and can be described by the equation:

$$Unsaturated Pledge Benefit Penalty = \frac{R}{1 + a0} \cdot p \cdot a0 \cdot \frac{p \frac{z0 - o}{z0}}{z0}$$

See [UnsaturatedPledgeBenefitPenalty.xlxs](./UnsaturatedPledgeBenefitPenalty.xlxs) to calculate the current unsaturated pledge benefit penalty for a stakepool.

### Goldfinger Attacks

The minimum fixed fee and current pledge benefit introduce a potential security threat to the Cardano protocol: Goldfinger attacks. The current reward scheme puts all small stakepools at a disadvantage regardless of pledge centralizing the network around established stakepools rather than pools with the most attractive pledge and fee combination. When SPOs with low stake (pledge) in the protocol are allowed to dominate consensus, they have a potential alternative incentive to attack the network in order make profit by shorting ADA. Because these stakepools have low stake in the protocol (operate with low or even zero pledge), they would be able make profit without any significant loss other than future staking rewards. With leverage, the attackers could make significantly more profit shorting ADA than years of staking.

### The Optimal Number of Stakepools, k

The current setting of k, the parameter representing the optimal number of stakepools, is set too low to provide an effective pledge benefit leaving little incentive for pools to increase pledge over time. Specifically, with a small k value, a fully pledged pledge benefit is far from achievable for most SPOs. An ineffective pledge benefit leads to the formation of multipools with high leverage, as operators can split their pledge into multiple stakepools without a significant decrease in ROS in the resulting stakepools.

### The Pledge Influence Parameter, a0

The current setting of a0, the pledge influence parameter, gives an unnecessarily large boost in rewards to very high pledge and fully pledged private pools. Specifically, the current setting of a0 results in approximately 30% greater rewards for fully pledged private pools. This boost in rewards unfortunately results significantly less rewards for public pools that commonly have low pledges relative to the saturation point. Given that most Cardano users are delegators and not SPOs, this exclusive boost for high pledge pools decreases Cardano’s overall attractiveness as a staking protocol. Additionally, having a high a0 setting only accelerates the wealth (ADA) disparity between large entities operating private pools and delegators who make up majority of the ecosystem.

## Specification

### The Proposed Rewards Calculation Equation

The proposed rewards calculation equation is a modification of the current equation that removes the unsaturated pledge benefit penalty:

$$maxPool = \frac{R}{1 + a0} \cdot (o + p \cdot a0 \cdot \frac{o}{z0})$$

where: $maxPool$ = maximum rewards for a stake pool, $R = ((reserve \cdot rho) + fees) \cdot (1 - tau), o = min(poolstake / totalstake, z0) = z0$ for fully saturated pool, $p = min(pledge / totalstake, z0)$, and $z0 = 1 / k$

### The Proposed Parameter Values

The proposed parameter values are the following:

| Name of the Parameter | New Parameter (Y/N) | Deleted Parameter (Y/N) | Proposed Value | Summary Rationale for Change |
|-----------------------|---------------------|-------------------------|----------------|------------------------------|
| minPoolCost           | N                   | Y                       | N/A            | See Rationale Section.       |
| stakePoolTargetNum    | N                   | N                       | 1000           | See Rationale Section.       |
| poolPledgeInfluence   | N                   | N                       | 0.2            | See Rationale Section.       |

## Rationale: how does this CIP achieve its goals?

### Principles

The main goal of this proposal is to ensure fairness in stakepool rewards. This is achieved by including these principles in the design:

1.	Eliminate all anticompetitive features. These include any parts of the design that treat stakepools differently based on anything other than pledge or declared fees.
2.	Ensure that the pledge benefit is fair and corresponds to a consistent boost in ROS no matter pool size. In other words, assuming the same fees, two pools with the same pledge should always offer the same ROS.
3.	Ensure that the pledge benefit is effective and incentivizes increasing pledge over time.
4.	Reduce the large rewards disparity between private pools and delegators and increase Cardano’s overall attractiveness as a staking protocol.

### Explanation

The current reward sharing scheme includes two notable anticompetitive features. These features are the minimum fixed fee and the unsaturated pledge benefit penalty. This proposal removes these features to ensure fairness and promote adequate competition between stakepools. Removing these anticompetitive features promotes delegation to pools with most attractive pledge and fee combinations rather than established large pools and multipools. This results in fairer competition among stakepools and lower possibility of Goldfinger attacks as the pledge benefit is effective at all stakepool sizes. Greater decentralization is also possible as small stakepools will be able to offer competitive returns and potentially extract delegation from low pledge multipools.

To ensure a more effective pledge benefit and incentivize increasing pledge over time, this proposal increases the current value of k from 500 to 1000. This allows a fully pledged pledge benefit to be closer for all SPOs and will force multipools to split pledge and reduce pledge benefit if they wish to continue operating with the same leverage. Additionally, a change in the value of k will give many stagnant delegators an incentive to reconsider their delegations giving smaller stakepools an opportunity at increasing delegation.

Finally, to reduce the large rewards disparity between private pools and delegators, this proposal reduces the setting of a0 from 0.3 to 0.2. The current setting of a0 results in approximately 30% greater rewards for fully pledged private pools. This proposal reduces this disparity to 20% to create a fairer rewards distribution. The result is an overall increase in rewards for delegators as most public pools operate with low pledges relative to the saturation point. Given that delegators make up majority of users, this reduction in a0 will make Cardano a much more competitive staking investment in contrast to other blockchains.

These proposed changes to Cardano’s reward sharing scheme are aimed at ensuring fairness, increasing decentralization, and creating a more egalitarian staking ecosystem.

### Test Cases

Stakepool viability and competitive points can give some insight into the fairness of the reward scheme. These points are essentially start-up costs required to run viable and competitive stakepools. These points are very high and out of reach for many SPOs with the current scheme. This proposal effectively minimizes these points.

Current stakepool viability point: ~625,000 ADA

Current stakepool competitive point: ~19,000,000 ADA

Proposal stakepool viability point: 1 ADA

Proposal stakepool competitive point: 1 ADA

See [FairStakepoolRewards.xlxs](./FairStakepoolRewards.xlsx) to compare stakepool ROS between the current and proposed scheme.

### Backward Compatibility

This proposal includes parameter changes, one parameter removal, and a change to the rewards calculation. Because of the parameter removal and changes to the rewards calculation, a hardfork will be necessary for implementation.

## Path to Active

### Acceptance Criteria

Each stage will be an individual protocol update. The first two updates will be protocol parameter updates. The third and final update will require a hardfork.

Before implementation, engineering and research teams must review the feasibility and potential consequences of the proposal, create the implementation for each update, and decide on the time interval between updates.

1. The protocol update is created, including all necessary changes.
2. The raw transaction for the protocol update is built.
3. Transaction is signed.
4. Transaction is submitted.
5. Protocol update is confirmed.

### Implementation Plan

Implementation can be staged to reduce shock to the network:

1.	Decrease minPoolCost from 340 ADA to 100 ADA and increase k from 500 to 750.
2.	Increase k from 750 to 1000, decrease minPoolCost from 100 ADA to 0 ADA, and decrease a0 from 0.3 to 0.2.
3.	Remove minPoolCost from the protocol and implement the new rewards calculation equation.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0080/README.md
---

- --
CIP: 80
Title: Transaction Serialization Deprecation Cycle
Status: Active
Category: Ledger
Authors:
- Jared Corduan <jared.corduan@iohk.io>
Implementors: N/A
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/372
Created: 2022-11-09
License: CC-BY-4.0
- --

## Abstract

This CIP specifies a policy for the backwards compatibility of the serialization scheme of
Cardano transactions.

## Motivation: why is this CIP necessary?

Transactions on Cardano are sent on the wire using CBOR and are specified with CDDL.
The first scheme was introduced with the Byron phase.
This scheme was changed dramatically with the introduction of the Shelley phase.
As of the time of the writing of this CIP, however, every new scheme has been backwards
compatible with the original scheme from the Shelley phase.
The intention is still to maintain backwards compatibility to the extent reasonable,
and to make explicit our policy for breaking backwards compatibility when deemed necessary.

## Specification

Problems with serialization fall into two categories:
- flaws in the implementation
- flaws is the CDDL specification

Note that at the time of the writing of this CIP, there is only one implementation of the Cardano
node, and we do not yet need to consider inconsistencies between different implementations.

The policy for maintaining backwards compatibility with the transaction serialization will be
as follows.

### Serious Flaws

A **serious flaw** in the serialization is an issue which could have a large and negative impact
on the network, and which requires a hard fork to fix.
These will almost always be problems with the serialization and not the specification.
It is up to human discretion to determine what constitutes a serious flaw,
mostly likely by the core developers.

Backwards compatibility can be abandoned in the case of a serious flaw,
and **the fix will occur at the next available hard fork**.

### Non-Serious Flaws

A **non-serious flaw** in the serialization is an issue which is not safety critical,
but is problematic enough to merit breaking backwards compatibility.
This is again a human judgment.

Backwards compatibility can be abandoned in the case of a non-serious flaw,
but there must be a deprecation cycle:
- In the case of a **soft fork** (meaning that the change is backwards incompatible for
  block producers but *not* block validators),
  a new format can be introduced at the next major or minor protocol version,
  at which time the old format can be abandoned.
- In the case of a **hard fork** (meaning that the change is backwards incompatible for
  both block producers and block validators),
  a new format can be introduced at the next major protocol version,
  but the old format must be supported for at least **six months**.
  After six months, the old format can be abandoned at the next possible fork.

#### Examples

A good example of a non-serious flaw is the CDDL specification of the transaction output in the
Alonzo ledger era:

```
alonzo_transaction_output = [ address, amount : value, ? datum_hash : $hash32 ]
```

There is nothing inherently wrong with this scheme, but it caused a problem in the Babbage ledger
era with the addition of inline datums and script references.
In particular, there were two new optional fields, and there was mutual exclusivity.
In order to maintain backwards compatibility, Babbage introduced this scheme:

```
transaction_output = alonzo_transaction_output / babbage_transaction_output

babbage_transaction_output =
  {   0 : address
  ,   1 : value
  , ? 2 : [ 0, $hash32 // 1, data ]
  , ? 3 : script_ref
  }
```

In other words, a new format was created, but the legacy format was still supported.
The new format, `babbage_transaction_output`, was introduced 2022-09-22 with the Vasil hard fork,
The old format, `alonzo_transaction_output`, can be retired after 2023-03-22.

Note that this example required a **hard fork**.

A good example of a non-serious flaw requiring a **soft fork** is the removal
of zero-valued multi-assets in the mint field of the transaction body.

In the Babbage ledger era, a multi-asset value was defined as:

```
value = coin / [coin,multiasset<uint>]
```

Zero values can be confusing inside of things like explorers, so in the Conway era they are removed:

```
natNum = 1 .. 4294967295
value = coin / [coin,multiasset<natNum>]
```

Notice that block validators will not notice this change, though block producers will notice it.

### Summary

- We should strive to maintain backwards compatibility.
- Serious flaws can be fixed immediately (at the next hard fork), and can break backwards
  compatibility.
- Non-Serious flaws can be fixed (at the next hard fork), but the old format
  must be supported for at least six months with support ending at the next hard fork event after
  the six months have passed.

## Rationale: how does this CIP achieve its goals?

It seems clear that security issues merit breaking backwards compatibility and should be fixed
as soon as possible.
The six month compatibility window for non-serious flaws is mostly
arbitrary, but we need to allow enough time for people to migrate.
It would be great to have more explicit definitions for "serious" and "non-serious" flaws,
but this seems very difficult.

## Path to Active

### Acceptance criteria

- [x] The proposal is accepted and recognized by the ledger team.

### Implementation plan

N/A

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode)

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0082/README.md
---

- --
CIP: 82
Title: Improved Rewards Scheme Parameters
Status: Proposed
Category: Ledger
Authors:
- Tobias Fancee <tobiasfancee@gmail.com>
Implementors: []
Discussions:
- https://forum.cardano.org/t/cip-improved-rewards-scheme-parameters/112409
Created: 2022-01-03
License: CC-BY-4.0
- --

## Abstract

The current parameter settings of Cardano's rewards sharing scheme leave much to be desired in terms of fairness and promoting decentralization. minPoolCost puts small stakepools at a significant disadvantage. Replacing minPoolCost with a minPoolRate will ensure a level playing field for stakepools while providing sufficient sybil attack resistance. Additionally, the current setting of k, the optimal number of stakepools, is too low to provide an adequate pledge benefit. Increasing k will make the pledge benefit more effective and get delegations moving in hopes of helping single pool operators gain delegations.

The parameter changes in this proposal are an optimization of the current settings and are meant to improve the fairness and decentralization of Cardano. Furthermore, all changes suggested in this proposal have been specifically voiced by the Cardano community.

## Motivation: why is this CIP necessary?

### Definitions

- *Stakepool Operator (SPO)**
- The operator of a stakepool, can be a single person or an entity.

- *Multipool**
- A group or brand of stakepools operated by a single entity or stakepool operator.

- *ROS**
- Return on staking (often annualized and represented as a percentage of the initial investment).

- *Stakepool Viability Point**
- The amount of pledge required for a stakepool with zero delegations to distribute nonzero rewards to delegators (assuming minimum stakepool fees and ignoring luck in VRF block production, i.e., rewards are exactly proportional to stake).

- *Stakepool Competitive Point**
- The amount of pledge required for a stakepool with zero delegations to offer the same ROS as a fully-saturated stakepool with zero pledge (assuming minimum stakepool fees).

- *Stakepool Saturation Point**
- The maximum amount of total stake in a stakepool before total stakepool rewards are capped and ROS diminishes.

- *Minimum Attack Vector (MAV)**
- Also known as the Nakamoto Coefficient, the MAV is the minimum number of entities required to capture more than 50% of a network. In the context of Cardano, this refers to the minimum number of SPOs required to capture more than 50% of active stake.

- *Sybil Attack**
- An attack on an online system where an entity tries to take over the network by creating many identities or nodes.

### Cardano’s Declining MAV

Cardano’s declining MAV is a real problem as the network matures and gains more users. Ideally MAV should increase or stay consistent over time. However, this is currently not the case. This trend points to potential problems with the parameters set in the rewards sharing scheme. Now that staking on the network has matured, it is time to re-examine the rewards sharing scheme parameters and seek optimization. This proposal suggests changes that aim to increase the fairness and decentralization of the Cardano network.

### minPoolCost

minPoolCost sets a minimum fixed fee that a stakepool must collect from an epoch’s rewards before distributing to delegators. This parameter was added to the rewards sharing scheme to provide additional sybil attack protection. While this parameter has been successful at deterring sybil attacks on the Cardano network, it has been at the expense of fairness and decentralization. minPoolCost imposes a proportionally greater staking fee on small stakepools in contrast to larger pools. This discrepancy results in many small pools being unviable and the network centralizing around established pools. It is not uncommon for small stakepools with higher pledge to offer inferior rewards to that of large stakepools with much lower pledge. In this way, minPoolCost reduces the effectiveness of pledge in the rewards scheme. In order to create a level playing field for stakepools and increase the effectiveness of pledge, minPoolCost must be removed from the protocol.

### minPoolRate

The current pledge benefit favors established pools close to saturation as a means of sybil attack protection. Specifically, this property combats high pledge sybil pools (~1 mil ADA pledge) that could be set up by large entities such as centralized exchanges. In contrast to minPoolCost, the pledge benefit’s built-in sybil attack protection is significantly less aggressive and does not affect the viability of stakepools. In this way, it is a useful mechanism to combat sybil pools. However, the pledge benefit on its own has no means to combat zero fee sybil pools. Without minPoolCost, zero fee sybil pools could offer greater rewards than that of established stakepools that must set fees to pay for continued operation. In order to combat zero fee sybil pools without minPoolCost, minPoolRate must be added to the protocol. minPoolRate will set a minimum margin or percentage fee that operators can extract from an epoch’s staking rewards. This new parameter will protect established stakepools by ensuring that they collect sufficient revenue from operation while offering a competitive ROS. Additionally, minPoolRate can be updated to reflect current economic conditions. As minPoolRate enforces a proportional minimum fee, it does not affect the viability of stakepools. Unlike minPoolCost, minPoolRate will be able to provide sufficient sybil attack protection with the pledge benefit while maintaining a level playing field for stakepools. minPoolRate was originally proposed in CIP-0023.

### k, the optimal number of stakepools

k represents the optimal number of stakepools that the Cardano network can support. This is achieved through a saturation mechanism where after exceeding a saturation point a stakepool will earn no additional rewards. A stakepool larger than the saturation point will offer lower rewards than a stakepool at the saturation point. The parameter k is used to tune the saturation point. In addition to tuning the saturation point, k also affects the pledge benefit. The maximum pledge benefit exists when a pool is fully saturated. Therefore, increasing k will increase the number of optimal stakepools, decrease the saturation point, and make it easier for stakepools to achieve the maximum pledge benefit. Increasing the effect of the pledge benefit on rewards is imperative, as pledge currently has little effect on rewards allowing low pledge pools to proliferate through marketing alone. Furthermore, increasing k can be used to get stale delegations moving. This is especially useful in the case of saturated multipools, where delegators would have to reconsider their delegation potentially assisting small and medium sized pools. Any delegation flow from multipools to single pool operators will increase the decentralization of Cardano.

## Specification

This CIP proposes several parameter changes. In order to give SPOs and delegators enough time to react to the changes, this CIP is divided into 4 stages. The proposed time interval between stages is 3 epochs or 15 days. However, it is up to the implementors to determine the best time interval between stages. Parameters are changed in the following stages:

### Stage 1: minPoolCost decreased to 170 ADA

| Name of the Parameter | New Parameter (Y/N) | Deleted Parameter (Y/N) | Proposed Value | Summary Rationale for Change |
|-----------------------|---------------------|-------------------------|----------------|------------------------------|
| minPoolCost           | N                   | N                       | 170000000      | See Rationale Section.       |

### Stage 2: minPoolCost deleted, minPoolRate of 3% implemented (requires hardfork)

| Name of the Parameter | New Parameter (Y/N) | Deleted Parameter (Y/N) | Proposed Value | Summary Rationale for Change |
|-----------------------|---------------------|-------------------------|----------------|------------------------------|
| minPoolCost           | N                   | Y                       | N/A            | See Rationale Section.       |
| minPoolRate           | Y                   | N                       | 0.03           | See Rationale Section.       |

In order to ensure the compatibility of existing stakepool registration certificates with this CIP, the variable poolRateEff must be added to the protocol. This variable will be the effective margin used during stakepool fee calculation. Following the hardfork, poolRate will only be the value set by SPOs. poolRate will be superseded by minPoolRate if its value is lower than minPoolRate. This is demonstrated in the definition of poolRateEff:

$$poolRateEff = max(poolRate, minPoolRate)$$

### Stage 3: k increased to 750

| Name of the Parameter | New Parameter (Y/N) | Deleted Parameter (Y/N) | Proposed Value | Summary Rationale for Change |
|-----------------------|---------------------|-------------------------|----------------|------------------------------|
| stakePoolTargetNum    | N                   | N                       | 750            | See Rationale Section.       |

### Stage 4: k increased to 1000

| Name of the Parameter | New Parameter (Y/N) | Deleted Parameter (Y/N) | Proposed Value | Summary Rationale for Change |
|-----------------------|---------------------|-------------------------|----------------|------------------------------|
| stakePoolTargetNum    | N                   | N                       | 1000           | See Rationale Section.       |

## Rationale: how does this CIP achieve its goals?

### Principles

1.	Propose changes voiced by the community.
2.	Make Cardano staking fairer by eliminating aggressive anticompetitive features.
3.	Increase the effect of the pledge benefit on staking rewards.
4.	Get stale delegations moving and allow users to reconsider their delegation.
5.	Maintain sufficient sybil attack protection.

### Explanation

#### Stage 1: minPoolCost is decreased to 170 ADA

Stage 1 reduces minPoolCost from 340 ADA to 170 ADA. 170 ADA is proposed because it is half of the current minPoolCost and is close to what the USD value of minPoolCost was during the launch of Shelley. This value is more than sufficient to allow established community pools to stay profitable while enabling smaller pools to be more competitive. This value also maintains sybil attack resistance.

#### Stage 2: minPoolCost is deleted, minPoolRate of 3% is implemented (requires hardfork)

Stage 2 introduces the largest change to the network by deleting minPoolCost from the protocol in favor of a minPoolRate of 3%. 3% is proposed, as it allows established community pools to stay profitable when competing against minimum fee pools. This in combination with the pledge benefit provide sufficient sybil attack resistance while leveling the playing field for all stakepools. As pool size is significantly less important following this stage, pledge becomes a more important factor in choice of delegation. In contrast, Lido, Ethereum’s most popular liquid staking DApp, applies a 10% fee on participants' staking rewards.

#### Stage 3: k is increased to 750

Stage 3 increases k from 500 to 750. The purpose of stage 3 and stage 4 is two-fold. Firstly, increasing k increases the pledge benefit. The more effective the pledge benefit, the greater Cardano’s sybil attack resistance. Secondly, increasing k may get stale delegations moving again by oversaturating large pools. This will cause many delegators to reconsider their delegation, potentially helping smaller community pools find delegations. Increasing k is split into two stages to give SPOs sufficient time to react to the change. Furthermore, it is imperative that k is increased after stage 2, as small stakepools will only be competitive after minPoolCost has been removed.

#### Stage 4: k is increased to 1000

Stage 4 increases k from 750 to 1000. Stage 4 will further improve the pledge benefit and get more delegations moving. This is the final stage of this proposal.

### Test Cases and Sybil Attack Resistance

Below are test cases for the current rewards scheme and each stage of this proposal. The calculated values assume all pools are operating with minimum fees. Sybil pools are assumed to be small pools with no delegations. Community pools are assumed to be pools with significant delegation. As demonstrated below, this proposal allows community pools to have sufficient revenue (higher than the USD value of minPoolCost at the launch of Shelley) while creating a level playing field for all stakepools. Sybil attack resistance is maintained at every stage, as community pool ROS remains higher than sybil pool ROS. Data used for calculations was approximated from epoch 385. See [ImprovedRewardsSchemeParameters.xlxs](./ImprovedRewardsSchemeParameters.xlsx) for more test cases.

#### Current Rewards Scheme

Stakepool Viability Point: ~670,000 ADA

Stakepool Competitive Point: ~20,000,000 ADA

| Pool Type | Pool Stake        | Pool Pledge      | Pool Epoch Revenue | Delegator ROS         |
|-----------|-------------------|------------------|--------------------|-----------------------|
| Sybil     | 100,000.00 ADA    | 100,000.00 ADA   | 340 ADA            | Below Viability Point |
| Sybil     | 1,000,000.00 ADA  | 1,000,000.00 ADA | 340 ADA            | 1.2339%               |
| Community | 10,000,000.00 ADA | 100,000.00 ADA   | 340 ADA            | 3.5213%               |
| Community | Saturated         | 1,000,000.00 ADA | 340 ADA            | 3.7567%               |

#### Proposed - Stage 1

Stakepool Viability Point: ~335,000 ADA

Stakepool Competitive Point: ~16,500,000 ADA

| Pool Type | Pool Stake        | Pool Pledge      | Pool Epoch Revenue | Delegator ROS         |
|-----------|-------------------|------------------|--------------------|-----------------------|
| Sybil     | 100,000.00 ADA    | 100,000.00 ADA   | 170 ADA            | Below Viability Point |
| Sybil     | 1,000,000.00 ADA  | 1,000,000.00 ADA | 170 ADA            | 2.4977%               |
| Community | 10,000,000.00 ADA | 100,000.00 ADA   | 170 ADA            | 3.6498%               |
| Community | Saturated         | 1,000,000.00 ADA | 170 ADA            | 3.7749%               |

#### Proposed - Stage 2

Stakepool Viability Point: 1 ADA

Stakepool Competitive Point: 1 ADA

| Pool Type | Pool Stake        | Pool Pledge      | Pool Epoch Revenue | Delegator ROS |
|-----------|-------------------|------------------|--------------------|---------------|
| Sybil     | 100,000.00 ADA    | 100,000.00 ADA   | 1.52 ADA           | 3.6615%       |
| Sybil     | 1,000,000.00 ADA  | 1,000,000.00 ADA | 15.24 ADA          | 3.6617%       |
| Community | 10,000,000.00 ADA | 100,000.00 ADA   | 152.42 ADA         | 3.6631%       |
| Community | Saturated         | 1,000,000.00 ADA | 1081.06 ADA        | 3.6773%       |

#### Proposed - Stage 3

Stakepool Viability Point: 1 ADA

Stakepool Competitive Point: 1 ADA

| Pool Type | Pool Stake        | Pool Pledge      | Pool Epoch Revenue | Delegator ROS |
|-----------|-------------------|------------------|--------------------|---------------|
| Sybil     | 100,000.00 ADA    | 100,000.00 ADA   | 1.52 ADA           | 3.6615%       |
| Sybil     | 1,000,000.00 ADA  | 1,000,000.00 ADA | 15.24 ADA          | 3.6620%       |
| Community | 10,000,000.00 ADA | 100,000.00 ADA   | 152.49 ADA         | 3.6639%       |
| Community | Saturated         | 1,000,000.00 ADA | 720.44 ADA         | 3.6853%       |

#### Proposed - Stage 4

Stakepool Viability Point: 1 ADA

Stakepool Competitive Point: 1 ADA

| Pool Type | Pool Stake        | Pool Pledge      | Pool Epoch Revenue | Delegator ROS |
|-----------|-------------------|------------------|--------------------|---------------|
| Sybil     | 100,000.00 ADA    | 100,000.00 ADA   | 1.52 ADA           | 3.6615%       |
| Sybil     | 1,000,000.00 ADA  | 1,000,000.00 ADA | 15.24 ADA          | 3.6624%       |
| Community | 10,000,000.00 ADA | 100,000.00 ADA   | 152.52 ADA         | 3.6646%       |
| Community | Saturated         | 1,000,000.00 ADA | 542.82 ADA         | 3.6932%       |

### Backward Compatibility

This proposal includes several parameter changes and changes to ledger rules. Specifically, stage 2 of this proposal will require a hardfork to introduce a new parameter, delete a parameter, and modify the stakepool fee calculation equation. As mentioned in the specification section, the stakepool fee calculation equation must be modified in order to ensure current stakepool registration certificates are compatible with this CIP.

## Path to Active

### Acceptance Criteria

#### For Stages 1, 3, and 4
1. The raw transaction for the parameter update is built.
2. Transaction is signed.
3. Transaction is submitted.
4. Parameter update is accepted by majority of the network.
5. Parameter update is confirmed.

#### For Stage 2

1. Necessary research and development is completed for the changes to the ledger rules.
2. New version of cardano-node supporting the changes to the ledger rules is released.
2. Raw transaction signaling the hardfork is built.
3. Transaction is signed.
4. Transaction is submitted.
5. Hardfork is accepted by majority of the network.
6. Hardfork and changes to ledger rules are confirmed.

### Implementation Plan

Each stage will be an individual update. Stages 1, 3, and 4 will be parameter updates. Stage 2 will require a hardfork.

Before implementation, engineering and research teams must review the feasibility and potential consequences of the proposal, create the implementation for each update, and decide on the time interval between updates.

As previously mentioned, implementation will occur in the following stages:

1. minPoolCost is decreased to 170 ADA
2. minPoolCost is deleted, minPoolRate of 3% is implemented (requires hardfork)
3. k is increased to 750
4. k is increased to 1000

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0083/README.md
---

- --
CIP: 83
Title: Encrypted Transaction message/comment metadata (Addendum to CIP-0020)
Status: Active
Category: Metadata
Authors:
- Martin Lang <martin@martinlang.at>
- Ola Ahlman <ola@ahlnet.nu>
- Andrew Westberg <andrewwestberg@gmail.com>
- Adam Dean <adam@crypto2099.io>
Implementors:
- Cardano Explorer <https://cexplorer.io>
- StakePoolOperator Scripts <https://github.com/gitmachtl/scripts>
- AdaStat.net <https://adastat.net>
- Eternl Wallet <https://eternl.io>
- CNTools <https://cardano-community.github.io/guild-operators/#/Scripts/cntools>
- JorManager <https://bitbucket.org/muamw10/jormanager/>
- Cardanoscan.io <https://cardanoscan.io>
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/394
Created: 2022-12-08
License: CC-BY-4.0
- --


## Abstract

This CIP is an addendum to the original [CIP-0020](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0020), which is active since mid 2021 and widely used across many entities.
It describes the JSON schema to add encrypted messages/comments/memos as transaction metadata. It is fully backwards compatible and requires no changes in existing tools, explorers, wallets.
Tools/Wallets that do not have an implementation to decrypt this format will just show the encrypted base64 as the message, but it will not break any existing processes.

## Motivation: why is this CIP necessary?

### Current state of transaction messages

Transaction messages/comments/memos via CIP-0020 are widely used across the Cardano Blockchain. For example DEXs are using it to comment there payouts to the customers.
Individual users are using it to send funds across the network to other users with attached information about it. Users are buying goods and pay directly in ADA, attaching payment informations
via an added message.

Theses and many other usecases are actively happening on the blockchain right now and are a valuable addition to the core functions.

### What is the issue with the current implementation?

Metadata is attached as a CBOR bytearray in the auxiliary dataset of a transaction. So the encoding is just done from UTF8-Text to Hex-Code/Bytes and after that it is sent in plaintext over the network/blockchain.
To seek further adoption of blockchain usage, privacy features are a must in the future. Having cleartext information in a TCP packet might not be an issue for many things, but it is an issue if you wanna convince
users to use the blockchain and their transaction feature like users using it now with bank transfers.

It is easy for 3rd-party entities like Internet Service Providers, Datacenters or basically any Man-In-The-Middle to collect data that is sent in cleartext.
Data such as bank-account-numbers, email-addresses, telephone numbers, etc. which are parts of transaction messages.

### What benefits/features would this CIP have on transaction messages?

As pointed out above, everyone that is having access to the datastream and of course the publicly distributed ledger can extract the cleartext data of the transaction messages.
Because there must not even be a specific approach to get such transaction message data out of a TCP stream, just a simple filter for email addresses (example) is enough.
Even with a simple encryption of such messages - and publicly known passphrase - it is much more complicated for the Man-In-The-Middle listener to collect data on the fly.

- *Targeted benefits:**
- By using a default passphrase, Man-In-The-Middle "sniffer" cannot extract/parse data like email-addresses, invoice-numbers on the fly that easily. They would need to search for a cardano-node transmission and decrypt each message. Public explorers like Cexplorer.io, Cardanoscan, etc. can still show the decrypted message content via there https connection to the user. So no cleartext transmission at all.
- Different users can transfer funds with encrypted messages attached between each other, using a preshared passphrase. Only theses users need to know the content. Example: A user buys goods from an online-store, the store provides a preshared-passphrase to the user on their website or via email, the user sends the payment with payment-information encrypted with this passphrase to the store.
- Keeping the usecase of a transaction private does not only belong to different entities, but to a single user too. Example: If a user sends funds to a Dex or wants to lend some fund to a friend, he just can add information like 'Sent xxx ADA to bob for xxx' to the outgoing transaction as a documentation using an own choosen private passphrase. This information is stored on the chain and so in the wallet, only the user itself can review the use case of these transactions.
- Backwards compatible with CIP-0020
- Easy implementation by using well known tools like OpenSSL

### What this CIP is not trying to do

This addition to the original CIP-0020 should not be seen as the end-all-be-all security solution for privacy on the blockchain. There's better options and upcoming Midnight for that. The transaction messages are also not intended to act like chat messages on the chain.

# Specification - Encrypted message

The specification update for encrypted messages takes advantage of the simple original design, which is leaving room for additional json-keys not affecting the parsing of the content at all. The only outcome if a receiver does not process the encrypted content is, that the encrypted message is shown instead of an maybe autodecrypted one. But even the encrypted base64 strings fit into the max. 64char long string restriction. So it does not break any tools. More on the autodecryption later.

### Format:
``` json
{
  "674":
         {
           "enc": "<encryption-method>",
           "msg":
                  [
                    "base64-string 1", "base64-string 2", "base64-string 3" ...
                  ]
         }
}
```
The format is identical to the original one, with a simple addition of the `enc` (encryptionmode) entry.

The value given in the `enc` field references the type of encryption is used. Starting with a simple implementation named `basic`. There is room to add additional encryption method in the future like using ChaCha20/Poly1305 or using public/private key encryption. Also there is the possibility to not encode the metadata in the standard JSON format, but using CBOR encoding instead.


### Encryption methods:

#### **plain** - no encryption at all

  | Parameter | Value |
  | :--- | :--- |
  | method | **plain** |
  |description|plaintext, no encryption|

  This is not really an encryption mode, but included as a backwards compatible entry to signal this message as an unencrypted one. The entry is not needed and fully optional for unencrypted messages.

#### **basic** - aes-256-cbc salted symmetric encryption via passpharse (+default passphrase)

  | Parameter | Value |
  | :--- | :--- |
  | method | **basic** |
  | description | symmetrical encryption via openssl and a passphrase |
  | default passphrase | cardano |
  | type | openssl |
  | cipher | aes-256-cbc (salted) |
  | digest | pdkdf2 |
  | padding | PKCS#7 |
  | iterations | 10000 (default) |
  | key + iv | 32 bytes key + 16 bytes iv |
  | salt | 8 bytes |
  | prefix | `Salted__` |
  | encoding | base64|

OpenSSL was choosen, because its fast and widely available also for all kind of different platforms, web frontends, etc. Encryption algo is **AES-256-CBC** (salted) using `pdkdf2` to derive the key from the given passphrase. 10000 Iterations is the default value for this encryption method. The format of the encoded output is base64 format.

The encryption is based on a given passphrase, which can be choosen by the user. However, a default-passphrase "cardano" should be used to encrypt/decrypt if no other passphrase is provided or known.

OpenSSL uses [PKCS#7](https://datatracker.ietf.org/doc/html/rfc5652#section-6.3) as padding. The adopted cipher accepts only multiple of 16-byte blocks. Not fitting messages to be encrypted are filled with the number of padding bytes that are needed to form multiple of 16-bytes. So if 1 byte of padding is required, the padding "01" is added. If 2 bytes of padding are needed, "02 02" is added. If no padding is required, an extra block of 0x10 bytes is added, meaning sixteen "10" bytes. In order to be interoperable with OpenSSL this kind of padding is a requirement.

##### Why a default passphrase?

As pointed out above, its way harder for man-in-the-middle listeners, to decrypt every single message on the fly. So by using a default passphrase, tools can encrypt messages and explorers/wallets can autodecrypt such messages trying to use the default passphrase. In that way, the displayed message is automatically readable to the user. If a more protected communication is needed, the sender can choose a custom passphrase and communicate that to the receiver as a preshared passphrase.

What part is uses for the encryption?

The **whole content** of the unencrypted normal transaction **metadata `msg:` key is used**, thats the array with the message string(s). (Example below)

##### Is there sample code?

Yes, example implementations for node.js, PHP, bash, etc. can be found in the [codesamples](./codesamples/) folder. They are showing how to encrypt/decrypt text with the right parameters set for this basic mode.

- *warning**

Message decryption should be done on the user frontend if possible, not via server callbacks.**

#### Encryption/Decryption example on the console - basic mode

First, generate a normal metadata transaction message.

- *normal-message-metadata.json**:
``` json
{
  "674": {
    "msg": ["Invoice-No: 123456789","Order-No: 7654321","Email: john@doe.com"]
  }
}
```

The **encryption** is done on the **whole content of the `msg:` key**, so this is

`["Invoice-No: 123456789","Order-No: 7654321","Email: john@doe.com"]`

in our example.

- *Encrypt** this content via openssl, the default passprase **cardano**, iteration set to 10000 and key-derivation via pbkdf2:
``` console
echo -n '["Invoice-No: 123456789","Order-No: 7654321","Email: john@doe.com"]' | openssl enc -e -aes-256-cbc -pbkdf2 -iter 10000 -a -k "cardano"
```

The encrypted result are the **base64 encoded strings**:
```
U2FsdGVkX1/5Y0A7l8xK686rvLsmPviTlna2n3P/ADNm89Ynr1UPZ/Q6bynbe28Y
/zWYOB9PAGt+bq1L0z/W2LNHe92HTN/Fwz16aHa98TOsgM3q8tAR4NSqrLZVu1H7
```

Compose the JSON by **using the base64 encoded encrypted strings now for the `msg:` part**.

Also add the value `basic` for the `enc:` key, to mark this transaction message as encrypted with basic mode.

- *encrypted-message-metadata.json**:
``` json
{
  "674":
         {
           "enc": "basic",
           "msg":
                 [
                   "U2FsdGVkX1/5Y0A7l8xK686rvLsmPviTlna2n3P/ADNm89Ynr1UPZ/Q6bynbe28Y",
                   "/zWYOB9PAGt+bq1L0z/W2LNHe92HTN/Fwz16aHa98TOsgM3q8tAR4NSqrLZVu1H7"
                 ]
         }
}
```

Console one-liner:
``` console
jq ".\"674\".msg = [ $(jq -cjrM .\"674\".msg normal-message-metadata.json | openssl enc -e -aes-256-cbc -pbkdf2 -iter 10000 -a -k "cardano" | awk {'print "\""$1"\","'} | sed '$ s/.$//') ]" <<< '{"674":{"enc":"basic"}}' | tee encrypted-message-metadata.json | jq
```
## 


A **decryption** can be done in a similar way:
``` console
jq -crM ".\"674\".msg[]" encrypted-message-metadata.json | openssl enc -d -aes-256-cbc -pbkdf2 -iter 10000 -a -k "cardano"
```

Which results in the original content of the **msg** key:

`["Invoice-No: 123456789","Order-No: 7654321","Email: john@doe.com"]`

## Rationale

This design is simple, so many tools on the cardano blockchain can adopt it easily and a few have already started to implement it.
The original CIP-0020 design allowed the addition of new entries like the `"enc":` key for encrypted messages in this CIP. Therefore the encoding format of the encrypted message was choosen to be UTF-8 instead of bytearrays, because it would break the backwards compatibility to CIP-0020. But maybe more important, it gives the user a simple text-format to handle such messages. Users can copy and paste the base64 encoded string(s) using there own tools for creation and verification. For example, a user can simply copy the encrypted format from an explorer and verify it with an external own local tool. Such messages are usally pretty short. Yes, the benefit of using bytearrays is to have less data (around -33% over base64), but the decision was made to sacrifice this benefit in favor of the base64 format for the reasons pointed out before.

There is also for example [CIP-8](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0008), but CIP-8 doesn't really fulfill the simplicity of just providing encrypted messages. CIP-8 is focused on Signing, which is not needed for encryption. The method to generate encrypted messages here is not intended to verify the owner of a message via signing. There is no need that everything on Cardano must be difficult. Also using such CBOR encoded structures would break all currently implemented transaction message solutions. This CIP uses openssl and base64 encoding, and endusers could even copy&paste such text into other tools, etc. Future updates may include the option to mix encrypted and unencrypted messages by adding another key like `msgclear` to support such a mixed message style format.

### Implementation suggestions

Wallets/Tools can implement an autodecryption attempt with the default passphrase on such messages, to give the user a more streamlined experience. The communication should be done via https or similar to make sure the message cleartext is not exposed again during the transmission.
Additionally the Tools can prompt for an input and decrypt the message once the user has requested it, this decryption should be done on the user frontend for further security.

### Handling ill-formed 674 metadata ##

Like with CIP-0020, it is up to the wallet-/display-/receiver-implementor to parse and check the provided metadata. As for the current state, its not possible to have the same label "674" more than once in a cardano transaction. So a check about that can be ignored at the moment. This CIP provides the correct implementation format, the parsing should search for the "674" metadata label, the "msg" and the "enc" key underneath it. There should also be a check, that the provided data within that "msg" key is an array. All other implementations like a missing "msg" key, or a single string instead of an array, should be marked by the display-implementor as "invalid". Additional keys within the "674" label should not affect the parsing of the "msg" and the "enc" key. As written above, we will likely see more entries here in the future like a "version" key for example, so additional keys should not harm the parsing of the "msg" and "enc" key.

### Implementation conclusion ##

An encrypted transaction message should be considered valid if the following apply:

- Label = 674.
- has property "enc".
- enc property contains a supported method like `basic`
- has property "msg".
- msg property contains an array of strings, even for a single-line message.
- Each line has a maximum length of 64 characters.
- If there are additional properties, they don't invalidate the message. They can just be ignored.

If any of the above is not met, ignore the metadata as an encrypted transaction message. Can still be displayed as general metadata to the transaction.

The implementation format in this CIP should be the ground base for encrypted transaction messages/comments/memos and should be respected by creator-/sender-implementations as well as in wallet-/receiver-/display-implementations.

## Path to Active

### Acceptance Criteria

The acceptance criteria to be `Active` should already have been met, because the following Implementors using this CIP on the Cardano Blockchain:

- Cardano Explorer (https://cexplorer.io)
- StakePoolOperator Scripts (https://github.com/gitmachtl/scripts)
- AdaStat.net (https://adastat.net)
- Eternl Wallet (https://eternl.io)

#### Integration examples for encrypted messages

- *Cexplorer.io**: With the implementation of the **encrypted message decoding**.
![image](https://user-images.githubusercontent.com/47434720/204560392-f45bbe4f-7f78-48fa-9e47-4d3b104685bf.png)

- *StakePool Operator Scripts**: It works on the commandline like any other script of the collection by just adding the `"enc: basic"` parameter, you can provide an individual passphrase by using the `"pass:<passphrase>"` parameter. This automatically generates the needed metadata.json structure with the encrypted message in it and attaches it to the transaction itself.
![image](https://user-images.githubusercontent.com/47434720/205442737-748a7fb0-90fc-4cc3-898c-98b06894a900.png)

- *Eternl.io**:
![image](https://user-images.githubusercontent.com/47434720/210166917-8af475fe-5cda-46f5-bd8d-3fc4c2c12482.png)

- *AdaStat.net**: With the implementation of the **encrypted message decoding** using a pure **frontend solution**.
![image](https://user-images.githubusercontent.com/47434720/206574191-22aa490a-5870-4853-906b-443284458987.png)
![image](https://user-images.githubusercontent.com/47434720/206574354-5dd81551-efc6-4f69-a2aa-282bb40e5084.png)


### Implementation Plan

The following Projects have committed to also implement it:

- CNTools (https://cardano-community.github.io/guild-operators/#/Scripts/cntools)
- JorManager (https://bitbucket.org/muamw10/jormanager/)
- Cardanoscan.io (https://cardanoscan.io)

The plan is to reach out to other projects - which already supporting the normal transaction messages - too. And of course also to new ones.

There are various **code samples available** in the [**codesamples**](codesamples/) folder to make it as easy as possible for integrators to implement it.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode)

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0084/README.md
---

- --
CIP: 84
Title: Cardano Ledger Evolution
Status: Active
Category: Meta
Authors:
- Jared Corduan <jared.corduan@iohk.io>
Implementors: N/A
Created: 2023-01-30
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/456
License: CC-BY-4.0
- --


## Abstract

This CIP provides guidance for future CIPs concerning the Cardano ledger.

## Motivation: why is this CIP necessary?

The ledger is responsible for processing transactions and updating the shared state of the network.
It also processes block headers and handles the state transformation from one epoch to the next
(e.g. computing the staking rewards).

Most of the state maintained by the ledger relates to the
[Extended UTxO accounting model](https://iohk.io/en/research/library/papers/the-extended-utxo-model/) and
support for [Ouroboros](https://iohk.io/en/research/library/papers/ouroboros-a-provably-secure-proof-of-stake-blockchain-protocol/),
the proof-of-stake consensus mechanism used in Cardano.

This CIP aims to give guidance for future CIPs related to the ledger,
making it a registered category of the CIP process[^1].
[^1]: See [CIP-1](https://github.com/cardano-foundation/CIPs/blob/cip-cps-rework/CIP-0001/README.md#categories).
While nothing new is added to the usual CIP process,
expectations for ledger CIPs are made explicit and some background information is provided.

Many thanks to Arnaud Bailly and Michael Peyton Jones for all their help reviewing and providing
feedback on the first versions of this CIP.

## Specification

### Terminology

Context for the terminology used in this document is given in [CIP-59].

### Specifications

The ledger is specified as a state transition system using a
[small step operational semantics](https://github.com/input-output-hk/cardano-ledger/releases/latest/download/small-step-semantics.pdf).
We refer to this framework as the *Small Step Semantics for Cardano*, or the *STS* for short.
An understanding of the existing STS specifications for the
existing ledger eras is often required to fully understand the implications of changes to the ledger
(though an understanding of the Haskell implementation is a fair substitute).

The STS framework leaves both cryptographic primitives and the serialization format abstract.
The STS specifications need to be complete enough to realize a full implementation of the ledger
given the cryptographic primitives and serialization format.
The cryptographic primitives are described as appendices to the STS specifications,
and the serialization format is given as a
[CDDL file](https://www.rfc-editor.org/rfc/rfc8610).
There SHOULD be one STS specification per ledger era.

From the Byron to the Babbage ledger eras, the STS frameworks were written in $\LaTeX$.
Starting in the Conway ledger era, literate Agda will be used.
During the transition from $\LaTeX$ to literate Agda, we will take advantage
of the ability to substitute $\LaTeX$ in place of Agda code when needed for expedience.
With time, the Agda specification will not only be used to provide PDF specifications,
but also reference implementations.

### Ledger eras

A ledger era is a collection of features added to the ledger which are introduced at a hard fork.
The existing ledger eras, with very simplistic descriptions, are given below.

|name|new features|link|
| --- | --- | --- |
|Byron|initial UTxO ledger|[spec](https://github.com/input-output-hk/cardano-ledger/releases/latest/download/byron-ledger.pdf)|
|Shelley|decentralized block production, stake delegation|[spec](https://github.com/input-output-hk/cardano-ledger/releases/latest/download/shelley-ledger.pdf)|
|Allegra|timelock scripts|-|
|Mary|multi-assets|[spec](https://github.com/input-output-hk/cardano-ledger/releases/latest/download/mary-ledger.pdf)|
|Alonzo|Plutus scripts|[spec](https://github.com/input-output-hk/cardano-ledger/releases/latest/download/alonzo-ledger.pdf)|
|Babbage|improved Plutus script contexts|[spec](https://github.com/input-output-hk/cardano-ledger/releases/latest/download/babbage-ledger.pdf)|
|Conway|governance|[spec WIP](https://github.com/input-output-hk/formal-ledger-specifications)|

Note that there is no Allegra specification.
The Allegra era consists entirely of the addition of timelocks to the MultiSig script
introduced in the Shelley ledger era
(See figure 12 of the Mary specification).

Note that small, isolated changes can be made within a ledger era
by way of an intra-era hard fork. See [CIP-59] for more details.

#### Ledger events

Some provenance about the ledger calculations is provided by ledger events.
Sometimes these events have clear triggers (e.g. Plutus script execution)
and sometimes they provide intermediate calculations performed by the ledger rules
(e.g. the reweard calculation).
The events come with zero cost to a running node if not used and are not stored in the ledger state.
Documentation about the existing events can be found
[here](https://github.com/input-output-hk/cardano-ledger/blob/master/docs/LedgerEvents.md).

### Soft forks and Hard forks

Since most ledger CIPs will involve backwards incompatible changes,
the following two definitions are helpful:

- *Hard fork** - A *hard fork* is a change to the protocol (not restricted to the ledger)
resulting in a single new block definition becoming valid.

Alternatively, a hard fork is a backwards incompatible change for both
block producers and block validators.

- *Soft fork** - A *soft fork* is a change to the protocol (not restricted to the ledger)
resulting in fewer blocks being valid.

Alternatively, a soft fork is a backwards incompatible change for
block producers, but a backwards compatible change for block validators.

### Serialization

Transactions and blocks are serialized with
[CBOR](https://www.rfc-editor.org/rfc/rfc7049)
and specified with
[CDDL file](https://www.rfc-editor.org/rfc/rfc8610).

Serialization changes to the ledger are discussed in
[CIP-80](https://github.com/cardano-foundation/CIPs/pull/372).

Note that the serialisation format of the ledger state is unspecified and left as an
implementation detail (unlike the format of blocks).

### The ledger-script interface

The ledger and Plutus scripts have a common interface, described in [CIP-35].
CIPs relating to this inteface are relevant to both the ledger and to the Plutus CIP categories.

Additionally, there is significant overlap with the Ledger category around the ledger-script
interface and the protocol parameters.
CIPs which change these parts of Cardano should generally use the Plutus category and not the
Ledger category, although the Editors may ask the Ledger reviewers to comment.


### What merits a ledger CIP?

The criterion for deciding if a change to the ledger merits a CIP is as follows:
changes to the ledger require going through the CIP process whenever
every implementation of the Cardano ledger needs to be standardized on the details.

Bug fixes are an exception to this criterion, they do not merit a CIP except in the case
that the fix is substantially complicated.
A "bug fix" is a change to behavior where:
- The implemented behavior does not match the specification; or
- The specified behavior is clearly wrong (in the judgment of relevant experts)

Serialization changes are another possible exception to the criterion.
Many serialization changes can be handled as a part of the normal development process
without the need for a CIP.
Dramatic changes to the serialization, however, may benefit from the CIP process.

The ledger rules MUST be standardized in order for consensus to be maintained,
but things like the ledger events are more open to debate.

Changes to the protocol parameter values do not require a CIP since they are
a governance issue (see [CIP-1694]).

### Expectations for ledger CIPs

- Familiarity with the existing ledger specifications is required to propose changes to the ledger.
- The CIP specifications for ledger CIPs must be sufficiently detailed for inclusion in
  a formal ledger specification.
- Though proposals can be accepted solely on the basis of peer and Ledger team review, some areas (e.g. changes to the incentives model) might only considered ready for implementation if accompanied by an opinion from an expert designated by the implementor (e.g. with a proper game theoretic analysis).

### The Ledger reviewers

The following table gives the current set of reviewers for Ledger CIPs.

| Name               | Email                      | GitHub username |
|--------------------|----------------------------|-----------------|
| Andre Knispel      | andre.knispel@iohk.io      | @WhatisRT       |
| Alexey Kuleshevich | alexey.kuleshevich@iohk.io | @lehins         |

## Rationale: how does this CIP achieve its goals?

### There is only one implementation, why limit the scope of ledger CIPs in this way?

Even though there is currently only one implementation, this provides us with a clear
definition of what is essential to the ledger.
It also provides a clear path for future implementations.

### Why is the specification vague about the role of ledger events in the CIP process?

This decision should be left to the community as more use cases emerge.

### Why is familiarity with the formal specifications required?

It is not always clear which seemingly small details can make a large difference
to the many consumers of the ledger.
It is better that the CIP process achieve consensus on all the details than for
these decisions to be made during the implementation phase.

## Path to Active

### Acceptance Criteria

This CIP requires the acceptance of the Ledger team, which it has in virtue of its authorship.

### Implementation Plan

No implementation is required.

## Copyright

This CIP is licensed under [CC-BY-4.0][].

[CC-BY-4.0]: https://creativecommons.org/licenses/by/4.0/legalcode
[CIP-35]: https://github.com/cardano-foundation/CIPs/tree/master/CIP-0035
[CIP-59]: https://github.com/cardano-foundation/CIPs/tree/master/CIP-0059
[CIP-1694]: https://github.com/cardano-foundation/CIPs/tree/master/CIP-1694

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0085/README.md
---

- --
CIP: 85
Title: Sums-of-products in Plutus Core
Authors:
- Michael Peyton Jones <michael.peyton-jones@iohk.io>
Implementors:
- Michael Peyton Jones <michael.peyton-jones@iohk.io>
Status: Active
Category: Plutus
Created: 2023-01-30
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/455
License: CC-BY-4.0
- --

## Abstract

Plutus Core does not contain any native support for datatypes.
Instead, users who want to use structured data must _encode_ it; a typical approach is to use [Scott encoding][].
This CIP proposes to add native support for datatypes to Plutus Core using sums-of-products (SOPs), and to use that support more efficient scripts, and better code-generation for compilers targeting Plutus Core.

## Motivation: why is this CIP necessary?

### Background

#### 1. Datatypes in Plutus Core

The first designs of Plutus Core had native support for datatypes.
In the interest of keeping the language as small and simple as possible, this was removed in favour of encoding datatype values using [Scott encoding][].

[Experiments](https://github.com/input-output-hk/plutus/blob/96fd25649107658fc911c53e347032bedce624e9/doc/notes/fomega/scott-encoding-benchmarks/results/test-results.md) at the time showed that the performance penalty of using Scott encoding over native datatypes was not too large, and so we could realistically use Scott encoding.
But we might expect that we should be able to do better with a native implementation, and indeed we can.

#### 2. 'Lifting' and 'Unlifting'

This section talks about Haskell, but the same problem applies in other languages.

Given a Haskell value, how do you translate it into the equivalent Plutus Core term ('lifting')?
It's clear what to do for a Haskell value of one of the Plutus Core builtin types (just turn it into a constant), but it's much more complicated for a Haskell value that is a datatype value: you have to know how to do all the complicated Scott-encoding work.

For example:
- `1` lifts to `(con integer 1)` (easy enough)
- `Just 1` lifts to `(delay (lam case_Nothing (lam case_Just [case_Just (con integer 1)])))` (much more complicated)

This means that it's difficult to specify how to do lifting for structured data.
For example, if we want users (or the ledger!) to create Plutus Core terms representing structured data, it will be difficult to explain how to do it.

There is also the opposite direction: how can we turn a Plutus Core term back into a Haskell value ('unlifting)'?
Again, it's clear how to do this for builtin types and very unclear how to do it for datatypes.
Doing anything with a Scott-encoded datatype usually requires applying it to arguments and _evaluating_ it.
It's not reasonable to require the Plutus Core evaluator just to work out what a term means.

For example:
- `(con integer 1)` clearly unlifts to `1`
- `(delay (lam case_Nothing (lam case_Just [caseJust (con integer 1)])))` should unlift to `Just `, but this is hard to see. Scott encodings are not even canonical (there can be many terms that represent the same Scott-encoded value), so it is hard to know what they represent in general.

In practice this makes Plutus Core terms very opaque and difficult to work with.

#### 3. The `Data` Type

The design of the EUTXO model rests on passing arguments to the script for the redeemer, datum, and script context.
But we hit upon the problem of how to _represent_ this information.

The first design was to encode each argument as a Plutus Core term (using Scott-encoding for structued data), and pass it directly to the script.
However, this had several problems:
1. At that point, the on-chain form of Plutus Core was typed, and we would ideally want to typecheck the application before peforming it. But this could be expensive.
2. As we saw above, Plutus Core terms representing structured data are very opaque, so using them for redeemers and datums would make those values very opaque to users.
3. Creating the script context would require the ledger to take a Haskell value and lift it into a Plutus Core term, which as we've seen is non-trivial.

The solution that we chose was to pick a single generic structured data type which would be used for data interchange between the ledger and scripts.
This is the `Data` type.

However, `Data` is not the natural representation of structured values inside a script, that would be as datatypes.
So this means we often want a decoding step where the script translates `Data` into its own representation.

### Why change anything?

#### 1. Everything Can Always be Faster

The Plutus Core interpreter has got a lot faster over the last few years (at least one order of magnitude, possibly two by now).
However, there continues to be relentless pressure on script resource usage.
This will always be the case: no matter how fast we make things, things can _always_ be faster or cheaper and it will improve throughput and save people money.

That means that significant performance gains are always compelling.

#### 2. Creating and Analyzing Plutus Core Terms is Difficult

As discussed above, both lifting and unlifting are currently very tricky or impossible.

#### 3. Working with Data is Clumsy and Slow

The `Data` type is used in many places as a representation of structured data:
1. The script context is represented as `Data`
2. Some language toolchains that target PLC use `Data` for all structured data, rather than Scott encoding or similar.

However, working with `Data` is not very pleasant.
Deconstructing a datatype encoded as `Data` requires multiple steps, each of which has to go through the builtins machinery, which imposes additional overhead.
Our benchmarks show that this is very 3x slower even than Scott encoding datatypes.

## Specification

### Plutus Core Changes

The following new term constructors are added to the Plutus Core language:[^typed-plutus-core]
```

t ::=
  ...

- - Packs the fields into a constructor value tagged with i
  | (constr i t...)
- - Inspects the tag on t and passes its fields to the corresponding case branch
  | (case t t...)
```

Tags start from 0, with 0 being the first case branch, and so on.

[^typed-plutus-core]: See Appendix 1 for changes to Typed Plutus Core.

The small-step reduction rules for these terms are:

$$
\mathrm{CONSTR\_{SUB\_i}}\frac{i \in [0, n], t_i \rightarrow t'_i}{(\mathrm{constr}\ e\ t_0 \ldots t_n) \rightarrow (\mathrm{constr}\ e\ t_0 \ldots t_i' \ldots t_n)}
$$

$$
\mathrm{CASE\_SUB}\frac{t \rightarrow t'}{(\mathrm{case}\ t\ c_0 \ldots c_m) \rightarrow (\mathrm{case}\ t'\ c_0 \ldots c_m)}
$$

$$
\mathrm{CASE\_EVAL}\frac{e \in [0,m]}{(\mathrm{case}\ (\mathrm{constr}\ e\ t_0 \ldots t_n)\ c_0 \ldots c_m) \rightarrow [c_e\ t_0 \ldots t_n]}
$$

$$
\mathrm{CASE\_NOBRANCH}\frac{e \notin [0,m]}{(\mathrm{case}\ (\mathrm{constr}\ e\ t_0 \ldots t_n)\ c_0 \ldots c_m) \rightarrow (\mathrm{error})}
$$

Note that `CASE_SUB` does not reduce branches and `CASE_EVAL` preserves only the selected case branch.
This means that only the chosen case branch is evaluated, and so `case` is _not_ strict in the other branches.[^strict]

[^strict]: See 'Why is case not strict?' for more discussion.

Also, note that case branches are arbitrary terms, and can be anything which can be applied.
That means that, for example, it is legal to use a builtin as a case branch, or an expression which evaluates to a function.
However, the most typical case will probably be a literal lambda.

A new Plutus Core minor language version 1.1.0 is added to indicate support for these terms.
They are illegal before that version.

#### Costing

All steps in the CEK machine have costs, all of which are constant.
There are cost model parameters which set the costs for each step.
This will therefore be new cost model paramters governing the costs for the steps for evaluating `constr` and `case`.

There is a potential problem because `constr` and `case` have a variable number of children, unlike all the existing constructs. The risk is that we could end up doing a linear amount of work but only paying a constant cost.

However:
- `constr`s arguments are all evaluated in turn, so we are sure to pay a linear price.
- `case`'s branches are _not_ all evaluated, but the only place we could do a linear amount work is in selecting the chosen branch. But this can be avoided in the implementation by storing the cases in a datastructure with constant-time indexing.

## Rationale: how does this CIP achieve its goals?

### Benefits

#### 1. Faster Processing of Structured Data

Most Plutus Core programs spend a significant amount of their time operating on datatypes, so we would expect that a performance improvement here would make a significant difference.

Indeed, this seems to be true.
Our benchmarks show that this CIP leads to:

1. A 0-30% real-time speedup in example programs that do generic computation work, versus Scott encoding[^scott-vs-data] (the 0% is a primality tester that is mostly doing arithmetic, the 30% is mostly doing data manipulation).
2. A small global slowdown of 2-4%.

Note that the speedup from point 1 is inclusive of the slowdown from point 2.

[^scott-vs-data]: See "Can't we just implement datatypes using `Data` itself?" for comparison with using `Data` directly.

#### 2. Simple Lifting and Unlifting

Today, lifting is complicated, and unlifting is mostly impossible.
With this CIP lifting becomes very simple, and unlifting becomes not only possible but simple.
Our prototype implementation contains code-generation to generate lifting and unlifting code in Haskell, but it is straightforward enough that you could easily do it in any language.

There are still some limitations, e.g. you cannot lift or unlift structures that contain functions, but this is not that unusual (similarly, you typically can't serialise such structures).

### Costs

The costs of this proposal are substantial.

Plutus Core today has only 8 kinds of term, this proposal adds 2 more, an increase of 25%.
Additionally, the new term types are the first term types which have a variable number of children.
Both of these changes increase implementation complexity throughout the system: everything that processes Plutus Core terms must now handle the new cases, and handle the variable numbers of children.

This change also modestly expends our novelty budget.
Plutus Core is a deliberately conservative language: for the most part it is just the untyped lambda calculus.
The proposed new features for sums-of-products are also conservative, and follow a very typical pattern.
But they are not _quite_ as standard as the untyped lambda calculus.

Moreover, a change like this would be very painful to reverse, so we should seriously consider the costs before proceeding.

### Discussion

#### Why is case not strict?

This makes it different to all other Plutus Core terms, but we believe it is the expected behaviour: _no_ language has strict case destructuring.

Consider e.g.

```haskell
case (Just 1) of
   Just x -> print "Success!"
   Nothing -> error "Failure!"
```

Even in a strict language like Rust, nobody would expect this to evaluate the `Nothing` branch and evaluate to an error.

Furthermore, this additional laziness has significant performance benefits, see Appendix 2 for more discussion.

#### Can't we just implement datatypes using `Data` itself?

`Data` is expressive enough to encode datatypes, indeed this is why it is possible to encode the script context into it.
It's performant enough that people who work with the script context as `Data` are able to get by.
Moreover, lifting and unlifting to `Data` is very easy.
So why not just use it to encode _all_ datatypes?[^aiken]

[^aiken]: The Aiken language does this.

There are two reasons:

1. `Data` cannot contain functions, so it's less expressive than either SOPs or Scott-encoding.
2. The performance is much worse.

To justify 2, we benchmarked some list processing using datatypes implemented with SOPs and with `Data`, and the `Data` version is over 3x worse (not accounting for overhead, so the real figure may be higher).

### Alternatives

#### 1. Use Sums _and_ Products Instead

The current solution is a sums-of-products solution, i.e. we have an introduction and elimination form for objects that have _both_ a tag and a list of fields.
We could instead separate these two parts out and have an introduction and elimination form for sums, likewise for products.

That would look something like this:
```
t ::=
  ...

- - Tags a term with a tag
  | (tag i t)
- - Inspects a tagged term for the tag, and passes the
- - inner term to the case function corresponding to the tag
  | (case t t...)

- - Constructs a product with the given fields
  | (prod t...)
- - Accesses the i'th field of the given product
  | (index i t)
```

This is cleaner in some ways, and was the first prototype we implemented, but it has the following problems:
- It adds two more term constructors to the language
- It is significantly slower in practice, because the combined operation of processing a sum-of-products is extremely common

#### 2. Bind Case Variables in the Syntax for Case Branches

The design presented here allows the case branches to be arbitrary terms, which must be evaluated and then applied to the fields.

A more complex design would be to have the bindings for the variables be part of the case expression instead. That is, the current design does this:

```
(case (constr 1 a b) (\x -> t1) (\x y -> t2))
```
(the literal lambdas here could be arbitrary terms)

whereas we could do this:

```
(case (constr 1 x y) (alt x t1) (alt x y t2))
```

The effect of this is that we know statically that we have a function, and we can jump right into evaluating the _body_ of the case branch instead of first having to evaluate it to a function and apply the arguments.

This saves us a small but meaningful amount of overhead at the cost of making the implementation significantly more complex.

We prototyped this version and it was noticeably faster (~10%).
However, performance investigations showed that we can realize a significant amount of the performance gain through other means, leaving only about 3% unclaimed.[^see-appendix-2]
We think that this is an acceptable cost for a simpler implementation.

[^see-appendix-2]: See Appendix 2 for more details.

#### 3. Unsaturated `constr` and `case`

Both `constr` and `case` in this proposal are _saturated_, meaning that they have all of the arguments that they need explicitly provided in the AST.
This is not the only option.

We could easily have unsaturated `constr`, by making `(constr i)` a _function_ that then needs to be applied to its arguments.
This would be mildly more complicated to implement, since we wouldn't know how _many_ arguments to expect, and so we would need to always be able to add aditional arguments to a `constr` value, but this would be manageable.

Unsaturated `case` is more complicated.
While the tag on the scrutinee `constr` value tells us which branch we are going to take in the end, we don't know how many branches we need in total.
In principle we could extend the tags on constructors to include not only the selected tag but also the maximum tag.
That would allow us to know how many case branches we need.
A more serious problem is that we would not be able to be non-strict in the branches any more, as they would be passed as function arguments and hence forced.

The main advantage of unsaturated `constr` and `case` is that it would avoid the need for n-ary terms in the language, as both would then have a fixed number of children.
However, it makes them more complex to work with, and likely less efficient to implement.
Finally, this is simply a less common design, and so conservatism suggests sticking to the more standard approach unless there is a compelling reason not to.

## Path to Active

### Acceptance Criteria

- [x] `plutus` changes
- [x] Specification
- [x] Production implementation
- [x] Costing of the new operations
- [x] `cardano-ledger` changes
- [x] Implementation of new ledger language including SOPs
- [x] Further benchmarking
- [x] Ensure that regressions on existing scripts do not occur
- [x] Check additional real-world examples
- [x] Release
- [x] New Plutus language version supported in a released node version
- [x] New ledger language supported in a released node version

### Implementation Plan

This plan will be implemented by Michael Peyton Jones and the Plutus Core team with assistance from the Ledger team.
The changes will be in the `PlutusV3` ledger language at the earliest, which will probably arrive in the Conway ledger era.

## Appendices

### Appendix 1: Changes to Typed Plutus Core

While Typed Plutus Core is not part of the specification of Cardano, it is still interesting and informative to give the changes here.
Plutus Core was originally conceived of as a typed language, and making sure that we can express the changes cleanly in a typed setting means that we ensure that the semantics make sense and that it will continue to be easy to compile to Plutus Core from typed languages.

We add one new type constructors and one auxiliary constructor to the type language of Typed Plutus Core:
```
- - List of types. This is an auxiliarry syntactic form, not a type!
tyl ::= [ty...]

ty ::=
  ...

- - Sum-of-products type, has n children, each of which is a list of types
  | (sop tyl...)
```
This corresponds to a sum-of-products type, and it has one list of types for each constructor, giving the argument types.

We add the following new term constructors to the Typed Plutus Core language:
```
t ::=
  ...

  | (constr ty i t...)
  | (case ty t t...)
```

These are identical to their untyped cousins, except that they include a type annotation for the type of the whole term.
These make the typing rules much simpler, as otherwise we lack enough information to pin down the whole type.

The typing rules for the new terms are:

$$
\frac{\Gamma \vdash t_i : p_i,\ ty_{\mathrm{result}} = (\mathrm{sop}\ s_0 \ldots s_e \ldots s_m), s_e = [p_0 \ldots p_n]}{\Gamma \vdash (\mathrm{constr}\ ty_{\mathrm{result}}\ e\ t_0 \ldots t_n) : ty_{\mathrm{result}}}
$$

$$
\frac{\Gamma \vdash t : ty_{\mathrm{scrutinee}},\ ty_{\mathrm{scrutinee}} = (\mathrm{sop}\ s_0 \ldots s_n), s_i = [p_{i_0} \ldots p_{i_m}],\ \Gamma \vdash c_i : p_{i_0} \rightarrow \ldots \rightarrow p_{i_m} \rightarrow ty_{\mathrm{result}}}{\Gamma \vdash (\mathrm{case}\ ty_{\mathrm{result}}\ t\ c_0 \ldots c_n) : ty_{\mathrm{result}}}
$$

The reduction rules are essentially the same since the types do not affect evaluation.

### Appendix 2: Performance Analysis

Performance testing of the various prototypes revealed the following interesting facts:
1. Scott encoding is surprisingly performant.
2. It's tricky to say why sums-of-products is so much better.
3. Binding case variables makes a modest difference, but we can achieve the same in other ways.

Let's look at these in turn.

#### Why is Scott encoding so good?

To see why Scott encoding is so good, let's look at what happens when we 1) construct, and 2) destruct a typical datatype value, using sums-of-products but _without_ binding case variables.
We'll do this in the typed setting for clarity.

Let’s consider four programs, corresponding to creation and destruction of `data XYZ = MkXY X Y | MkZ Z`.

- *P1: Scott-encoded construction**

```
[
- - Scott-encoded constructor for MkXY
  (\(x : X) (y : Y) . (/\ R . \(xyc : X -> Y -> R) (zc : Z -> R) . xyc x y)
  xx
  yy
]
```

- *P2: Scott-encoded destruction**

```
[
  { P1 r }
- - case alternatives
  (\(x : X) (y : Y) -> b1)
  (\(z : Z) -> b2
]
```

- *P3: Explicit construction**

```
constr 0 xx yy
```

- *P4: Explicit matching**

```
case P3 (\(x : X) (y : Y) . b1) (\(z : Z) . b2)
```

P1 vs P3

- P1
- Evaluate the function, evaluate the argument, apply the argument (x2 for two arguments)
- Return a closure containing the two arguments in the environment
- P3
- Allocate an array
- Evaluate the argument and put it into the array (x2 for two arguments)
- Return the value containing the tag and the array

P2 vs P4

- P2
- Evaluate the scrutinee
- Force the scrutinee (type instantiation)
- Evaluate the resulting function, evaluate the argument, apply the argument (x2 for two branch arguments)
- Evaluate the branch function, evaluate the argument, apply the argument (x2 for two constructor arguments)
- Enter the branch body
- P4
- Evaluate the scrutinee
- Look at the tag
- Evaluate the case branch (x2 for two branch arguments)
- Apply the branch function to the constructor arguments (x2 for two constructor arguments)
- Enter the branch body

Scott encoding isn't doing anything clearly unnecessary: it's quite efficient at constructing values (because it just returns a function closure right away), and it's quite efficient at deconstructing values (because it just loads the constructor arguments into an environment and then applies the branch).

In particular, both ways we have to do a similar amount of work in a) evaluating the branch function, b) applying it to its arguments.

#### Why is sums-of-products better at all?

There are a few advantages for sums-of-products.
The most important is that it does not evaluate all the case branches, only the one it needs. Whereas Scott-encoding passes the case branches as simple function arguments, so they are always evaluated before we proceed.

This makes a surprising amount of difference.
We are performing so few steps already for each datatype match that adding a few more to evaluate unused case branches makes a difference.

#### What about binding case variables?

Binding case variables allows us to get rid of some of the work identified in the previous sections.
A `constr` that binds case variables has two differences:
1. The body of the case branch is _known_, rather than potentially being an arbitrary lambda that we will need to resolve by doing evaluation.
2. The constructor arguments can be loaded into the environment _all in one go_, rather than taking requiring multiple steps through the evaluator for each evaluation.

In practice it seems that difference 1 makes a significant amount of difference, because even if the case branch is a literal lambda, we are forced to allocate a value for the lambda.
Optimizing the evaluator to avoid this allocation removes a significant part of the advantage.

Difference 2 does not seem to make as large a difference. If we did see a big difference here then we might want to investigate adding multi-lambdas to Plutus Core in order to gain this benefit in other places. In practice, however, it does not seem to be that significant, and prototypes of multi-lambdas have not performed well.

### Appendix 3: Benchmark results

Throughout, the following commits are referenced:
- `master`: `df9b23f59852d11776fde382720df830c6163238`
- `sums-of-products`: `e98b284204070053b2e64bb66c7aa0832520afec`

These represent somewhat arbitrary snapshots.
The `sums-of-products` branch represents the current prototype, and `master` is it's merge-base with `plutus`'s `master` branch.
These may be updated at a future date.

#### Nofib

These are benchmarks taken from the `nofib` benchmark suite used by GHC.
They are defined in `plutus-benchmark/nofib`.
They are not totally comprehensive, but they represent a reasonable survey of programs that do various kinds of general computation.

These benchmarks are re-compiled from Haskell each time, so the comparison does not represent faster evaluation of the same script, but rather than we can now compile datatype operations using the new terms, which are faster overall.

| Script             | `master` | `sums-of-products` | Change |
|:-------------------|:--------:|:------------------:|:------:|
| clausify/formula1  | 18.99 ms | 14.07 ms           | -25.9% |
| clausify/formula2  | 24.33 ms | 18.10 ms           | -25.6% |
| clausify/formula3  | 66.30 ms | 48.93 ms           | -26.2% |
| clausify/formula4  | 96.71 ms | 72.88 ms           | -24.6% |
| clausify/formula5  | 417.8 ms | 302.7 ms           | -27.5% |
| knights/4x4        | 59.00 ms | 49.18 ms           | -16.6% |
| knights/6x6        | 152.2 ms | 127.8 ms           | -16.0% |
| knights/8x8        | 248.4 ms | 207.6 ms           | -16.4% |
| primetest/05digits | 32.39 ms | 32.33 ms           | -0.2%  |
| primetest/08digits | 58.97 ms | 59.02 ms           | +0.1%  |
| primetest/10digits | 82.44 ms | 82.83 ms           | +0.5%  |
| primetest/20digits | 168.8 ms | 169.3 ms           | +0.3%  |
| primetest/30digits | 246.7 ms | 248.9 ms           | +0.9%  |
| primetest/40digits | 338.8 ms | 341.7 ms           | +0.9%  |
| primetest/50digits | 329.1 ms | 331.1 ms           | +0.6%  |
| queens4x4/bt       | 10.03 ms | 8.904 ms           | -11.2% |
| queens4x4/bm       | 14.18 ms | 12.58 ms           | -11.3% |
| queens4x4/bjbt1    | 12.80 ms | 11.16 ms           | -12.8% |
| queens4x4/bjbt2    | 13.42 ms | 11.90 ms           | -11.3% |
| queens4x4/fc       | 31.96 ms | 28.30 ms           | -11.5% |
| queens5x5/bt       | 132.9 ms | 113.7 ms           | -14.4% |
| queens5x5/bm       | 167.2 ms | 143.1 ms           | -14.4% |
| queens5x5/bjbt1    | 160.8 ms | 137.0 ms           | -14.8% |
| queens5x5/bjbt2    | 167.4 ms | 145.6 ms           | -13.0% |
| queens5x5/fc       | 398.8 ms | 351.5 ms           | -11.9% |

The results indicate that the speedup is more associated with programs that do lots of datatype manipulation, rather than those that do a lot of numerical work (which in Plutus Core means calling lots of builtin functions).
However, we don't see any regression even in the primality tester, which is very numerically heavy (the noise threshold for the benchmarks is ~1%).

#### Validation

These benchmarks are some real-world examples taken from `plutus-use-cases`.
They are defined in `plutus-benchmark/validation`.
They thus represent real-world workloads.

The validation benchmarks are _not_ recompiled, they are specific saved Plutus Core programs.
These benchmarks thus only show changes in the performance of the Plutus Core evaluator itself.

| Script                   | `master` | `sums-of-products` | Change |
|:-------------------------|:--------:|:------------------:|:------:|
| auction_1-1              | 150.3 μs | 158.6 μs           | +5.5%  |
| auction_1-2              | 652.2 μs | 684.8 μs           | +5.0%  |
| auction_1-3              | 638.2 μs | 678.7 μs           | +6.3%  |
| auction_1-4              | 194.9 μs | 205.9 μs           | +5.6%  |
| auction_2-1              | 153.7 μs | 161.9 μs           | +5.3%  |
| auction_2-2              | 648.8 μs | 687.2 μs           | +5.9%  |
| auction_2-3              | 858.0 μs | 895.6 μs           | +4.4%  |
| auction_2-4              | 638.2 μs | 676.8 μs           | +6.0%  |
| auction_2-5              | 195.1 μs | 205.1 μs           | +5.1%  |
| crowdfunding-success-1   | 182.6 μs | 187.6 μs           | +2.7%  |
| crowdfunding-success-2   | 182.4 μs | 187.9 μs           | +3.0%  |
| crowdfunding-success-3   | 182.2 μs | 187.8 μs           | +3.1%  |
| currency-1               | 237.9 μs | 249.5 μs           | +4.9%  |
| escrow-redeem_1-1        | 331.7 μs | 342.8 μs           | +3.3%  |
| escrow-redeem_1-2        | 330.5 μs | 343.4 μs           | +3.9%  |
| escrow-redeem_2-1        | 386.0 μs | 403.0 μs           | +4.4%  |
| escrow-redeem_2-2        | 385.0 μs | 404.3 μs           | +5.0%  |
| escrow-redeem_2-3        | 386.4 μs | 403.7 μs           | +4.5%  |
| escrow-refund-1          | 134.9 μs | 140.9 μs           | +4.4%  |
| future-increase-margin-1 | 237.8 μs | 250.3 μs           | +5.3%  |
| future-increase-margin-2 | 522.0 μs | 538.2 μs           | +3.1%  |
| future-increase-margin-3 | 521.7 μs | 536.5 μs           | +2.8%  |
| future-increase-margin-4 | 489.9 μs | 508.8 μs           | +3.9%  |
| future-increase-margin-5 | 859.0 μs | 873.8 μs           | +1.7%  |
| future-pay-out-1         | 237.7 μs | 248.7 μs           | +4.6%  |
| future-pay-out-2         | 524.5 μs | 540.3 μs           | +3.0%  |
| future-pay-out-3         | 525.8 μs | 537.7 μs           | +2.3%  |
| future-pay-out-4         | 862.0 μs | 878.5 μs           | +1.9%  |
| future-settle-early-1    | 237.7 μs | 248.6 μs           | +4.6%  |
| future-settle-early-2    | 521.7 μs | 537.5 μs           | +3.0%  |
| future-settle-early-3    | 525.5 μs | 537.1 μs           | +2.2%  |
| future-settle-early-4    | 642.1 μs | 656.0 μs           | +2.2%  |
| game-sm-success_1-1      | 379.3 μs | 391.5 μs           | +3.2%  |
| game-sm-success_1-2      | 166.4 μs | 178.3 μs           | +7.2%  |
| game-sm-success_1-3      | 639.2 μs | 669.1 μs           | +4.7%  |
| game-sm-success_1-4      | 193.5 μs | 206.6 μs           | +6.8%  |
| game-sm-success_2-1      | 379.7 μs | 389.6 μs           | +2.6%  |
| game-sm-success_2-2      | 166.0 μs | 178.5 μs           | +7.5%  |
| game-sm-success_2-3      | 641.2 μs | 670.1 μs           | +4.5%  |
| game-sm-success_2-4      | 193.3 μs | 207.1 μs           | +7.1%  |
| game-sm-success_2-5      | 644.1 μs | 668.8 μs           | +3.8%  |
| game-sm-success_2-6      | 193.6 μs | 206.5 μs           | +6.7%  |
| multisig-sm-1            | 394.5 μs | 405.5 μs           | +2.8%  |
| multisig-sm-2            | 385.2 μs | 392.8 μs           | +2.0%  |
| multisig-sm-3            | 386.5 μs | 394.5 μs           | +2.1%  |
| multisig-sm-4            | 385.8 μs | 404.2 μs           | +4.8%  |
| multisig-sm-5            | 567.7 μs | 583.0 μs           | +2.7%  |
| multisig-sm-6            | 391.7 μs | 405.5 μs           | +3.5%  |
| multisig-sm-7            | 382.7 μs | 394.3 μs           | +3.0%  |
| multisig-sm-8            | 389.3 μs | 395.8 μs           | +1.7%  |
| multisig-sm-9            | 387.1 μs | 404.4 μs           | +4.5%  |
| multisig-sm-10           | 567.4 μs | 583.3 μs           | +2.8%  |
| ping-pong-1              | 320.6 μs | 327.4 μs           | +2.1%  |
| ping-pong-2              | 319.1 μs | 327.0 μs           | +2.5%  |
| ping-pong_2-1            | 182.2 μs | 190.6 μs           | +4.6%  |
| prism-1                  | 139.2 μs | 150.2 μs           | +7.9%  |
| prism-2                  | 404.5 μs | 412.2 μs           | +1.9%  |
| prism-3                  | 339.4 μs | 359.4 μs           | +5.9%  |
| pubkey-1                 | 118.6 μs | 123.7 μs           | +4.3%  |
| stablecoin_1-1           | 962.4 μs | 976.3 μs           | +1.4%  |
| stablecoin_1-2           | 163.7 μs | 174.2 μs           | +6.4%  |
| stablecoin_1-3           | 1.103 ms | 1.120 ms           | +1.5%  |
| stablecoin_1-4           | 174.0 μs | 185.4 μs           | +6.6%  |
| stablecoin_1-5           | 1.391 ms | 1.418 ms           | +1.9%  |
| stablecoin_1-6           | 215.4 μs | 227.5 μs           | +5.6%  |
| stablecoin_2-1           | 962.4 μs | 981.3 μs           | +2.0%  |
| stablecoin_2-2           | 163.5 μs | 174.2 μs           | +6.5%  |
| stablecoin_2-3           | 1.099 ms | 1.117 ms           | +1.6%  |
| stablecoin_2-4           | 173.6 μs | 184.8 μs           | +6.5%  |
| token-account-1          | 174.1 μs | 181.2 μs           | +4.1%  |
| token-account-2          | 312.8 μs | 334.3 μs           | +6.9%  |
| uniswap-1                | 408.0 μs | 425.4 μs           | +4.3%  |
| uniswap-2                | 203.9 μs | 211.7 μs           | +3.8%  |
| uniswap-3                | 1.779 ms | 1.830 ms           | +2.9%  |
| uniswap-4                | 282.6 μs | 297.6 μs           | +5.3%  |
| uniswap-5                | 1.139 ms | 1.180 ms           | +3.6%  |
| uniswap-6                | 276.2 μs | 288.4 μs           | +4.4%  |
| vesting-1                | 339.8 μs | 356.0 μs           | +4.8%  |

This is an average slowdown of 4%, which is not good at all. We do not want to have a negative impact on scripts that don't use the new constructs.

However, this slowdown is very difficult to avoid.
The GHC Core (GHC's intermediate language for Haskell programs) for both versions looks nearly identical, with the only differences being the introduction of new code for the new cases.
We believe that this indicates that GHC simply produces slightly slower code when we have more constructs, even if those code paths are not used.
In particular, there are some threshold effects when you cross certain numbers of constructors.

We tested this by doing an experiment that simply adds new unused constructors to the Plutus Core term type and evaluator frame type.
This caused a slowdown of on average 2%.
That's not enough to completely explain the loss, but we suspect that similar causes account for the rest (investigation is ongoing).

This is still bad -- a slowdown is still a slowdown -- but it's less bad because it's unavoidable if we ever want to increase the size of the language.
We will pay this cost whenever we decide expand the language.
Especially if the cost comes from threshold effects, it may be a one-time cost that we just have to pay on some occasion.

#### Datatypes using 'Data'

This benchmark compares lists implemented three ways: using SOPs, using builtin lists, and using `Data`.
It is defined in `plutus-benchmark/lists`.
The benchmark task is summing a list of 100 integers.
All three versions are using the Plutus Tx compiler, so any overhead is identical, and the only difference is how the list operations are implemented in the end.
There almost certainly is a decent amount of overhead (we did not attempt to measure it here), so the proportional difference in the underlying operations may in fact be greater.

| Benchmark     | CPU budget usage | Memory budget usage |
|:-------------:|:----------------:|:-------------------:|
| SOP lists     | 136797800        | 505300              |
| Builtin lists | 165182654        | 524632              |
| `Data` lists  | 427357685        | 1360262             |

Using `Data` is much worse.
This is not terribly surprising: pattern-matching on a datatype encoded using `Data` requires multiple builtin calls:

- A call to `ChooseData` to identify the type of `Data`
- A call to `UnConstrData` to get the tag and arguments as a builtin pair
- A call to `Fst` to get the tag
- Some number of calls to builtin operations on integers to work out which branch to take given the tag
- A call to `Snd` to get the args
- Some number of calls to `Head`/`Tail` to extract the arguments to be used

On the other hand, for SOPs this is a single machine step.

## Copyright

This CIP is licensed under [CC-BY-4.0][].

[CC-BY-4.0]: https://creativecommons.org/licenses/by/4.0/legalcode
[Scott encoding]: https://en.wikipedia.org/wiki/Mogensen%E2%80%93Scott_encoding


---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0086/README.md
---

- --
CIP: 86
Title: NFT Metadata Update Oracles
Status: Proposed
Category: Metadata
Authors:
- Nicolas Ayotte <nick@equine.gg>
- George Flerovsky <george@mlabs.city>
- Samuel Williams <samuel@mlabs.city>
- Nathaniel Lane <nathaniel@mlabs.city>
Implementors:
- Equine <https://www.equine.gg/>
- MLabs <https://mlabs.city/>
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/430
Created: 2022-11-01
License: CC-BY-4.0
- --

## Abstract

This proposal extends the CIP-25 standard for defining
and updating token metadata via transaction metadata,
by providing a new mechanism to update token metadata
without having to mint or burn tokens,
while maintaining full backward compatibility with CIP-25.
The new mechanism is capable of expressing metadata updates
more efficiently than CIP-25 updates.

## Motivation: why is this CIP necessary?

On Cardano’s eUTxO ledger, native tokens exist
without any inherently attached metadata.
The ledger does not provide a direct method for
preserving any information associated with an asset class of native tokens,
as transactions move the tokens from one UTxO to another.

The Media NFT Metadata Standard
([CIP-25](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0025))
proposed an indirect way to attach metadata to an asset class —
using the metadata field (CBOR tag 721) of its minting transactions.
When there are multiple minting transactions for the same asset class,
the latest minting transaction’s metadata overrides
all previous metadata defined for that asset class.

CIP-25 is widely supported across the Cardano community by blockchain indexers,
wallet providers, marketplace applications, and other stakeholders.
It has served the community quite well so far,
in particular for non-fungible tokens (NFTs) with static metadata
(i.e. intended to be mostly immutable after minting).

However, the CIP-25 metadata update mechanism is suboptimal
because it requires tokens of the asset class to be minted or burned
whenever its metadata is updated.
This is incongruous with the often purely-informational
purpose of metadata update transactions.
While it may sometimes be convenient
to combine token minting and metadata updates
within the same transaction (e.g. to save on transaction fees),
there is a broad range of applications where
metadata updates need to be more frequent and independent from minting events.

In practice, there are three main drawbacks to the CIP-25 update mechanism:

1. The metadata authority for an asset class must
retain the ability to mint/burn more tokens of the asset class
if it wants to retain the ability to post metadata updates/corrections.
This compromises the token issuers’ guarantees to token holders
that the token supply (or NFT collections) will not be diluted in the future.
Token holders are being asked to trust
that the metadata authority will not abuse its power
to mint new tokens of the same asset class.
2. Executing a minting policy script to mint or burn tokens
every time that an asset class’ metadata is updated
incurs script execution fees, particularly if the script is Plutus-based,
far exceeding the fees corresponding to
the actual informational content of the metadata update.
This cost can become severe for large collections of thousands of NFTs,
and it may be prohibitive for implementing
any dynamic fields for such large NFT collections.
3. An asset class’ metadata can only be updated as a whole.
This is inefficient (both in terms of transaction fees and ledger bloat)
when only a small subset of the metadata fields needs to be updated
for a large collection of NFTs,
and it can lead to errors
if the unmodified fields are improperly copied over to the update.

We encountered these drawbacks during the development
of an NFT gaming application,
where thousands of NFTs correspond to game assets with properties
(e.g. horse age, physical and mental abilities)
that evolve both over time and as the NFTs participate
in various game-related events on-chain.
These properties do not need to interact with smart contract logic directly,
which makes transaction metadata a more appropriate place to define them,
rather than UTxO datums with complicated state-evolution logic.

Our NFT gaming application is leveraging the Cardano immutable ledger,
rather than an off-chain database,
as the single source of truth to track the evolution
of the game assets’ most important properties.
This reassures users that the history of metadata states for a game NFT
cannot be retroactively erased or altered by the game admins —
although the game admins have the authority to modify metadata going forward,
they are retroactively accountable for all past updates.

We expect that this proposal will be useful to many other applications
that need to track dynamic on-chain metadata for large NFT collections,
particularly in the growing NFT gaming sector.
It may also be useful for artists to update on-chain metadata
about their art NFTs (e.g. royalty payment receiving address)
without having to burn or mint anything.

## Specification

Our proposal extends CIP-25 with a new update mechanism:

- A metadata oracle can be assigned for a given policy ID.
- For a policy with a metadata oracle assigned,
the metadata oracle can post CIP-86 updates to add, remove, or modify
any fields of the token’s CIP-25 metadata,
without needing to mint or burn any tokens in the metadata update transactions.
- The current metadata for a token can be deterministically reconstructed
by starting from the latest CIP-25 update to the token,
and applying the subsequent CIP-86 metadata updates
in ascending blockchain transaction order.

Both CIP-25 and CIP-86 updates affect the token metadata:

- A CIP-25 update removes all fields in the current metadata state
and replaces them with a new complete definition of the metadata state.
- A CIP-86 update selectively adds, removes, or modifies fields
relative to the current metadata state.
It does not apply to any asset class that has not had tokens previously minted
or any asset class that has not had CIP-25 metadata previously defined,
before the CIP-86 update.

However, we recommend that CIP-86 adopters use
the new metadata update mechanism exclusively
to manage all updates to their token metadata
after the initial CIP-25 metadata is set.

Our proposal only affects the `86` top-level CBOR tag
of the metadata field in Cardano transactions.
(Note: we’re using `86` as a stand-in for the CIP number
that will eventually be assigned to this proposal.)

### Assigning a metadata oracle for a token policy ID

A token metadata oracle is defined via two addresses:

- An oracle update address that is authorized to post metadata updates.
For example, this address could be controlled by a bot that automatically
posts token metadata updates as needed to the blockchain.
- An oracle main address that is used to update the oracle update address.
This address should be controlled via private keys
held in cold storage or hardware wallets.

A metadata oracle can be explicitly assigned to a policy ID
by setting the following metadata in a transaction
that mints or burns tokens for that policy:

```json
{
  "86": {
    "assign_metadata_oracle": {
      "<policyId>": {
        "main_address": "<Shelley_address>",
        "update_address": "<Shelley_address>"
      }
    }
  }
}
```

While a metadata oracle is not explicitly assigned
to a native-script-based policy ID,
the policy ID is implicitly assigned a metadata oracle
with both addresses set to an address derived from the native script.
Specifically, they are both set to an Enterprise address constructed
by setting the payment key to the verification key from the native script
and keeping the staking key empty.

### Updating an oracle assignment

The metadata oracle assignment for a policy ID can be updated
via a transaction signed by the oracle main address key.
The oracle assignment transaction must be signed
by the signing key of the main oracle address,
but it may contain any inputs and outputs,
as these are ignored for the purposes of metadata oracle assignment.

The schema for updating an oracle assignment is the same as
for the initial assignment in the minting transaction:

```json
{
  "86": {
    "assign_metadata_oracle": {
      "<policyId>": {
        "main_address": "<Shelley_address>",
        "update_address": "<Shelley_address>"
      }
    }
  }
}
```

The `main_address` or the `update_address` fields for a `<policyID>`
can be omitted, in which case the addresses for the omitted fields
remain the same for that policy ID.

If a metadata oracle was implicitly assigned to a policy ID
before the assignment update,
then the implicit assignment is replaced
by the new explicit assignment.

### Simple metadata updates

A simple metadata update transaction must be signed
by the signing key of the oracle update address,
but it may contain any inputs and outputs,
as these are ignored for the purposes of updating token metadata.

The schema for simple metadata updates in CIP-86
is similar to the CIP-25 schema,
but it is nested under `86.simple_metadata_update`
in the transaction metadata object.

```json
{
  "86": {
    "simple_metadata_update": {
      "<policyId>": {
        "<tokenName>": {
          "<metadataField>": "<metadataValue>"
        }
      }
    }
  }
}
```

To remove a metadata field,
set its value explicitly to `null` in the metadata update.

### Regex metadata updates

The schema for regex metadata updates is as follows:

```json
{
  "86": {
    "regex_metadata_update": {
      "<policyId>": {
        "<tokenNameRegex>": {
          "<metadataField>": "<metadataValue>"
        }
      }
    }
  }
}
```

A regex metadata update transaction must be signed
by the signing key of the oracle update address,
but it may contain any inputs and outputs,
as these are ignored for the purposes of updating token metadata.

The only difference from the simple metadata update is that
here the token names are defined in terms of PCRE regular expressions (regex).
The regex metadata update applies to any previously minted token
whose policy ID matches `<policyID>`
and whose token name matches the `<tokenNameRegex>` regular expression.

For example, the following metadata update would apply
to every Equine pioneer horse
between `EquinePioneerHorse05000` and `EquinePioneerHorse05999`:

```json
{
  "86": {
    "regex_metadata_update": {
      "30ed3d95db1d6bb2c12fc5228a2986eab4553f192a12a4607780e15b": {
        "^EquinePioneerHorse05\\d{3}$": {
          "age": 2
        }
      }
    }
  }
}
```

The regular expression pattern in `<tokenNameRegex>`
is defined according to the grammar in:

- European Computer Manufacturers Association,
"ECMAScript Language Specification 5.1 Edition",
ECMA Standard ECMA-262, June 2011. Section 15.10.
[https://www.ecma-international.org/wp-content/uploads/ECMA-262_5.1_edition_june_2011.pdf](https://www.ecma-international.org/wp-content/uploads/ECMA-262_5.1_edition_june_2011.pdf)

### Tabular metadata updates

Tabular metadata updates use a condensed rectangular format
to specify new values for a fixed set of fields for a large number of assets.
Specifically, for each policy ID we provide an object
with the following three fields:

- `field_paths` contains an array of paths pointing
to possibly-nested fields within a token metadata object.
Each of these field paths is a dot-separated list of field names
(e.g. `"images.background.sunset.url"`)
that lead from the top of the metadata object (for asset classes of the policy)
into a targeted field within that object.
- `token_names` contains an array of token names.
- `values` contains a table of values, represented by an array of arrays.
For each token name in `token_names`,
the outer array in `values` contains one element (an inner array)
of metadata values to which the fields targeted by `field_paths`
should be updated for that token name under the policy ID.
The outer array of `values` must be equal in length to `token_names`
and each inner array of `values` must be equal in length to `field_paths`.

```json
{
  "86": {
    "tabular_metadata_update": {
      "<policyId>": {
        "field_paths": [
          "<fieldPath>"
        ],
        "token_names": [
          "<tokenName>"
        ],
        "values": [
          ["<metadataValue>"]
        ]
      }
    }
  }
}
```

A tabular metadata update transaction must be signed
by the signing key of the oracle update address,
but it may contain any inputs and outputs,
as these are ignored for the purposes of updating token metadata.

For example, the following update would apply updates to six metadata fields
of five Equine horse NFTs:

```json
{
  "86": {
    "tabular_metadata_update": {
      "<policyId>": {
        "field_paths": [
          "age",
          "stats.acceleration",
          "stats.agility",
          "stats.endurance",
          "stats.speed",
          "stats.stamina"
        ],
        "token_names": [
          "EquinePioneerHorse00000",
          "EquinePioneerHorse00012",
          "EquinePioneerHorse00315",
          "EquinePioneerHorse01040",
          "EquinePioneerHorse09175"
        ],
        "values": [
          [3,34,16,18,51,33],
          [2,24,48,12,32,18],
          [3,33,34,41,14,31],
          [4,19,22,21,21,50],
          [1,24,11,36,22,14]
        ]
      }
    }
  }
}
```

It is equivalent to the following simple metadata update:

```json
{
  "86": {
    "simple_metadata_update": {
      "<policyId>": {
        "EquinePioneerHorse00000": {
          "age": 3,
          "stats": {
            "acceleration": 34,
            "agility": 16,
            "endurance": 18,
            "speed": 51,
            "stamina": 33
          }
        },
        "EquinePioneerHorse00012": {
          "age": 2,
          "stats": {
            "acceleration": 24,
            "agility": 48,
            "endurance": 12,
            "speed": 32,
            "stamina": 18
          }
        },
        "EquinePioneerHorse00315": {
          "age": 3,
          "stats": {
            "acceleration": 33,
            "agility": 34,
            "endurance": 41,
            "speed": 14,
            "stamina": 31
          }
        },
        "EquinePioneerHorse01040": {
          "age": 4,
          "stats": {
            "acceleration": 19,
            "agility": 22,
            "endurance": 21,
            "speed": 21,
            "stamina": 50
          }
        },
        "EquinePioneerHorse09175": {
          "age": 1,
          "stats": {
            "acceleration": 24,
            "agility": 11,
            "endurance": 36,
            "speed": 22,
            "stamina": 14
          }
        }
      }
    }
  }
}
```

### Versions 1 and 2
Like CIP-25, CIP-86 supports two different methods of
representing policy IDs and token name:

- In version 1,
policy IDs and token names must be expressed as text
(see [cddl/version_1.cddl](./cddl/version_1.cddl)).
- In version 2,
policy IDs and token names must be expressed as raw bytes
(see [cddl/version_2.cddl](./cddl/version_2.cddl)).

By default, all CIP-86 metadata updates use version 1.
However, version 2 can be used if the `version` field of the object
under the top-level `"86"` CBOR tag is set to `2`.

For example:

```json
{
  "86": {
    "version": 2,
    "simple_metadata_update": {
      "<policyIdRawBytes>": {
        "<tokenNameRawBytes>": {
          "<metadataField>": "<metadataValue>"
        }
      }
    }
  }
}
```

Regex updates are disallowed in version 2, because it is unclear
how to apply regular expressions to non-UTF-8 bytestrings (or their
corresponding hex encodings).

### Order of application for updates

Up to network consensus, the Cardano blockchain imposes
a total ordering on transactions added to the chain —
each transaction can be indexed by the slot number
of the block that added it to the chain,
and the transaction’s index within that block.
Network nodes may disagree about
which blocks have been added most recently to the blockchain;
however, the disagreement about whether a particular transaction was added
at a particular position in the total order
decreases exponentially as more and more blocks are added to the chain.

To reconstruct the metadata state for a given asset class,
scan through the sequence of transactions in a Cardano node’s blockchain,
applying the CIP-25 and CIP-86 updates
in the order that they are encountered in this sequence.
Should a transaction contain both CIP-25 and CIP-86 updates,
then the CIP-25 updates should be applied first,
followed by the CIP-86 updates.
If a transaction contains both a CIP-25 update
and a CIP-86 explicit oracle assignment,
then the CIP-25 update will be applied as usual
and the oracle addresses (main and/or update) will be set to
those of the explicit CIP-86 oracle assignment.
If the Cardano node rolls back some blocks from the chain tip,
then roll back the updates from those blocks as well.

We recommend that token metadata oracle operators wait for
their metadata update transactions to be confirmed at a sufficient block depth
before submitting any subsequent metadata updates for the same asset classes.
Doing so should minimize any confusion about
the order of simultaneous pending metadata updates
while the Cardano network settles toward consensus.

A transaction can contain CIP-86 token metadata updates of different types,
plus oracle assignment updates.
In this case, apply the updates in the following sequence:

1. Apply the CIP-86 regex update.
2. Apply the CIP-86 tabular update.
3. Apply the CIP-86 simple update.
4. Apply the CIP-86 oracle assignment update.

We recommend that token metadata oracle operators not mix multiple update types
in the same transaction, unless they have a clear understanding of
the outcome of applying the updates in the above sequence.

### Token metadata indexer

The CIP-86 token metadata indexer begins with
a configuration of the policy IDs for which it will be tracking metadata.
Optionally, it can be configured to track metadata for all tokens on Cardano.

The indexer monitors the blockchain for minting transactions.
If such a minting transaction mints tokens of a tracked policy ID and contains
an explicit oracle assignment and/or CIP-25 metadata for that policy ID,
then the indexer caches that assignment and metadata in its database.
If the transaction does not contain
an explicit oracle assignment for the policy ID,
and there is no prior oracle assignment,
then the indexer caches the implicit oracle assignment
for the policy ID in its database.

The indexer continues to monitor minting transactions for the policy IDs
that it’s tracking, applying oracle assignment updates
and CIP-25 metadata updates accordingly in its database.
A CIP-25 metadata update is applied as a wholesale replacement of the metadata
cached in the indexer database for the respective asset classes.

For each oracle main address currently assigned to a policy ID,
the indexer monitors the blockchain for transactions
that contain oracle assignment updates in their metadata
and are signed by the signing key of the main address.
The indexer updates its database to reflect these explicit oracle assignments
and removes any implicit assignments that were replaced by explicit assignments.

For each oracle update address currently assigned to a policy ID,
the indexer monitors the blockchain for transactions
that contain CIP-86 token metadata updates
and are signed by the signing key of the update address.
The indexer applies these metadata updates in the order defined in
[Order of application for updates](#order-of-application-for-updates).
CIP-86 metadata updates are applied to the asset classes and metadata fields
that they target, while keeping all other fields the same.

To be able to handle blockchain rollbacks, the indexer keeps track of
past metadata states for its policy IDs,
going back 2160 blocks (~12 hours) from the current blockchain tip.
Cardano’s `securityParam` Shelley Genesis parameter prevents nodes
from rolling back more than 2160 blocks.
If the Cardano node informs the indexer of a rollback,
the indexer restores the past metadata state that existed immediately before
all the metadata updates in the rolled-back blocks were applied.

For compatibility with existing applications
that are already relying on CIP-25 metadata indexers,
the CIP-86 indexer provides a similar API
so that those applications can get and display
the current CIP-86 token metadata
in the same way that they have been for CIP-25 metadata.
The indexer indicates that it is following the CIP-86 standard.

## Rationale: how does this CIP achieve its goals?

We pursued the following design goals in our solution:

1. Maintain backward compatibility with CIP-25 —
tokens that only use CIP-25 updates should have token metadata displayed
identically by CIP-25 indexers and CIP-86 indexers.
2. Ensure that the current token metadata can be reconstructed
by only looking at the blockchain, without accessing any external resources.
3. Allow token metadata to be updated
by designated authorities without minting or burning tokens.
4. Allow existing token issuers to opt into and out of
the CIP-86 update mechanism
without having to remint all of their existing tokens.
5. Allow designated authorities to securely rotate any keys that they use
in their automated and networked processes
(e.g. oracle update address used by a bot to update token metadata).
6. Allow token metadata to be updated more efficiently than
by a wholesale replacement of the entire metadata object for an asset class.
7. Minimize the time and resource usage required
for an indexer to apply CIP-25 and CIP-86 updates
for an asset class and then serve the resulting token metadata to applications.
8. Gracefully handle blockchain rollbacks that modify
the sequence of CIP-25 and CIP-86 metadata updates for an asset class.

### Backward compatibility

We maintain full backward compatibility with CIP-25:

- CIP-25 updates are respected and applied in the same way
as before by the CIP-86 indexer.
- CIP-86 updates are namespaced under a different top-level CBOR tag
than CIP-25, in order to prevent any clashes between
field names, policy IDs, and token names.
- The CIP-86 indexer provides the accumulated CIP-25
and CIP-86 metadata via the same API as the CIP-25 indexer.

### Using an assigned oracle for token metadata updates

We recognize CIP-86 updates only if they are issued
by an oracle currently assigned for the corresponding policy IDs.
This assignment is either directly authorized
by the token issuer (explicitly or implicitly)
or else indirectly authorized by the token issuer
via the delegated authority that the token issuer placed
in the originally assigned oracle to transfer the assignment to other oracles.
Therefore, all authority to post valid CIP-86 updates about a token
originates from the token issuer.

An alternative design could have oracles that declare themselves
as sources of metadata for tokens, without authorization from anyone,
and then users could voluntarily subscribe to
the metadata oracles that they wish to follow.
However, such an approach would make it very difficult for the Cardano ecosystem
to converge on a single objective metadata state for a token,
as each user would have his own subjective view of token metadata
based on his oracles subscriptions.
This alternative approach could be interesting to allow
secondary/auxiliary metadata to be defined for tokens,
but it is unsuitable for the primary metadata
that CIP-25 and CIP-86 seek to manage.

### Identifying oracles by addresses

We use two addresses to identify an oracle when assigning it to a policy ID.
It would be simpler to use a single oracle address,
but we chose to separate the main address (authorized to reassign the oracle)
from the update address (authorized to post updates) for an oracle.
We did this to mitigate the risk of using the update address
in an automated process on a network machine,
and to allow the update address to be safely rotated
via a transaction that can only be signed on
a cold storage or hardware wallet device using the main address.

Instead of using addresses to identify oracles,
we could have identified oracles by minting policies
(not the minting policies to which oracles are assigned).
In this alternative design, a minting policy `X` could have
an assigned oracle identified by minting policy `A`.
Under such an assignment, a CIP-86 update for a token under `X`
would be valid if the update transaction consumed a utxo
that contained a token of minting policy `A`.
In other words, the holder of an `A` token would be allowed
to post metadata updates about any `X` tokens.

This minting-policy-based alternative for identifying oracles may be
more advantageous for more flexibly managing oracle authorization
(including rate-limiting, time-boxing, etc.)
and proving data provenance to on-chain scripts.
These advantages are relevant for oracles that provide information
in utxo datums meant for use in smart contracts,
but they are not as relevant for this CIP,
where we seek to provide a method to provide updateable token metadata
via transaction metadata (as a direct extension of CIP-25).
Furthermore, it is easier to track transactions originating
from a given address in an indexer
than to keep track of all the people who control various authorization tokens,
at a given time.

### Removing the original CIP-86 update transaction restriction

In the first draft of this proposal,
we prohibited CIP-86 oracle assignment updates and metadata updates
from occurring in transactions that mint tokens,
in order to avoid awkward clashes with CIP-25 metadata transactions.
This was removed because, while it likely does not make sense
to create CIP-25 and CIP-86 metadata for the same token in one transaction,
it could feasibly make sense to want to update the metadata
for other tokens while minting another one.

We also originally required CIP-86 updates to occur in transactions
that only send ADA from an oracle address (main or update, as appropriate),
to prevent unforeseen interactions with other mechanisms
that may have negative consequences.
This requirement was removed for two reasons.
First, the reasoning above does not establish any specific issues
with other transaction types. Second, it is too restrictive
and creates unnecessary additional transactions for long sequences of updates,
causing the update issuer to spend unnecessary transaction fees.
Therefore, we decided to remove these unnecessary restrictions
on CIP-86 update transactions.

### Implicit oracle assignment

The implicit method of assigning a metadata oracle is needed
to allow existing token issuers to opt into the CIP-86 update mechanism.
Their minting policies may no longer allow any more token minting or burning,
which would prevent the token issuers from being able to
explicitly assign an oracle via a CIP-25 update for those policies.
The implicit assignment method bootstraps
the CIP-86 update mechanism for these policies.

The implicit address is of the Enterprise address type,
to avoid having to deal with staking keys.
If needed, the metadata oracle operator can send ADA to the Enterprise address,
and then spend it if the operator still controls
the payment key of that Enterprise address.

### Opting out of CIP-86

Opting out of the CIP-86 update mechanism can be done
by explicitly assigning an oracle with addresses
from which ADA cannot be spent (e.g. Plutus AlwaysFails).
If the minting policy does not allow any more minting or burning,
then this is an irreversible opt-out.

### Regex metadata updates

When managing large collections of thousands of NFTs,
one often needs to set a given field to the same value for many NFTs.
Doing this individually for each NFT
via CIP-25 updates or CIP-86 simple updates is inefficient,
so we have proposed the regex metadata update as a succinct way
to specify a mapping from multiple token names to a single metadata update.

### Tabular metadata updates

Another common use case for dynamic token metadata involves
having a set of volatile fields that should receive relatively frequent updates,
but where those updates should be different for each NFT.
Labeling each field value update with its field name
for each NFT is very verbose,
especially if the field is deeply nested within the metadata schema for the NFT.
For this use case, we have proposed the tabular metadata update format
as a way to avoid this repetition —
field names/paths are defined once in the column names of a rectangular table
and applied consistently for each row of updated metadata field values.

Rectangular tables are a standard format used in the data analytics field
for these situations.

## Path to Active

### Acceptance Criteria

This proposal may be considered active if:

1. The solution meets the design goals listed in the
[Rationale](#rationale) section to a satisfactory degree.
2. The indexer and simple tools to construct CIP-86 update transactions
(as described in the [Specification](#specification) section)
are fully implemented and provided in an open-source (Apache 2.0 licensed)
repository with sufficient documentation.
3. The CIP-86 metadata format, indexer, and/or indexer API
are used by several stakeholders in the Cardano ecosystem,
including dApps, blockchain explorers, analytics platforms, etc.

### Implementation Plan

Equine and MLabs are collaborating on
developing the indexer described in this CIP
and the Equine NFT gaming application will be
using CIP-86 updates to manage metadata updates
for its large collection of thousands of NFTs under multiple minting policies.

We will include detailed documentation, example configurations,
and tutorials on how to adapt the tools to new projects.

We are actively engaged in discussions with other stakeholders
in the Cardano ecosystem that are interested in adopting this CIP
to their projects, platforms, and tools.

## Copyright

This CIP is licensed under
the Creative Commons Attribution 4.0 International Public License
 ([CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode)).
## 


---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0088/README.md
---

- --
CIP: 88
Title: Token Policy Registration
Category: Tokens
Status: Proposed
Authors:
- Adam Dean <adam@crypto2099.io>
Implementors: []
Discussions:
- https://github.com/cardano-foundation/cips/pull/467
Created: 2023-02-27
License: CC-BY-4.0
- --

## Abstract

Currently, token projects (NFT or FT) have no mechanism to register the intent, details, or feature set of their minting
policy on-chain. This CIP will aim to create a method that is backwards compatible and enable projects to declare, and
update over time, the supported feature set and metadata details pertaining to their tokens.

This CIP will aim to make use of a hybrid (on and off-chain) information schema to enable maximum flexibility and
adaptability as new and novel use cases for native assets expand and grow over time.

## Motivation: Why is this CIP necessary?

This CIP was borne out of a distaste for the lack of on-chain token policy intent registration that has been cited as
both a centralization and security concern at various points over the preceding two years of native asset history on
Cardano.

- **Example 1: The Cardano Token Registry***

Many Fungible Token (FT) projects require special treatment of their native assets such as decimal places for proper
display and formatting, project information, and token logo. As it stands, these projects must currently register via a
GitHub repository ([Cardano Token Registry](https://github.com/cardano-foundation/cardano-token-registry)) in order to
have their token properly appear in wallets. This GitHub repository is currently managed by the Cardano Foundation (CF).
While there is no reason to believe that the CF would take any malicious or nefarious action, forcing token projects to
register in this fashion introduces a point of centralization, _potential_ gate keeping, and ultimately reliance on the
interaction of a 3rd party (the human at CF responsible for merging pull requests) in order for projects to register, or
update, their information.

- **Note:*** The original intent of the *CIP-26 Cardano Token Registry* was never to have a sole provider or
controller of the repository. However, time has shown that there is little interest from the community or various
service providers to contribute or participate in this or alternative solutions.

This CIP attempts to provide a decentralized solution to this problem.

- **Example 2: Token Metadata Insecurity***

One of the stated rationales for [CIP-68](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0068) was that the
"original" Cardano NFT Metadata Standard ([CIP-25](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0025)) was
insecure in some example use cases. This is due to the link between the transactional metadata and the minting of native
assets. For example, a smart contract that cannot/does not validate against transaction metadata (i.e. Liquidity Pool
tokens) could have a malicious user inject CIP-25 metadata into the transaction, potentially inserting illicit, illegal,
or otherwise nefarious metadata information tied to the tokens which may be picked up and displayed to end users
unwittingly by explorers and wallets.

- **Example 3: De-duplication of Data***

Similar to **Example 1** above, when it comes to Non-Fungible Token (NFT) projects
currently operating on the chain there is usually a desire to provide some level of information that pertains to all
tokens under the given policy. Examples include project/collection names, social media handles, and miscellaneous
project registration information. At current, this is generally solved by adding "static" fields in the metadata of
every token. By moving this project-specific information a layer higher, not only do we achieve a path to dynamism
but also reduce ledger bloat size by de-duplicating data.

Similarly, there are currently multiple marketplaces and decentralized exchanges (DEXes) in operation in the Cardano
ecosystem. At current, most DEXes pull information from the _Cardano Token Registry_ but there is no similar function
for NFT projects. As such, much information must be manually provided to individual marketplaces by the token projects
creating an undue burden on the project creators to provide a largely static amount of information via different web
forms and authentication schemes rather than simply publishing this information to the blockchain directly.

### CPS-0001: Metadata Discoverability and Trust

[CPS-0001](https://github.com/cardano-foundation/CIPs/pull/371) presents a problem of metadata discoverability and
trust.
This CIP attempts to address and solve several of the issues proposed in CPS-0001 but is most likely not a "complete"
solution and is rather narrowed (for the time being) to the scope of token projects although with some refinement to
the schema could potentially be expanded to support additional scopes.

#### Discoverability

The primary purpose of this CIP is to enhance discoverability of token projects utilizing and parsing only the
information contained on-chain. In the first version of this CIP we address the discoverability of top-level information
related to "Token Projects" such as NFT and FT projects needing to provide social media handles, human-friendly names,
etc.

The goal of both minimizing redundant data stored on-chain and enhancing discoverability of projects for platforms
like DExes and NFT Marketplaces is specifically referenced in Example #3 above.

Note that while some external chain indexing and validation will ultimately be required, there is no off-chain,
centralized or decentralized trusted repository of additional information required (although aspects of the metadata
provided may rely on off-chain storage solutions).

#### Correctness

This CIP aims to ensure metadata "correctness" on two different fronts.

1. **Actual Data Correctness**
- This CIP utilizes a strongly-typed, numerically indexed data structure that should minimize common errors and
      omissions observed in less strictly-typed standards. Parsers of the data presented within the scope of this
      standard should ignore any non-specified or non-conforming data.
2. **Data Provenance**
- Specifically in the context of correctness via proving provenance of the metadata, this CIP aims to address
      correctness via the same data witness standards utilized by CIP-26 although with a slightly modified data
      structure.
      Currently existing solutions for things like NFT Project verification standards rely on trust methods such as
      publishing a special message on your website, send us a DM from your Twitter account, and other less secure means
      of validating provenance of the data.

#### Trust

As mentioned in the *Data Provenance* note on Data Correctness above, this CIP minimizes the trust required by relying
on a verifiable witness signature versus currently existing solutions which largely rely on off-chain trust mechanisms
for proof of provenance. Therefore, we increase trust in the data by describing a relatively simple means of data
validation while decreasing the need for trust outside the scope of the on-chain metadata.

## Specification

Where applicable the 0 (zero) index of all specification documents is reserved for an optional integer version
identifier to enable future extensions to this and CIP-specific sub-standards.

A numeric-indexed structure is used to support required and optional fields in a format that is compatible with both
CBOR and JSON transport formats with minimal changes to the data structure and to minimize the possibility of
misspelling or capitalization issues.

### Modification and Extension of This Standard

This standard is likely to need frequent extension and modification, particularly relating to
[CIP-Specific Information](#6-cip-specific-information). Any group or individual wishing to extend or modify this
standard MUST comply to the following criteria:

- [ ] New CIPs SHOULD achieve the `Active` status prior to being included and documented in this directory after
  undergoing the
  regular community feedback and review process.
- [ ] Any change or modification to `required` fields in the root standard or a CIP's specific details MUST be written
  as a new `version`, MUST increment the version number by `1`, and MUST include new, versioned documentation for
  the CIP while leaving previous version documentation intact for backwards compatibility.
- [ ] Submissions for addition to this CIP MUST be made via a separate, dedicated pull request against this repository
  so that the format and documentation pertaining to CIP-88 specifically can undergo community review and feedback
  prior to inclusion here.
- [ ] Whenever possible, extensions to this CIP SHOULD attempt to introduce new indices and object definitions without
  changing or modifying existing data structures to enable new functionality and existing implementations to operate
  until newer standards can be adopted and enhance functionality rather than change it.
- [ ] New CIP submissions MUST follow the same paradigms and documentation examples as those found within the
  [CIPs](./CIPs) directory including:
- [ ] a README.md document describing the fields, values, and rationale
- [ ] a CBOR CDDL specification file
- [ ] a JSON format schema file (_optional_)
- [ ] a JSON example file showing all defined fields (_optional_)

### Registration Metadata Format

`Version: 1`

| Index | Name                                                 | Type  | Required | Notes                                                        |
|-------|------------------------------------------------------|-------|----------|--------------------------------------------------------------|
| 0     | Version                                              | UInt  | Yes      |                                                              |
| 1     | [Registration Payload](#registration-payload-object) | Map   | Yes      | Object describing and providing details for the token policy |
| 2     | [Registration Witness](#registration-witness-array)  | Array | Yes      | Array of witness signatures used to validate the payload     |

### Registration Payload Object

The Token Registration Payload Object (TRPO) consists of 4 required fields and optional additional fields to
provide context and information. The top-level metadata label of **867** has been chosen for the purposes of this
standard.

#### Fields

| Index | Name                                        | Type   | Required | Notes/Examples                                                                                                                                                                                                                                    |
|-------|---------------------------------------------|--------|----------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 1     | Scope                                       | Array  | Yes      | An array defining the scope of this registration (for greater compatibility with CPS-0001). The first entry should be an unsigned integer value identifying the type of scope while the second entry addresses the specific scope of registration |
| 2     | Feature Set                                 | Array  | Yes      | An array of unsigned integers specifying none or more CIP standards utilized by the tokens of this project. Should reference the assigned CIP number.                                                                                             |
| 3     | Validation Method                           | Array  | Yes      | How should this payload be validated.                                                                                                                                                                                                             |
| 4     | Nonce                                       | UInt   | Yes      | A simple cache-busting nonce. Recommend to use the blockchain slot height at the time of submission. Only the highest observed nonce value should be honored by explorers.                                                                        |
| 5     | Oracle URI                                  | Array  | No       | Reserved for future use, URI to an informational oracle API for this policy                                                                                                                                                                       |
| 6     | [CIP Details](#6--cip-specific-information) | Object | No       | If one or more of the CIPs addressed in the Feature Set have additionally defined metadata, it may be added here                                                                                                                                  |

The following fields are required in all token registration submissions.

##### 1. Scope

Currently, this CIP concerns itself with the scope of *Tokens* with relation to CPS-0001 as described in the Motivation
section. However, the specification is left flexible to encapsulate additional scopes and contexts (Stake Pools, dApps,
etc.) should the specification become adopted and the community desire to expand the scope of this CIP.

- *Scopes**

| ID  | Scope         | Format                             |
|-----|---------------|------------------------------------|
| 0   | Native Script | `[0, h'policyID', [h'policyHex']]` |

0. **Native Scripts**: Native scripts should be specified as an array with the first entry indicating the type (Native
Script), the second entry indicating the script hash (Policy ID) and the third entry consisting of an array with one or
more 64-byte strings constituting the hex-encoded CBOR representation of the Native Script itself. In this way, CIP-88
registration may be submitted on-chain prior to any tokens being minted and can be used by validators to confirm the
legitimacy of the certificate without any secondary information source.

- *Example:**

`[0, h'3668b628d7bd0cbdc4b7a60fe9bd327b56a1902e89fd01251a34c8be', h'8200581c4bdb4c5017cdcb50c001af21d2488ed2e741df55b252dd3ab2482050']`

#### 2. Feature Set

The _Feature Set_ is a simple array of unsigned integer values representing the CIP standards that should be applied to
the subject scope.

- *Example:**

`[25, 27]`

#### 3. Validation Method

In order to minimize issues relating to capitalization and misspellings, we should use a well-defined map of integer
values for validation methods that will be utilized by third party observers and processors to authenticate the payload.
The validation method entry should always be provided as an array with the first element being an unsigned integer
representing the method and additional entries providing additional context to the validation as required.

- **Proposed Validation Methods***

| ID  | Type                   | Format                              | Notes                                                                                                                                                                                                                                                                                                                                                                                                                                      |
|-----|------------------------|-------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 0   | Ed25519 Key Signature  | `[0]`                               | The most basic and simplistic approach to signing and validation. In this case the Registration Witness object could contain one or more pubkey + signed witness objects. The payload to be signed should be the hex-encoded CBOR representation of the Registration Payload object.                                                                                                                                                       |
| 1   | Beacon/Reference Token | `[1, [h'<policyId>',h'<assetId>']]` | Similar to the approach utilized by [CIP-27](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0027). We could attach this metadata during a mint transaction for a specially formatted token under the policy ID in question. CIP-27 uses a "nameless" token that has an empty "Asset ID" for example. This may be a validation method that lends itself better to supporting token projects that are minted via Smart Contract. |

- *Examples:**

`[0]`,
`[1, [h'<policyId>',h'<assetId>']]`

#### 4. Nonce

The nonce value is utilized to prevent a replay attack vector. The nonce value should be an unsigned integer value that
is always at least one greater than the previously registered value.

- *Example:**

`12345`

#### 5. Data Oracle URI

To be utilized and expanded upon in a separate CIP, this should be a valid URI pointing to a source of additional,
potentially dynamic information relating to the project and/or the tokens minted under it.

- *Example:**

`[
"https://",
"oracle.mytokenproject.io/"
]`

#### 6. CIP-Specific Information

This entry, if present, should be a CIP ID indexed object containing additional information pertaining to that CIP.
When and where possible the CIP-Specific registration should follow the CBOR-like declaration syntax to ensure that
the content is well-formed and easily parseable.

| CIP | Name                          | Version | Status | CDDL                                       | Rationale             |
|-----|-------------------------------|---------|--------|--------------------------------------------|-----------------------|
| 25  | Token Metadata                | 1       | Active | [CIP25_v1.cddl](./CIPs/0025/CIP25_v1.cddl) | [CIP-25](./CIPs/0025) |
| 26  | Fungible Token Information    | 1       | Active | [CIP26_v1.cddl](./CIPs/0026/CIP26_v1.cddl) | [CIP-26](./CIPs/0026) |
| 27  | Token Royalties               | 1       | Active | [CIP27_v1.cddl](./CIPs/0027/CIP27_v1.cddl) | [CIP-27](./CIPs/0027) |
| 48  | Metadata References           | 1       | Draft  | [CIP48_v1.cddl](./CIPs/0048/CIP48_v1.cddl) | [CIP-48](./CIPs/0048) |
| 60  | Music Token Metadata          | 1       | Draft  | [CIP60_v1.cddl](./CIPs/0060/CIP60_v1.cddl) | [CIP-60](./CIPs/0060) |
| 68  | Datum Token Metadata          | 1       | Active | [CIP68_v1.cddl](./CIPs/0068/CIP68_v1.cddl) | [CIP-68](./CIPs/0068) |
| 86  | Token Metadata Update Oracles | 1       | Active | [CIP86_v1.cddl](./CIPs/0086/CIP86_v1.cddl) | [CIP-86](./CIPs/0086) |

- **Note: CIP-0068 Tokens***

Due to a lack of clarity in the original language of CIP-0068, standards for fungible, non-fungible, and "rich" fungible
tokens have been added to the core standard. To accommodate for this, projects should use the CIP-68 data when using the
`222` (NFT) or `444` (RFT) tokens for top-level project information. Projects utilizing the `333` (FT) style tokens
should utilize the CIP-26 data structure to provide fungible token context.

- **Beacon Token Registration***

Where applicable to a specific CIP, the CIP-specific registration may refer to a "beacon token". This is standardized
in this CIP as a two-element array consisting of the hex-encoded policy ID and asset ID of the token to be used as a
beacon token for the purposes of smart contract interactions. e.g. `[
h'<policy_id>',
h'<asset_id>'
]`

- *Multiple Feature Set Example (CBOR):**

```cbor
{
  25: {
    0: 1,
    1: {
      0: "Cool NFT Project",
      1: [
        "This is a description of my project",
        "longer than 64 characters so broken up into a string array"
      ],
      2: [
        "https://",
        "static.coolnftproject.io",
        "/images/icon.png"
      ],
      3: [
        "https://",
        "static.coolnftproject.io",
        "/images/banner1.jpg"
      ],
      4: 0,
      5: [
        [
          "twitter",
          [
            "https://",
            "twitter.com/spacebudzNFT"
          ]
        ],
        [
          "discord",
          [
            "https://",
            "discord.gg/spacebudz"
          ]
        ]
      ],
      6: "Virtua Metaverse"
    }
  },
  27: {
    0: 1,
    1: {
      1: "0.05",
      2: [
        "addr_test1qqp7uedmne8vjzue66hknx87jspg56qhkm4gp6ahyw7kaahevmtcux",
        "lpy25nqhaljc70094vfu8q4knqyv6668cvwhsq64gt89"
      ]
    }
  }
}
```

### Registration Witness Array

#### (Native Scripts)

The Witness Array included in the on-chain metadata should consist of an array of arrays with two elements, the public
key of the signing key and the signed key witness. If a script requires multiple signatures, enough signatures to meet
the criteria of the script should be included and required for proper validation of an updated token registration.

The signing payload should be the hex-encoded Token Registration Payload Object.

- *Example**

```cbor
[
  [
    h'02b76ae694ce6549d4a20dce308bc7af7fa5a00c7d82b70001e044e596a35deb',
    h'23d0614301b0d554def300388c2e36b702a66e85432940f703a5ba93bfb1659a0717962b40d87523c507ebe24efbb12a2024bb8b14441785a93af00276a32e08'
  ],
  [
    h'26bacc7b88e2b40701387c521cd0c50d5c0cfa4c6c6d7f0901395757',
    h'secondSignatureByteString'
  ]
]
```

### Example NFT Token Registration Metadata

Below is a complete example of the hypothetical metadata payload for an NFT project registering their policy on-chain.

```cbor
{
  867: {
    0: 1,
    1: {
      1: [
        0,
        h'3668b628d7bd0cbdc4b7a60fe9bd327b56a1902e89fd01251a34c8be',
        h'8200581c4bdb4c5017cdcb50c001af21d2488ed2e741df55b252dd3ab2482050'
      ],
      2: [
        25,
27
      ],
      3: [0],
      4: 12345,
      5: [
        "https://",
        "oracle.mycoolnftproject.io/"
      ],
      6: {
        25: {
          0: 1,
          1: {
            0: "Cool NFT Project",
            1: [
              "This is a description of my project",
              "longer than 64 characters so broken up into a string array"
            ],
            2: [
              "https://",
              "static.coolnftproject.io",
              "/images/icon.png"
            ],
            3: [
              "https://",
              "static.coolnftproject.io",
              "/images/banner1.jpg"
            ],
            4: 0,
            5: [
              [
                "twitter",
                [
                  "https://",
                  "twitter.com/spacebudzNFT"
                ]
              ],
              [
                "discord",
                [
                  "https://",
                  "discord.gg/spacebudz"
                ]
              ]
            ],
            6: "Virtua Metaverse"
          }
        },
        27: {
          0: 1,
          1: {
            1: "0.05",
            2: [
              "addr_test1qqp7uedmne8vjzue66hknx87jspg56qhkm4gp6ahyw7kaahevmtcux",
              "lpy25nqhaljc70094vfu8q4knqyv6668cvwhsq64gt89"
            ]
          }
        }
      }
    },
    2: [
      [
        h'02b76ae694ce6549d4a20dce308bc7af7fa5a00c7d82b70001e044e596a35deb',
        h'23d0614301b0d554def300388c2e36b702a66e85432940f703a5ba93bfb1659a0717962b40d87523c507ebe24efbb12a2024bb8b14441785a93af00276a32e08'
      ],
      [
        h'26bacc7b88e2b40701387c521cd0c50d5c0cfa4c6c6d7f0901395757',
        h'secondWitnessByteString'
      ]
    ]
  }
}
```

### Example Fungible Token Registration Metadata

```cbor
{
  867: {
    0: 1,
    1: {
      1: [
        0,
        h'3668b628d7bd0cbdc4b7a60fe9bd327b56a1902e89fd01251a34c8be',
        h'8200581c4bdb4c5017cdcb50c001af21d2488ed2e741df55b252dd3ab2482050'
      ],
      2: [
26
      ],
      3: [0],
      4: 12345,
      5: [
        "https://",
        "oracle.tokenproject.io/"
      ],
      6: {
        26: {
          0: 1,
          1: [
            {
              0: [
                h"d894897411707efa755a76deb66d26dfd50593f2e70863e1661e98a0",
                h"7370616365636f696e73"
              ],
              1: "spacecoins",
              2: [
                "the OG Cardano community token",
                "-",
                "whatever you do, your did it!",
                "",
                "Learn more at https://spacecoins.io!"
              ],
              3: "SPACE",
              4: 0,
              5: [
                "https://",
                "spacecoins.io"
              ],
              6: [
                "ipfs://",
                "bafkreib3e5u4am2btduu5s76rdznmqgmmrd4l6xf2vpi4vzldxe25fqapy"
              ],
              7: [
                [
                  "ipfs://",
                  "bafkreibva6x6dwxqksmnozg44vpixja6jlhm2w7ueydkyk4lpxbowdbqly"
                ],
                "3507afe1daf05498d764dce55e8ba41e4acecd5bf42606ac2b8b7dc2eb0c305e"
              ],
              8: [
                h"d894897411707efa755a76deb66d26dfd50593f2e70863e1661e98a0",
                h"7370616365636f696e74"
              ]
            }
          ]
        }
      }
    },
    2: [
      [
        h'02b76ae694ce6549d4a20dce308bc7af7fa5a00c7d82b70001e044e596a35deb',
        h'23d0614301b0d554def300388c2e36b702a66e85432940f703a5ba93bfb1659a0717962b40d87523c507ebe24efbb12a2024bb8b14441785a93af00276a32e08'
      ]
    ]
  }
}
```

## Rationale: how does this CIP achieve its goals?

For this specification, I have drawn inspiration from
[CIP-36: Catalyst/Voltaire Registration Metadata Format](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0036)
which succinctly and canonically publishes data to the main chain (L1) via a metadata transaction and without any
required modification or customization to the underlying ledger.

By leveraging the existing signing keys present in native asset scripts from the beginning of the Mary Era on Cardano we
can enable all projects to update and provide additional, verified information about their project in a canonical,
verifiable, and on-chain way while also providing for additional off-chain information.

This makes this CIP backwards compatible with all existing standards (CIP-25, 26, 27, 68, etc) while also providing the
flexibility for future-proofing and adding additional context and information in the future as additional use cases,
utility, and standards evolve.

## Path to Active

### Acceptance Criteria

- [X] This CIP should receive feedback, criticism, and refinement from: CIP Editors and the community of people involved
  with token projects (both NFT and FT) to review any weaknesses or areas of improvement.
- [ ] Guidelines and examples of publication of data as well as discovery and validation should be included as part of
  criteria for acceptance.
- [X] Specifications should be updated to be written in both JSON Schema and CBOR CDDL format for maximum compatibility.
- [ ] Implementation and use demonstrated by the community: Token Projects, Blockchain Explorers, Wallets,
  Marketplaces/DEXes.

#### TO-DO ACCEPTANCE ACTIONS ####

- [ ] Publish instructions and tooling for publication and verification of certificates
- [ ] Develop standard for validation of Smart Contract minted tokens

### Implementation Plan

1. Publish open source tooling and instructions related to the publication and verification of data utilizing this
   standard.
2. Achieve "buy in" from existing community actors and implementors such as: blockchain explorers, token marketplaces,
   decentralized exchanges, wallets.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0089/README.md
---

- --
CIP: 89
Title: Distributed DApps & Beacon Tokens
Category: Tools
Status: Active
Authors:
- fallen-icarus <modern.daidalos@gmail.com>
- zhekson1 <zhekson@nomadpool.io>
Implementors: []
Discussions: []
Created: 2023-02-21
License: CC-BY-4.0
- --

## Abstract

This CIP describes a general design pattern for creating DApps where all users are given their own
personal DApp addresses. These DApps are called distributed-DApps (dDApps) because assets are
distributed across all of the user addresses. By giving everyone their own DApp address, users are
able to maintain custody *and* delegation control of their assets at all times. In other words, this
CIP describes how to design DeFi so that it does *not* centralize Proof-of-Stake blockchains like
Cardano. And by using beacon tokens, users are able to interact fully peer-to-peer - no third-party
indexing system is required. As a result, dDApps are essentially just extensions to a cardano node.

## Motivation: why is this CIP necessary?

To date, there are very few DeFi Dapps in which users maintain full delegation control of their
assets. In fact, sacrificing delegation control is so common in the blockchain space that many
Proof-of-Work proponents argue that DeFi will ultimately kill Proof-of-Stake blockchains. But this
actually isn't the fault of DeFi; **it's entirely the fault of how DeFi DApps are designed**.

The problem is that DApps are consistently designed as *concentrated-DApps*. Concentrated-DApps have
all users share a smart contract address; any assets than must be locked up for the DApp, are locked
up inside this shared smart contract address. But Cardano's delegation is by address, so sharing a
smart contract address fundamentally requires users to sacrifice delegation control of any assets
they lock up. In order for DeFi to not destroy Cardano's Proof-of-Stake and its on-chain government,
users **must** get their own addresses for DeFi DApps.

Some DeFi DApps have already switched to giving everyone their own DApp addresses, but then the
indexing piece is still an issue. At a high-level, the problem is: *if users are spread out across
many different addresses, how do they find each other?* So far, most DApps that have tried the
distributed approach have relied on a third-party for this (e.g., centralized indexers or a
data-availability layer).

This CIP describes a general design to distributed-DApps **that does not required a third-party
indexing system.** Users can find and interact with each other *directly*. No middlemen/middlebots
are required. The only thing users need is the current UTxO set which all nodes should already have.
And this design is generalizable enough to be used for all kinds of DeFi Dapps: DEXs, p2p lending,
options trading, etc.

## Specification

> [!IMPORTANT]
> All dDApps are essentially standards - all users of the DApp must use the same smart contracts
> despite getting their own DApp addresses.

### Personal DApp Addresses

All Cardano addresses have two parts:

1. A payment credential
2. An optional staking credential

For dDApps, the payment credential is the DeFi DApp's spending smart contract. But the staking
credential is the user's staking credential. The staking credential can be anything: a pubkey,
native script, or a plutus script.

The spending smart contract *must delegate owner-related spending authorization to the DApp address'
staking credential*. So if Alice wants to close her limit orders that are locked at her personal DEX
address, the staking credential she used for the DEX address must approve the transaction:

- Pubkeys must sign the transaction.
- Scripts must be successfully executed in the transaction (the withdraw 0 trick can be used for
this).

### Peer-to-Peer Discoverability

Finding each other can be done using *Beacon Tokens*.

The main idea is that the current UTxO set can be easily filtered for UTxOs that contain a specific
native asset. All Cardano database/API services support this ability. Beacon tokens are native
assets whose main purpose it to tag UTxOs.

> [!NOTE]
> The concept is the same as categorizing CIPs so that they can be filtered later. In essence, the
> UTxO set is used as a public message board and beacon tokens allow users to easily find the
> messages relevant to them.

At a high-level, when Alice creates a limit order, she will store a beacon token *with* the limit
order. It is the DApp's responsibility to ensure that beacon tokens are only ever found with
- *valid** DApp UTxOs. For example, the DApp should not allow Alice to store a beacon token with an
invalid limit order (e.g., it must have a price).

Beacon tokens **should never be circulating**. They should only ever be found inside valid DApp
UTxOs. This means beacons must be minted whenever DApp UTxOs are created, and burned whenever they
are spent. It is fine if the net change is zero, like when updating a limit order's price, but the
dDApp must still ensure beacon tokens are properly stored.

At a low-level, this requires the DApp's spending smart contract to work together with a set of
minting policies. Some dDApps may only require one minting policy while others may require several.
Generally, the pattern is to pass the spending script's hash into the minting policies as an extra
parameter. The minting policies must ensure that new beacon tokens are only stored with valid DApp
UTxOs. Valid DApp UTxOs are:

- Properly configured for the DApp. The minting policy ID should be included in the DApp UTxOs' datums.
- Only ever created at addresses using the DApp's spending script and a valid staking credential.
- *Creating DApp UTxOs at a DApp address without a staking credential should not be allowed.**

The first bullet will depend on the DeFi DApp. For example, a valid limit order should have a
positive price and have the asset in question. But all DApps should include the required minting
policy IDs so that the spending script can ensure beacons are burned whenever DApp UTxOs are spent.

The second bullet is required so that all DApp UTxOs discovered with the beacon tokens are
guaranteed to be governed by the same spending conditions. In other words, users know exactly what
behavior to expect.

DApp UTxOs must go to addresses with staking credentials because, if there is no staking credential,
there is technically no address owner. There is no one to contol assets locked at this address.
Therefore, there is never a reason for users to store assets in this address.

> [!TIP]
> In order to secure DApp UTxO updates where beacons can be re-used (e.g., updating a limit order's
> price), the minting policies can be executed as staking/observer scripts.

### Sharing Reference Scripts

Since all users use the same spending smart contract and minting policies, they can all use the same
reference script UTxOs. **These reference scripts must always be available.** To make them
permanently available, they should be deliberately locked forever inside the DApp address *without*
a staking credential. The reference script UTxOs can be deliberately stored with an invalid datum to
permanently lock them.

Doing this allows frontends to hard-code the reference script UTxOs and prevents any possible
service disruptions due to the reference script UTxOs being spent. By storing it inside the DApp
address without a staking credential, the dDApp standard is effectively "batteries-included".

### Upgrades

Distributed-DApp upgrades occur like with cardano-node:

    Users must decide when and if to upgrade their DApp addresses.
    No DAO or multisig is needed.

For example, if Alice has v1 limit orders, she must personally decide to close these limit orders
and create equivalent v2 limit orders. No one can force her to upgrade if she doesn't want to.

> [!IMPORTANT]
> dDApps should take care to ensure backwards compatibility. For example, it should still be
> possible to construct the full order book even if some users are using v1 while others are now
> using v2.

## Rationale: how does this CIP achieve its goals?

By giving all users their own personal DApp addresses, DeFi no longer undermines Cardano's
Ouroboros. And by using beacon tokens to enable easy filtering of the current UTxO set, users can
find and interact with each other fully peer-to-peer. There is absolutely no need for a third-party
indexing system to facilitate interactions.

> [!IMPORTANT]
> In order for the dDApp to work properly, all users must agree on which beacon tokens to use.
> Otherwise, filtering the UTxO set will be very complicated. This is why dDApps are effectively
> standards.

### Censorship Resistant

dDApps, as described in this CIP, use on-chain intents meaning limit orders are directly posted
on-chain. Because of this, on-chain intents are as censorship resistant as the Cardano's base layer.
However, this censorship resistance only applies when it comes to processing intents.

Another possible avenue for censorship is when trying to find the current on-chain intents.
Third-party indexing systems make it possible to censor users by *hiding intents*. For example, the
indexing system can refuse to tell Alice about particular limit orders. By using beacon tokens
instead, no one can stop Alice from finding out about all current on-chain intents.

- *dDApps are maximally censorship resistant.**

### Easy Address Recovery

While only knowing the user's staking credential, the user's DApp address can easily be discovered.
This means there is no extra burden on users and wallets. The user's seed phrase is enough to
recover all of their assets in the dDApp.

### Extensions to a Cardano Node

Since dDApps only require filtering the current UTxO set, dDApps are essentially **extensions to a
cardano node**. They do not require any DApp connector. Instead, they are meant to be built directly
into frontends similar to how WiFi support is built into all devices.

> [!IMPORTANT]
> Even light nodes should be able to integrate them.

### Zero Operating Costs

Filtering the current UTxO set can be done client side by nodes. For example,
[kupo](https://github.com/CardanoSolutions/kupo) is a light-weight version of db-sync that can use
native assets to decide what information to add to a local database. This means wallets can support
features like *Trading Pair Watchlists* where users can keep track of the current order book for
their favorite trading pairs. **This is possible without relying on any third-parties.**

Because of how easy it is to filter the UTxO set, there are no operating expenses for dDApps.

### Naturally DDos Resistant

Since all DApp UTxOs must be stored with beacon tokens which are native assets, the Cardano
blockchain requires each UTxO to be stored with a minimum amount of ADA. This minimum amount of ADA
helps prevent creating valid dust DApp UTxOs that would make local filtering of the UTxO set hard.

> [!IMPORTANT]
> The beacon tokens themselves prevent creating *invalid* DApp UTxOs. The minimum UTxO value
> requirement helps prevent creating absurd amounts of *valid* DApp UTxOs.

### Other CIPs for Discoverability

The main difference between beacon tokens and CIPs like
[CIP-68](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0068/README.md) is the
scenarios they were designed to operate in. There are effectively two scenarios:

    Scenario 1: only a single entity can mint a tag for a given use case.
    Scenario 2: anyone can mint a tag for a given use case.

This simple difference results in completely different game theories for each scenario. CIP-68 was
designed for scenario 1 where the business owner (or owners) are the only ones that can mint the
reference NFT. Since the business itself depends on the reference NFT being properly used, proper
usage is naturally incentivized.

For scenario 2, there is no natural incentive for proper usage of the tag. Consider Cardano-Swaps, a
distributed DEX that would be competing with several other DEXs. The value of any activity that
occurs through Cardano-Swaps does not go to the other DEX entities. Therefore, the other DEXs have
an incentive to destroy Cardano-Swaps in a kind of goldfinger attack. Since anyone can mint a tag,
the easiest way to destroy Cardano-Swaps would be to denial-of-service attack it by creating a lot
of mis-tagged UTxOs. Then, when users try to interact with each other, there would be too much noise
in the filtering results to find each other.

CIP-68 does not defend against this kind of attack; it doesn't need to since it operates in
scenario 1. Distributed-DApps operate in scenario 2 and therefore need more assurances than what
CIP-68 can offer.

## Path to Active

This CIP is already active.

- [Cardano-Swaps](https://github.com/fallen-icarus/cardano-swaps) is a distributed order book DEX.
- [Cardano-Loans](https://github.com/fallen-icarus/cardano-loans) is a distributed credit market.
- [Cardano-Options](https://github.com/fallen-icarus/cardano-options) is a distributed options trading
protocol.
- [Cardano-Aftermarket](https://github.com/fallen-icarus/cardano-aftermarket) is a distributed secondary
market for financial assets.

Finally, the above four protocols were built into a prototype desktop light wallet called
[p2p-wallet](https://github.com/fallen-icarus/p2p-wallet) which showcase how easy it is to integrate
dDApps directly into wallets *without* using a DApp connector.

## Copyright
[CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode)

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0091/README.md
---

- --
CIP: 91
Title: Don't force Built-In functions
Category: Plutus
Status: Proposed
Authors:
- Niels Mündler <n.muendler@posteo.de>
Implementors: []
Discussions:
- https://github.com/cardano-foundation/cips/pulls/459
Created: 2023-02-05
License: CC-BY-4.0
- --

## Abstract
The Untyped Plutus Language Core (UPLC) has established itself as the target language for a host of emerging Smart Contract Languages. These languages implement type safety by checking the types of variables at compile time. In the compiled output, type information is absent and no longer required or checked. This proposal suggests replacing or enhancing the set of builtin functions with untyped builtin functions, whose arguments are devoid of any type instantiations. This change aims to improve performance and reduce resource costs.

## Motivation: why is this CIP necessary?
Many currently available UPLC builtin functions require forcing between 1 and 3 times to eliminate type instantiations checked at a higher level language of the toolstack (PLC), which most third-party tools do not use. These forces only burn cycles of nodes that evaluate contracts, since there is no actual type instantiation happening internally. By removing the need for these no-op force operations, this proposal aims to enhance performance and reduce resource costs.

There is one data point as to how much performance improvement this may bring in the non-optimized case [here](https://github.com/input-output-hk/plutus/issues/4183#issuecomment-957934430). However, the performance improvement in the optimized case is generally constant: One can bind the forced builtins to a variable at the outermost layer of the UPLC program and from there on just use the forced builtins.

## Specification

For all existing UPLC Builtin Functions _x_ that require _n > 0_ forces for evaluation, this proposal suggests to implement the builtin function _x'_
without any required forces.

This proposal suggests that all existing UPLC Builtin Functions _x_ be *replaced* by _x'_. Generally, this proposal also suggests that no further Builtin Functions be defined that require `force`.

## Rationale: how does this CIP achieve its goals?

This proposal reduces the resources needed to evaluate builtin functions by removing the need to apply no-op force operations to them. However, the actual performance impact might be negligible, and the main impact could be on simplifying the language and making it easier for compiler writers. These are weaker reasons than widespread performance improvements. Implementing this proposal may also require a new Plutus ledger language, as described in CIP-35, due to the non-backwards-compatible changes.

The implementation will break the backward compatibility for future Plutus Smart Contracts.

## Path to Active

### Acceptance Criteria

- [ ] `plutus` changes
- [ ] Specification
- [ ] Production implementation
- [ ] Costing of the new operations
- [ ] `cardano-ledger` changes
- [ ] Specification, _including_ specification of the script context translation to a Plutus Core term
- [ ] Implementation of new ledger language, including new ledger-script interface
- [ ] Further benchmarking
- [ ] Check additional real-world examples
- [ ] Release
- [ ] New Plutus language version supported in a released node version
- [ ] New ledger language supported in a released node version

### Implementation Plan

It is currently not planned to implement this proposal.

## Copyright
This CIP is licensed under [CC-BY-4.0]: https://creativecommons.org/licenses/by/4.0/legalcode

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0093/README.md
---

- --
CIP: 93
Title: Authenticated Web3 HTTP requests
Category: Tools
Status: Proposed
Authors:
- Juan Salvador Magán Valero <jmaganvalero@gmail.com>
Implementors: []
Discussions:
- https://github.com/cardano-foundation/cips/pulls/442
Created: 2022-12-27
License: CC-BY-4.0
- --

## Abstract

The proposed Cardano Improvement Proposal (CIP) outlines a conventional structure for data payloads that are signed by wallets, which can be used by decentralized application (dApp) servers to authenticate user requests. By leveraging the Cardano blockchain as an identity provider, dApp servers can securely and trustlessly verify user identities without relying on centralized servers or third-party services. This CIP aims to provide a standard approach for implementing wallet signature authentication in dApps, improving the security and reliability of user interactions with decentralized systems.

## Motivation

The cardano wallets have the ability to sign arbitrary piece of data as we can see in the [Message signing CIP-0008](./CIP-0008/README.md). All wallets implement the method ```api.signData(addr: Address, payload: Bytes): Promise<DataSignature>``` defined in [Cardano dApp-Wallet Web Bridge CIP-0030](./CIP-0030/README.md).

dApps generate arbritary payloads as byte arrays. These payloads are signed and included in the protected headers of the signatures. The wallets are responsible for showing the payload data to the user, who will proceed to sign or reject the payload. It's a common practice to encode a string or a JSON string but there isn't any standard for the way to construct and to show this data.

The current implementations for web3 applications use static strings. This is dangerous because if a bad actor intercepts the signed message then it can be used in a replay attack by the bad actor. That's why it is very important to produce a dynamic payload rather a static string.

Another problem with the current approach is how the wallets show the information contained in the payload. The payload is a encoded byte array and it could contain anything. If Alice want to call an endpoint and Bob has the ability to change the message before Alice gets it. Alice must be notified somehow that she is signing a potentially malicious payload. A simple hex-encoded representation of the payload isn't enough to ensure a safe interaction.

## Specification

This specification involves multiple parties: Wallet/Client, dApp Server and Blockchain.

1. **Wallet/Client**: The Wallet/Client is responsible for managing the user's cryptographic keys. Anyone can create a wallet using the CIP-0030 API interface, but it may produce invalid or malicious data sent to the dApp. This CIP aims to validate the ownership and veracity of the data provided by wallets. Additionally, it establishes guidelines for mitigating common wallet attacks, improving security for user interaction.

2. **dApp Server**: The dApp Server represents the server-side infraestructure that supports decentralized applications (dApps). It communicates with the blockchain to retrieve stored data and validate wallet status. It must enforce minimum payload requirements to ensure authenticity and protect users from malicious actors.

3. **Blockchain**: The Blockchain is the underlying distributed ledger technology that forms the foundation of decentralized systems. It is a decentralized and immutable ledger that securely records all transactions and data in a chronological and transparent manner. The Blockchain can be utilized for authentication, providing user identity, and for authorization, tracking user history and current status.

```
- -----------+               +---------------+              +----------------+
|  Wallet/  |               |  dApp Server  |              |   Blockchain   |
|  Client   |               |               |              |                |
- -----------+               +---------------+              +----------------+
      |                              |                               |
      |                              |                               |
      | 1. Create payload            |                               |
      |------------+                 |                               |
      |            |                 |                               |
      |<-----------+                 |                               |
      |                              |                               |
      | 2. Request signature         |                               |
      |------------+                 |                               |
      |            |                 |                               |
      |<-----------+                 |                               |
      |                              |                               |
      | 3. Send signed payload       |                               |
      |----------------------------->|                               |
      |                              |                               |
      |                              | 4. Verify signature           |
      |                              |------------+                  |
      |                              |            |                  |
      |                              |<-----------+                  |
      |                              |                               |
      |                              | 5. Check blockchain (optional)|
      |                              |------------------------------>|
      |                              |                               |
```

### Requirement Levels

The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT", "SHOULD", "SHOULD NOT", "RECOMMENDED", "NOT RECOMMENDED", "MAY", and "OPTIONAL" in this document are to be interpreted as described in [RFC 2119](https://tools.ietf.org/html/rfc2119).

### Payload structure

The payload MUST be encoded as a JSON string. JSON strings are semi-structured data that are human-readable. So that with a straightforward decoding into text, it will be understandable for the readers. This feature also allows the users to debug it in an easy manner, for example, with the browser debugger tools.

The content of the payload will be included in the protected header of the COSESign1 signature, hence the content effects directly the behavior and security of the system. The payload MUST have the following fields:

1. The field `uri` MUST contain the full path to the endpoint, where the payload will be processed.

2. Sometimes, the endpoint `uri` field is not enough to determine its purpose. The user should understand perfectly the objective of the payload which he or she is signing. That's why the payload MUST contain an `action` field with a descriptive text containing the purpose of the payload. For example, if someone calls the endpoint `/users` without an action field, they can create or delete users. However, by including the action field in the payload, it not only provides additional clarity but also effectively limits the scope of the payload.

3. In order to improve globalization, the payload MAY include an `actionText` field that represents the action in the locale of the user. When present, the wallet MUST display this field to the user. By including the `actionText` field, the wallet facilitates the processing of the action field, eliminating the need for the server to be aware of the user's locale and the possible variants of the action text.

4. The payload MUST include either a UNIX `timestamp` or a `slot` number. The `slot` field represents a specific time in the blockchain and serves as a reference for synchronization between the client and the server. The `timestamp` or `slot` number is also used as a nonce and serves as an indicator for payload expiration in case the payload is compromised.

Additional fields MAY be included in the payload, and these fields can be string fields or objects. Depending on the specific process or use case, including additional fields in the protected header of the signature can provide valuable functionality and security enhancements. For example, in a registration request, it may be useful to include the email information as an additional field in the protected header. By doing so, the payload can be uniquely associated with that specific email, ensuring its integrity and preventing tampering.

#### JSON Schema
```JSON
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "object",
  "properties": {
    "uri": {
      "type": "string",
      "format": "uri"
    },
    "action": {
      "type": "string"
    },
    "actionText": {
      "type": "string"
    },
    "timestamp": {
      "anyOf": [
        {
          "type": "integer"
        },
        { "type": "string", "pattern": "^\\d+$" }
      ]
    },
    "slot": {
      "anyOf": [
        {
          "type": "integer"
        },
        { "type": "string", "pattern": "^\\d+$" }
      ]
    }
  },
  "required": ["uri", "action"],
  "oneOf": [{ "required": ["timestamp"] }, { "required": ["slot"] }],
  "additionalProperties": {
    "type": ["string", "object"]
  }
}
```
#### Minimum payload:

```JSON
{
    "uri": "http://example.com/signin",
    "action": "Sign in",
    "timestamp": 1673261248,
}
```
#### Sign up payload examples:
```JSON
{
    "uri": "http://example.com/signup",
    "action": "Sign up",
    "timestamp": "1673261248",
    "email": "email@example.com"
}

{
    "uri": "http://example.com/signup",
    "action": "SIGN_UP",
    "actionText": "Registrar",
    "slot": 94941399
}
```

### Wallet specification

The wallets can improve the overall security implementing the following guidelines. We RECOMMEND to show in a structured way the payload information for sake of clarity. This information should be well understood by the users before the payload is signed.

The `uri` field provides information about the hostname of the application. This hostname MUST be included in the wallet allow list. If a known domain A tries to sign a payload for an unknown domain B, you will be prompted with permission popup making more obvious the cross-domain interaction. When possible, the wallet SHOULD warn the user if a payload is for a different domain.

The wallet SHOULD update the `timestamp` field to the current time just before the signature. This field ideally should match the moment just before the signature such that the server receives fresh payload.

### dApp Server processing

The server has ultimate responsibility of processing correctly the requests. We use the content to validate the payload. The request will be processed with the following steps:

1. The server MUST check the action and the endpoint included in the request. Each route to an endpoint MUST have an associated action and a URI. The first step is to check that they match with the parameterized action.

2. The server MUST check the expiration of the payload. The expiration SHOULD be enough to give time to the user to introduce the wallet password but it SHOULD NOT be too long, we RECOMMEND not more than 5 minutes.

3. The server MUST validate the COSESign1 signature and check that the address inside the protected map of the signature corresponds to the public key in the COSEKey.

Additionally the server COULD extract the payload content and pass it through the server logic.

## Rationale: how does this CIP achieve its goals?

CIP-0008 enhances authentication by enabling individuals to prove ownership of addresses, identities, and off-chain data through message signing. It provides a reliable means of authentication by allowing individuals to attach a public key to their data and sign messages associated with that data, thereby establishing ownership and ensuring the integrity of the authentication process.

Additionally, This specification provides the general guidelines and necessary recommendations for performing secure authenticated web3 requests in the Cardano ecosystem. It covers the two main desired characteristics for a secure payload: It must expire and it must be non-static. Moreover, the signature method proposed in this CIP does not require users to spend funds in a transaction, which further lowers the cost and barriers to entry for users.

Another important aspect for security is how wallets process the payload. They can improve the security using the data inside the payload to warn the users about possible malicious interactions. This specification emphasizes the importance of informing users clearly about the purpose of the payloads and how wallets can use the URI field to apply allow-lists and/or cross-domain policies. It establishes also the requirements and recommendations for server side processing. The server must also ensure the validity of the signature and the payload, as well as of its purpose in order to accomplish the authentication.

In addition to the aforementioned aspects, this CIP also aims to promote decentralization and enhance security and privacy by enabling users to sign and verify transactions without relying on external servers or third parties. By allowing users to create and sign their own payloads, this specification reduces the dependency on centralized authorities and enhances the security and privacy of the transactions.

### Alternative implementations

During discussions about this specification, the possibility of modifying CIP-0008 to incorporate the standards defined here was considered. However, a thorough evaluation revealed that this approach would require extensive modifications to CIP-0008 and CIP-0030, leading to significant changes in the Cardano wallet API. Moreover, it would result in a lengthy waiting period for browser wallet developers to implement the necessary requirements. This could potentially bypass important security measures outlined in this CIP, such as the requirement for human readability.

While this alternative approach brings advantages, such as defining the payload in CBOR, which aligns well with Cardano, it also presents challenges. JSON and CBOR offer different levels of expressiveness, and the choice between the two depends on the specific needs of the application. JSON provides a more flexible and widely supported data format, whereas CBOR offers a more compact and efficient representation, particularly beneficial when working with the blockchain.

Considering these factors, it was concluded that deploying this standard as it currently stands, while coexisting with a future version that allows users to choose between JSON and CBOR payloads, would be the most practical approach. This would provide sufficient time for modifying CIP-0008 and CIP-0030, enabling browser wallet developers to fulfill the requirements for human readability and make necessary adjustments to the wallet API. Consequently, a version 2 of this CIP can be introduced, incorporating COSESign and CBOR, accommodating both realms, and ensuring broad support.

### Common usage

The payload signature ensures wallet ownership without incurring transaction fees. However, requiring the user to enter their spending password for every authenticated request can be inconvenient for the user experience. To address this, it is recommended to restrict the use of the payload signature to only important requests such as login, sign up, or other critical operations depending on the dApp requirements.

A common practice is to request the user's signature for the login process, and once authenticated, the dApp can issue a session token, such as a JSON Web Token (JWT), to manage the session. By implementing this approach, future non-critical requests can be performed using standard web 2.0 methods, eliminating the need to enter the spending password for each step. This significantly enhances the usability of the application, providing a smoother user experience.


### Version history

| Version | Date      | Author                         | Rationale              |
|:-------:|-----------|--------------------------------|------------------------|
| v1      |2022-12-27 | Juan Salvador Magán Valero     | Initial release        |


### Reference implementation

- [jmagan/passport-cardano-web3](https://github.com/jmagan/passport-cardano-web3)
- [jmagan/cardano-express-web3-skeleton](https://github.com/jmagan/cardano-express-web3-skeleton)
- [jmagan/cardano-nextjs-web3-skeleton](https://github.com/jmagan/cardano-nextjs-web3-skeleton)

## Path to Active

### Acceptance Criteria

- [X] At least one library should implement this authentication method.
- [ ] The 80% users should have wallets implementing the following requirements:
1. It MUST detect when the payload is formatted using this specification.
2. The information contained in the payload MUST be parsed and formatted in the signing pop-up.
3. The wallet SHOULD update the timestamp just before the payload is signed.
4. The wallet MUST detect if the URI is in the allow list.
5. The wallet SHOULD warn the user against cross-domain requests.
- [ ] A detailed documentation about web3 standards should be published. This documentation will include this standard and further best practices for web3 technologies.

### Implementation Plan

- [X] Create a library for processing payload according to this specification.
- [X] Open a conversation about this specification and its possible improvements.
- [X] Talk about further web3 standards and new specifications.
- [ ] Write the documentation for web3 developers.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0094/README.md
---

- --
CIP: 94
Title: On-chain SPO polls
Category: Tools
Status: Active
Authors:
- Matthias Benkort <matthias.benkort@cardanofoundation.org>
- Markus Gufler <markus.gufler-ext@cardanofoundation.org>
Implementors:
- Matthias Benkort <matthias.benkort@cardanofoundation.org>
- Ashish Prajapati <https://cardanoscan.io/contactUs>
- Dmytro Stashenko <https://preprod.adastat.net/about>
Discussions:
- https://github.com/cardano-foundation/cips/pull/496
- https://github.com/cardano-foundation/cips/pull/102
- https://github.com/input-output-hk/cardano-node/pull/5050
- https://github.com/input-output-hk/cardano-node/pull/5132
- https://forum.cardano.org/t/entering-voltaire-on-chain-poll-for-spos/117330
- https://forum.cardano.org/t/entering-voltaire-poll-experiment-live-on-mainnet/117879
Created: 2023-03-21
License: CC-BY-4.0
- --

## Abstract

The Cardano Foundation proposes a mechanism for polling Cardano stake pool operators on specific topics. Polls are done on-chain through transaction metadata and authenticated through stake pool credentials (Ed25519 cold key). The goal is to gather opinions on governance matters such as protocol parameter updates. This standard is an inclusive interim solution while the work on a larger governance framework such as [CIP-1694][] continues.

## Motivation: why is this CIP necessary?
<!-- A clear explanation that introduces the reason for a proposal, its use cases and stakeholders. If the CIP changes an established design, then it must outline design issues that motivate a rework. For complex proposals, authors must write a Cardano Problem Statement (CPS) as defined in CIP-9999 and link to it as the `Motivation`. -->

Governance is difficult. Discussions on CIP-1694 can attest to that quite clearly. There are constant debates within the Cardano community about changing protocol parameters, and the decision ultimately falls -- at this stage still -- onto the three genesis entities: Input Output, The Cardano Foundation and Emurgo. Yet, at this stage, few governance tools are at their disposal to make educated decisions. Besides Twitter polls, newsletter surveys, and SPO town halls on Discord, we have identified a gap and an opportunity to engage with the Cardano community through means currently at our disposal.

Conducting an on-chain poll between SPOs can also be seen as an experiment and an evaluation of the network's participation and engagement in the governance questions. Even though we only propose to poll one particular group of the Cardano community (the SPOs), such events can help to provide actual data to fuel the conversations around CIP-1694.

In summary, the goals are:

1. [x] to make some first experimental baby steps in the realm of governance;
1. [x] to be achievable _now_ (or an in immediate future);
1. [x] to capture participation data from SPOs;
1. [x] to raise awareness amongst SPOs regarding their future role in governance;
1. [x] to keep the Voltaire dynamics up in the ecosystem while other efforts are being pursued;
1. [x] to improve relations between the Cardano Foundation & SPOs for better mutual understanding and fruitful conversations.


## Specification

### Overview

Polls will be multiple-choice questions by The Cardano Foundation with pre-defined answers to choose from.

Here's an example of a question and answers:

- _Pineapples on pizza?_
- [ ] yes
- [ ] no

The serialised question and answers will be posted on-chain and signed by one of the delegate genesis keys owned by The Cardano Foundation. Answers will be provided on-chain by participating SPOs via transaction metadata referring to:

- The question and answers
- The index of the chosen answer from the available choices
- A digital signature (EdDSA) from the SPO's current cold key

> **Note**
> In this document, every time we refer to a _serialized object_, we refer to its **canonical** CBOR representation. In particular, keys in a map are always ordered alphabetically.

### Question structure

A question is posted in a transaction's metadata using the metadata label `94` and the following metadata structure:

```cbor
question =
  { 0: prompt
  , 1: [ * choice ]
  , ? "_": nonce
  }

prompt =
  [ * text .size (0..64) ]

choice =
  [ * text .size (0..64) ]

nonce =
  uint
```

A nonce is optionally included to provide non-replayability should the same question and answers be asked multiple times over different periods. The transaction carrying a question **must** be signed by one of the genesis delegate keys to be considered valid. This genesis key signature isn't captured in the metadata but in the transaction itself as an extra signatory.

For example:

<table>
<thead>
  <th>CBOR diagnostic</td>
  <th>Base16-encoded</th>
</thead>
<tbody>
 <tr>
  <td>
<pre>
{ 94:
  { 0: [ "Pineapples on pizza?" ]
  , 1:
    [ [ "yes" ]
    , [ "no" ]
    ]
  }
}
</pre>
  </td>
  <td>
   <a target="_blank" href="https://cbor.me/?bytes=A1185EA200817450696E656170706C6573206F6E2070697A7A613F0182816379657381626E6F">
<pre>
A1185EA200817450696E656170706C6573206F
6E2070697A7A613F0182816379657381626E6F
</pre>
   </a>
  </td>
 </tr>
</tbody>
</table>

### Answer structure

Similarly, an answer to a question is posted as transaction's metadata using the label `94` and the following metadata structure:

```cbor
answer =
  { 2: question_hash
  , 3: choice
  }

question_hash =
  bytes .size 32
```

Some remarks:

1. The field `2` (`question_hash`) is a blake2b-256 hash digest, whose preimage is the entire serialised question metadata payload (with the `94` top-level label).
1. The field `3` represents the 0-based index of the chosen answer from the available choices (from field `1` of the target poll).

For example:

<table>
<thead>
  <th>CBOR diagnostic</td>
  <th>Base16</th>
</thead>
<tbody>
 <tr>
  <td>
<pre>
{
  94: {
    2: h'29093fd43fc30ba31e306af06ce8537390e1668ae7496fe53d53684683c3762c',
    3: 0
  }
}
</pre>
  </td>
  <td>
   <a target="_blank" href="https://cbor.me/?bytes=A1185EA202582029093FD43FC30BA31E306AF06CE8537390E1668AE7496FE53D53684683C3762C0300">
<pre>
A1185EA202582029093FD43FC30BA31E306AF06CE
8537390E1668AE7496FE53D53684683C3762C0300
</pre>
   </a>
  </td>
 </tr>
</tbody>
</table>

The transaction carrying the answer metadata must then **be signed using a stake pool operator cold key**. Because cold key are not payment keys, it is required to specify an extra required signer on the transaction (transaction's field number 14 as per [Babbage's CDDL](https://github.com/input-output-hk/cardano-ledger/blob/cffa75fdbd800cda60997791e51bf02f2af0c42b/eras/babbage/test-suite/cddl-files/babbage.cddl#L66)) to prevent malicious nodes from potentially propagating transactions without the necessary key witnesses.

Alternatively, operators that are unable to sign arbitrary transactions due to hardware limitations can opt for stake pool update-registration certificate and attach the transaction metadata to it. Because an update-registration requires a signature from the cold key, the extra required signer field is redundant in that situation.

Regardless of the method, the signature shall be produced in an air-gapped environment only.

> **Warning**
>
> Only the first answer to a poll for each credential shall be considered. If multiple answers are found, only the first answer submitted (transaction & block ordering tallying) shall be considered.

### Adding context

It is possible to optionally attach extra context to the transaction as metadata following the procedure described in [CIP-0020](../CIP-0020/). Beside the structure specified in CIP-0020, such extra metadata is free-form and can be used to signal an intention behind a choice, or to voice a concern, or simply to give extra context. This is totally optional though we encourage SPOs to use this to inform their delegators of their choices.

### Procedure & Duration

A poll starts when a valid transaction with a question is posted on-chain. Answers can be submitted until the end of the following epoch, so there is always at least one whole epoch to answer the poll.

After one or more epochs in which the Stake Pool Operators have cast their answers, there follows a period of one or more epochs in which Cardano delegators may respond: If they disagree with the choice of their current stake pool, they can delegate to another pool. This changes the stake weight and thus influences the result. At the current state, the epochs for the answer and redelegation phase are only defined off-chain. In the future, they could also be defined as part of the signed question.

![process diagram](./CIP-0094_procedure-duration.png "Epoch poll phases example")

Indirectly, this results in the possibility of participation for all Ada holders.

### Outcome

The outcome of a poll will depend on its level of participation (in **terms of stake**). It is essential to understand that we explicitly call this a _poll_ / _survey_ and not a _vote_ to dispel any possible confusion. So it is akin to `1 Lovelace = 1 Voice` although we may chose to interpret data using different equations (e.g. giving more weight to pledged stake). How the data is interpret is deemed out of the scope of this proposal which aims mainly at producing the data-points. Further conversations and debates will be needed regarding interpretation of the data-points.


This proposal does not introduce a change in the current governance scheme: it is still up to the three genesis entities to make a final call based on the poll results. Poll results will provide new data points to feed into the conversation. But, regardless of the outcome, any decision will be explained and motivated by other auditable sources of information. And on-chain polls will provide such an auditable source.

## Rationale: how does this CIP achieve its goals?

### Recording question & answers

The proposed process will permanently record questions and their answers on-chain by leveraging existing transaction metadata. Note that we consciously do not record any element as datums. There are several reasons for this:

1. Datums offer extra programmability (for being available in Plutus script context); this is not needed at this stage.
1. Following a _keep-it-simple_ strategy, we propose relying on well-known and well-supported transaction features (a.k.a metadata) for producers and consumers.
1. Storing data in datums / UTxO has a non-negligible cost; naive datum storage would create thousands of new dummy UTxO on each poll. Transactions are cheaper to store and consume.
1. Polls rely on slot order when tallying answers, which means that chain sync is needed anyway, and there's no strong argument for having this information readily available in the UTxO graph.

### Cold key signing vs VRF proving

There have been several (on-and-off-the-record) discussions regarding using the cold key (Ed25519) vs the VRF key as authentication instruments; and arguments for both.

On the one hand, some prefer the use of the cold key because:

- The cold key is meant to authenticate stake-pools activity (e.g. certificate registrations/updates).
- It is ultimately the cold key that identifies a pool; its hash is the _pool id_.
- The VRF is more likely to be compromised, hence granting rights to participate in a poll to potential adversaries.
- Cold keys are Ed25519 keys, which allows piggybacking on the existing protocol's capabilities for transaction witnesses (extra required signer + verification key witnesses).

On the other hand, arguments for using the VRF key were already discussed as part of [CIP-0022][]:

- Because it's a hotkey, the VRF is usually more accessible, so it is more likely to lead to higher participation in surveys and no exposure of the cold key is needed.
- Blocks contain VRF proofs, which serve as explicit pool identifiers.
- It is only necessary to check that a key is correct at the moment of the poll, making VRF keys perfectly suitable.

We originally opted for a hybrid solution (as visible in input-output-hk#5050) but later decided to drop the VRF option to rely solely on cold key signing (see input-output-hk#5132). The reason for that regards the possible uncertainty of promoting (ab)use of VRF proving in the cardano-cli on such a short time period (see also [Insecurity of secret key re-usage](https://www.essentialcardano.io/article/insecurity-of-secret-key-re-usage)).

This has the unfortunate effect of making this participation procedure harder for SPOs relying on cold storage but we are open to the idea of proxy-keys authenticated off-chain through a challenge similar to [CIP-0022][].

#### KES Signing

There's a third on-chain element which we could use for identifying SPOs which is a digital signature from their KES credentials. It is however a bit more annoying to leverage mainly because KES are meant to expire and are only loosely tied to pools by operational certificate. Thus, verifying KES signatures on a survey requires a more complex setup and monitoring to keep track of operational certificates and their validity at the time of the survey.

If this CIP was meant to NOT be an interim solution, this is something we would likely consider. However, given the timeframe we're looking at and the overall trade-offs in complexity, we have opted out of using the KES as an authentication mechanism in this iteration.

#### Proxy keys

Another possible alternative to what's described in the CIP would be to have SPOs register a proxy Ed25519 key and use that proxy key onward. The validity of the proxy key registration would be conditionned to the production of an associated VRF proof or a digital signature from the cold key (very much like it's done for operational certificate).

Yet, like the KES alternative, this option is in conflict with some of the design goals of this CIP: simplicity. All the more so given that we want to maximise participation of SPOs to the various surveys. We aim to make the process of participating to the survey as simple as possible, without compromising on security.

> **Note** Both alternative options for KES Signing and Proxy Keys may be re-considered in a future version of the survey. Especially if the solution turns out to be not _as temporary as intended_. Fortunately, the current design decisions do not preclude this from happening as it shall be possible to introduce two new witness types `6` and `7` for those purpose. The KES registration can be handled through a separate on-chain event.

### Security

#### Replayability

Questions are meant to be unique, achieved using an optional nonce. It is up to the genesis entity conducting the poll to ensure the formulated question is unique. If the same question is asked several times, the nonce provides non-replayable protection.

Then, because every answer contains a (unique) hash of the question, answers are unique too. Yet, it still means that the same answer can be recast multiple times (possibly, by another system actor), so we do not allow answers to be changed/cast multiple times. The only exception is when answers are authenticated again using a cold key.

#### Credentials exposure

Exposure to SPOs' secret credentials must be limited, and their manipulation shall be done carefully. This potential attack vector is why we propose to extend the `cardano-cli` and have support for these features scrutinised by existing core maintainers and other open source actors.

Other tools are then free to replicate the approach taken in the cardano-cli, but we recommend that SPOs proceed with extreme caution when using third-party tools. In particular, any tool should be able to work fully offline to produce the required metadata. Final transaction construction and submission shall be made in any suitable environment, yet the metadata's production shall be done only in air-gapped systems.

## Path to Active

### Acceptance Criteria

- [x] The Cardano Foundation has conducted a first trial poll on mainnet ([CardanoScan](https://cardanoscan.io/spo-polls/96861fe7da8d45ba5db95071ed3889ed1412929f33610636c072a4b5ab550211) / [AdaStat](https://preprod.adastat.net/polls/62c6be72bdf0b5b16e37e4f55cf87e46bd1281ee358b25b8006358bf25e71798))
- [x] Visible agreement and engagement from a large set of SPOs
- [x] Multiple SPOs workshops
- [x] ~800 stake pools participating on the first mainnet poll
- [x] ~11B stake answered the first mainnet poll

### Implementation Plan

- [x] Provide a reference implementation for the signing method
- [x] [`cardano-cli`](https://github.com/input-output-hk/cardano-node/tree/master/cardano-cli#readme) [has been updated](https://github.com/input-output-hk/cardano-node/pull/5050) to provide support for constructing and signing relevant transactions.
- [x] Created [scripts to crawl the chain](https://github.com/cardano-foundation/CIP-0094-polls/tree/main/crawler#cip-0094-chain-crawler) for results.

- [ ] Possibly add support for KES signing as an alternative to EdDSA from the cold key and the VRF proving.

#### Tools Support

- [x] [`cncli`](https://github.com/cardano-community/cncli) has been updated with similar support
- [x] [`CardanoScan`](https://cardanoscan.io/spo-polls) now lists available and past polls directly on their web UI.
- [x] [`AdaStat`](https://preprod.adastat.net/polls) now lists available and past polls directly on their web UI.
- [ ] [`cardano-signer`](https://github.com/gitmachtl/cardano-signer) might be updated with similar support

#### Test runs

- [x] Announce a testnet run (on Preprod) and invite SPOs to a workshop session to conduct a testnet poll.
- See the [Preprod poll on AdaStat](https://preprod.adastat.net/polls/62c6be72bdf0b5b16e37e4f55cf87e46bd1281ee358b25b8006358bf25e71798).
- [ ] ~~Possibly do a second test run, but on mainnet this time.~~

## Copyright

This CIP is licensed under [CC-BY-4.0][].

[CIP-1694]: https://github.com/cardano-foundation/CIPs/pull/380
[CIP-0022]: https://github.com/cardano-foundation/CIPs/pull/102
[CC-BY-4.0]: https://creativecommons.org/licenses/by/4.0/legalcode
[Apache-2.0]: http://www.apache.org/licenses/LICENSE-2.0

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0095/README.md
---

- --
CIP: 95
Title: Web-Wallet Bridge - Conway ledger era
Category: Wallets
Status: Active
Authors:
- Ryan Williams <ryan.williams@intersectmbo.org>
Implementors:
- Eternl <https://eternl.io/>
- GeroWallet <https://gerowallet.io>
- Lace <https://www.lace.io/>
- Mesh <https://meshjs.dev/>
- mLabs <https://mlabs.city/>
- NuFi <https://nu.fi/>
- Ryan Williams <ryan.williams@intersectmbo.org>
- Typhon <https://typhonwallet.io/>
- Lido Nation <https://www.lidonation.com/>
- Vespr <https://vespr.xyz/>
- Yoroi <https://yoroi-wallet.com/>
Discussions:
- https://github.com/cardano-foundation/cips/pulls/509
- https://discord.com/channels/826816523368005654/1101547251903504474/1101548279277309983
- https://discord.com/channels/826816523368005654/1143258005354328156/1143272934966837309
Created: 2022-02-24
License: CC-BY-4.0
- --

## Abstract

This document describes an interface between webpage/web-based stacks and
Cardano wallets. This specification defines the API of the javascript object
that needs to be injected into web applications.

These definitions extend
[CIP-30 | Cardano dApp-Wallet Web Bridge](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0030/README.md)
to provide support for
[CIP-1694 | A First Step Towards On-Chain Decentralized Governance](https://github.com/cardano-foundation/CIPs/blob/master/CIP-1694/README.md)
focussed web-based stacks. Here we aim to support the requirements of Ada
holders and DReps in the Conway Ledger era, this specification is based on the
[Conway Ledger Era Specification](https://github.com/IntersectMBO/cardano-ledger/blob/dcacf044c8d38362edc57a761e027953aab3f335/eras/conway/impl/cddl-files/conway.cddl).

For the many contributors to this proposal, see [Acknowledgements](#acknowledgements).

## Motivation: why is this CIP necessary?

CIP-1694 introduces many new concepts, entities and actors to Cardano;
describing their implementation at the ledger level. This creates the need for
new tooling with respect to governance. For the average ecosystem participant,
the details should be abstracted away, enabling them to leverage the new ledger
features more effectively. This specification allows for creation of web-based
tools for the utilization of CIP-1694's governance features.

Whilst CIP-30 facilitated the launch of dApp development on Cardano, it's
functionality is limited in scope. It was written well before the emergence of
the Conway Ledger Era and thus lacks the required methods to support full user
interaction. We believe that expecting existing CIP-30 implementors to upgrade
implementations is unfeasible, thus we must extend it's functionality with this
API.

This proposal enables Ada holders, and DReps to engage web-based tooling through
wallets. Thus the primary stakeholders for this proposal are tool developers and
wallet providers. Here we aim to outline all endpoints needed to be exposed to
web based tools to support all the needs Ada holders and DReps to engage with
CIP-1694's governance design.

## Specification

We define the following section as an extension to the specification described
within CIP-30. Although currently CIP-30 acts as the defacto Cardano dApp-wallet
connector, this specification could be applied to similar standards.

> **Note** This specification will evolve as the proposed ledger governance
> model is finalized.

### Data Types

#### CIP-30 Inherited Data Types

From
[CIP-30's Data Types](https://github.com/cardano-foundation/CIPs/tree/master/CIP-003/README.md#data-types)
we inherit:

##### [Address](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0030/README.md#address)

A string representing an address in either bech32 format, or hex-encoded bytes.
All return types containing `Address` must return the hex-encoded bytes format,
but must accept either format for inputs.

##### [Bytes](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0030/README.md#bytes)

A hex-encoded string of the corresponding bytes.

##### [cbor\<T>](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0030#cbort)

A hex-encoded string representing [CBOR](https://tools.ietf.org/html/rfc7049)
corresponding to `T` defined via [CDDL](https://tools.ietf.org/html/rfc8610)
either inside of the
[Shelley Multi-asset binary spec](https://github.com/IntersectMBO/cardano-ledger-specs/blob/0738804155245062f05e2f355fadd1d16f04cd56/shelley-ma/shelley-ma-test/cddl-files/shelley-ma.cddl)
or, if not present there, from the
[CIP-0008 signing spec](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0008/CIP-0008.md).
This representation was chosen when possible as it is consistent across the
Cardano ecosystem and widely used by other tools, such as
[cardano-serialization-lib](https://github.com/Emurgo/cardano-serialization-lib),
which has support to encode every type in the binary spec as CBOR bytes.

##### [DataSignature](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0030/README.md#datasignature)

```ts
type DataSignature = {|
  signature: cbor<COSE_Sign1>,
  key: cbor<COSE_Key>,
|};
```

##### [Extension](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0030/README.md#extension)

An extension is an object with a single field `"cip"` that describe a CIP number
extending the API (as a plain integer, without padding). For example:

```ts
{ "cip": 30 }
```

#### CIP-95 Data Types

##### PubDRepKey

```ts
type PubDRepKey = string;
```

A hex-encoded string representing 32 byte Ed25519 DRep public key, as described
in [CIP-0105 | Conway Era Key Chains for HD Wallets](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0105/README.md).

#### DRepID

```ts
type DRepID = string;
```

A hex-encoded string representing a registered DRep's ID which is a Blake2b-224
hash digest of the above mentioned 32 byte Ed25519 public key, as described in
[CIP-1694 Registered DReps](https://github.com/cardano-foundation/CIPs/blob/430f64d3e86dd67903a6bf1e611c06e5343072f3/CIP-1694/README.md#registered-dreps).

##### PubStakeKey

```ts
type PubStakeKey = string;
```

A hex-encoded string representing 32 byte Ed25519 public key used as a staking
credential.

### Error Types

For the methods described in
[Governance Extension API](#governance-extension-api), we inherit APIError,
DataSignError and TxSignError from
[CIP-30's Error Types](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0030#error-types).

> **Note** We choose to reword some descriptions from CIP-30, to improve
> clarity.

#### [APIError](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0030#apierror)

We repurpose this error type from CIP-30, extending it's functionality. We
extend the `Refused` error code to also include the case of the extension no
longer being enabled.

```ts
APIErrorCode {
	InvalidRequest: -1,
	InternalError: -2,
	Refused: -3,
	AccountChange: -4,
}
type APIError {
	code: APIErrorCode,
	info: string
}
```

- `InvalidRequest` - Inputs do not conform to this specification or are
  otherwise invalid.
- `InternalError` - An internal wallet error occurred during execution of this
  API call.
- `Refused` - The request was refused due to lack of access - e.g. wallet
  disconnects or extension is no longer enabled.
- `AccountChange` - The account has changed. The client application should call
  `wallet.enable()` to reestablish connection to the new account. The wallet
  should not ask for confirmation as the user was the one who initiated the
  account change in the first place.

#### [TxSignError](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0030#txsignerror)

We repurpose this error type from CIP-30, extending it's functionality. We
extend the `ProofGeneration` error code to also include cases where DRep secret
key is not available. We also add one new error code `DeprecatedCertificate`.

```ts
TxSignErrorCode = {
  ProofGeneration: 1,
  UserDeclined: 2,
  DeprecatedCertificate: 3,
};
type TxSignError = {
  code: TxSignErrorCode;
  info: String;
};
```

- `ProofGeneration` - User has accepted the transaction sign, but the wallet was
  unable to sign the transaction. This is because the wallet does have some of
  the private keys required.
- `UserDeclined` - User declined to sign the transaction.
- `DeprecatedCertificate` - Returned regardless of user consent if the
  transaction contains a deprecated certificate.

#### [DataSignError](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0030#datasignerror)

We repurpose this error type from CIP-30, extending it's functionality. We
extend the `ProofGeneration` error code to also include cases where DRep secret
key is not available.

```ts
DataSignErrorCode {
	ProofGeneration: 1,
	AddressNotPK: 2,
	UserDeclined: 3,
}
type DataSignError = {
	code: DataSignErrorCode,
	info: String
}
```

- `ProofGeneration` - Wallet could not sign the data; because the wallet does
  not have the secret key to the associated with the address or DRep ID.
- `AddressNotPK` - Address was not a P2PK address and thus had no SK associated
  with it.
- `UserDeclined` - User declined to sign the data.

### Governance Extension API

These are the CIP-95 methods that should be returned as part of the API object,
namespaced by `cip95` without any leading zeros.

For example: `api.cip95.getPubDRepKey()`

To access these functionalities a client application must present this CIP-95
extension object, as part of the extensions object passed at enable time:

```ts
cardano.{wallet-name}.enable({ extensions: [{ cip : 95 }]})
```

#### `api.cip95.getPubDRepKey(): Promise<PubDRepKey>`

The connected wallet account provides the account's public DRep Key, derivation
as described in [CIP-0105](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0105/README.md).

These are used by the client to identify the user's on-chain CIP-1694
interactions, i.e. if a user has registered to be a DRep.

##### Returns

The wallet account's public DRep Key.

##### Errors

<!-- prettier-ignore-start -->
| Error Type   | Error Code       | Return Condition                                                                          |
| ------------ | ---------------- | ----------------------------------------------------------------------------------------- |
| `APIError`   | `InvalidRequest` | Returned if a input parameter is passed.                                                  |
| `APIError`   | `InternalError`  | Returned if there is a generic internal error occurred during execution of this API call. |
| `APIError`   | `Refused`        | Returned if there is a refusal, could be wallet disconnection or extension is revoked.    |
| `APIError`   | `AccountChange`  | Returned if wallet has changed account, meaning connection should be reestablished.       |
<!-- prettier-ignore-stop -->

#### `api.getRegisteredPubStakeKeys(): Promise<PubStakeKey[]>`

The connected wallet account's registered public stake keys. These keys may or may not control any Ada, but they must all have been registered via a stake key registration certificate. This includes keys which the wallet knows are in the process of being registered (already included in a pending stake key registration certificate).

If none of the wallets stake keys are registered then an empty array is returned.

These keys can then be used by the client to identify the user's on-chain CIP-1694
interactions, and also create vote delegation certificates which can be signed via `.signTx()`.

##### Returns

An array of the connected user's registered public stake keys.

##### Errors

<!-- prettier-ignore-start -->
| Error Type   | Error Code       | Return Condition                                                                          |
| ------------ | ---------------- | ----------------------------------------------------------------------------------------- |
| `APIError`   | `InvalidRequest` | Returned if a input parameter is passed.                                                  |
| `APIError`   | `InternalError`  | Returned if there is a generic internal error occurred during execution of this API call. |
| `APIError`   | `Refused`        | Returned if there is a refusal, could be wallet disconnection or extension is revoked.    |
| `APIError`   | `AccountChange`  | Returned if wallet has changed account, meaning connection should be reestablished.       |
<!-- prettier-ignore-stop -->

#### `api.cip95.getUnregisteredPubStakeKeys(): Promise<PubStakeKey[]>`

The connected wallet account's unregistered public stake keys. These keys may or may not control any Ada. This includes keys which the wallet knows are in the process of becoming unregistered (already included in a pending stake key unregistration certificate).

If the wallet does not know the registration status of it's stake keys then it should return them as part of this call. If all of the wallets stake keys are registered then an empty array is returned.

These keys can then be used by the client to identify the user's on-chain CIP-1694
interactions, i.e if a user has delegated to a DRep.

##### Returns

An array of the connected user's unregistered stake keys.

##### Errors

<!-- prettier-ignore-start -->
| Error Type   | Error Code       | Return Condition                                                                          |
| ------------ | ---------------- | ----------------------------------------------------------------------------------------- |
| `APIError`   | `InvalidRequest` | Returned if a input parameter is passed.                                                  |
| `APIError`   | `InternalError`  | Returned if there is a generic internal error occurred during execution of this API call. |
| `APIError`   | `Refused`        | Returned if there is a refusal, could be wallet disconnection or extension is revoked.    |
| `APIError`   | `AccountChange`  | Returned if wallet has changed account, meaning connection should be reestablished.       |
<!-- prettier-ignore-stop -->

#### `api.signTx(tx: cbor<transaction>, partialSign: bool = false): Promise<cbor<transaction_witness_set>>`

This endpoint requests the wallet to inspect and provide appropriate witnesses
for a supplied transaction. The wallet should articulate this request from
client application in a explicit and highly informative way.

Here we extend the capabilities of
[CIP-30's `.signTx()`](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0030/README.md#apisigntxtx-cbortransaction-partialsign-bool--false-promisecbortransaction_witness_set).
To allow signatures with `drep_credential` and recognition of Conway ledger
era transaction fields and certificates.

##### Expected Inspection Support

As read from
[cardano-ledger Conway _draft_ specification](https://github.com/IntersectMBO/cardano-ledger/blob/dcacf044c8d38362edc57a761e027953aab3f335/eras/conway/impl/cddl-files/conway.cddl).

Supporting wallets should be able to recognize and inspect
all the following certificates and data contained in transaction bodies, in any
combination.

| Index | Supported Pre-Conway Certificates |
| ----- | --------------------------------- |
|   0   | `stake_registration`              |
|   1   | `stake_deregistration`            |
|   2   | `stake_delegation`                |
|   3   | `pool_registration`               |
|   4   | `pool_retirement`                 |

| Index | Supported Conway Certificates   |
| ----- | ------------------------------- |
|   5   | `reg_cert`                      |
|   6   | `unreg_cert`                    |
|   7   | `vote_deleg_cert`               |
|   8   | `stake_vote_deleg_cert`         |
|   9   | `stake_reg_deleg_cert`          |
|  10   | `vote_reg_deleg_cert`           |
|  11   | `stake_vote_reg_deleg_cert`     |
|  12   | `auth_committee_hot_cert`       |
|  13   | `resign_committee_cold_cert`    |
|  14   | `reg_drep_cert`                 |
|  15   | `unreg_drep_cert`               |
|  16   | `update_drep_cert`              |

| Transaction Index | Supported Conway Transaction Field Data |
| ----------------- | --------------------------------------- |
|        19         | `voting_procedure`                      |
|        20         | `proposal_procedure`                    |
|        21         | `coin`         (current treasury value) |
|        22         | `positive_coin`          (new donation) |

All other potential transaction field inclusions
[0-18](https://github.com/IntersectMBO/cardano-ledger/blob/master/eras/conway/test-suite/cddl-files/conway.cddl#L54-#L69),
should be able to be recognized by supporting wallets.

##### Unsupported Inspection

In the Conway ledger era two certificate types are deprecated `genesis_key_delegation` and `move_instantaneous_rewards_cert`.
If the wallet receives a transaction containing a deprecated certificate it should return a `TxSignError` with an error code of `DeprecatedCertificate`.

| Index | Unsupported Pre-Conway Certificates |
| ----- | ----------------------------------- |
|   5   | `genesis_key_delegation`            |
|   6   | `move_instantaneous_rewards_cert`   |

##### Expected Witness Support

Although constitutional committee certificates and stake pool certificates should be able to be recognized they should not be able to be correctly witnessed by wallets following this API.
Wallet's should only support witnesses using payment, stake and DRep keys.

##### Returns

The portions of the witness set that were signed as a result of this call are
returned. This encourages client apps to verify the contents returned by this
endpoint before building the final transaction.

##### Errors

<!-- prettier-ignore-start -->
| Error Type    | Error Code               | `partialSign`     | Return Condition                                                                                                                  |
| ------------- | ------------------------ | ----------------- | --------------------------------------------------------------------------------------------------------------------------------- |
| `APIError`    | `InvalidRequest`         | `true` or `false` | Returned if an erroneous parameter is passed, wrong type or too many etc.                                                         |
| `APIError`    | `InternalError`          | `true` or `false` | Returned if there is a generic internal error occurred during execution of this API call.                                         |
| `APIError`    | `Refused`                | `true` or `false` | Returned if there is a refusal, could be wallet disconnection or the extension is revoked.                                        |
| `APIError`    | `AccountChange`          | `true` or `false` | Returned if wallet has changed account, meaning connection should be reestablished.                                               |
| `TxSignError` | `ProofGeneration`        | `false`           | Returned if user has accepted transaction to sign, but the wallet is unable to sign because it does not have the required key(s). |
| `TxSignError` | `UserDeclined`           | `true` or `false` | Returned if user has declined to sign the transaction.                                                                            |
| `TxSignError` | `DeprecatedCertificate` | `true` or `false` | Returned regardless of user consent if the transaction contains a deprecated certificate.                                        |
<!-- prettier-ignore-stop -->

If `partialSign` is `true`, the wallet only tries to sign what it can. If
`partialSign` is `false` and the wallet could not sign the entire transaction,
`TxSignError` shall be returned with the `ProofGeneration` code.

#### `api.cip95.signData(addr: Address | DRepID, payload: Bytes): Promise<DataSignature>`

Errors: `APIError`, `DataSignError`

This endpoint requests the wallet to inspect and provide a DataSignature for the supplied data. The wallet should articulate this request from client application
in a explicit and highly informative way.

Here we extend the capabilities of
[CIP-30's `.signData()`](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0030/README.md#apisigndataaddr-address-payload-bytes-promisedatasignaturet).
To allow for signatures using DRep keys.

This endpoint utilizes the
[CIP-0008 | Message Signing](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0008/README.md)
for standardization/safety reasons. It allows the client app to request the user to
sign a payload conforming to said spec.

##### Supported Credentials

Here we define how each key is identified by an `Address` in relation to
[CIP-0019 | Cardano Addresses](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0019/README.md),
these are all Shelley key-hash-based addresses.

We allow for `DRepID` to be passed in the `addr` field to signal signature using the associated DRep key.

To construct an address for DRep Key, the client application should construct a
[type 6 address](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0019/CIP-0019-cardano-addresses.abnf#L7C8-L7C93).
Using an appropriate
[Network Tag](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0019/CIP-0019-cardano-addresses.abnf#L13)
and a hash of a public DRep Key.

<!-- prettier-ignore-start -->
| Key         | Identifying `addr` |
| ----------- | ------------------ |
| Payment Key | Address types: [0](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0019/CIP-0019-cardano-addresses.abnf#L1), [2](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0019/CIP-0019-cardano-addresses.abnf#L3), [4](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0019/CIP-0019-cardano-addresses.abnf#L5), [6](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0019/CIP-0019-cardano-addresses.abnf#L7C27-L7C72). |
| Stake Key   | Address type: [14](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0019/CIP-0019-cardano-addresses.abnf#L10). |
| DRep Key | [`DRepID`](#drepid) |
<!-- prettier-ignore-end -->

These keys will be used to sign the `COSE_Sign1`'s `Sig_structure` with the
following headers set:

- `alg` (1) - must be set to `EdDSA` (-8)
- `kid` (4) - Optional, if present must be set to the same value as in the
  `COSE_key` specified below. It is recommended to be set to the same value as
  in the `"address"` header.
- `"address"` - must be set to the raw binary bytes of the address as per the
  binary spec, without the CBOR binary wrapper tag

The payload is not hashed and no `external_aad` is used.

##### Returns

The return shall be a `DataSignature` with `signature` set to the hex-encoded
CBOR bytes of the `COSE_Sign1` object specified above and `key` shall be the
hex-encoded CBOR bytes of a `COSE_Key` structure with the following headers set:

- `kty` (1) - must be set to `OKP` (1).
- `kid` (2) - Optional, if present must be set to the same value as in the
  `COSE_Sign1` specified above.
- `alg` (3) - must be set to `EdDSA` (-8).
- `crv` (-1) - must be set to `Ed25519` (6).
- `x` (-2) - must be set to the public key bytes of the key used to sign the
  `Sig_structure`.

##### Errors

<!-- prettier-ignore-start -->

| Error Type      | Error Code        | Return Condition                                                                                                               |
| --------------- | ----------------- | ------------------------------------------------------------------------------------------------------------------------------ |
| `APIError`      | `InvalidRequest`  | Returned if a input parameter is passed.                                                                                       |
| `APIError`      | `InternalError`   | Returned if there is a generic internal error occurred during execution of this API call.                                      |
| `APIError`      | `Refused`         | Returned if there is a refusal, could be wallet disconnection or extension is revoked.                                         |
| `APIError`      | `AccountChange`   | Returned if wallet has changed account, meaning connection should be reestablished.                                            |
| `DataSignError` | `ProofGeneration` | Returned if user has accepted to sign, but wallet could not sign the data; because the wallet does not have the required keys. |
| `DataSignError` | `AddressNotPK`    | Returned if Address was not a P2PK address and thus had no SK associated with it.                                              |
| `DataSignError` | `UserDeclined`    | Returned if the user declined to sign the data.                                                                                |

<!-- prettier-ignore-stop -->

### Versioning

Whilst this CIP is in it's unmerged proposed state, it remains very fluid and
substantial changes can happen, so we would advise against any implementation.
Once more feedback is received, maturing this design, implementations can safely
emerge, alongside this proposal's merger into the CIPs repository. Once merged
only small necessary changes should be made, ideally in backwards compatible
fashion.

This, in tandem with, maturing implementations should move this proposal to an
active state where only small backwards compatible changes can be made. If any
large changes are needed once active then a new proposal should be made to
replace this one. This we believe aligns with the (new) extendibility design of
CIP-0030.

### Examples of Flows

#### Connection and Login

This describes a potential flow of connection between CIP-95 compatible client
application and wallet, then a subsequent _login_.

1. **Connection:** User indicates to the client their intent to connect, causing
   client offer a list of supported wallets, user selects their desired wallet.
   The client will then invoke
   `.{wallet-name}.enable({extensions: [{ "cip": 95 }]})` from the shared
   `cardano` namespace, ensuring to pass in the CIP-95 extension object.
2. **Wallet Confirmation:** The wallet indicates through its UI the clients
   intent to connect, the user should then grant permission.
3. **Share Credentials:** The client invokes both `.getRegisteredPubStakeKeys()`
   and `.getPubDRepKey()`, causing the connected wallet to share relevant
   credentials.
4. **Chain Lookup:** The client uses a chain indexer to work out the governance
   state of the provided credentials. The results of the lookup are then shown
   to the user, acting as a `login`.

#### Vote Delegation

Assume a _DRep Aggregator and Delegation_ specialized client app, that
aggregates DRep metadata from DRep registration certificates and renders this
metadata to show prospective delegators. Assume that connection to a users
wallet has already been made via
`cardano.{wallet-name}.enable({extensions: [{ cip: 95 }]})`.

1. **Choose DRep:** User browses DReps and selects one which align's with their
   values to delegate too. It is up to the client application to choose and
   manage which stake key should be used for this delegation, this could be with
   or without user input.
2. **Construct Delegation:** The client application uses CIP-30 endpoints to
   query the wallet's UTxO set and payment address. A DRep delegation
   certificate
   ([`vote_deleg_cert`](https://github.com/IntersectMBO/cardano-ledger/blob/1beddd3d9f10d8fcb163b5e83985c4bac6b74be7/eras/conway/test-suite/cddl-files/conway.cddl#L303))
   is constructed by the app using the chosen DRep's ID and wallet's stake
   credential. A transaction is constructed to send 1 ADA to the wallet's
   payment address with the certificate included in the transaction body.
3. **Inspect and Sign:** The app passes the transaction to the wallet via
   `.signTx()`. The wallet inspects the content of the transaction, informing
   the user of the client app's intension. If the user confirms that they are
   willing to sign, the wallet returns the appropriate witnesses, of payment key
   and stake key.
4. **Submit:** The app will add the provided witnesses into the transaction body
   and then pass the witnessed transaction back to the wallet for submission via
   `.submitTx()`.
5. **Feedback to user:** The wallet returns the submitted transaction's hash,
   the app can use this to track the status of the transaction on-chain and
   provide feedback to the user.

#### DRep Registration

Assume a _DRep Registration_ specialized client app, that allows people to
register as a DRep. Assume that connection to a users wallet has already been
made via `cardano.{wallet-name}.enable({extensions: [{ cip: 95 }]})` and that
the user is not a registered DRep.

1. **User Indicates Intent:** User indicates to the client that they wish to
   register as a DRep. The client asks the user to provide metadata anchor, this
   is bundled with DRepID the client generates from the wallet's public DRep Key
   provided via `.getPubDRepKey()`.
2. **Construct Registration**: The client application uses CIP-30 endpoints to
   query the wallet's UTxO set and payment address. A DRep registration
   certificate
   ([`reg_drep_cert`](https://github.com/IntersectMBO/cardano-ledger/blob/1beddd3d9f10d8fcb163b5e83985c4bac6b74be7/eras/conway/test-suite/cddl-files/conway.cddl#L312))
   is constructed by the app using the wallet's DRep ID and the provided
   metadata anchor. A transaction is constructed to send 1 ADA to the wallet's
   payment address with the certificate included in the transaction body.
3. **Inspect and sign:** The app passes the transaction to the wallet via
   `.signTx()`. The wallet inspects the content of the transaction, informing
   the user of the client app's intension. If the user confirms that they are
   happy to sign, the wallet returns the appropriate witnesses, of payment key
   and DRep key (`drep_credential`).
4. **Submit:** The app will add the provided witnesses into the transaction body
   and then pass the witnessed transaction back to the wallet for submission via
   `.submitTx()`.
5. **Feedback to user:** The wallet returns the submitted transaction's hash,
   the app can use this to track the status of the transaction on-chain and
   provide feedback to the user.

## Rationale: how does this CIP achieve its goals?

The principle aim for this design is to reduce the complexity for wallet
implementors whilst maintaining backwards compatibility with CIP-30
implementations. This is motivated by the necessity for users to be able to
interact with the age of Voltaire promptly, by keeping the wallet's providers
ask small we aim to reduce implementation time.

This design aims to make the tracking of a user's governance state an optional
endeavour for wallet providers. This is achieved by placing the responsibility
on clients to track a user's governance state, i.e. if a wallet user is a DRep,
what DRep a wallet user has delegated to, etc.

Despite only defining the minimal set of endpoints required, we do not wish to
discourage the creation of subsequent CIPs with a wider range of governance
functionality. Nor does this specification aim to discourage wallet providers
from fully integrating governance features, side-stepping the necessity for this
API (matching how staking is achieved).

### Why Web-based Stacks?

Web-based stacks, with wallet connectivity, are a familiar place for users to be
able to interact with Cardano. These tools lower the technical bar to engage
with the ecosystem. Thus we believe encouraging further adoption of this
approach is beneficial.

The primary alternative approach is for wallet providers to integrate this
functionality fully inside of wallet software, matching how staking is often
implemented. We deem this approach as preferable from a security standpoint, we
would encourage wallet providers to pursue this. But we understand that this
adds significant overhead to wallet designs, so we offer this API as an
alternative.

### Why DReps and Ada Holders?

This proposal only caters to two types of governance actor described in
CIP-1694; Ada holders and DReps, this decision was three fold. Primarily, this
is to allow these groups to utilize a web-based client to participate in
Cardano's governance. These groups are likely less comfortable utilizing
command-line interfaces than other groups, thus making alternatives from them is
a priority. Secondly, the other types of actor (constitutional committee members
and SPOs) are identified by different credentials than Ada holders and DReps,
making their integration in this specification more complex. These alternative
credentials are unlikely to be stored within standard wallet software which may
interface with this API. Thirdly, Ada holders and DReps likely represent the
majority of participants thus we aim to cast a wide net with this specification.

#### Unsupported Items

In this specification we have placed explicit boundaries on what should not be
supported with `.signTx()`. Those being not witnessing
[stake pool](https://github.com/IntersectMBO/cardano-ledger/blob/1beddd3d9f10d8fcb163b5e83985c4bac6b74be7/eras/conway/test-suite/cddl-files/conway.cddl#L294C1-L295C43)
or
[constitutional committee](https://github.com/IntersectMBO/cardano-ledger/blob/1beddd3d9f10d8fcb163b5e83985c4bac6b74be7/eras/conway/test-suite/cddl-files/conway.cddl#L310C1-L311C61),
certificates and not inspecting genesis key delegation or MIR certificates.

From speaking to CIP-30 implementors it seems reasonable that there does not
exist implementations or motivation to support witnessing stake pool
certificates via wallet web bridges. This is because stake pool operators much
prefer the utility and security advantages not operating via light wallets. Due
to the [Lack of Specificity](#lack-of-specificity) of CIP-30 we felt it
necessary to explicitly state the lack of support in this extension.

[Constitutional committee certificates](https://github.com/IntersectMBO/cardano-ledger/blob/1beddd3d9f10d8fcb163b5e83985c4bac6b74be7/eras/conway/test-suite/cddl-files/conway.cddl#L310C1-L311C61)
are not supported by this specification's `.signTx()` for two reasons. First,
this specification is only focussed on the need's of Ada holders and DReps.
Secondly, the credentials used by the constitutional committee, are a hot and
cold key setup. Hot and cold keys are not suited for standard light wallets.

Genesis key delegation and move instantaneous reward certificates (see in
[Shelley spec](https://github.com/IntersectMBO/cardano-ledger/blob/0738804155245062f05e2f355fadd1d16f04cd56/shelley-ma/shelley-ma-test/cddl-files/shelley-ma.cddl#L117#L118))
are not supported here because they have been deprecated in the Conway ledger
era. Furthermore, due to the lack of accessibility (require access to genesis
keys) for these certificates it is extremely unlikely any CIP-30 implementations
supported these.

### The Role of the Wallet

The endpoints specified here aim to maintain the role of the wallet as: sharing
public keys, transaction inspecting, transaction signing and transaction
submission.

#### Transaction Inspection

In a previous design we had stipulated the precise information that must be
shown to user by wallets at signature time. This was discussed during the
wallets and tooling hackathon and consensus was reached that is not the place of
these APIs to prescribe such details to wallets. Rather this specification
should be describing the interface between web-based stacks and wallets and not
telling wallets what UI elements should be used. It is in a wallet's best
interest to always adequately inform the user, with varying levels of detail
based on the wallet's discretion.

#### Transaction Construction

By not placing the burden of transaction construction onto the wallet, we move
the application specific complexity from wallet implementations and onto
applications. This has a number of benefits, primarily this should lower the bar
for wallet adoption. But this also helps in the creation of iterative updates,
all wallet implementers do not need to update if the format of these
transactions is adjusted during development.

Here we also benefit from imitating the existing flows which have been utilized
by CIP-30 compliant systems. Reusing existing flows is beneficial for developer
adoption as it enables straight forward code reuse.

One argument against this design is that, if wallets are required to be able to
inspect and thus understand these application specific transactions then they
may as well build the transaction. Ultimately, we have taken the design decision
to leave transaction construction to the applications.

### CIP-30 Reuse

Whilst CIP-30 facilitated the launch of dApp client development on Cardano, it's
functionality is limited in scope. Although it does offer generic functions,
these cannot satisfy the problem that this proposal tackles in a backwards
compatible manner. Thus extending it's functionality is a necessity.

#### Lack of Specificity

The CIP-30 specification has required amendments to add clarification to it's
ambiguity. There is further ambiguity around what is and is not supported via
`.signTx()`. The specification does not explicitly list the transaction
artifacts wallets has to be able to inspect and witness. Whilst for most use
cases this is likely fine and has served the community well. We forsee issues
around large ledger upgrades which introduce new types of transaction fields and
certificate. Without explicit mention of what is and is not supported deltas
between expected and actual functionality become common and hazardous. This is
why we choose to explicitly list those items that wallet have to support when
complying with this API.

#### Extension Design

With this specification we chose to extend CIP-30's functionalities. There would
be two competing designs to this approach. One; move to have this specification
included within CIP-30. Two; deploy this specification as it's own standalone
web-bridge.

It would be undesirable to include this functionality within the base CIP-30 API
because it would force all wallets supporting CIP-30 to support this API. This
is undesirable because not all client apps or wallets will have the need or
desire to support this specification.

The reason we chose to not deploy this specification on its own is because it is
unlikely that clients implementing this API will not want to also use the
functionality offered by CIP-30. Additionally, CIP-30 offers a extensibility
mechanism meaning that the initial handshake connection is defined and thus wont
be needed to be defined within this specification.

Furthermore, another benefit of utilizing the CIP-30 extensibility mechanism is
the potential for siloing of wallet capabilities between client apps. By having
to request access to each extension wallets and users are able to silo which
extensions they allow to each client application. An example of this could be
only allowing the CIP-95 API with governance related applications and not
decentralized extensions.

#### Backwards Compatibility

The primary issue with just using CIP-30 to inspect, sign and submit Conway
transactions/certificates is that wallet implementations are likely
incompatible. This is because such certificates/transactions were not part of
the ledger design at time of original CIP-30 implementation. Furthermore, CIP-30
was written and implemented before voting credentials were defined and thus it
would be impossible to provide signatures with this credential to votes, DRep
registrations and DRep retirements.

Although it is likely that some of the capabilities of this API can be achieved
by existing CIP-30 implementations it is not certain how much. We would like to
avoid the potential mismatching of capabilities between CIP-30 implementations,
as this creates unpredictable wallet behavior for client applications. Such
behavior was a primary motivator to introduce such an extendability mechanism to
CIP-30.

#### Namespacing

In this specification we have chosen to explicitly namespace all endpoint except
`.signTx()` where we omit the namespacing. By not namespacing `.signTx()` we
intend to offer client apps an override of the CIP-30 `.signTx()`. We chose to
do this because this `.signTx()` extends the CIP-30 functionality in a backwards
compatible way. All other endpoints are namespaced to avoid possible collisions
with other future extensions.

#### Inheriting Endpoints

In this design we chose to extend the capabilities of CIP-30's `.signTx()` and
`.signData()` rather than introducing new endpoints for signing and submission.
This was a result of community discussion at the wallets and tooling hackathon,
leading to a more straight forward design. Originally we had individual
endpoints for sign and submitting of DRep registration, DRep retirement, votes,
governance actions and vote delegations.

Whilst individual endpoints seem like a simpler solution they would likely
introduce more complexities for wallet implementors. As constraining what
transactions an endpoint would require additional validation complexities and
error handling from wallets. For example; what should a wallet do if it is
passed a DRep registration certificate via `.signTx()`? should it witness or
reject and only witness with a dedicated DRep registration endpoint?

Furthermore individual restrictive endpoints limit how much can be done in a
single transaction. These methods would not allow multiple certificates to be
supplied at once. Thus client apps would be limited to a single governance
artifact per transaction. This is limiting as it means users have to submit
multiple transactions to achieve what is possible in one.

#### CIP-95 as a Conway Flag

By providing _updated_ CIP-30 endpoints we essentially use the CIP-95 extension
object as a flag to signal to apps at connection time a wallet's compatibility
with Conway leger era. This goes against how past ledger feature upgrades have
rolled out. Rather in the past, existing CIP-30 implementors have just updated
their implementations, we believe this to be an error prone approach. This
undoubted introduces deltas between what a client application expects a wallet
to be able to do and what it can do. There is no way for a client application to
know what a wallet is capable of.

Despite this we do not discourage CIP-30 implementors from updating their
implementations to support Conway artifacts to `.signData()` and `.signTx()`.
But if so they must support the CIP-95 object flag at connection time, so that
clients are aware of this functionality.

### Multi-stake Key Support

Although
[multi-stake key wallets](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0018)
are not widely adopted across Cardano, we make an effort to support them here.
This is because a single stake key can delegate to a single DRep. By allowing
users to spread stake across multiple stake keys it allows different weighted
delegation to different DReps, this could be a very desirable feature.

### Types of credential

This specification does not cater for wallets who manage non-key-based stake
credentials and those who wish to handle non-key-based DRep credentials. This
does limit the usefulness of this specification. But the complexities that would
be introduced by generalization this specification to these credentials is
unlikely to yield much benefit since these types of wallet are not prevalent in
Cardano.

Although this means that we are likely excluding tooling for DAOs from being
supported through this standard. The argument could be made that such entities
generally prefer to use more advanced wallet tooling rather than relying on
interaction with web-based stacks, thus it is not even certain DAOs would want
to use such a standard.

#### Encoding

Unlike the CIP-30 specification we have made an decision, where possible to
represent keys as raw hex rather than encoded representations. We believe that
it is not the role of the wallet to encode such credentials, encoding needs
should be at the application's discretion. Introducing different encodings into
this API would add unneeded complexity.

Furthermore, in this API we chose to return public keys over key-hashes or
addresses. Again we believe wallets should just serve public key information and
it is up to the application to encode and derive addresses as needed. This
simplifies the overall design and makes implementations easier for wallets.

### Splitting of Stake Key endpoint

For stake keys we have chosen to implement two endpoints where wallets can share
registered and unregistered stake keys. Originally we had a single endpoint
which only allowed sharing of registered stake keys. This was problematic for
wallets which had no registered stake keys, and thus the second endpoint was
introduced.

We chose to keep a single endpoint for DRep keys, although it would have been
possible to introduce a second to allow for wallets to activity of their DRep
keys. This was just for the simplicity of the API. Furthermore, due to the
design of this proposal it is unlikely that wallets will implement methods to
track a user's DRep state.

### Backwards Compatibility

This proposal should not effect the backwards compatibility of either clients or
wallet implementors.

#### CIP-62?

[CIP-62? | Cardano dApp-Wallet Web Bridge Catalyst Extension](https://github.com/cardano-foundation/CIPs/pull/296)
is another extension to the CIP-30 API, this proposal is independent of CIP-95.
The CIP-95 specification does not rely on any of the implementation defined in
CIP-62?. We have attempted to avoid any collisions of naming between these
proposals, this was motivated by a desire to make wallet implementations more
straight forward for wallets implementing both APIs.

### Open Questions

- <s>The burden of transaction building to be placed on dApps or wallets?</s>
- As we are replacing CIP-30's signTx it makes sense to follow the same flow
    and place the burden on the client applications.
- <s>Does supporting governance action submission a necessary burden for the
  scope of this proposal?</s>
- Since moving burden of transaction construction from wallet to app, this
    becomes much less of an issue as the complex error checking should now be
    done by the application.
- <s>should provide support for combination certificates?</s>
- Yes we will support ALL conway ledger era Tx/Certs, this will allow for
    CIP95 to be "the Conway compatible" wallet web bridge.
- <s>Is it necessary to provide a method to prove ownership of DRep key?</s>
- Yes, this will be a useful add.
- <s>Is it sensible to place multi-stake key burden onto clients?</s>
- Yes, seems like a reasonable approach. If wallets want to manage it, they
    can only provide the keys they wish.
- <s>Do we need to share stake keys or can we just reuse reward addresses?</s>
- Reusing CIP30's `.getRewardAddresses()` may act as an alternative, but it is
    unclear how implementors have supported this function and thus its reuse
    maybe a mistake.
- It is a more reasonable approach to share public key material instead of
    addresses as it gives the client application more freedom.
- <s>Should this proposal cater for non-key-based stake credential?</s>
- We can leave this for a future iteration.
- <s>Move DRep key definitions be moved into another CIP?</s>
- Yes, this is a cleaner approach, as we keep the purity of this proposal to
    being a wallet web bridge.
- <s>Should there be a way for the optional sharing of governance state, from
  wallet to client?</s>
- We leave this for future CIPs.
- <s>Should DRep key be moved into CIP-1852?</s>
- Yes it will be moved to it's own CIP with reference added to
    CIP-1852.

## Path to Active

### Acceptance Criteria

- [x] The interface is supported by three wallet providers.
- [Nufi](https://assets.nu.fi/extension/sanchonet/nufi-cwe-sanchonet-latest.zip)
- [Lace](https://github.com/input-output-hk/lace)
- [Yoroi](https://github.com/Emurgo/yoroi)
- [demos wallet](https://github.com/Ryun1/cip95-demos-wallet)
- [x] The interface is used by one web application to allow users to engage with
      the Conway ledger design.
- [SanchoNet GovTool](https://sanchogov.tools)
- [GovTool](https://gov.tools)
- [cip95-cardano-wallet-connector](https://github.com/Ryun1/cip95-cardano-wallet-connector/tree/master)
- [drep-campaign-platform](https://github.com/IntersectMBO/drep-campaign-platform)
- [x] The interface is supported via libraries.
- [Cardano JS-SDK](https://github.com/input-output-hk/cardano-js-sdk)
- [purescript-cip95](https://github.com/mlabs-haskell/purescript-cip95)
- [Mesh SDK](https://github.com/MeshJS/mesh)

### Implementation Plan

- [x] Provide a public Discord channel for open discussion of this
      specification.
- See
    [`wallets-sanchonet`](https://discord.com/channels/826816523368005654/1143258005354328156/1143272934966837309)
    channel in the [IOG Technical Discord](https://discord.gg/inputoutput) under
    (to view you have to opt-in to the Sanchonet group in the start-here
    channel).
- [x] Author to engage with wallet providers for feedback.
- [x] Author to run a hackathon workshop with wallet providers.
- In person and online hackathon run 2023.07.13, outcomes presented here:
    [CIP-95 pull request comment](https://github.com/cardano-foundation/CIPs/pull/509#issuecomment-1636103821).
- [x] Resolve all [Open Questions](#open-questions).
- [x] Author to provide test dApp to test against.
- See
    [cip95-cardano-wallet-connector](https://github.com/Ryun1/cip95-cardano-wallet-connector/tree/master).
- [x] Author to provide a reference wallet implementation.
- See [cip95-demos-wallet](https://github.com/Ryun1/cip95-demos-wallet/).
- [ ] Author to produce a set of test vectors for wallets to test against.
- [x] Author to move DRep key definitions to a separate CIP.
- via the addition of [CIP-105 | Conway era Key Chains for HD Wallets](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0105/README.md) via [CIPs PR #597](https://github.com/cardano-foundation/CIPs/pull/597).

## Acknowledgments

<details>
  <summary><strong>Wallets and Tooling Hackathon</strong></summary>

  On 2023.07.13 a online and in person community hackathon took place, aims of
  this event included maturation of the design of this specification.

  We would like to thank the following attendees for providing their valuable
  insights:

- Piotr Czeglik - Lace
- Mircea Hasegan - Lace
- Alex Apeldoorn - Lace
- Michal Szorad - Yoroi
- Javier Bueno - Yoroi
- Ed Eykholt - Blocktrust
- Vladimir Volek - Five Binaries
- Marek Mahut - Five Binaries
- Markus Gufler - Cardano Foundation
- Michal Ciborowski - BinarApps

</details>

## Copyright

This CIP is licensed under
[CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0099/README.md
---

- --
CIP: 99
Title: Proof of Onboarding
Status: Active
Category: Wallets
Authors:
- Adam Dean <adam@crypto2099.io>
- Carl <vegas@hosky.io>
- Alex Dochioiu <alex@dochioiu.com>
Implementors:
- VESPR Wallet <https://www.vespr.xyz/>
- Yoroi Wallet <https://yoroi-wallet.com/>
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/546
Created: 2023-06-20
License: CC-BY-4.0
- --

## Abstract

Since at least 2021 when Cardano entered the Mary Era and implemented Native Assets, projects and creators building on
the network have sought a means to distribute their tokens efficiently to users. Of particular frustration has been the
ability to onboard new users at real-world events. Solutions for this historically have been to create and preload
"paper wallets" where a seed phrase and accompanying password is generated by the project, pre-populated with tokens and
ADA, and then delivered over to attendees of said event.

The creation of this addendum to the [CIP-13 Cardano URI scheme](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0013/README.md) seeks to minimize this friction and take advantage of
existing technology to enable a new era of user onboarding, particularly at real world events through the use of a
defined URI scheme enabling instant and frictionless communication between a project's "token fountain" and the user's
wallet to reward, incentivize, and onboard new users easier than ever.

This CIP defines an extension to the CIP-13 URI Scheme as well as an API specification to facilitate and streamline
communications between wallets and project servers.

## Motivation: Why is this CIP necessary?

By leveraging the power of Cardano-specific URIs (CIP-13) and the modern technological advances of mobile devices and
wallets we can provide a framework for Cardano projects to attend real world events, incentivize or reward attendees via
their Native Assets, and have facts and figures to help support and analyze the impact that their attendance had (Proof
of Onboarding).

## Specification

### CIP-13 Cardano URI Extensions

Distributing Native Assets (and/or ADA) to attendees of IRL events has historically been a pain point in the ecosystem.
Some implemented solutions have included: Pre-generating wallet seed phrases and pre-populating these wallets with a
minimum amount of ADA as well as the desired Native Assets, (re)creating token fountain/faucet designs which can be
cumbersome and not user-friendly to instruct individuals to install a wallet, visit a website, enter a code and claim
tokens.

The Cardano Token Claim URI schema is proposed to allow wallets (particularly mobile wallets) to implement a QR-friendly
URI structure allowing for easy onboarding and distribution of Native Assets and/or ADA to individuals in a variety of
situations but not least of which being at IRL events specifically tailored and geared to onboarding new users to the
ecosystem.

Examples:
```html
<!-- Token Claim URIs -->
<a href="web+cardano://claim/v1?faucet_url=https%3A%2F%2Fclaim.hosky.io&code=consensus2023">Claim $HOSKY</a>
<a href="web+cardano://claim/v1?faucet_url=https%3A%2F%2Fclaim.hosky.io%2Fconsensus23&code=ABC123">Claim $HOSKY</a>
<a href="web+cardano://claim/v1?faucet_url=https%3A%2F%2Fclaim.nftxlv.com&code=ABC123&invoice=123456">Claim NFTxLV Commermorative NFT!</a>
```

#### ABNF Grammar

- For a token claim URI (authority = `claim`), a versioning path, a `faucet_url` and a required `code`.
- Additional query parameters may be provided and should be passed through to the provided `faucet_url` without modification.

```
cardanourn = "web+cardano:" claimtokenref

claimtokenref = "//claim" claimversion claimquery
claimversion = "/v1"
claimquery = ( "?" claimurl) ( "&" claimcode)
claimurl = "faucet_url=" text
claimcode = "code=" text
```

#### Token Claim URI Queries

- *All arguments for Token Claim URIs should be URL-encoded**

- **Version 1 URIs***

Version 1 URIs must include a `faucet_url` and a `code` as required parameters.

URIs may include additional arguments to suit the needs of the project's faucet API.

#### Handling Token Claim URI Queries

The token claim URI should consist of a required versioning `path` (i.e. `/v1`) as well as one or more required
or optional URL-encoded arguments.

All Token Claim URIs must include a URL-encoded `faucet_url` argument as well as a `code` argument.

The wallet provider should send a POST request to the provided `Faucet URL` that includes:
- The change/receipt wallet address of the user
- Any additional arguments specified in the URI as key: value pairs

- *Example:**

```
URI: web+cardano://claim/v1?faucet_url=https%3A%2F%2Fclaim.hosky.io%2Fconsensus23&code=ABC123
Version: 1
URL: https://claim.hosky.io/consensus23
CODE: ABC123
JSON POST Data:
```
```json
{
  "address": "addr1abc...xyz",
  "code": "ABC123"
}
```

##### Note on the `address` field

The wallet should always send the recipient address in bech32 format. If a particular token faucet implementation wishes
to restrict or limit access to their faucet based on staking key or individual wallet address, this should be handled at
the server end.

- *Supported Addresses**

- Shelley-era `enterprise` address consisting of only a payment key
- Shelley-era `staking` address consisting of a payment and staking key

##### Note on the `code` field

The code is required. Specifying a `code` allows for reliable tracking and/or limiting of claims to the faucet host.
Codes can be used to identify attendees of particular events (i.e. CODE = `consensus2023`) or can be a unique, one-time
code per user (i.e. CODE = `abc123xyz987`). In this way we leave the code to be flexible to match a variety of
analytical use cases depending upon the needs of the implementing project.

#### Security Considerations

1. Wallets should prompt/warn users prior to sending potentially sensitive information (wallet address + code) via the
   token claim URI. An informational pop-up or confirmation modal should be displayed to users such as:
   `We are about to send your address and code 123456 to https://claim.hosky.io. Are you sure you want to proceed?`

### Process Flow

The envisioned process flow for the POO Protocol is as follows:
1. The project set asides some amount of budget (tokens + Lovelace [minUTxO]) for a given marketing push or IRL event
2. If desired, one or more `codes` are generated to help track and analyze claiming figures
3. QR Code(s) may be generated, printed, and otherwise displayed or given to users during the course of events
4. Users scan the code with their mobile light wallet
5. The light wallet makes a POST request to the API endpoint specified in the Cardano URI containing the user's wallet address and the included code (if present)
6. The project API returns a documented status code indicating the success or failure of the operation
7. If a successful status is detected and returned, the project issues tokens to the specified address per their campaign settings

### URI Format

The URI format consists of the CIP-13 `web+cardano://` scheme, followed by the `claim` authority, then a `version` path.

- **NOTE: ALL ARGUMENTS SHOULD BE URL-ENCODED***

#### Version 1

Version 1 URIs must include `/v1` as the path of the URI.

Version 1 URIs must include two required arguments:

- `faucet_url` as a fully-typed URL (i.e. https://claim.hosky.io)
- `code` as either a campaign identifier or unique, one-time use code

Version 1 URIs may include additional query parameters that should be passed through to the api server.

_Version 1 Examples:_

```html
<!-- A Cardano Claim URI with campaign identifier code -->
<a href="web+cardano://claim/v1?faucet_url=https%3A%2F%2Fclaim.hosky.io&code=consensus2023">Thanks for attending Consensus 2023!</a>

<!-- A Cardano Claim URI with unique, one-time use code -->
<a href="web+cardano://claim/v1?faucet_url=https%3A%2F%2Fclaim.hosky.io&code=dff6508d8dfb4e128fd67e9ff54af147">Claim your $HOSKY now!</a>

<!-- A Cardano Claim URI with a campaign-specific code and optional user_id argument -->
<a href="web+cardano://claim/v1?faucet_url=https%3A%2F%2Fclaim.hosky.io&code=NFTxLV2023&user_id=Idjiot1337">Get your $HOSKY!</a>
```

### Wallet Requests

Light wallets that detect and support `web+cardano` URIs as well as mobile wallets who detect either a QR code or other
link with this format should parse the URI and send a `POST` request to the specified URL containing a JSON payload
including:

- The user's wallet receive address
- The code
- Additional URI query parameters passed through

#### Examples

##### _Faucet URL + Campaign Code_
- URI: ```web+cardano://claim/v1?faucet_url=https%3A%2F%2Fclaim.nftxlv.com&code=NFTxLV2023```
- Faucet URL: ```https://claim.nftxlv.com```
- POST JSON Data:
```json
{
  "address": "addr1abc...xyz",
  "code": "NFTxLV2023"
}
```

##### _Faucet URL + Unique Code_
- URI: ```web+cardano://claim/v1?faucet_url=https%3A%2F%2Fclaim.nftxlv.com&code=NFTxLV2023```
- Faucet URL: ```https://claim.hosky.io```
- POST JSON Data:
```json
{
  "address": "addr1abc...xyz",
  "code": "ABC123"
}
```

##### _Faucet URL + Campaign Code + Custom User ID_
- URI: ```web+cardano://claim/v1?faucet_url=https%3A%2F%2Fclaim.nftxlv.com&code=NFTxLV2023&user_id=Adam1337```
- Faucet URL: ```https://claim.nftxlv.com```
- POST JSON Data:
```json
{
  "address": "addr1abc...xyz",
  "code": "NFTxLV2023",
  "user_id": "Adam1337"
}
```

### API Server Response Codes

The API server is expected to return one of the following defined status blocks in `application/json` format. Any other
responses from the API server should be considered invalid and discarded or display an error.

The expected API that any token fountain implementation should follow and wallet integrators should expect is documented
on Swagger!

- [Version 1](https://app.swaggerhub.com/apis/CatastrophicCardano/FaucetAPI/1)

#### Successful Responses

##### Valid (200)

First Successful Request

```json
{
  "code": 200,
  "lovelaces": "2000000",
  "queue_position": 23,
  "status": "accepted",
  "tokens": {
    "a0028f350aaabe0545fdcb56b039bfb08e4bb4d8c4d7c3c7d481c235.484f534b59": "29433292000000"
  }
}
```

##### Valid Queued (201)

Subsequent Successful Request (Address + Code Match) prior to token distribution

```json
{
  "code": 201,
  "lovelaces": "2000000",
  "queue_position": 1,
  "status": "queued",
  "tokens": {
    "a0028f350aaabe0545fdcb56b039bfb08e4bb4d8c4d7c3c7d481c235.484f534b59": "29433292000000"
  }
}
```

##### Valid Complete (202)

Subsequent Successful Request (Address + Code Match) after token(s) are distributed

```json
{
  "code": 202,
  "lovelaces": "2000000",
  "status": "claimed",
  "tokens": {
    "a0028f350aaabe0545fdcb56b039bfb08e4bb4d8c4d7c3c7d481c235.484f534b59": "29433292000000"
  },
  "tx_hash": "TX1234"
}
```

#### Error Responses

##### Bad Request - Invalid Address (400)

The provided address is not a valid Cardano address

```json
{
   "code": 400,
   "status": "invalidaddress"
}
```

##### Bad Request - Missing Code (400)

No code was provided in the request

```json
{
   "code": 400,
   "status": "missingcode"
}
```

##### Bad Request - Invalid Network (400)

The wallet provided is from the wrong network (testnet/mainnet)

```json
{
   "code": 400,
   "status": "invalidnetwork"
}
```

##### Invalid - Not Known (404)

The specified code does not exist

```json
{
  "code": 404,
  "status": "notfound"
}
```

##### Invalid - Already Claimed (409)

An address was already used (if not code present) or the code presented was found but the address did not match

```json
{
  "code": 409,
  "status": "alreadyclaimed"
}
```

##### Invalid - Expired (410)

For time-limited fountains, a code of 410 means that the period for redemption has expired

```json
{
  "code": 410,
  "status": "expired"
}
```

##### Invalid - Too Early (425)

For time-limited fountains, a code of 425 means that the period for redemption has not begun yet

```json
{
  "code": 425,
  "status": "tooearly"
}
```

##### Invalid - Rate Limited (429)

Rate limiting settings and details are left to the discretion and implementation of individual projects. A status code
of 429 or this status response should be considered as a rate limiting response.

```json
{
  "code": 429,
  "status": "ratelimited"
}
```

##### Server Error (500)

Implementations should of course be prepared to handle situations where a server is non-responsive for any reason and
be prepared to handle any other, non-specified error codes including 500 codes.

### Versioning & Modification Rules

If there is sufficient justification in the future for modification of this standard to the point that a "Version 2"
would be necessary, those changes MUST be submitted as a new, separate CIP to this repository and follow all applicable
CIP standards for acceptance. Examples of "major" changes that might justify a new version of this CIP include:
fundamentally altering the URI structure, adding or removing a **required** field, or any other non-backwards compatible
changes to the [Process Flow](#process-flow).

Minor changes for grammar, clarity, or functionality that fall within the scope of "Version 1" of this document may be
made by editing this document directly. Such changes include: grammatical or exposition changes to improve readability
or clarity of communication, improvements to documented code examples, additional or optional server response information,
etc.

## Rationale: How does this CIP achieve its goals?

By creating a well-defined standard for both a CIP-13 URI scheme and the expected API response(s) we can create a
framework that both wallets and projects can utilize to encourage and onboard new users into the ecosystem via Native
Asset incentive models without needlessly and constantly reinventing the wheel for each product or project.

Furthermore, the aforementioned "paper wallet" technique has many drawbacks including:
- The person(s) responsible for generating the paper wallets at some point have access to the seed phrases generated, leading to a potential security vulnerability
- Projects would need to preload these wallets with funds/tokens; this makes it difficult and/or impossible to reliably know how many of the paper wallets were ever actually claimed
- For those wallets that go forever unclaimed, this essentially creates a permanent "burn" of both Lovelace and the native assets of the project; less than ideal

By utilizing this framework, projects can have accurate, measurable analytics into the success of various real-world
marketing and event efforts: Proof of Onboarding.

## Path to Active

### Acceptance Criteria

- [X] Demonstrate a working MVP
- [X] Open source an MVP example of token faucet [server-side code](https://github.com/HOSKYToken/poo-genericClaim)
- [X] Receive feedback and iterate based on community feedback

### Implementation Plan

- [X] VESPR Mobile Wallet supports the Proof of Onboarding Protocol.
- [X] Yoroi Mobile Wallet supports the Proof of Onboarding Protocol.
- [X] HOSKY Project has released an open source server-side implementation software that may be used as a proof of concept for any interested projects.
- [X] Multiple projects at multiple, global events have successfully deployed Proof of Onboarding.
- [X] Onboard additional wallet providers, server/service providers, and redemption methods.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0100/README.md
---

- --
CIP: 100
Title: Governance Metadata
Category: Metadata
Status: Active
Authors:
- Pi Lanningham <pi@sundaeswap.finance>
Implementors: []
Discussions:
- https://github.com/cardano-foundation/cips/pulls/556
Created: 2023-04-09
License: CC-BY-4.0
- --

## Abstract

This Cardano Improvement Proposal (CIP) introduces a standardized and flexible metadata format for governance events in the Cardano ecosystem. While the ledger does not enforce the structure of metadata, providing a standard for metadata will enable better tooling, improved interoperability, and more consistent display across wallets, blockchain explorers, and other tools. This CIP aims to strike a balance between flexibility and structure to facilitate high-quality tooling, without limiting expressivity with regards to user expression.

For the many contributors to this proposal, see [Acknowledgements](#acknowledgements).

## Motivation

With the advent of the Voltaire era, Cardano is moving towards a decentralized governance model. CIP-1694 addresses a potential technical implementation of ledger rules for creating, voting on, and ratifying proposed changes to the ledger. The ledger has no mechanism or desire to validate this metadata, and as a result, the official specification leaves the format of this metadata unspecified.

To facilitate rich user experiences for the various governance actors, however, it would be beneficial to have a suggested universal format for this metadata, allowing deep and interconnected discovery and exploration of this metadata. This CIP seeks to provide that standard format, and a minimal set of fields for various governance actions, leaving the true depth of metadata to be defined later through the extensibility mechanism outlined below.

While this specification is written with CIP-1694 in mind, many of the ideas should be equally suitable for any other governance proposal, provided that proposal has a mechanism for attaching metadata to a governance action.

Explicitly, here are the goals this CIP is trying to satisfy:
- Standardize a format for rich metadata related to cardano governance
- Standardize a minimal and uncontroversial set of fields to support "Minimal Viable Governance"
- Leave that format open to extension and experimentation
- Enable tooling and ecosystem developers to build the best user experiences possible
- Provide a set of best practices that tooling and ecosystem developers can rely on for the health and integrity of this data

## Specification

This section outlines the high level format and requirements of this standard.

      The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL
      NOT", "SHOULD", "SHOULD NOT", "RECOMMENDED",  "MAY", and
      "OPTIONAL" in this document are to be interpreted as described in
      [RFC 2119](https://www.rfc-editor.org/rfc/rfc2119).

- On chain metadata actions in CIP-1694 (and likely any alternative proposals) have the notion of an "Anchor"; this is the URL and a hash for additional, non-ledger metadata about the action.
- Tools which publish governance related transactions SHOULD publish metadata via these fields.
- While that content MAY be in any format, following any standard or non-standard, for the remainder of this document SHOULD/MUST will refer to documents that are following this specification.
- The content hosted at the anchor URL MUST be a JSON-LD document according to the rest of this specification.
- This document SHOULD include `@context` and `@type` fields below to aid in interpretation of the document.
- The JSON document SHOULD be formatted for human readability, for the sake of anyone who is manually perusing the metadata.
- That content SHOULD be hosted on a content addressable storage medium, such as IPFS or Arweave, to ensure immutability and long term archival.
- The hash in the anchor MUST be the hash of the the raw bytes of the content, using the hashing algorithm specified in the `hashAlgorithm` field. Currently only blake2b-256 is supported.
- For the purposes of hashing and signature validation, we should use the [canonical RDF triplet representation](https://www.w3.org/TR/rdf-canon/), as outlined in the JSON-LD specification.

### Versioning

[JSON-LD](https://json-ld.org/) is a standard for describing interconnected JSON documents that use a shared vocabulary.

In a JSON-LD document, every field is uniquely tied to some globally unique identifier by means of an Internationalized Resource Identifier (IRI). Different machine-consumers can then know that they agree on the interpretation of these fields.

The shared vocabulary of fields is standardized within the scope of a document via a `@context` field. This allows a compositional / extensible approach to versioning, similar to the recent changes to [CIP-0030](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0030/README.md). Rather than specifying a version number and forcing competing standards to compete for what the "next" version number will include, instead a wide variety of standards are allowed and encouraged. Tool authors MAY support those which are the most beneficial or common. This creates an organic, collaborative evolution of the standard.

Note: Any URI's in the @context field SHOULD be content-addressable and robustly hosted; losing access to the schema is less dangerous than losing access to the metadata itself, but should still prefer strong and immutable storage options for the preservation of context.

- A governance metadata document MAY include a `@context` field.
- A governance metadata document MAY include a `@type` field, referring to a specific type of document from the included `@context`s.
- The `@context` field, if included, MUST be a valid JSON-LD @context field; the basics are described below.
- It MAY be a string, in which case it MUST be the IRI of a jsonld document describing the shared vocabulary to assume when interpreting the document.
- It MAY be an object, where each key refers to a property name, and the value is either an IRI to a schema describing that field, or an object with that definition inlined.
- It MAY be an array, including multiple contexts, which are merged in order with a "most-recently-defined-wins" mechanism.
- For a full understanding of the @context field, refer to the [JSON-LD specification](https://www.w3.org/TR/json-ld/).
- Each IRI in the `@context` field SHOULD refer to a schema hosted via a robust, content addressable, and immutable storage medium such as IPFS, Arweave, etc.
- If the metadata document is missing the `@context` field, it will be assumed to refer to [./cip-0100.common.jsonld]
- Future CIPs may standardize common contexts, and SHOULD attempt to reuse common terminology and SHOULD avoid naming collisions.
- Tool authors MAY choose which contexts to support, but MUST make a best effort to display the metadata in the presence of unrecognized context, up to and including gracefully falling back to a raw display of the JSON document.

- *Extensions to the governance metadata standard can take one of two forms**:
- CIP-0100 itself can be updated through the normal CIP process to provide additional clarity on any concepts that are giving people trouble with adoption, or to correct inaccuracies.
- A new CIP, which defines new JSON-LD vocabulary to extend this one, which seeks broad adoption.
- A new context document, used in the wild, but not officially standardized, and which doesn't seek broad adoption.

### Hosting

In CIP-1694 (and likely any alternative or future evolution of it), there are a number of certificates that can be attached to a transaction pertaining to governance; each of these is equipped with an "anchor", which is a URI at which can be found additional metadata.

While this metadata can be published anywhere, external hosting may be unavailable to some users. Therefore, we recognize the transaction metadata as an effective tool for a "common town square" for hosting and discoverability, and reserve [metadatum label 1694](../CIP-0010) for publishing governance related metadata on-chain.

With the help of [CIP-?](https://github.com/cardano-foundation/CIPs/pull/635), the anchor can then refer to the metadata of another transaction, or even the metadata of the transaction being published itself.

When published on-chain, the CBOR encoding of this metadata, when published on-chain, follows [the standard convention](https://developers.cardano.org/docs/get-started/cardano-serialization-lib/transaction-metadata/#json-conversion) used for converting JSON to CBOR in transaction metadata.

### Augmentations

Additionally, someone may wish to augment a previous piece of metadata with new information, divorced from the transaction that initially published it; this may be useful, for example, provide additional arguments in favor of or against a proposal, provide translations to other languages, provide a layman's explanation of a highly technical document, etc.

These use the same format, but leverage the `references` field to point to the documents that they wish to extend.

These can, in theory, be published anywhere, but the use of metadatum label 1694 mentioned above is particularly useful in this case.

### Context and Schema

We expect a rich ecosystem of CIPs to emerge defining different extensions, so this CIP provides only an initial base-line MVP of fields, defined in the following JSON-LD and JSON Schema files:

- [JSON-LD Context](./cip-0100.common.jsonld)
- [JSON Schema](./cip-0100.common.schema.json)

The rest of this document will provide a high level description of how these fields should be interpreted

### High level description

The following properties are considered common to all types of transactions, and the minimal set needed for "minimum viable governance":

- `hashAlgorithm`: The algorithm used to hash the document for the purposes of the on-chain anchor; currently only supports blake2b-256
- `authors`: The primary contributors and authors for this metadata document
- An array of objects
- Each object MAY have a `name` property, which serves as a display name
- Each object MUST have a `witness` field, which contains a signature of the `body` of the document
- The witness may define a `@context`, describing the manner in which the document has been witnessed
- A default witness scheme context is described in a later section
- Tooling authors SHOULD validate witnesses they understand
- If witnesses aren't validated, tooling authors SHOULD emphasize this to the user
- If absent or invalid, tooling authors SHOULD make this clear to the user
- `body`: The material contents of the document, separate for the purposes of signing
- `references`
- An array of objects
- Each object specifies a `@type`, which is either "GovernanceMetadata" or "Other"
- Each object specifies a `label`, which serves as a human readable display name
- Each object specifies a `uri`; when the type is set to "GovernanceMetadata", the URI should point to another CIP-0100 compliant document
- `comment`
- Freeform text attached to the metadata; richer structures for justifying the transaction this is attached to is left to future CIPs
- Tooling authors SHOULD emphasize that these comments represent the *authors* views, and may contain bias.
- `externalUpdates`
- A array of objects
- Each object can have a `@context` defining how to interpret it
- by default, assumed to just be an object with a `title` and `uri` field
- The purpose is to allow "additional updates", that aren't written yet, such as a blog or RSS feed
- Tooling authors MAY fetch and parse this metadata according to this standard,
- If so, Tooling authors MUST emphasize that this information is second-class, given that it might have changed

Additionally, we highlight the following concepts native to json-ld that are useful in the context of governance metadata:
- [@language](https://www.w3.org/TR/json-ld11/#string-internationalization)
- The `@context` field SHOULD specify a `@language` property, which is an ISO 639-1 language string, to define a default language for the whole document
- Specific sub-fields can specify different languages
- The `@context` field may specify a `@container` property set to `@language`, in which case the property becomes a map with different translations of the property
- Tooling authors MAY provide automatic translation, but SHOULD make the original prose easily available

### Hashing and Signatures

When publishing a governance action, the certificate has an "anchor", defined as a URI and a hash of the content at the URI.

For CIP-0100 compliant metadata, the hash in the anchor should be the blake2b-256 hash of the raw bytes received from the wire.
Hashing directly the original bytes ensures that there are no ambiguities, since the process doesn't depend on parsing the metadata, which can be the source of conflicts in different implementations.

A metadata has a number of authors, each of which MUST authenticate their endorsement of the document in some way.

This is left extensible, through the use of a new context, but for the purposes of this CIP, we provide a simple scheme.

Each author should have a witness. The witness will be an object with an `witnessAlgorithm` (set to ed25519), a `publicKey` (set to a base-16 encoded ed25519 public key), and a `signature`, set to the base-16 encoded signature of the body field.

Because the overall document may change, it is neccesary to hash a subset of the document that is known before any signatures are collected. This is the motivation behind the `body` field.

The signature is an ed25519 signature using the attached public key, and the payload set to the blake2b-256 hash of the `body` field. Specifically, this field is canonicalized in the following way.
- Canonicalize the whole document according to [this](https://w3c-ccg.github.io/rdf-dataset-canonicalization/spec/) specification.
- Identify the node-ID of the `body` node
- Filter the canonicalized document to include the body node, and all its descendents
- Ensure the file ends in a newline
- Hash the resulting file with blake2b-256

### Best Practices

This section outlines a number of other best practices for tools and user experiences built on top of this standard, as brainstormed by the Cardano community.

- If the hash in the anchor doesn't match the computed hash of the content, it is imperative to make that obvious to the user.
- Without this being obvious, there are severe and dramatic attack vectors for manipulating user votes, delegators, etc.
- NOTE: The term "MUST" in the RFC-2119 sense isn't used here because it's unenforcable anyway, but if these hashes *aren't* checked, you **SHOULD** inform the user that you are not checking the integrity of the data.
- You MAY do this by displaying a prominent warning, or potentially fully barring access to the content.
- You SHOULD provide a way to access the raw underlying data for advanced or diligent users.
- This MAY be in the form of a JSON viewer, or a simple link to the content.
- You SHOULD gracefully degrade to a simple raw content view if the metadata is malformed in some way, or not understood.
- For example, a proposal has added an extra unexpected field, in addition to expected ones. The tool SHOULD gracefully render expected fields and show users a raw view of unexpected fields.
- You SHOULD provide links and cross references whenever the metadata refers to another object in some way
- For example, a proposal may link to the sponsoring DReps, which may have their own view within the tool you're building
- If you are hosting the content for the user, you SHOULD use a content-addressable hosting platform such as IPFS or Arweave
- If the content is self-hosted, you SHOULD take care to warn the user about changing the content
- For example, you CAN detect well-known content-addressable file storage platforms such as IPFS or Arweave, and display an extra warning if the content is not hosted on one of those

## Rationale: how does this CIP achieve its goals?

Here are the goals this CIP seeks to achieve, and the rationale for how this specific solution accomplishes them:

- Standardize a format for rich metadata related to cardano governance
- Standardizing on JSON-LD provides an industry standard, yet highly flexible format for effectively arbitrary structured data
- Standardize a minimal and uncontroversial set of fields to support "Minimal Viable Governance"
- This CIP specifies a minimal number of fields: hash-algorithm, authors, justification, external-updates, and @language
- Each of these fields is essential for the global accessibility of this data, and enables tooling that promotes a well-informed voting populace
- Leave that format open to extension and experimentation
- JSON-LD has, built in, a mechanism for extending and experimenting with new field types
- Anyone can extend this metadata, even independent of the CIP process, with their own field definitions
- Tooling authors can, independently, choose which extensions to support natively, while also surfacing fields they don't recognize in more generic ways.
- Enable tooling and ecosystem developers to build the best user experiences possible
- The `@context` field of a JSON-LD document allows tooling authors to confidently and consistently interpret a known field within the metadata, with reduced risk of misinterpreting or misrepresenting the authors intent
- This metadata can also reference other objects and documents in the ecosystem, providing for rich exploration needed for an informed voting populace.
- Provide a set of best practices that tooling and ecosystem developers can rely on for the health and integrity of this data
- This CIP has an explicit section of best practices, brainstormed with attendees of a workshop dedicated to the purpose.

The following alternatives were considered, and rejected:
- Plain JSON documents
- While ultimately flexible and simple, there is a risk that with no way to structure what is *officially* supported, and the interpretation of each field, tooling authors would have one hand tied behind their back, and would be limited to a minimum common denominator.
- Canonicalising the whole document before hashing it
- Canonicalising requires initially parsing the file as a json, which can by itself cause ambiguities
- A custom JSON format, with reference to CIPs
- An initial draft of this proposal had an `extensions` field that operated very similar to `@context`
- Instead, this CIP chose to go with an industry standard format to leverage the existing tooling and thought that went into JSON-LD
- CBOR or other machine encoding
- The metadata in question, despite being proliferous, is not expected to to be an undue storage burden; It's not, for example, video data, or storing billions of records.
- It is more important, then, that the metadata be human readable, so that tooling authors have the option to show this data in its raw format to a user, and for it to be loosely understandable even by non-technical users.

### Test Vectors

See [test-vector.md](./test-vector.md) for example.

## Path to Active

The path for this proposal to be considered active within the community focuses on 4 key stages: Feedback, Implementation, Adoption, and Extension.

### Acceptance Criteria

In order for this standard to be active, the following should be true:

- [x] At least 1 month of feedback has been solicited, and any relevant changes with broad consensus to the proposal made
- [ ] At least 2 client libraries in existence that support reading an arbitrary JSON file, and returning strongly typed representations of these documents
- [cardano-governance-metadata-lib](https://github.com/SundaeSwap-finance/cardano-governance-metadata)
- [x] At least 1 widely used *producer* of governance metadata (such as a wallet, or the cardano-cli)
- [1694.io](https://www.1694.io)
- [GovTool](https://gov.tools)
- [Tempo.vote](https://tempo.vote)
- [x] At least 1 widely used *consumer* of governance metadata (such as a blockchain explorer, governance explorer, voting dashboard, etc)
- [1694.io](https://www.1694.io)
- [Adastat](https://adastat.net)
- [Cardanoscan](https://cardanoscan.io)
- [GovTool](https://gov.tools)
- [Tempo.vote](https://tempo.vote)
- [x] At least 1 CIP in the "Proposed" status that outlines additional fields to extend this metadata
- [CIP-108 | Governance Metadata - Governance Actions](../CIP-0108/README.md)
- [CIP-119 | Governance metadata - DReps](../CIP-0119/README.md)
- [CIP-136 | Governance metadata - Constitutional Committee votes](../CIP-0136/README.md)

### Community Tooling

Below you can find a growing list of community tools which let you sign / verify / canonize / manipulate governance metadata JSON-LD data:
- [cardano-signer](https://github.com/gitmachtl/cardano-signer?tab=readme-ov-file#cip-100--cip-108--cip-119-mode) : A tool to sign with author secret keys, verify signatures, canonize the body content (Linux/Arm/Win/Mac)
- [cardano-governance-metadata-lib](https://github.com/SundaeSwap-finance/cardano-governance-metadata) : A rust library for interacting with Cardano Governance Metadata conforming to CIP-100 (rust)

### Implementation Plan

The key stages to get this proposal to active, and the motivation for why each stage is valuable, is outlined below:

- Solicitation of feedback
- While this proposal represents the input of many prominent community members, it is by no means exhaustive
- This CIP should receive a moderate, but not egregious, amount of scrutiny and feedback within it's initial goals
- Implementation
- The effectiveness of this standard is greatly amplified if tools and SDKs are built which allow parsing arbitrary data according to this standard
- Sundae Labs has offered to build Rust and Golang libraries that capture the types outlined above, and implementations in other languages are welcome
  <!-- In particular, I would love to find someone to own the Haskell implementation for easy integration into the cardano-cli -->
- Adoption
- This standard is most effective if it receives widespread adoption
- Therefore, a path to active includes engaging prominent members of the ecosystem, such as wallets and explorers, to begin producing and consuming documents in accordance with the standard.
- Extension
- Finally, this standard chooses to fully specify very little of the total surface area of what could be expressed in governance metadata
- Therefore, to prove the extensibility of the standard, at least one follow-up CIP should be drafted, extending the set of fields beyond "Minimum Viable Governance"

## Acknowledgements

<details>
  <summary><strong>First draft/workshop</strong></summary>

  I would like to thank those that contributed to this first draft during the online workshop that was held on 2023-04-12.

- CHIL Pool
- Alex Djuric
- Cody Butz
- Felix Weber
- Leo Pienasola
- Markus Gufler
- Michael Madoff
- Mohamed Mahmoud
- Thomas Upfield
- William Ryan
- Santiago Carmuega

</details>

<details>
  <summary><strong>Second draft</strong></summary>

  The following people helped with the second draft, out of band at at the Edinburgh workshop on 2023-07-12.

- Ryan Williams
- Matthias Benkort
- All Edinburgh Workshop attendees

</details>
<details>
  <summary><strong>Second Workshop</strong></summary>

  The following people helped with the third draft during the online workshop held on 2023-11-02.

- Mike Susko
- Thomas Upfield
- Lorenzo Bruno
- Ryan Williams
- Nils Peuser
- Santiago Carmuega
- Nick Ulrich
- Ep Ep

</details>

<details>
  <summary><strong>Third Workshop</strong></summary>

  The following people helped with the third draft during the online workshop held on 2023-11-10.

- Adam Dean
- Rhys Morgan
- Thomas Upfield
- Marcel Baumberg

</details>

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0101/README.md
---

- --
CIP: 101
Title: Integration of keccak256 into Plutus
Status: Proposed
Category: Plutus
Authors:
- Sergei Patrikeev <so.patrikeev@gmail.com>
- Iñigo Querejeta-Azurmendi <inigo.querejeta@iohk.io>
Implementors:
- Michael Peyton Jones <michael.peyton-jones@iohk.io>
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/524
Created: 2023-06-13
License: Apache-2.0
- --

## Abstract
This CIP proposes an extension of the current Plutus functions to provide support for the [`keccak256`](https://keccak.team/files/Keccak-submission-3.pdf) hashing algorithm,
primarily to ensure compatibility with Ethereum's cryptographic infrastructure.

## Motivation: why is this CIP necessary?

The [integration](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0049/README.md) of the ECDSA and Schnorr signatures over the secp256k1 curve into Plutus was a significant
step towards interoperability with Ethereum and Bitcoin ecosystems. However, full compatibility is still impossible
due to the absence of the `keccak256` hashing algorithm in Plutus interpreter,
which is a fundamental component of Ethereum's cryptographic framework:
- data signing standard [EIP-712](https://eips.ethereum.org/EIPS/eip-712),
- `keccak256` is the hashing algorithm underlying Ethereum's ECDSA signatures.
- EVM heavily [depends](https://ethereum.github.io/yellowpaper/paper.pdf) on `keccak256` for internal state management

Adding `keccak256` to Plutus would enhance the potential for cross-chain solutions between Cardano and EVM-based blockchains.

A compelling integration that would greatly benefit from `keccak256` support on the Cardano blockchain is [Hyperlane](https://hyperlane.xyz/).
Hyperlane is a permissionless interoperability layer that facilitates communication of arbitrary data between smart contracts across multiple blockchains. Hyperlane's [interchain security modules](https://docs.hyperlane.xyz/docs/protocol/sovereign-consensus)
rely on the verification of specific cryptographic proofs from one chain to another. These proofs utilize the `keccak256` hash to calculate consistent cross-chain message IDs.
The multi-signature module verifies that a majority of off-chain validators have signed an ECDSA signature over a `keccak256` digest, a common practice in EVM.

While Hyperlane [can support](https://github.com/hyperlane-xyz/hyperlane-monorepo/issues/2399) different cryptographic primitives for non-EVM chains, doing so could compromise censorship resistance, resulting in only limited support for Cardano in Hyperlane. By implementing this CIP, Cardano could fully integrate with Hyperlane's security modules, enabling Cardano smart contracts to communicate with any blockchain supported by Hyperlane.

## Specification
This proposal aims to introduce a new built-in hash function `keccak_256`.

This function will be developed following the [`keccak256`](https://keccak.team/files/Keccak-submission-3.pdf) specification
and will utilize the [cryptonite](https://github.com/haskell-crypto/cryptonite/blob/master/Crypto/Hash/Keccak.hs) implementation.
Since `cryptonite` is already a part of the [`cardano-base`](https://github.com/input-output-hk/cardano-base/blob/master/cardano-crypto-class/src/Cardano/Crypto/Hash/Keccak256.hs),
this simplifies its integration into Plutus. The cost of the `keccak_256` operation will scale linearly with the length of the message.

More specifically, Plutus will gain the following primitive operation:

- `keccak_256` :: ByteString -> ByteString

The input to this function can be a `ByteString` of arbitrary size, and the output will be a `ByteString` of 32 bytes.
Note that this function aligns with the format of existing hash functions in Plutus, such as [blake2b_256](https://github.com/input-output-hk/plutus/blob/75267027f157f1312964e7126280920d1245c52d/plutus-core/plutus-core/src/Data/ByteString/Hash.hs#L25)

## Rationale: how does this CIP achieve its goals?
While the `keccak256` function might be implemented in on-chain scripts, doing so would be computationally unfeasible.

The library, cryptonite, is not implemented by and under control of the Plutus team. However,
- It is a library already used in the Cardano stack to expose SHA3, and can be considered as a trustworthy implementation.
- The function does not throw any exceptions as hash functions are defined to work with any ByteString input. It does not expect a particular particular structure.
- It's behaviour is predictable. As mentioned above, the cost of the function is linear with respect to the size of the message provided as input. This is the same behaviour that other hash functions exposed in plutus (blake, sha3) have.
## Path to Active
This CIP may transition to active status once the Plutus version containing the `keccak_256` function is introduced
in a node release and becomes available on Mainnet.

### Acceptance Criteria
- A Plutus binding is created for the `keccak256` function and included in a new version of Plutus.
- Integration tests, similar to those of the existing Plutus hash functions, are added to the testing infrastructure.
- The function is benchmarked to assess its cost. As for other hash functions available in Plutus (blake2b and sha256), we expect the cost of keccak to be linear with respect to the size of the message. The Plutus team determines the exact costing functions empirically.
- The ledger is updated to include new protocol parameters to control costing of the new builtins.

### Implementation Plan
The Plutus team will develop the binding, integration tests, and benchmarks. The E2E tests will be designed and implemented
collaboratively by the testing team, the Plutus team, and community members planning to use this primitive.

## Copyright
This CIP is licensed under [Apache-2.0][https://www.apache.org/licenses/LICENSE-2.0].

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0102/README.md
---

- --
CIP: 102
Title: Royalty Datum Metadata
Authors:
- Sam Delaney <sdelaney@ikigaitech.org>
Implementors:
- Grabbit <https://grabbit.market/>
- Nebula <https://github.com/spacebudz/nebula/>
Discussions:
- https://github.com/ikigai-github/CIPs/pull/1
- https://github.com/cardano-foundation/CIPs/pull/551
Status: Proposed
Category: Tokens
Created: 2023-08-08
License: CC-BY-4.0
- --

## Abstract

This proposal makes use of the onchain metadata pattern established in [CIP-0068][] to provide a way to store royalties with greater assurance and customizability.

## Motivation: why is this CIP necessary?

The inability to create trustless onchain royalty validation with [CIP-0027][] is a major drawback to Cardano NFTs. The pattern defined in CIP-68 represents an opportunity to upgrade the standard to support onchain validation. This CIP aims to eliminate that drawback and demonstrate better support for developers, NFT creators, and NFT collectors, ultimately attracting dapps & NFT projects that would otherwise have taken their talents to another blockchain.

In addition, this standard allows royalties to be split between multiple addresses, another limitation of the CIP-27 royalty schema. Future versions of this standard could  also easily support multiple royalty policies defined for a single collection, applied at the level of individual tokens.

## Specification

### 500 Royalty Datum Standard

The following defines the `500` Royalty NFT standard with the registered `asset_name_label` prefix value

| asset_name_label            | class        | description                                                          |
| --------------------------- | ------------ | -------------------------------------------------------------------- |
| 500                         | NFT          | Royalty NFT stored in a UTxO containing a datum with royalty information |

#### Class

The `royalty NFT` is an NFT (non-fungible token).

#### Pattern

The `royalty NFT` **must** have an identical `policy id` as the collection.

The `asset name` **must** be `001f4d70526f79616c7479` (hex encoded), it contains the [CIP-0067][] label `500` followed by the word "Royalty".

Example:\
`royalty NFT`: `(500)Royalty`\
`reference NFT`: `(100)Test123`

#### 500 Datum Metadata

The royalty info datum is specified as follows (CDDL):

```cddl
big_int = int / big_uint / big_nint
big_uint = #6.2(bounded_bytes)
big_nint = #6.3(bounded_bytes)

optional_big_int = #6.121([big_int]) / #6.122([])

royalty_recipient = #6.121([
              address,                    ; definition can be derived from:
                                          ; https://github.com/input-output-hk/plutus/blob/master/plutus-ledger-api/src/PlutusLedgerApi/V1/Address.hs#L31
              int,                        ; variable fee ( calculation: ⌊1 / (fee / 10)⌋ ); integer division with precision 10
              optional_big_int,           ; min fee (absolute value in lovelace)
              optional_big_int,           ; max fee (absolute value in lovelace)
            ])

royalty_recipients = [ * royalty_recipient ]

; version is of type int, we start with version 1
version = 1

; Custom user defined plutus data.
; Setting data is optional, but the field is required
; and needs to be at least Unit/Void: #6.121([])
extra = plutus_data

royalty_info = #6.121([royalty_recipients, version, extra])
```

#### Example of onchain variable fee calculation:

```cddl
; Given a royalty fee of 1.6% (0.016)

; To store this in the royalty datum
1 / (0.016 / 10) => 625

; To read it back
10 / 625 => 0.016
```
Because the computational complexity of Plutus primitives scales with size, this approach significantly minimizes resource consumption.

To prevent abuse, it is **recommended** that the `royalty NFT` is stored at the script address of a validator that ensures the specified fees are not arbitrarily changed, such as an always-fails validator.

### Reference Datum Royalty Flag

If not specified elsewhere in the token's datums, a malicious user could send transactions to a protocol which do not reference the royalty datum. For full assurances, a new optional flag should be added to the reference datum

```cddl
extra =
	{
		...

		? royalty_included : big_int
	}
```

- If the field is present and > 1 the validators must require a royalty input.
- If the field is present and set to 0 the validators don't need to search for a royalty input.
- If the field is not present, validators should accept a royalty input, but not require one.

### Examples

In-code examples can be found in the [reference implementation](https://github.com/SamDelaney/CIP_102_Reference).

#### Retrieve metadata as 3rd party

A third party has the following NFT `d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(222)TestToken` and they want to look up the royalties. The steps are

1. Construct `royalty NFT` from `user token`: `d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(500)Royalty`
2. Look up `royalty NFT` and find the output it's locked in.
3. Get the datum from the output and look up metadata by going into the first field of constructor 0.
4. Convert to JSON and encode all string entries to UTF-8 if possible, otherwise leave them in hex.

#### Retrieve metadata from a Plutus validator

We want to bring the royalty metadata of the NFT `d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(222)TestToken` in the Plutus validator context. To do this we

1. Construct `royalty NFT` from `user token`: `d5e6bf0500378d4f0da4e8dde6becec7621cd8cbf5cbb9b87013d4cc.(500)Royalty` (off-chain)
2. Look up `royalty NFT` and find the output it's locked in. (off-chain)
3. Reference the output in the transaction. (off-chain)
4. Verify validity of datum of the referenced output by checking if policy ID of `royalty NFT` and `user token` and their asset names without the `asset_name_label` prefix match. (on-chain)

## Rationale: how does this CIP achieve its goals?

The specification here is made to be as minimal as possible. This is done with expediency in mind and the expectation that additional changes to the specification may be made in the future. The sooner we have a standard established, the sooner we can make use of it. Rather than attempting to anticipate all use cases, we specify with forward-compatibility in mind.

### 500 Royalty Token Datum

This specification is largely based on [the royalty specification in Nebula](https://github.com/spacebudz/nebula/tree/main#royalty-info-specification), with a couple key departures:

- The royalty token is recommended to be locked at a script address, rather than stored in the user's wallet. This encourages projects to guarantee royalties won't change by sending their royalties to an always-fails (or similar) script address, but still allows for creative royalty schemes and minimizes disruption to existing projects.

- The policyId of the royalty NFT must match that of the reference NFT. This enables lookups based on the user token in the same way as is done for the tokens specified in the original CIP-68 standard.

### Reference Datum Flag

In addition to providing a way to create guaranteed royalties, this has several advantages:

- Backwards Compatibility - Existing royalty implementations will still work, just not have the same assurances.
- Minimal Storage Requirement - An optional boolean has about the smallest memory impact possible. This is especially important because it's attached to the - Reference NFT and will be set for each individual NFT.
- Intra-Collection Utility - This already allows for minting a collection with some NFTs with royalties and some without. A future version of this standard will likely make use of this field to allow for multiple versions of royalties for even more granular control.

### Backward Compatibility

To keep metadata compatibility with changes coming in the future, we introduce a `version` field in the datum.
## Extending & Modifying this CIP
See the [CIP-0068 Extension Boilerplate](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0068/extension_boilerplate.md)
## Path to Active

### Acceptance Criteria

- [x] This CIP should receive feedback, criticism, and refinement from: CIP Editors and the community of people involved with NFT projects to review any weaknesses or areas of improvement.
- [x] Guidelines and examples of publication of data as well as discovery and validation should be included as part of of criteria for acceptance.
- [x] Minimal reference implementation making use of [Lucid](https://github.com/spacebudz/lucid) (off-chain), [PlutusTx](https://github.com/input-output-hk/plutus) (on-chain): [Reference Implementation](https://github.com/SamDelaney/CIP_102_Reference).
- [ ] Implementation and use demonstrated by the community: NFT Projects, Blockchain Explorers, Wallets, Marketplaces.

### Implementation Plan

- [x] Publish open source reference implementation and instructions related to the creation, storage and reading of royalty utxos.
- [ ] Implement in open source libraries and tooling such as [Lucid](https://github.com/spacebudz/lucid), [Blockfrost](https://github.com/blockfrost/blockfrost-backend-ryo), etc.
- [ ] Achieve additional "buy in" from existing community actors and implementors such as: blockchain explorers, token marketplaces, minting platforms, wallets.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

[CIP-0027]: https://github.com/cardano-foundation/CIPs/tree/master/CIP-0027
[CIP-0067]: https://github.com/cardano-foundation/CIPs/tree/master/CIP-0067
[CIP-0068]: https://github.com/cardano-foundation/CIPs/tree/master/CIP-0068

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0103/README.md
---

- --
CIP: 103
Title: Web-Wallet Bridge - Bulk transaction signing
Category: Wallets
Status: Active
Authors:
- Martín Schere
- Ola Ahlman <ola.ahlman@tastenkunst.io>
Implementors:
- JPG Store <https://www.jpg.store>
- Eternl <https://eternl.io/>
- Typhon <https://typhonwallet.io/>
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/443
- https://github.com/cardano-foundation/CIPs/pull/587
Created: 2023-09-03
License: CC-BY-4.0
- --

## Abstract
This CIP extends [CIP-30 (Cardano dApp-Wallet Web Bridge)](https://cips.cardano.org/cips/cip30/) to provide an additional endpoint for dApp to sign multiple transactions in bulk.

## Motivation: why is this CIP necessary?
Currently, there is no way to sign multiple transactions in bulk, and the experience of signing a chain of transactions is suboptimal. We propose the addition of a signTxs endpoint that enable wallets to create an array of interconnected transactions and sign them all at once.

## Specification

### Data Types
#### TransactionSignatureRequest

```ts
type TransactionSignatureRequest = {|
  cbor: cbor<transaction>,
  partialSign: bool = false,
|};
```

Used to represent a single transaction awaiting a user's signature. More details on {partialSign} can be found in [api.signTx](https://cips.cardano.org/cips/cip30/#apisigntxtxcbortransactionpartialsignboolfalsepromisecbortransactionwitnessset) defined in CIP-30.

### Error Types

#### APIError
See [CIP-30 (Cardano dApp-Wallet Web Bridge) APIError](https://cips.cardano.org/cips/cip30/#apierror)

#### TxSignError
See [CIP-30 (Cardano dApp-Wallet Web Bridge) TxSignError](https://cips.cardano.org/cips/cip30/#txsignerror)

#### TxSendError
See [CIP-30 (Cardano dApp-Wallet Web Bridge) TxSendError](https://cips.cardano.org/cips/cip30/#txsignerror)

### `api.cip103.signTxs(txs: TransactionSignatureRequest[]): Promise<cbor<transaction_witness_set>[]>`

Errors: `APIError`, `TxSignError`

Signs a list of transactions where each transaction can either be independent and/or as a sequence of interconnected transactions where a subsequent transaction depends on a previous one. The returned array of witness sets directly correspond to the elements in the `txs` parameter, aligning the witness set at index 0 with the transaction at index 0, and so forth.

On sign error for any transaction in the array, no witnesses are to be returned. Instead a `TxSignError` is to be thrown. The error message thrown should include a reference to the transaction that caused the sign error, by including the index for failing transaction that map to the input array transaction list.

There are certain things that should be considered by wallets implementing this CIP, namely user visibility of what is signed. Though not explicitly specified in this CIP, as it would be up to the wallet to find a good solution, the wallet should make it clear to the user that multiple transactions are to be signed, and to give a clear overview/summary of what is signed. In addition to visibility, the wallet shall process the input transaction array in the same order as input parameter to allow transactions to be chained by accepting a previous transaction in the array to be used as input in a following transaction.

### `api.cip103.submitTxs(txs: cbor<transaction>[]): Promise<hash32[]>`

Errors: `APIError`, `TxSendError`

Extends [CIP-30 (Cardano dApp-Wallet Web Bridge) submitTx](https://cips.cardano.org/cips/cip30/#apisubmittxtxcbortransactionpromisehash32) with the ability to submit transactions in bulk. Transactions are to be submitted in the same order as provided as inputs. All transactions provided as input should be attempted to be submitted even in case of error. If all transactions are successfully submitted, an array of transaction ids is to be returned. In the case of one or multiple `Refused | Failure`, a `TxSendError | hash32` array is thrown. Each entry in the array represents either a successful submit with `hash32` (transaction id), or in the case of `Refused | Failure`, a `TxSendError` object. In both cases, the response array directly corresponds to the elements in the `txs` parameter.

### Versioning
Once approved and final, the proposal must be superseded by another if changes break the specification. Minor adjustments are allowed if needed to improve readability and resolve uncertainties.

## Rationale: how does this CIP achieve its goals?
Allowing for bulk signing and submission of transactions can greatly improve the user experience by reducing the amount of steps needed to sign more than one transaction. Allowing multiple transactions to be provided in the same API call also reduces the burden for transaction chaining that was previously mainly possible by keeping track of node mempool to know if input utxo's are available to be spent. Submitting multiple transactions would still be possible without `submitTxs` addition using the already defined [CIP-30 (Cardano dApp-Wallet Web Bridge) submitTx](https://cips.cardano.org/cips/cip30/#apisubmittxtxcbortransactionpromisehash32) endpoint by calling it multiple times. However, allowing a bulk submit endpoint speeds up submission when many transactions are to be submitted at once as you wouldn't have to await each individual submission.

## Path to Active

### Acceptance Criteria
In order for this standard to be active, the following should be true:
- [x] Implemented by at least two wallets.
- [x] Adopted and used by at least one dApp or infrastructure tool to prove usability.

### Implementation Plan
N/A

## Copyright
This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).



---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0104/README.md
---

- --
CIP: 104
Title: Web-Wallet Bridge - Account public key
Category: Wallets
Status: Proposed
Authors:
- Ola Ahlman <ola.ahlman@tastenkunst.io>
- Andrew Westberg <andrewwestberg@gmail.com>
Implementors:
- Eternl <https://eternl.io/>
- newm-chain <https://newm.io/>
Discussions:
- https://github.com/cardano-foundation/cips/pulls/588
Created: 2023-09-03
License: CC-BY-4.0
- --

## Abstract
This CIP extends [CIP-30 (Cardano dApp-Wallet Web Bridge)](https://cips.cardano.org/cips/cip30/) to provide an additional endpoint for dApp to get the extended account public key from a connected wallet.

## Motivation: why is this CIP necessary?
Normally it's up to the wallet to handle the logic for utxo selection, derived addresses etc through the established CIP-30 api. Sometimes however, dApp needs greater control due to subpar utxo selection or other specific needs that can only be handled by chain lookup from derived address(es). This moves the control and complexity from wallet to dApp for those dApps that prefer this setup. A dApp has better control and can make a more uniform user experience. By exporting only the account public key, this gives read-only access to the dApp.

## Specification
A new endpoint is added namespaced according to this cip extension number that returns the connected account extended public key as [`cbor<T>`](https://cips.cardano.org/cips/cip30/#cbort) defined in CIP30.

### 1. `api.cip104.getAccountPub(): Promise<cbor<Bip32PublicKey>>`

Errors: APIError

Returns hex-encoded string representing cbor of extended account public key. Throws APIError if needed as defined by CIP30.

Wallets implementing this CIP should, but not enforced, request additional access from the user to access this endpoint as it allows for complete read access to account history and derivation paths.

## Rationale: how does this CIP achieve its goals?
Raw cbor is returned instead of bech32 encoding to follow specification of other CIP30 endpoints.

## Path to Active

### Acceptance Criteria
In order for this standard to be active, the following should be true:
- Implemented by at least two wallets.
- Adopted and used by at least one dApp or infrastructure tool to prove usability.

### Implementation Plan
Communication with additional wallets established to widen availability

## Copyright
This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).


---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0105/README.md
---

- --
CIP: 105
Title: Conway era Key Chains for HD Wallets
Status: Active
Category: Wallets
Authors:
- Ryan Williams <ryan.williams@intersectmbo.org>
Implementors:
- Eternl <https://eternl.io/>
- Lace <https://www.lace.io/>
- Mesh <https://meshjs.dev/>
- NuFi <https://nu.fi/>
- Ryan Williams <ryan.williams@intersectmbo.org>
- Pawel Jakubas <pawel.jakubas-ext@cardanofoundation.org>
- Typhon <https://typhonwallet.io/>
- Vespr <https://vespr.xyz/>
- Yoroi <https://yoroi-wallet.com/>
Discussions:
- https://github.com/cardano-foundation/cips/pulls/597
Created: 2023-09-22
License: CC-BY-4.0
- --

## Abstract

The Conway Ledger era introduces many new features to Cardano, notably features to support community governance via CIP-1694.
This includes the introduction of the new first class credentials; `drep_credential`, `committee_cold_credential` and `committee_hot_credential`.

We propose a HD wallet key derivation paths for registered DReps and constitutional committee members to deterministically derive keys from which credentials can be generated.
Such keys are to be known as DRep keys, constitutional committee cold keys and constitutional committee hot keys.
Here we define some accompanying tooling standards.

> **Note** this proposal assumes knowledge of the Conway ledger design (see
> [draft ledger specification](https://github.com/IntersectMBO/cardano-ledger/blob/d2d37f706b93ae9c63bff0ff3825d349d0bd15df/eras/conway/impl/cddl-files/conway.cddl))
> and
> [CIP-1694](https://github.com/cardano-foundation/CIPs/blob/master/CIP-1694/README.md).

## Motivation: why is this CIP necessary?

In the Conway ledger era, DRep credentials allow registered DReps to be identified on-chain, in DRep registrations, retirements, votes, and in vote delegations from ada holders.
Whilst constitutional committee members can be recognized by their cold credentials within update committee governance actions, authorize hot credential certificate and resign cold key certificates.
Constitutional committee hot credential can be observed within the authorize hot key certificate and votes.

CIP-1694 terms these DRep credentials as DRep IDs, which are either generated from blake2b-224 hash digests of Ed25519 public keys owned by the DRep, or are script hash-based.
Similarly, both the hot and cold credentials for constitutional committee members can be generated from public key digests or script hashes.

This CIP defines a standard way for wallets to derive DRep and constitutional committee keys.

Since it is best practice to use a single cryptographic key for a single purpose, we opt to keep DRep and committee keys separate from other keys in Cardano.

By adding three paths to the [CIP-1852 | HD (Hierarchy for Deterministic) Wallets for Cardano](https://github.com/cardano-foundation/CIPs/blob/master/CIP-1852/README.md), we create an ecosystem standard for wallets to be able to derive DRep and constitutional committee keys.
This enables DRep and constitutional committee credential restorability from a wallet seed phrase.

Stakeholders for this proposal are wallets that follow the CIP-1852 standard and tool makers wishing to support DReps and or constitutional committee members.
This standard allows DReps and constitutional committee members to use alternative wallets whilst being able to be correctly identified.
By defining tooling standards, we enable greater interoperability between governance-focussed tools.

## Specification

### Derivation

#### DRep Keys

Here we describe DRep key derivation as it pertains to Cardano wallets that follow the CIP-1852 standard.

To differentiate DRep keys from other Cardano keys, we define a new `role` index of `3`:

`m / 1852' / 1815' / account' / 3 / address_index`

We strongly recommend that a maximum of one set of DRep keys should be associated with one wallet account, which can be achieved by setting `address_index=0`.

#### DRep ID

Tools and wallets can generate a DRep ID (`drep_credential`) from the Ed25519 public DRep key (without chaincode) by creating a blake2b-224 hash digest of the key.
As this is key-based credential it should be marked as entry `0` in a credential array. DRep Identifier is further specified in [CIP-0129](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0129/README.md).

#### Constitutional Committee Cold Keys

Here we describe constitutional committee cold key derivation as it pertains to Cardano wallets that follow the CIP-1852 standard.

To differentiate constitutional committee cold keys from other Cardano keys, we define a new `role` index of `4`:

`m / 1852' / 1815' / account' / 4 / address_index`

We strongly recommend that a maximum of one set of constitutional committee cold keys should be associated with one wallet account, which can be achieved by setting `address_index=0`.

#### Constitutional Committee Cold Credential

Tools and wallets can generate a constitutional committee cold credential (`committee_cold_credential`) from the Ed25519 public constitutional committee cold key (without chaincode) by creating a blake2b-224 hash digest of the key.
As this is key-based credential it should be marked as entry `0` in a credential array.

#### Constitutional Committee Hot Keys

Here we describe constitutional committee hot key derivation as it pertains to Cardano wallets that follow the CIP-1852 standard.

To differentiate constitutional committee hot keys from other Cardano keys, we define a new `role` index of `5`:

`m / 1852' / 1815' / account' / 5 / address_index`

We strongly recommend that a maximum of one set of constitutional committee hot keys should be associated with one wallet account, which can be achieved by setting `address_index=0`.

#### Constitutional Committee Hot Credential

Tools and wallets can generate a constitutional committee hot credential (`committee_hot_credential`) from the Ed25519 public constitutional committee hot key (without chaincode) by creating a blake2b-224 hash digest of the key.
As this is key-based credential it should be marked as entry `0` in a credential array.

### Bech32 Encoding

These are also described in [CIP-0005 | Common Bech32 Prefixes](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0005/README.md), but we include them here for completeness.

> **Note** we also include the prefixes for script-based credentials in the following subsections, for completeness.

#### DRep Keys

DRep keys and DRep IDs should be encoded in Bech32 with the following prefixes:

| Prefix        | Meaning                                                 | Contents                                                           |
| ------------- | --------------------------------------------------------| ------------------------------------------------------------------ |
| `drep_sk`     | CIP-1852’s DRep signing key                             | Ed25519 private key                                                |
| `drep_vk`     | CIP-1852’s DRep verification key                        | Ed25519 public key                                                 |
| `drep_xsk`    | CIP-1852’s DRep extended signing key                    | Ed25519-bip32 extended private key                                 |
| `drep_xvk`    | CIP-1852’s DRep extended verification key               | Ed25519 public key with chain code                                 |
| `drep`        | [DEPRECATED] DRep verification key hash (DRep ID)       | blake2b\_224 digest of a delegate representative verification key  |
| `drep_vkh`    | Delegate representative verification key hash           | blake2b\_224 digest of a delegate representative verification key  |
| `drep_script` | Delegate representative script hash                     | blake2b\_224 digest of a serialized delegate representative script |

#### Constitutional Committee Cold Keys

Constitutional cold keys and credential should be encoded in Bech32 with the following prefixes:

| Prefix           | Meaning                                                               | Contents                                                               |
| ---------------- | --------------------------------------------------------------------- | ---------------------------------------------------------------------  |
| `cc_cold_sk`     | CIP-1852’s constitutional committee cold signing key                  | Ed25519 private key                                                    |
| `cc_cold_vk`     | CIP-1852’s constitutional committee verification signing key          | Ed25519 private key                                                    |
| `cc_cold_xsk`    | CIP-1852’s constitutional committee cold extended signing key         | Ed25519-bip32 extended private key                                     |
| `cc_cold_xvk`    | CIP-1852’s constitutional committee extended verification signing key | Ed25519 public key with chain code                                     |
| `cc_cold`        | [DEPRECATED] Constitutional committee cold verification key hash (cold credential) | blake2b\_224 digest of a consitutional committee cold verification key |
| `cc_cold_vkh`    | Constitutional committee cold verification key hash (cold credential) | blake2b\_224 digest of a consitutional committee cold verification key |
| `cc_cold_script` | Constitutional committee cold script hash (cold credential)           | blake2b\_224 digest of a serialized constitutional committee cold script |

#### Constitutional Committee Hot Keys

Constitutional hot keys and credential should be encoded in Bech32 with the following prefixes:

| Prefix          | Meaning                                                               | Contents                                                              |
| --------------- | --------------------------------------------------------------------- | --------------------------------------------------------------------- |
| `cc_hot_sk`     | CIP-1852’s constitutional committee hot signing key                   | Ed25519 private key                                                   |
| `cc_hot_vk`     | CIP-1852’s constitutional committee verification signing key          | Ed25519 private key                                                   |
| `cc_hot_xsk`    | CIP-1852’s constitutional committee hot extended signing key          | Ed25519-bip32 extended private key                                    |
| `cc_hot_xvk`    | CIP-1852’s constitutional committee extended verification signing key | Ed25519 public key with chain code                                    |
| `cc_hot`        | [DEPRECATED] Constitutional committee hot verification key hash (hot credential) | blake2b\_224 digest of a consitutional committee hot verification key |
| `cc_hot_vkh`    | Constitutional committee hot verification key hash (hot credential)   | blake2b\_224 digest of a consitutional committee hot verification key |
| `cc_hot_script` | Constitutional committee hot script hash (hot credential)             | blake2b\_224 digest of a serialized constitutional committee hot script |

### Tooling Definitions

#### DRep Keys

Supporting tooling should clearly label these key pairs as "DRep Keys".

Examples of acceptable `keyType`s for supporting tools:

| `keyType`                                   | Description                                       |
| ------------------------------------------- | ------------------------------------------------- |
| `DRepSigningKey_ed25519`                    | Delegate Representative Signing Key               |
| `DRepExtendedSigningKey_ed25519_bip32`      | Delegate Representative Extended Signing Key      |
| `DRepVerificationKey_ed25519`               | Delegate Representative Verification Key          |
| `DRepExtendedVerificationKey_ed25519_bip32` | Delegate Representative Extended Verification Key |

For hardware implementations:

| `keyType`                     | Description                                       |
| ----------------------------- | ------------------------------------------------- |
| `DRepHWSigningFile_ed25519`   | Hardware Delegate Representative Signing File     |
| `DRepVerificationKey_ed25519` | Hardware Delegate Representative Verification Key |

#### Constitutional Committee Cold Keys

Supporting tooling should clearly label these key pairs as "Constitutional Committee Cold Keys".

Examples of acceptable `keyType`s for supporting tools:

| `keyType`                                                          | Description                                             |
| ------------------------------------------------------------------ | ------------------------------------------------------- |
| `ConstitutionalCommitteeColdSigningKey_ed25519`                    | Constitutional Committee Cold Signing Key               |
| `ConstitutionalCommitteeColdExtendedSigningKey_ed25519_bip32`      | Constitutional Committee Cold Extended Signing Key      |
| `ConstitutionalCommitteeColdVerificationKey_ed25519`               | Constitutional Committee Cold Verification Key          |
| `ConstitutionalCommitteeColdExtendedVerificationKey_ed25519_bip32` | Constitutional Committee Cold Extended Verification Key |

For hardware implementations:

| `keyType`                                            | Description                                             |
| ---------------------------------------------------- | ------------------------------------------------------- |
| `ConstitutionalCommitteeColdHWSigningFile_ed25519`   | Hardware Constitutional Committee Cold Signing File     |
| `ConstitutionalCommitteeColdVerificationKey_ed25519` | Hardware Constitutional Committee Cold Verification Key |

#### Constitutional Committee Hot Keys

Supporting tooling should clearly label these key pairs as "Constitutional Committee Hot Keys".

| `keyType`                                                         | Description                                            |
| ----------------------------------------------------------------- | ------------------------------------------------------ |
| `ConstitutionalCommitteeHotSigningKey_ed25519`                    | Constitutional Committee Hot Signing Key               |
| `ConstitutionalCommitteeHotExtendedSigningKey_ed25519_bip32`      | Constitutional Committee Hot Extended Signing Key      |
| `ConstitutionalCommitteeHotVerificationKey_ed25519`               | Constitutional Committee Hot Verification Key          |
| `ConstitutionalCommitteeHotExtendedVerificationKey_ed25519_bip32` | Constitutional Committee Hot Extended Verification Key |

For hardware implementations:

| `keyType`                                           | Description                                            |
| --------------------------------------------------- | ------------------------------------------------------ |
| `ConstitutionalCommitteeHotHWSigningFile_ed25519`   | Hardware Constitutional Committee Hot Signing File     |
| `ConstitutionalCommitteeHotVerificationKey_ed25519` | Hardware Constitutional Committee Hot Verification Key |

### Deprecated Governance ID Definition
The previous governance key IDs defined by this standard have been superseded by the definitions provided in [CIP-0129]. Tools implementing this standard are encouraged to consider adopting [CIP-0129]. Tools that already support [CIP-0129] maintain backward compatibility with the legacy formats specified below but should consider fully transitioning to [CIP-0129] to standardize key formats across the ecosystem. This will help avoid multiple formats and ensure consistency.

This CIP previously also lacked `_vkh` key definitions, which are now added above possible due to the upgrades defined in [CIP-0129]. For detailed information on the new specification and the rationale behind the upgrade, please refer to [CIP-0129].

#### DRep Keys

| Prefix          | Meaning                                                               | Contents                                                              |
| --------------- | --------------------------------------------------------------------- | --------------------------------------------------------------------- |
| `drep`        | Delegate representative verification key hash (DRep ID) | blake2b\_224 digest of a delegate representative verification key  |
| `drep_script` | Delegate representative script hash (DRep ID)           | blake2b\_224 digest of a serialized delegate representative script |

#### Constitutional Committee Cold Keys

| Prefix          | Meaning                                                               | Contents                                                              |
| --------------- | --------------------------------------------------------------------- | --------------------------------------------------------------------- |
| `cc_cold`        | Constitutional committee cold verification key hash (cold credential) | blake2b\_224 digest of a consitutional committee cold verification key   |
| `cc_cold_script` | Constitutional committee cold script hash (cold credential)           | blake2b\_224 digest of a serialized constitutional committee cold script |

#### Constitutional Committee Hot Keys

| Prefix          | Meaning                                                               | Contents                                                              |
| --------------- | --------------------------------------------------------------------- | --------------------------------------------------------------------- |
| `cc_hot`        | Constitutional committee hot verification key hash (hot credential) | blake2b\_224 digest of a consitutional committee hot verification key   |
| `cc_hot_script` | Constitutional committee hot script hash (hot credential)           | blake2b\_224 digest of a serialized constitutional committee hot script |

### Versioning

This CIP is not to be versioned using a traditional scheme, rather if any large technical changes are required then a new proposal must replace this one.
Small changes can be made if they are completely backwards compatible with implementations, but this should be avoided.

## Rationale: how does this CIP achieve its goals?

### Derivation

By standardizing derivation, naming, and tooling conventions we primarily aim to enable wallet interoperability.
By having a standard to generate DRep and constitutional committee credentials from mnemonics, we allow wallets to always be able to discover a user’s governance activities.

#### Why add a new roles to the 1852 path?

This approach mirrors how stake keys were rolled out, see [CIP-0011 | Staking key chain for HD wallets](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0011/README.md).
We deem this necessary since these credentials sit alongside each other in the Conway ledger design.

The alternative would be to define a completely different derivation paths, using a different index in the purpose field, similar to the specification outlined within [CIP-0036](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0036/README.md#derivation-path), but this could introduce complications with HW wallet implementations.

#### Why not multi-DRep/CC wallet accounts?

We believe the overhead that would be introduced by multi-DRep accounts or multi-constitutional-committee is an unjustified expense.
Future iterations of this specification may expand on this, but at present this is seen as unnecessary.
This avoids the need for DRep, cc hot or cc cold key discovery.

We model this on how stake keys are generally handled by wallets.
If required, another CIP could, of course, introduce a multi-DRep/CC method.

### Encoding

#### Why not allow network tags?

For simplicity, we have omitted network tags within the encoding.
This is because we have modeled DRep IDs and CC credentials on stake pool operator IDs, which similarly do not include a network tag.

The advantage of including a network tag would be to reduce the likelihood of mislabelling a DRep’s network of operation (eg Preview v Cardano mainnet).

### Test vectors

See [Test Vectors File](./test-vectors.md).

## Path to Active

### Acceptance Criteria

- [x] The DRep derivation path is used by three wallet/tooling implementations.
- [Nufi](https://assets.nu.fi/extension/sanchonet/nufi-cwe-sanchonet-latest.zip)
- [Lace](https://chromewebstore.google.com/detail/lace-sanchonet/djcdfchkaijggdjokfomholkalbffgil?hl=en)
- [Yoroi](https://chrome.google.com/webstore/detail/yoroi-nightly/poonlenmfdfbjfeeballhiibknlknepo/related)
- [demos wallet](https://github.com/Ryun1/cip95-demos-wallet)
- [x] The constitutional committee derivation paths are used by two implementations.
- [csl-examples](https://github.com/Ryun1/csl-examples/)
- [cardano-addresses](https://github.com/IntersectMBO/cardano-addresses/)

### Implementation Plan

- [x] Author to provide an example implementation inside a HD wallet.
- [csl-examples/cip-1852-keys.js](https://github.com/Ryun1/csl-examples/blob/main/examples/CIP-1852/cip-1852-keys.js)

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

[CIP-0129]: (https://github.com/cardano-foundation/CIPs/blob/master/CIP-0129/README.md)
[DEPRECATED]: #deprecated-governance-id-definition

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0106/README.md
---

- --
CIP: 106
Title: Web-Wallet Bridge - Multisig wallets
Status: Proposed
Category: Wallets
Authors:
- Leo
Implementors:
- BroClanWallet
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/617
Created: 2023-10-12
License: CC-BY-4.0
- --

## Abstract

This document describes a CIP-30 extension allowing webpages (i.e. dApps) to interface with Cardano Multisig-wallets. This document is a work in progress and is not yet finalized. It is expected to be updated as the ecosystem evolves.

## Motivation: why is this CIP necessary?

In order to facilitate future dApp development, we will need a way for dApps to communicate with multisig wallets, given the unique complexities of native script based addresses. Special provisions need to be made to make the connector compatible with them.

Specifically, apps building transactions need to be able to get the following information from the wallet:
- Script descriptor
- Any transaction consuming a UTXO from a Plutus-based address must attach the corresponding script.
- `ScriptRequirements`
- The `TxContext` that is required to be able to validate the transaction. It encompasses all the possible combinations of requirements for the transaction to be valid, as such it is represented by an array of `ScriptRequirement` objects.
- Change Datum
- The datum that will be used as the change output for the transaction. This is required for wallets based on Plutus V2 and before, as the change output must contain a datum to be valid and spendable.

Additionally, apps need to be able to submit a transaction to the wallet for signing in an asynchronous manner, as gathering of signatures can take a long time and each wallet provider will have its own way of handling this process.

Finally, the signTx() and signData() endpoints will have to be disabled when using this extension since they are not compatible with native script based addresses.

## Specification

### Data Types

#### KeyHash

A hex-encoded string of the corresponding bytes. This represents the hash of the public key used to sign transactions.

```ts
type KeyHash = String
```

#### ScriptRequirements

```ts
type ScriptRequirementsCode = {
    Signer: 1,
    Before: 2,
    After: 3,
}
type ScriptRequirement = {
    code: ScriptRequirementsCode,
    value: KeyHash|number,
}
```

### Aditional Error Types

#### CompletedTxError

```ts
CompletedTxErrorCode = {
	NotFound: 1,
	NotReady: 2
}
```

- NotFound - The transaction with the given id was not found.
- NotReady - The transaction with the given id is not ready yet.

### Additional API Endpoints

#### api.cip106.getCollateralAddress(): Promise\<Address>

For Plutus V2 and later, partial collateral is supported. This function returns an address that can be used to add collateral to a transaction. The address returned must be owned by one of the signers in the list of signers returned by `api.getScriptRequirements()`.

dApp developers can choose to use this address to add collateral to a transaction, or they can choose to use the `api.getCollateral()` function to get a list of UTXOs that can be used as collateral. If the dApp chooses to use this address, they must ensure that the address is not used for any other purpose, as the wallet may be using it to track collateral, and that the collateral return address is the same one.

#### api.cip106.getScriptRequirements: Promise\<ScriptRequirement[]>

Errors: `APIError`

Returns a list of ScriptRequirements that will be used to validate any transaction sent to the wallet.

#### api.cip106.getScript(): Promise\<cbor\<nativeScript>>

Errors: `APIError`

Returns the CBOR-encoded native script that controls this wallet.

#### api.cip106.submitUnsignedTx(tx: cbor\<unsignedTransaction>): Promise\<hash32>

Errors: `APIError`, `TxError`

Submits a transaction to the wallet for signing. The wallet should check that the transaction is valid, gather the required signatures, compose the finalized transaction, and submit the transaction to the network. If the transaction is valid and the wallet is able to sign it, the wallet should return the transaction hash. If the transaction is invalid or the wallet is unable to sign it, the wallet should throw a `TxError` with the appropriate error code. The wallet should not submit the transaction to the network if it is invalid or the wallet is unable to sign it.

If the transaction contains hidden metadata, the wallet should not submit the transaction when it is ready, but return it to the dApp when the dApp calls the `getCompletedTx` function.

#### api.cip106.getCompletedTx(txId: hash32): Promise\[<cbor\<transaction>,cbor<transaction_witness_set>>]

Errors: `APIError`, `CompletedTxError`

If the transaction is not ready, the wallet should throw a `CompletedTxError` with the appropriate error code. If the transaction is ready, the wallet should return the CBOR-encoded transaction and the signatures.

### Altered API endpoints

#### api.getCollateral(params: { amount: cbor\<Coin> }): Promise\<TransactionUnspentOutput[] | null>

Native script based addresses cannot provide collateral for transactions. Using this function, dApps can request the wallet to provide collateral for a transaction. The collateral must be a pure ADA UTXO, held by one of the signers in the list of signers returned by `api.getScriptRequirements()`.

### Disabled API endpoints

When connecting to a wallet using this extension the following endpoints will be disabled:

#### `api.signTx(tx: cbor<transaction>, partialSign: bool = false): Promise<cbor<transaction_witness_set>>`


#### `api.signData(addr: Address, payload: Bytes): Promise<DataSignature>`

These endpoints should return an error if called when using this extension.

## Rationale: how does this CIP achieve its goals?

See justification and explanations provided with each API endpoint.


## Path to Active

### Acceptance Criteria

- [ ] The interface is implemented and supported by multiple wallet providers.
- [ ] The interface is used by multiple dApps to interact with wallet providers.

### Implementation Plan

- [x] Provide some reference implementation of wallet providers
- [leo42/BroClanWallet](https://github.com/leo42/BroClanWallet)

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0107/README.md
---

- --
CIP: 107
Title: URI Scheme - Block and transaction objects
Status: Proposed
Category: Tools
Authors:
- Pi Lanningham <pi@sundaeswap.finance>
Implementors: N/A
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/556
Created: 2020-09-22
License: CC-BY-4.0
- --

## Abstract

This extends CIP-13, which describes a Cardano URI scheme, with several more objects that would be useful to be able to assign addresses to, in particular blocks, transactions, transaction metadata, and transaction outputs.

## Motivation: why is this CIP necessary?

CIP-13 defined two initial URL authorities, for payment links and delegating to a stakepool.

However, in a number of contexts, it would be useful to canonically link to other Cardano objects, such as:
- Providing links to a transaction, to be opened in a wallet or chain explorer of the users choice
- To provide richly interconnected metadata, such as in [CIP-100](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0100)

Without a canonical standard for how to define these URIs, these objects are either unaddressable, not machine-interpretable, or will suffer from a divergence of convention. Similarly, we seek to fit an existing structure, such as CIP-0013, to reduce the number of different conventions that need be supported by the ecosystem.

## Specification

We extend CIP-13 with 2 new authorities for referencing blocks and transactions.

Examples:
```
web+cardano://block?hash=c6a8976125193dfae11551c5e80a217403d953c08ebbd69bba904d990854011f
web+cardano://block?height=12345678890
web+cardano://transaction/7704a68404facf7126fa356f1b09f0e4c552aeef454cd0daba4208f3a64372e9
web+cardano://transaction/7704a68404facf7126fa356f1b09f0e4c552aeef454cd0daba4208f3a64372e9#1
web+cardano://transaction/7704a68404facf7126fa356f1b09f0e4c552aeef454cd0daba4208f3a64372e9/metadata
web+cardano://transaction/d3616b772c91f118346e74863d1722810a4858e4d7cc7663dc2eed345d7bca72/metadata/674
web+cardano://transaction/self/metadata/1694
```

### Grammar & interpretation

We extend the grammar from CIP-0013 with two new authorities:

```
authorityref = (... | blockref | transactionref)
```

#### Block queries

A link referencing a block by either block hash or height.

> WARNING: Referencing blocks by height is provided in the case of extremely tight space requirements, but referencing a recent block (within the rollback horizon of the chain) is unstable.  Due to chain re-orgs, it may refer to different content for different users, or at different times. This should only be used when this is not critical, or for blocks outside of the rollback horizon.

```
blockref = "//block" query
query = ( "?block=" block_hash | "?height=" block_height)

block_hash = 64HEXDIG
block_height = *digit
```

#### Transaction queries

A link referencing *this* transaction, another transaction, transaction output, the full transaction metadata, or specific tag within the transaction metadata.

```
transactionref = "//transaction/" (tx_id | utxo_id | tx_metadata | tx_metadata_tag)

tx_id = "self" | 64HEXDIG
utxo_id = tx_id "#" *digit
tx_metadata = tx_id "/" metadata
tx_metadata_tag = tx_id "/" metadata "/" *digit
```

> NOTE: tx_id can be set to "self", which is useful for making self-referential metadata, before the transaction ID is known.  For example, in CIP-100, you may want to store governance metadata on the transaction which casts the vote. The transaciton hash is not yet known, and so the `anchor` field cannot link to the transaction by transaction ID.

For grammar reference, see:

- [Wikipedia > Augmented Backus–Naur form](https://en.wikipedia.org/wiki/Augmented_Backus%E2%80%93Naur_form)
- [RFC 2234: Augmented BNF for Syntax Specifications: ABNF](https://datatracker.ietf.org/doc/html/rfc2234)
- [Unicode in ABNF](https://tools.ietf.org/html/draft-seantek-unicode-in-abnf-00)

## Rationale: how does this CIP achieve its goals?

This CIP defines a canonical format for URIs referencing four new Cardano objects: blocks, transactions, transaction metadata, and specific tags within the transaction metadata. It utilizes existing cardano standards (CIP-0013) and industry standards (URIs), minimizing the number of new concepts that a developer needs to learn. By utilizing URIs, it creates a natural path to integration with existing tools, such as browsers. And finally, it allows a canonical URI for these objects, such as storing CIP-100 metadata on-chain, and referring to it in the anchor field.
## Path to Active

### Acceptance Criteria

- [ ] CIP-100 is standardized utilizing these URI schemes for on-chain references.
- [ ] At least one governance tool utilizes these URI schemes
- [ ] At least one explorer or wallet utilizes these URI schemes

### Implementation Plan

The current community sentiment towards adopting this in CIP-100 is high (it was the original inspiration).

Advocacy and education about this format should be performed by:

- Implementors of CIP-100 and governance metadata tooling
- Wallet and Explorer developers

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0108/README.md
---

- --
CIP: 108
Title: Governance Metadata - Governance Actions
Category: Metadata
Status: Proposed
Authors:
- Ryan Williams <ryan.williams@intersectmbo.org>
Implementors: [ ]
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/632
- https://github.com/cardano-foundation/CIPs/pull/748
- https://github.com/cardano-foundation/CIPs/issues/757
Created: 2023-11-23
License: CC-BY-4.0
- --

## Abstract
The Conway ledger era ushers in on-chain governance for Cardano via [CIP-1694 | A First Step Towards On-Chain Decentralized Governance](https://github.com/cardano-foundation/CIPs/blob/master/CIP-1694/README.md), with the addition of many new on-chain governance artifacts.
Some of these artifacts support the linking off-chain metadata, as a way to provide context.

The [CIP-100 | Governance Metadata](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0100) standard provides a base framework for how all off-chain governance metadata should be formed and handled.
But this is intentionally limited in scope, so that it can be expanded upon by more specific subsequent CIPs.

This proposal aims to provide a specification for off-chain metadata vocabulary that can be used to give context to governance actions.
Without a sufficiently detailed standard for governance actions we introduce the possibility to undermine voters ability to adequately assess governance actions.
Furthermore a lack of such standards risks preventing interoperability between tools, to the detriment of user experiences.

For the many contributors to this proposal, see [Acknowledgements](#acknowledgements).

## Motivation: why is this CIP necessary?
Blockchains are poor choices to act as content databases.
This is why governance metadata anchors were chosen to provide a way to attach long form metadata content to on-chain events.
By only supplying an onchain hash of the off-chain we ensure correctness of data whilst minimizing the amount of data posted to the chain.

### For voters
When observing from the chain level, tooling can only see the content of the governance action and it's anchor.
These on-chain components do not give give any context to the motivation nor off-chain discussion of an governance action.
Although this information would likely be desired context for voters.
By providing rich contextual metadata we enable voters to make well informed decisions.

### For all participants
By standardizing off-chain metadata formats we facilitate interoperability for tooling which creates and/or renders metadata attached to governance actions.
This intern promotes a rich user experience between tooling.
This is good for all governance participants.

## Specification
Although there are seven types of governance action defined via CIP-1694, we focus this proposal on defining core properties which must be attached to all types.
We leave room for future standards to refine and specialize further to cater more specific for each type of governance action.

### New `witness` Type
Here we extend the potential witnesses, with a `witnessAlgorithm` that can be set to include support for [CIP-08 | Message Signing](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0008) standard, indicated by `CIP-0008`.
Here we mimic the restrictions as imposed over the CIP-30's implementation in [.signData()](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0030#apisigndataaddr-address-payload-bytes-promisedatasignature).

### Markdown Text Styling
This standard introduces the possibility of using [Github markdown text styling](https://docs.github.com/en/get-started/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax#styling-text) within fields.

### Extended Vocabulary
The following properties extend the potential vocabulary of [CIP-100](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0100)'s `body` property.

#### `title`
- A very short freefrom text field. Limited to `80` characters.
- This SHOULD NOT support markdown text styling.
- Authors SHOULD use this field to succinctly describe the governance action and its motivation.
- Authors SHOULD attempt to make this field unique whilst also avoiding hyperbolic language.
- i.e; Increase K protocol parameter to `100,000` to increase decentralization of Cardano.

#### `abstract`
- A short freefrom text field. Limited to `2500` characters.
- This SHOULD support markdown text styling.
- Authors SHOULD use this field to expand upon their `title` by describing the contents of the governance action, its motivation and rationale.

#### `motivation`
- A freeform text field.
- This SHOULD support markdown text styling.
- This SHOULD be used by the author to encapsulate all context around the problem that is being solved by the on-chain action.
- This SHOULD be used to outline the related stakeholders and use cases.

#### `rationale`
- A freeform text field.
- This SHOULD support markdown text styling.
- This SHOULD be used by the author to discuss how the content of the governance action addresses the problem outlined within the `motivation`.
- This field SHOULD justify the changes being made to Cardano.
- i.e "by decreasing X parameter by Y we increase Ada earned by SPOs, thus incentivising more people to become SPOs, leading to a more diverse network"
- This SHOULD provide evidence of consensus within the community and discuss significant objections or concerns raised during the discussion.
- This SHOULD include discussion of alternative solutions and related topics/ governance actions.
- This SHOULD include any recommendations made by relevant organizations or committees.

#### `references`
- We extend CIP-100's references field.
- This SHOULD NOT support markdown text styling.
- To be an OPTIONAL set of objects, using the `@set` property.
- Each object MUST have a `label` field to describe the reference, such as; "blog - Why we must continue to fund Catalyst".
- Each object MUST have a `uri` field.
- Each object MAY have a OPTIONAL `referenceHash` object.
- Each object MUST have a `hashDigest` field.
- Each object MUST have a `hashAlgorithm` field, which is inherited from CIP-100.
- This should be used by the author to link related or supporting work via the URI, and reference this via the index within their freefrom text fields.

### Application
Governance action metadata must include all compulsory fields to be considered CIP-0108 compliant.
As this is an extension to CIP-100, all CIP-100 fields can be included within CIP-108 compliant metadata.

### Test Vector
See [test-vector.md](./test-vector.md) for examples.

### Versioning
This proposal should not be versioned, to update this standard a new CIP should be proposed.
Although through the JSON-LD mechanism further CIPs can add to the common governance metadata vocabulary,

## Rationale: how does this CIP achieve its goals?
We intentionally have kept this proposal brief and uncomplicated.
This was to reduce the time to develop and deploy this standard.
We think it is better to have a base standard which can be improved, rather than meticulously craft a perfect single standard.
This way we enable tooling which depends on this standard to start development.
Furthermore, it is very difficult to predict future wants/needs now, so by allowing upgrades we build in the ability to improve the standard as new wants/needs arrive.

The fields which have been chosen for this standard heavily inspired to those used for CIPs.
We did this for two reasons; familiarity and competency.
Those who are involved in Cardano are familiar with the CIP format, meaning they will be intuitively understand these fields being reused here.
These fields in combination have also been fairly battle tested via the CIPs process and thus act as a good standard to describe problems and their solutions.

### New `witness` type
We introduce a new witness type to be able to reuse existing dApp-Wallet infrastructure.
The type as described allows for reuse of the CIP-30 signData standard, which means that governance dApps are able to implement this without needing any alterations to existing wallets.

### Markdown Text Styling
We choose to introduce rich text standard here because we see significant value in supporting it.
Rich text styling improves the ability for the author to express themselves.
Furthermore, most potential voters are use to such standards when reviewing metadata.

### Character Limits
With this design, we wanted to allow for quick and easy differentiation between governance actions.
We achieve this by facilitating users "layers of investigation", where some fields are limited in size.
This encourages tooling providers to show users the small fields before allowing deep investigation of the larger fields.
By allowing this we aim to improve the experience of prospective voters, when sorting though many governance actions.

The downside of highlighting some fields over others is that we incentivize hyperbolic and eye catching phrases.
Where authors want their governance action to standout in tooling so use overly dramatic phrasing.
This creates an environment where there is a race to the bottom on voter's attention.
Overall this could decrease the perceived legitimacy of the system.
The counter argument is that tooling providers should not use metadata to solely highlight proposals, rather other means such as cryptographically verified submitters.

### `title`
This should be used by voters to quickly and easily differentiate between two governance actions which may be having the same or similar on-chain effects.
This is why we have chosen a short character limit, as longer titles would reduce the ability for quick reading.

### `abstract`
This gives voters one step more detail beyond the `title`.
This allows for a compact description of the what, why and how without the voter having to read the larger fields.

### `motivation`
The `motivation` is a chance for the author to fully describe the problem that is being solved by the governance action.
This is important as all governance actions are a solution to a problem and thus this is a universal field.
By showing relation to stakeholders the author is able to show that they have performed adequate research on the problem.
Voters can use this field to determine if the problem is sizable enough to warrant voting on.

### `rationale`
This field gives the author the opportunity to explain how the onchain action is addressing the problem outlined in the motivation.
This gives the author a place to discuss any alternative designs or completing governance actions.
Voters should be able to use this field to evaluate the applicability of the solution to the problem.

### `references`
References give the author ability to point to supporting research or related work.
These should be used by voters to verify the content of supporting research.
The inclusion of a hash allows for the supporting documentation to be cryptographically verified.

### Open Questions
- <s>Should fields be optional or compulsory?</s>
- Title, abstract, motivation and rationale should be compulsory as they should be very important to the ability
- <s>How much vocabulary can be extended to other onchain governance events?</s>
- It is hard to predict how the scope of future standards before they have been developed.
- <s>How to integrate custom set of HTML tags? to allow formatting of longer text fields.</s>
- Since CIP-100 does not intend to support rich text fields such an inclusion would not fit, so we have included such format here.

## Path to Active

### Acceptance Criteria
- [X] This standard is supported by two different tooling providers used to submit governance actions to chain.
- [cardano-signer](https://github.com/gitmachtl/cardano-signer?tab=readme-ov-file#cip-100--cip-108--cip-119-mode) : A tool to sign with author secret keys, verify signatures, canonize the body content (Linux/Arm/Win/Mac)
- [spo-scripts](https://github.com/gitmachtl/scripts) : A collection of scripts to interact with governance. Create actions, vote on actions incl. signature verification
- [cardano-governance-metadata-lib](https://github.com/SundaeSwap-finance/cardano-governance-metadata) : A rust library for interacting with Cardano Governance Metadata conforming to CIP-100 (rust)

- [ ] This standard is supported by two different chain indexing tools, used to read and render metadata.
- DB-Sync via [db-sync-sancho-4.1.0](https://github.com/IntersectMBO/cardano-db-sync/releases/tag/sancho-4.1.0)

### Implementation Plan
Solicitation of feedback
- [x] Run two online workshops to gather insights from stakeholders.
- [x] Seek community answers on all [Open Questions](#open-questions).
Implementation
- [x] Author to provide example metadata and schema files.


## Acknowledgments

<details>
  <summary><strong>Governance Metadata Working Group - Workshop #1 2023-12-04</strong></summary>

  I would like to thank those that contributed to the Governance Metadata Working Group Workshop #1 hosted by Ryan Williams ([see presentation slides with notes](https://docs.google.com/presentation/d/18OK3vXexCc8ZXq-dC00RDPPKcy2Zu4DiMo8PeIZ47_4/)).

  Thank you to the co-hosts:
- Adam Dean
- Thomas Upfield

  Thank you to the participants:
- Carlos Lopez de Lara
- Igor Veličković
- Johnny Kelly
- Kenric Nelson
- Kevin Hammond
- Lorenzo Bruno
- Mike Susko
- Rhys Morgan
- Eric Alton
- Samuel Leathers
- Vladimir Kalnitsky

</details>

<details>
  <summary><strong>Governance Metadata Working Group - Workshop #2 2023-12-14</strong></summary>

  I would like to thank those that contributed to the Governance Metadata Working Group Workshop #2 hosted by Ryan Williams ([see presentation slides with notes](https://docs.google.com/presentation/d/1tFsyQnONjwyTm7zKrnxxedzWsoonO6-8vXw5vYzB3qs)).

  Thank you to the co-host:
- Adam Dean

  Thank you to the participants:
- Mark Byers
- Nils Codes

  Thank you to the bots that joined also.

</details>

## Copyright
This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0109/README.md
---

- --
CIP: 109
Title: Modular Exponentiation Built-in for Plutus Core
Status: Proposed
Category: Plutus
Authors:
- Kenneth MacKenzie <kenneth.mackenzie@iohk.io>
- Michael Peyton Jones <michael.peyton-jones@iohk.io>
- Iñigo Querejeta-Azurmendi <inigo.querejeta@iohk.io>
- Thomas Vellekoop <thomas.vellekoop@iohk.io>
Implementors:
- Michael Peyton Jones <michael.peyton-jones@iohk.io>
- Thomas Vellekoop <thomas.vellekoop@iohk.io>
Created: 2023-10-05
License: CC-BY-4.0
- --
## Abstract
This CIP proposes an extension of the current plutus functions to provide support for the efficient calculation of modular exponentiation with inverses.

## Motivation: why is this CIP necessary?
Modular exponentiation is a cornerstone operation in numerous cryptographic protocols. The availability of such a function directly within Plutus will provide a more efficient and reliable means to perform this crucial computation. Therefore, the integration of such a Plutus core built-in is imperative to enhance cryptographic functionalities within the ecosystem.

More concretely, the key area where this function would contribute is that of finite field arithmetic, which is a basis for elliptic curves. In this context, a finite field is a set of integers modulo a prime number `p`. On this set, we have the basic operations of addition, multiplication, additive inversion (negation) and the multiplicative inversion (reciprocal), all reduced modulo the prime number `p`.

With the current built-in functions, most of these operations can be implemented relatively cheaply, except the reciprocal. This function can be implemented via either the Extended Euclidean algorithm or using Fermat's little theorem. Using the preliminary cost-model for plutus V3, both implementations still [consume](https://github.com/perturbing/mod-exp-bench/tree/558b6a47cb18d063b6a7324a15087f87fa3da673) around ~5% and ~9% of the CPU budget on mainnet. These benchmarks are performed for the scalar field of the BLS12-381 curve, which has a 255 bit prime modulus.

The result is that doing such computation in a practical setting on-chain is costly, and other methods should be used to find this multiplicative inverse. A cumbersome method for solving this issue, is calculating the inverse off-chain and bringing it in scope via a redeemer as a claimed inverse of another element. This method works, as one can cheaply check that the claimed inverse is indeed the unique inverse with one multiplication, one modulo reduction and one equality check with the multiplicative identity. In math notation this means that for a field element `a`, we off-chain compute `claimed_inverse_a`, such that on-chain we can check: `claimed_inverse_a * a = id`.

The drawback of this technique is that by transferring the calculation off-chain, we incur additional fees due to the increased size of the transaction, as CPU units are cheaper. The creator of the transaction is also burdened by this computation, which often precedes more intricate, non-trivial cryptographic calculations. In the application of zero knowledge, this calculation if often implemented in low-level code, and used via bindings, meaning that extracting these intermediary values breaks existing code and interfaces.

In conclusion, integrating modular exponentiation as a core built-in within Plutus is not only essential for enhancing cryptographic capabilities, but also for optimizing on-chain computations. The current approach, which offloads certain calculations like finding multiplicative inverses to off-chain processes, is inefficient and costly in terms of transaction size and computational burden on the transaction creators. Incorporating this function directly into Plutus will streamline these operations, reduce transaction costs, and maintain the integrity of existing tools, thereby significantly advancing the Plutus ecosystem's functionality and user experience.

A nonexclusive list of cryptographic protocols that use a field and would benefit from having this built-in are:

1. The verification of pairing based zero-knowledge proofs over BLS12-381. This pairing curve has a base field and a scalar field, with primes of size 381 and 255 bits respectively. In, for example, the proof system plonk, a verifier performs one reciprocal for each public input in the 255 bit scalar field.
2. Onchain public key aggregation for Schnorr over SECP256k1: effectively, this aggregation is the point addition of the two keys on the curve, which requires one reciprocal in the SECP256k1 base field (using a 256 bit prime).
3. A more interoperable interface for the BLS-12-381 built-ins: currently, the BLS-12-381 built-ins only expose a compressed version of a point, containing the `x` coordinate and some marked bits to describe how one can find the corresponding `y` in the base field. This calculation of finding `y` requires modular exponentiation in the field.

## Specification
Modular exponentiation is mathematically defined as the equivalence relation

$$
a \equiv b^{e} (\bmod{m})
$$

Here we have that the base $b$ is an integer, the exponent $e$ a non-negative integer and the modulus $m$ a positive integer. To be more specific, we want $e$ to be larger or equal to zero, and $m$ is larger than zero. This is because what we are effectively doing, is multiplying $e$ copies of $b$ and a modulo reduction by $m$. In this context, multiplying a negative number of copies of $b$ has no definition.


$$
\begin{equation}
a \equiv \underbrace{(b \times b \times ... \times b)}_{e \textrm{ times}} (\bmod{m} )
\end{equation}
$$

That said, in the context of multiplicative inversion, a negative exponent can be interpreted as taking the exponent of the inverse of the base number. That is

$$
\begin{equation}
b^{-e} (\bmod{m}) := (b^{-1})^{e} (\bmod{m})
\end{equation}
$$

We propose to also include this extension to the plutus built-in, for optimized inversion when this is possible. This is inversion is not guaranteed to exist for all numbers $b$, only when the modulus $m$ is a prime number this is guaranteed. We also propose that the built-in fails if this inverse does not exist, and if a modulus is provided that is smaller than one.

### Function definition
With the above, we define a new Plutus built-in function with the following type signature

```hs
modularExponentiation :: Integer -> Integer -> Integer -> Integer
```
here the first argument is the base, the second the exponent and the third the modulus. As mentioned above, the behavior of this function is that it fails if the modulus is not a positive integer, or if the inverse of the base does not exist for a negative exponent. For the lower level implementation, we propose the usage of the `integerPowMod` function in the `ghc-bignum` packages. This function has the desired functionality, is optimized, and is easy to integrate in the plutus stack.

### Cost model
The computational impact of modular exponentiation is complexified by it having three arguments. That said, observe that the integers used can always be bound by the modulus. Preliminary [benchmarks](https://github.com/perturbing/expFast-bench/tree/cca69b842050de9523493d52c20384bc50c80b22) on the time consumption of this `integerPowMod` function show that it can be costed constant in the size of its first argument (the base) and linear in the other two.

## Rationale: how does this CIP achieve its goals?
Integrating this function directly into Plutus will streamline cryptographic operations, reduce transaction costs, and uphold the integrity of existing cryptographic interfaces. It addresses current inefficiencies and enhances the cryptographic capabilities of the Plutus platform.

For completeness and an historic perspective, the above functionality can also be attained by a new built-in function that performs normal exponentiation, after which one can reduce with the already present built-in function `ModInteger`. In the creation of this CIP, this possibility was discussed but put aside. This method has the flaw that the intermediate value of these integers is not bound. Meaning that memory consumption is not efficient for practical use in this setting.

## Path to Active

### Acceptance Criteria

We consider the following criteria to be essential for acceptance:

- [ ] The PR for this functionality is merged in the Plutus repository.
- [ ] This PR must include tests, demonstrating that it behaves as the specification requires in this CIP.
- [ ] A benchmarked use case is implemented in the Plutus repository, demonstrating that realistic use of this primitive does, in fact, provide major cost savings.

### Implementation Plan

- [x] IOG Plutus team consulted and accept the proposal.
- [x] Author to provide preliminary benchmarks of time consumption.
- https://github.com/perturbing/expFast-bench/tree/cca69b842050de9523493d52c20384bc50c80b22

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0110/README.md
---

- --
CIP: 110
Title: Plutus v1 Script References
Status: Active
Category: Plutus
Authors:
- Pi Lanningham <pi@sundaeswap.finance>
Implementors:
- Alexey Kuleshevich <alexey.kuleshevich@iohk.io>
Discussions:
- https://twitter.com/SmaugPool/status/1737454984147390905
- https://twitter.com/Quantumplation/status/1737704936089985339
- https://twitter.com/SmaugPool/status/1737814894710231161
- https://github.com/IntersectMBO/cardano-ledger/issues/3965
Created: 2023-12-20
License: CC-BY-4.0
- --

## Abstract

Despite making up less than half the transactions on Cardano, Plutus v1 scripts occupy around 40% of the total block space since chain inception, and sometimes higher during periods of peak activity. Increasing the space available to blocks is risky, as it impacts the block propagation time. This proposal puts forth a simple way to reduce this strain.

## Motivation: why is this CIP necessary?

Plutus v2 introduced a way to publish scripts on-chain, and *reference* those scripts to satisfy the witness requirement. However, because this was done via a new field on the transaction (i.e. "Reference Inputs"), which shows up in the script context, this feature is not backwards compatible with Plutus v1.

However, of the 151gb it takes to represent the 6 year history of the chain, roughly 60 gb of that (nearly 40%) can be attributed to the wasted space from repeating the same scripts in the last 2 years. This analysis was further confirmed by IOG in [this](https://github.com/IntersectMBO/cardano-ledger/issues/3965) issue.

It would be one thing if this was an issue mostly felt before or shortly after the Babbage hard-fork. It was assumed at the time that Plutus v2 would become a dominant player, and that usage of Plutus v1 contracts would die off. However, looking at recent trends in late 2023, it's clear this isn't happening.

Looking at recent trends [through late 2023](https://twitter.com/SmaugPool/status/1737454984147390905/photo/1), considering all scripts included in transactions, Plutus v1 hovered at or slightly below 50% of all scripts in transactions. However, comparing the size of transactions which execute scripts scripts, [Plutus v1 scripts make up 90% of that space](https://twitter.com/SmaugPool/status/1737454984147390905/photo/2).

Looking at periods of saturation can also help us understand where the limits of the chain are. Periods where activity is low and the chain is underutilized can skew our view of the problem: it largely doesn't matter to end users what percentage of space is occupied by different script versions, because the user experience is largely unimpacted and transactions are able to fit into the next block regardless. However, in periods of high activity, when blocks are nearly full, we get a better picture of where user activity is allocating that space, and where the cost savings would be most beneficial. For example, [epoch 455](https://twitter.com/SmaugPool/status/1737814898648691195) saw nearly 100% block usage for a full epoch, of which an average of 48% (and sometimes as high as 75%) of the space was occupied by scripts, presumably much of that Plutus v1 scripts.

This problem isn't going away: while protocols may migrate to new Plutus v2 or v3 scripts, these old protocols will exist forever. Liquidity locked in these scripts, sometimes permanently, will mean that there is always an arbitrage opportunity that incentivizes a large portion of the block to be occupied by continually republishing these v1 scripts.

Additionally, raising the block size is considered incredibly sensitive, as it impacts block propagation times.

A simple, backwards compatible mechanism for plutus v1 protocols to satisfy the script witness requirement, without changing the script context and causing breaking changes for Plutus v1 scripts, would alleviate quite literally millions of dollars worth of storage requirements, user pain, and developer frustration.

## Specification

We propose relaxing the ledger rule that fails Plutus v1 scripts in transactions that have reference inputs, and to construct a script context that excludes reference inputs.

The ledger rule shouldn't change in other ways: for example, Plutus v1 scripts should still fail in the presence of inline datums or reference scripts on spent transaction inputs.

## Rationale: how does this CIP achieve its goals?

The main concern with this relates to backwards compatibility. The ledger makes very strong commitments regarding the behavior of scripts: any observable change represents a risk that there is some script out there that will either be unspendable when it should be, or spendable when it should not be.

Because of this, any such change which violates this must satisfy a burden of proof with regards to both the benefits and the risks. This was in fact considered [in the original CIP](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0031#how-should-we-present-the-information-to-scripts), and at the time, it was decided that the justification would likely not meet that high bar.

Thus, as a rationale for this CIP, we repeat that analysis, hopefully with a different conclusion.

In terms of benefit, this approach would immediately allow all major plutus v1 dApps to reduce their transaction sizes dramatically. Some napkin math for both Sundae and Minswap shows that this would cut around 85% of the transaction size for each transaction; Considering the portion of block space currently taken up by Plutus v1 scripts, this represents a significant savings.

It is hard to overstate the long-term positive impact that this change could have for real users of the Cardano blockchain.

In terms of the risks, there are four main risks to consider:

- Funds that should be spendable are suddenly not spendable;

   In this case, a user could simply continue to use the existing witness mechanism to provide the scripts, and those funds become spendable again.

- Funds that should not be spendable are suddenly spendable;

   In this case, it is very hard to imagine a scenario where this would be true that isn't crafted intentionally. It would have to be some script that was dependent on the transaction fee being above a certain threshold, which is already a dangerous assumption to make given the updatable protocol parameters. In other instances (such as the change to how the minimum UTXO output is calculated) this kind of risk hasn't been an obstacle.

- The execution units change, without changing the outcome, resulting in a different cost for the user;

   In this case, the cost would only go down, and it is again hard to imagine a scenario where this is at material risk of violating some protocols integrity in a way that is not already compromised.

Given the parallel plans to include reference scripts in the cost of the transaction, outlined [here](https://github.com/IntersectMBO/cardano-ledger/issues/3952), further mitigates these concerns.

## Path to Active

### Acceptance Criteria

- [x] Review of this proposal by the relevant subject matter experts
- [x] Implement the change in the cardano-ledger and cardano-node repositories
- [x] Include this change in a relevant hard fork
- Included within the Chang #1 hardfork

### Implementation Plan

- [x] Update the formal Agda specification
- [x] Implement [minFeeRefScriptCoinsPerByte] or similar approach, as described [here](https://github.com/IntersectMBO/cardano-ledger/issues/3952)
- [x] Update the implementation [here](https://github.com/IntersectMBO/cardano-ledger/blob/fdc366df654fc02b1668012342732d41eaa099fe/eras/babbage/impl/src/Cardano/Ledger/Babbage/TxInfo.hs#L94-L97)
- [x] Update property based tests to cover these scenarios

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0112/README.md
---

- --
CIP: 112
Title: Observe Script Type
Status: Proposed
Category: Plutus
Authors:
- Philip DiSarro <info@anastasialabs.com>
Implementors: []
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/418/
Created: 2024-01-08
License: CC-BY-4.0
- --

## Abstract
We propose to introduce a new Plutus scripts type `Observe` in addition to those currently available (spending, certifying, rewarding, minting, drep). The purpose of this script type is to allow arbitrary validation logic to be decoupled from any ledger action.
Since observe validators are decoupled from actions, you can run them in a transaction without needing to perform any associated action (ie you don't need to consume a script input, or mint a token, or withdraw from a staking script just to execute this validator).
Additionally, we propose to introduce a new assertion to native scripts that they can use to check that a particular script hash is in `required_observers` (which in turn enforces that the script must be executed successfully in the transaction). This addresses a number of technical issues discussed in other CIPs and CPS such as the redundant execution of spending scripts, and the inability to effectively use native scripts in conjunction with Plutus scripts.

## Motivation: why is this CIP necessary?
Often in a plutus validator you want to check "a particular (different) Plutus script checked this transaction", but it's annoying (and wasteful) to have to have to lock an output in a script and then check if that output is consumed, or mint a token, or whatever else just to trigger script validation.

Currently the main design pattern used to achieve this is a very obscure trick involving staking validators and the fact that you can withdraw 0 from a staking validator to trigger the script validation. A summary of the trick is:
Implement all the intended validation logic in a Plutus staking validator, we will call this validator `s_v`. To check that this validator was executed in the transaction you check if the credential of `s_v` (`StakingCredential`) is present in `txInfoWdrl`, this guarantees that `s_v` was checked in validation.
This relies on the fact that unlike in `txInfoMint` the ledger does not filter out 0 amount entries in `txInfoWdrl`. This means that you are allowed to build transactions that withdraw zero from a staking credential which in-turn triggers the staking script associated with that credential to execute in the transaction,
which makes it available in `txInfoWdrl`. This is a enables a very efficient design pattern for checking logic that is shared across multiple scripts.

For instance, a common design pattern is a token based forwarding validator in which the validator defers its logic to another validator by checking that a state token is present in one of the transaction inputs:
```haskell
forwardNFTValidator :: AssetClass -> BuiltinData -> BuiltinData -> ScriptContext -> ()
forwardNFTValidator stateToken _ _ ctx = assetClassValueOf stateToken (valueSpent (txInfo ctx)) == 1
```
This pattern is common in protocols that use the batcher architecture. Some protocols improve on the pattern by including the index of the input with the state token in the redeemer:
```haskell
forwardNFTValidator :: AssetClass -> BuiltinData -> Integer -> ScriptContext -> ()
forwardNFTValidator stateToken _ tkIdx ctx =  assetClassValueOf stateToken (txInInfoResolved (elemAt tkIdx (txInfoInputs (txInfo ctx)))) == 1

forwardMintPolicy:: AssetClass -> Integer -> ScriptContext -> ()
forwardMintPolicy stateToken tkIdx ctx =  assetClassValueOf stateToken (txInInfoResolved (elemAt tkIdx (txInfoInputs (txInfo ctx)))) == 1
```
The time complexity of this validator is **O(n)** where n is the number of tx inputs. This logic is executed once per input being unlocked  / currency symbol being minted.
The redundant execution of searching the inputs for a token is the largest throughput bottleneck for these DApps; it is **O(n*m)** where n is the number of inputs and m is the number of `forwardValidator` inputs + `forwardValidator` minting policies.
Using the stake validator trick, the time complexity of the forwarding logic is improved to **O(1)**. The forwardValidator logic becomes:
```haskell
forwardWithStakeTrick:: StakingCredential -> BuiltinData -> BuiltinData -> ScriptContext -> ()
forwardWithStakeTrick obsScriptCred tkIdx ctx = fst (head stakeCertPairs) == obsScriptCred
  where
    info = txInfo ctx
    stakeCertPairs = AssocMap.toList (txInfoWdrl info)

stakeValidatorWithSharedLogic :: AssetClass -> BuiltinData -> ScriptContext -> ()
stakeValidatorWithSharedLogic stateToken _rdmr ctx = assetClassValueOf stateToken (valueSpent (txInfo ctx)) == 1
```
For the staking validator trick (demonstrated above), we are simply checking that the StakingCredential of the the staking validator containing the shared validation logic is in the first pair in `txInfoWdrl`. If the StakingCredential is present in `txInfoWdrl`, that means the staking validator (with our shared validation logic) successfully executed in the transaction. This script is **O(1)** in the case where you limit it to one shared logic validator (staking validator), or if you don't want to break composability with other staking validator,
then it becomes **O(obs_N)** where `obs_N` is the number of Observe validators that are executed in the transaction as you have to verify that the StakingCredential is present in `txInfoWdrl`.

The proposed changes in this CIP enable this design pattern to exist indepedently from implementation details of stake validators and withdrawals, and improve efficiency and readability for validators that implement it. Furthermore, with the proposed extension to native scripts, we are able to completely get rid of the redundant spending script executions like so:
```haskell
observationValidator ::  AssetClass -> BuiltinData -> ScriptContext -> ()
observationValidator stateToken _redeemer ctx = assetClassValueOf stateToken (valueSpent (txInfo ctx)) == 1
```
We simply include the script hash of the above `observationValidator` in the `required_observers` field in the transaction body and we lock
all the UTxOs that we would like to share the same spending condition into the following native script:
```json
{
  "type": "observer",
  "keyHash": "OUR_OBSERVATION_SCRIPT_HASH"
}
```
The above solution (enabled by this CIP) is more clear, concise, flexible and efficient than the alternatives discussed above.

## Specification
The type signature of this script type will be consistent with the type signature of minting and staking validators, namely:
```haskell
Redeemer -> ScriptContext -> ()
```

The type signature of the newly introduced `Purpose` will be:
```haskell
Observe Integer -- ^ where integer is the index into the observations list.
```

### Script context

Scripts are passed information about transactions via the script context.
We propose to augment the script context to include information about the observation scripts that are executed in the transaction.

Changing the script context will require a new Plutus language version in the ledger to support the new interface.
The change is: a new field is added to the script context which represents the list of observers that must be present in the transaction.

The interface for old versions of the language will not be changed.
Scripts with old versions cannot be spent in transactions that include observation scripts, attempting to do so will be a phase 1 transaction validation failure.

A new field will be introduced into the script context:

```haskell
- - | TxInfo for PlutusV3
data TxInfo = TxInfo
  { txInfoInputs                :: [V2.TxInInfo]
  , txInfoReferenceInputs       :: [V2.TxInInfo]
  , txInfoOutputs               :: [V2.TxOut]
  , txInfoFee                   :: V2.Value
  , txInfoMint                  :: V2.Value
  , txInfoTxCerts               :: [TxCert]
  , txInfoWdrl                  :: Map V2.Credential Haskell.Integer
  , txInfoValidRange            :: V2.POSIXTimeRange
  , txInfoSignatories           :: [V2.PubKeyHash]
  , txInfoRedeemers             :: Map ScriptPurpose V2.Redeemer
  , txInfoData                  :: Map V2.DatumHash V2.Datum
  , txInfoId                    :: V2.TxId
  , txInfoVotingProcedures      :: Map Voter (Map GovernanceActionId VotingProcedure)
  , txInfoProposalProcedures    :: [ProposalProcedure]
  , txInfoCurrentTreasuryAmount :: Haskell.Maybe V2.Value
  , txInfoTreasuryDonation      :: Haskell.Maybe V2.Value
  , txInfoObservations          :: [V2.Credential] -- ^ newly introduced list of observation scripts that executed in this tx.
  }
```

### CDDL

The CDDL for transaction body will change as follows to reflect the new field.
```
transaction_body =
  { 0 : set<transaction_input>             ; inputs
  , 1 : [* transaction_output]
  , 2 : coin                               ; fee
  , ? 3 : uint                             ; time to live
  , ? 4 : certificates
  , ? 5 : withdrawals
  , ? 7 : auxiliary_data_hash
  , ? 8 : uint                             ; validity interval start
  , ? 9 : mint
  , ? 11 : script_data_hash
  , ? 13 : nonempty_set<transaction_input> ; collateral inputs
  , ? 14 : required_observers              ; Upgraded `required_signers`
  , ? 15 : network_id
  , ? 16 : transaction_output              ; collateral return
  , ? 17 : coin                            ; total collateral
  , ? 18 : nonempty_set<transaction_input> ; reference inputs
  , ? 19 : voting_procedures               ; Voting procedures
  , ? 20 : proposal_procedures             ; Proposal procedures
  , ? 21 : coin                            ; current treasury value
  , ? 22 : positive_coin                   ; donation
  }
; addr_keyhash variant is included for backwards compatibility and will be
; deprecated in the future era, because `credential` already contains `addr_keyhash`.
required_observers = nonempty_set<credential / addr_keyhash>
```

We rename the `required_signers` field to `required_observers`, promoting it from a list of public key hashes to a list of credentials (i.e. either a KeyHash or ScriptHash). This is consistent with other parts of the transaction that are unlocked by a script or a key witness. `required_observers` (field 14) is a set of credentials that must be satisfied by the transaction. For public key credentials, if the corresponding signature is not in the witness set, the transaction will fail in phase 1. For script credentials, if the associated scripts is not present in the witness set or as a reference script and executed in the transaction, the transaction will fail in phase 1 validation. This way Plutus scripts can check the script context to know which observation scripts were executed in the transaction. Similarly, since native script conditions use the `required_observers` field, it is natural that they are now able to require that other scripts observed the transaction (an extension of the ability to check for the presence of key signatures).

### Native Script Extension

The BNF notation for the abstract syntax of native scripts change as follows to reflect the new field.

```BNF
<native_script> ::=
             <RequireSignature>  <vkeyhash>
           | <RequireObserver>   <scripthash>
           | <RequireTimeBefore> <slotno>
           | <RequireTimeAfter>  <slotno>

           | <RequireAllOf>      <native_script>*
           | <RequireAnyOf>      <native_script>*
           | <RequireMOf>        <num> <native_script>*
```

Native scripts are typically represented in JSON syntax. We propose the following JSON representation for the `RequireObserver` constructor:
```JSON
{
  "type": "observer",
  "keyHash": "OBSERVATION_SCRIPT_HASH"
}
```


## Rationale: how does this CIP achieve its goals?

Currently Plutus scripts (and native scripts) in a transaction will only execute when the transaction performs the associated ledger action (ie. a Plutus minting policy will only execute if the transaction mints or burns tokens with matching currency symbol). The only exception is the withdraw zero trick which relies on an obscure mechanic where zero amount withdrawals are not filtered by the ledger. Now using `required_observers` we can specify a list of scripts (supports both native and Plutus scripts) to be executed in the transaction independent of any ledger actions. The newly introduced `txInfoObservations` field in the script context provides a straightforward way for scripts to check that "a particular script validated this transaction".

This change is not backwards-compatible and will need to go into a new Plutus language version.

### Alternatives

- We could decide to accept the withdraw-zero staking script trick as an adequate solution, and just preserve the nonsensical withdraw zero case in future language versions.
- The staking script trick could be abstracted away from the developer by smart contract languages that compile to UPLC.
- This can be dangerous since by distancing the developer from what is actually happening you open up the door for the developer to act on misguided assumptions.

## Path to Active

### Acceptance Criteria
- [ ] These rules included within a official Plutus version, and released via a major hard fork.

### Implementation Plan
- [ ] Passes all requirements of both Plutus and Ledger teams as agreed to improve Plutus script efficiency and usability.

## Copyright
This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

[CC-BY-4.0]: https://creativecommons.org/licenses/by/4.0/legalcode
[Apache-2.0]: http://www.apache.org/licenses/LICENSE-2.0

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0114/README.md
---

- --
CIP: 114
Title: CBOR Tags Registry
Status: Proposed
Category: Tools
Authors:
- Steven Johnson <steven.johnson@iohk.io>
Implementors: N/A
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/752
Created: 2020-01-25
License: CC-BY-4.0
- --

## Abstract

Cardano typically uses CBOR to encode structured data.
For example, the Block data and individual transactions are all CBOR encoded.

CIPs define many of the individual data structures used by both the Ledger and Metadata attached to transactions.
This CIP defines a set of policies for the creation and maintenance of new CBOR tags for Cardano Data structures.
It also collates a list of all currently defined Tags for easy reference.
Finally, it recommends best practice for registering those Tags with IANA.

## Motivation: why is this CIP necessary?

This CIP is motivated by the problems outlined in [CPS-0014].

## Specification

[CBOR] defines a schemaless method of encoding data.
Part of that specification (3.4) defines how generic data can be tagged to assist in the proper interpretation of the encoded data.
Currently all Cardano data structures in both the Ledger and metadata do not widely use Tags.
Nor is there any standardized way to create a new tag for a Cardano data structure.

[IANA] maintains a registry of all [CBOR] Tags.
Anyone can submit a request to register a tag in the range of 32768-18446744073709551615.
These are known as *First Come First Served* tags.
To register a tag, an applicant must demonstrate where the Tag is defined, and also the format of the data.
If the data structure is defined by a CIP, the natural place to define its Tag is also in a CIP.

### Defining Tags

Tags can be defined for any data structure defined by a CIP or commonly used by Cardano.
Examples of commonly used data structures could be those encoded inside the Cardano block or transaction.
Tags should be defined by the following process:

1. If a [CBOR] tag is desired for an application, first the applicant MUST check that one does not already exist.
   If it does, then the pre-existing TAG must be used, as well as the defined canonical encoding.
   This is to prevent redundant and wasteful re-tagging of the same data structures.
2. Otherwise the application may define a [CBOR] Tag for the necessary data structures as follows:
1. The CIP which defines the data structure can also define a Tag and its encoding in [CBOR]. **(Preferred)**
2. Alternatively, if the data structure is already defined by a CIP, then there are two possibilities.
1. The original CIP is edited to add Tags and their canonical representation in [CBOR].
2. A supplementary CIP can be made that simply defines the TAG and canonical representation.
        Such a CIP would reference the original CIP to define the data structure itself.
3. CIP authors would select an unused [CBOR] Tag in the [IANA Registry][IANA], and assign it to their data structure.
1. The CIP should add to its "Path to Active" that the selected Tag/s have been successfully registered with [IANA].
         This allows the CIP to be edited with the final Tag once [IANA] have accepted and published the registration.
4. CIP Authors would include in their PR an addition to the Cardano [CBOR Tag Registry](#tag-registry).

### Registration of TAGS with IANA

Registration of the Tag with IANA is the sole responsibility of the CIP author.
CIPs should not proceed to Active if they define unregistered Tags on data structures.
This is to prevent abuse of the Tag number space.

- NOTE: Tags defined by any CIP or marked as not registered with IANA in the `registry.json` MUST NOT be used outside of testing.
Tags not registered with IANA are subject to change during the IANA registration process.*

### Usage of Tags within a CIP

CIPs, such as metadata CIPs can freely define when and how tags are used with any CBOR data structure.
For example, a CIP may require that a particular field is always tagged.
They may also define that a field need not be tagged if it is the typical data structure.
Only less common or unusual data structures would be tagged in this case.
A CIP may also define that Tags are not used, and only the canonical encoding.

### Canonical Encoding

When a Tag is defined, its Canonical encoding must also be defined.
This is to ensure that all data that is tagged is encoded in a uniform manor.
Even if a CIP does NOT use a tag, it should preferably use the canonical encoding for the data structure.
This is to prevent fragmentation and confusion amongst compliant encoders and decoders of the various data structures.

If a pre-existing data structure is being tagged, then its most common current encoding should be used as the Canonical encoding.
It should not be re-defined.

For example `ED25519-BIP32` public keys are commonly encoded as a byte array.
The array is 32 bytes long with the most significant byte of the key appearing first in the array.
If a Tag is defined for this public key, it should simply define the Tag.
Its Canonical encoding MUST follow this common encoding scheme.

If a CIP does not refer to a Tag, nor the Canonical encoding specification for the data structure,
AND it does not define an alternative encoding.
Then the application implementing the encoding should assume it is encoded canonically.
This helps ensure backward compatibility with pre-existing CIPs where Tags are not used.

### Tag Registry

Similar to [CIP-0010] this CIP defines a registry of all known tags.
The format of the registry is defined by the json schema: [CIP Tag Registry Schema].
New entries MUST be added to the [CIP Tag Registry] in a PR for a CIP that first defines a new CBOR Tag.
They MUST be updated when the Tag is accepted or rejected by IANA.
The registry clearly notes if the tag is currently known to be registered or not.
If a Tag is not yet registered then any implementor must be aware that its possible the Tag number could change and is not final.
Unregistered tags MUST not be used in any main net on-chain metadata or data structures.
They should only be used for testing purposes until registration is complete.

## Rationale: how does this CIP achieve its goals?

Creating a [registry][CIP Tag Registry] for `CIP Tag` values has the following benefit:

1) It makes it easy for developers to know which `CIP Tags` to use, and if they have been registered or not.
2) It makes it easy to avoid collisions with other standards that use CIP Tags.
3) It helps CIP authors to find appropriate CIP tags for their use case, or to define new tags.

The process for defining and registering Tags should help provide clarity about how a CIP tag can be defined.
It also provides clarity on the responsibility CIP authors have to register the tags they create.
If a CIP author is not prepared to take on that responsibility they should not create a Tag.

## Path to Active

### Acceptance Criteria

- [ ] [CPS-0014] is accepted.
- [ ] At least 1 CIPs are accepted into the Tag registry for historical tags.
- [ ] At least 1 CIPs are accepted into the Tag registry for new tags.
- [ ] At least 3 CIPs of any kind are accepted into the Tag registry.

### Implementation Plan

- [ ] Author to write the first CBOR tag CIP.

## References

- [CPS-0014 -  Register of CBOR Tags for Cardano Data structures][CPS-0014]
- [RFC8949 - Concise Binary Object Representation (CBOR)][CBOR]
- [IANA CBOR Tag Registry][IANA]

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode)

[CPS-0014]: https://github.com/cardano-foundation/CIPs/tree/master/CPS-0014
[CBOR]: https://datatracker.ietf.org/doc/rfc8949/
[IANA]: https://www.iana.org/assignments/cbor-tags/cbor-tags.xhtml
[CIP Tag Registry Schema]: ./registry.schema.json
[CIP Tag Registry]: ./registry.json

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0115/README.md
---

- --
CIP: 115
Title: CBOR tag definition - ED25519-BIP32 Keys
Category: Tools
Status: Proposed
Authors:
- Steven Johnson <steven.johnson@iohk.io>
Implementors: N/A
Discussions:
- https://github.com/cardano-foundation/cips/pulls/753
Created: 2024-01-19
License: CC-BY-4.0
- --

## Abstract

[CIP-0003] defines [ED25519-BIP32] Keys, Key derivation and a signature scheme.
This CIP defines CBOR Tags and formalizes the Canonical encoding of data structures for [CIP-0003].
The intention is to have these tags registered in the [IANA CBOR Tag Registry].

## Motivation: why is this CIP necessary?

Project Catalyst is in the process of defining new CBOR data structures.
We needed a way to reliably disambiguate different 32 byte strings.
Rather than making a non-standard encoding scheme specific to our structures we would like to use standard [CBOR] Tags.

This CIP is informed by [CPS-0014] and [CIP-0114].

Without this Tag definition, a metadata CIP which uses [ED25519-BIP32] public keys:

- Is likely to just encode public keys as a byte string of 32 bytes; and
- Needs to redundantly define how the keys are encoded in the byte string.
- May encode these keys differently to another CIP, which can lead to confusion and potential error.

[BIP32] also defines `secp256k1` keys which are also 32 bytes long.
This CIP would help disambiguate between these keys and inform the decoder which key is being utilized.

## Specification

| Type | [CBOR] Tag | Type | Size | [IANA CBOR Tag Registry] |
| -- | -- | -- | -- | -- |
| [ED25519-BIP32 Private Key](#ed25519-bip32-private-key) | 32771 | bstr | 32 | To submit |
| [ED25519-BIP32 Extended Private Key](#ed25519-bip32-extended-private-key) | 32772 | bstr | 64 | To submit |
| [ED25519-BIP32 Public Key](#ed25519-bip32-public-key) | 32773 | bstr | 32 | To submit |
| [ED25519-BIP32 Signature](#ed25519-bip32-signature) | 32774 | bstr | 64 | To submit |

- NOTE: These tags are preliminary and subject to change until IANA registration is complete.
They MUST not be used outside of testing purposes.
They MUST not be used in any data intended to be posted to main-net.*

### ED25519-BIP32 Private Key

This key is defined in [ED25519-BIP32].

This is encoded as a byte string of size 32 bytes.

#### [CDDL]

```cddl
ed25519_private_key = #6.32771(bstr .size 32)
```

Data for the key inside the byte string is encoded in [network byte order].

### ED25519-BIP32 Extended Private Key

This key is defined in [ED25519-BIP32].

This is encoded as a byte string of size 64 bytes.

#### [CDDL]

```cddl
ed25519_extended_private_key = #6.32772(bstr .size 64)
```

Data for the key inside the byte string is encoded in [network byte order].

### ED25519-BIP32 Public Key

This key is defined in [ED25519-BIP32].

This is encoded as a byte string of size 32 bytes.

#### [CDDL]

```cddl
ed25519_public_key = #6.32773(bstr .size 32)
```

Data for the key inside the byte string is encoded in [network byte order].

### ED25519-BIP32 Signature

[ED25519-BIP32] defines how signatures can be generated on data from private keys.
These signatures are defined to be 64 bytes long.

Signatures are encoded as a byte string of size 64 bytes.

#### [CDDL]

```cddl
ed25519_bip32_signature = #6.32774(bstr .size 64)
```

Data for the signature inside the byte string is encoded in [network byte order].

## Rationale: how does this CIP achieve its goals?

By defining concrete CBOR tags, it is possible for metadata to unambiguously mark the kind of data encoded.
This is conformant with the intent of Tags in [CBOR], and aligns with [CIP-CBOR-TAGS].

An official published spec is required to register these Tags with [IANA][IANA CBOR Tag Registry].
This document also serves that purpose.

## Path to Active

### Acceptance Criteria

- [ ] These tags to be included in [CIP-CBOR-TAGS].
- [ ] One downstream CIP uses at least one of the tags defined in this CIP.
- [ ] IANA register all the tags as defined herein.

### Implementation Plan

- [ ] Tags are to be used by Project Catalyst for CBOR data structure definitions.
- [ ] Project Catalyst will also make the application to [IANA][IANA CBOR Tag Registry] to register the Tags.

## References

- [CPS-0014 -  Register of CBOR Tags for Cardano Data structures][CPS-0014]
- [CIP-0003 - Wallet Key Generation][CIP-0003]
- [CIP-0114 - CBOR Tags Registry][CIP-0114]
- [RFC8949 - Concise Binary Object Representation (CBOR)][CBOR]
- [RFC8610 - Concise Data Definition Language (CDDL)][CDDL]
- [RFC1700 Data Notations (Network Byte Order)][network byte order]
- [BIP32 - Hierarchical Deterministic Wallets][BIP32]
- [ED25519-BIP32 - Hierarchical Deterministic Keys over a Non-linear Keyspace][ED25519-BIP32]
- [IANA CBOR Tag Registry]

## Copyright

This CIP is licensed under [CC-BY-4.0]

Code samples and reference material are licensed under [Apache 2.0]

[CC-BY-4.0]: https://creativecommons.org/licenses/by/4.0/legalcode
[Apache 2.0]: https://www.apache.org/licenses/LICENSE-2.0.html
[CBOR]: https://www.rfc-editor.org/rfc/rfc8949.html
[CDDL]: https://www.rfc-editor.org/rfc/rfc8610
[CIP-0003]: https://cips.cardano.org/cip/CIP-0003
[BIP32]: https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki
[IANA CBOR Tag Registry]: https://www.iana.org/assignments/cbor-tags/cbor-tags.xhtml
[network byte order]: https://datatracker.ietf.org/doc/html/rfc1700
[ED25519-BIP32]: https://github.com/input-output-hk/adrestia/raw/bdf00e4e7791d610d273d227be877bc6dd0dbcfb/user-guide/static/Ed25519_BIP.pdf
[CPS-0014]: https://github.com/cardano-foundation/CIPs/tree/master/CPS-0014
[CIP-0114]: https://github.com/cardano-foundation/CIPs/tree/master/CIP-0114

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0116/README.md
---

- --
CIP: 116
Title: Standard JSON encoding for Domain Types
Category: Tools
Status: Proposed
Authors:
- Vladimir Kalnitsky <klntsky@gmail.com>
Implementors:
- Vladimir Kalnitsky <klntsky@gmail.com>
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/742
- https://github.com/cardano-foundation/CIPs/pull/766
Created: 2024-02-22
License: CC-BY-4.0
- --

## Abstract

Canonical JSON encoding for Cardano domain types lets the ecosystem converge on a single way of serializing data to JSON, thus freeing the developers from repeating roughly the same, but slightly different encoding/decoding logic over and over.

## Motivation: why is this CIP necessary?

Cardano domain types have canonical CDDL definitions (for every era), but when it comes to use in web apps, where JSON is the universally accepted format, there is no definite standard. This CIP aims to change that.

The full motivation text is provided in [CPS-11 | Universal JSON Encoding for Domain Types](https://github.com/cardano-foundation/CIPs/tree/master/CPS-0011).

## Specification

This CIP is expected to contain multiple json-schema definitions for Cardano Eras and breaking intra-era hardforks starting from Babbage.

| Ledger era | Hardfork | Ledger Commit | Schema | Changelog Entry |
| --- | --- | --- | --- |--- |
| Babbage | Vasil | [12dc779](https://github.com/IntersectMBO/cardano-ledger/blob/12dc779d7975cbeb69c7c18c1565964a90f50920/eras/babbage/impl/cddl-files/babbage.cddl) | [cardano-babbage.json](./cardano-babbage.json) | N/A |

### Tests & utilities for JSON validation

[`cip-0116-tests`](https://github.com/mlabs-haskell/cip-0116-tests) repo contains utility functions and a test suite for the schema. In particular, there's a `mkValidatorForType` function that builds a validator function for any type defined in the schema.

### Scope of the Schema

The schemas should cover `Block` type and all of its structural components, which corresponds to the scope of CDDL files located in [the ledger repo](https://github.com/IntersectMBO/cardano-ledger/).

### Schema Design Principles

Below you can find some principles outlining the process of schema creation / modification. They are intended to be applied when there is a need to create a schema for a new Cardano era.

#### Uniqueness of encoding

Every transaction (i.e. CBOR-encoded binary) must have exactly one valid JSON encoding, up to entry ordering in mappings (that are represented as key-value pairs).

For a single JSON fixture, however, there are multiple variants of encoding it as CBOR.

#### Consistency with the previous versions

To simplify transitions of dApps between eras, the scope of changes introduced to the schemas SHOULD be limited to the scope of CDDL changes.

### Schema Conventions

These conventions help to keep the schema uniform in style.

#### Encoding of binary types

Binary data MUST be encoded as lower-case hexademical strings. Restricting the character set to lower-case letters (`a-f`) allows for comparisons and equality checks without the need to normalize the values to a uniform case.

#### Encoding of mapping types

`Map`-like container types MUST be encoded as arrays of key-value pairs.

```json
"Map": {
    "type": "array",
    "items": {
        "type": "object",
        "properties": {
            "key": ...,
            "value": ...
        },
        "required": [
          "key",
          "value"
        ],
        "additionalProperties": false
    }
}
```

Uniqueness of `"key"` objects in a map MUST be preserved (but this property is not expressible via a schema).

Implementations MUST consider mappings with conflicting keys invalid.

Some mapping-like types, specifically `Mint`, allow for duplicate keys. Types like these should not be encoded as maps, instead, `key` and `value` properties should be named differently.

#### Encoding of variant types

Encoding types with variable payloads MUST be done with the use of `oneOf` and an explicit discriminator property: `tag`:

```json
{
    "Credential": {
      "type": "object",
      "discriminator": {
        "propertyName": "tag"
      },
      "oneOf": [
        {
          "type": "object",
          "properties": {
            "tag": {
              "enum": [
                "pubkey_hash"
              ]
            },
            "value": {
              "$ref": "cardano-babbage.json#/definitions/Ed25519KeyHash"
            }
          },
          "required": ["tag", "value"],
          "additionalProperties": false
        },
        {
          "type": "object",
          "properties": {
            "tag": {
              "enum": [
                "script_hash"
              ]
            },
            "value": {
              "$ref": "cardano-babbage.json#/definitions/ScriptHash"
            }
          },
          "required": ["tag", "value"],
          "additionalProperties": false
        }
      ]
    }
}
```

Other properties of a tagged object MUST be specified in lower-case snake-case.

#### Encoding of enum types

Enums are a special kind of variant types that carry no payloads. These MUST be encoded as string `enum`s.

Lowercase snake case identifiers MUST be used for the options, e.g.:

```json
{
    "Language": {
      "title": "Language",
      "type": "string",
      "enum": [
        "plutus_v1",
        "plutus_v2"
      ]
    }
}
```

#### Encoding of record types

All record types MUST be encoded as objects with explicit list of `required` properties, and `additionalProperties` set to `false` (see "absence of extensibility" chapter for the motivation behind this suggestion).

#### Encoding of nominal type synonyms

Some of the types have identical representations, differing only by nominal name. For example, `Slot` domain type is expressed as `uint` in CDDL.

For these types, their nominal name SHOULD NOT have a separate definition in the json-schema, and the "representation type" should be used via a `$ref` instead. The domain type name SHOULD be included as `title` string at the point of usage.

### Additional format types

Some non-standard `format` types are used:

- `hex` - lower-case hex-encoded byte string
- `bech32` - [bech32](https://en.bitcoin.it/wiki/Bech32) string
- `base58` - [base58](https://bitcoinwiki.org/wiki/base58) string
- `uint64` - 64-bit unsigned integer
- `int128` - 128-bit signed integer
- `string64` - a unicode string that must not exceed 64 bytes when utf8-encoded.
- `posint64` - a positive (0 excluded) 64-bit integer. `1 .. 2^64-1`

### Limitations

JSON-schema does not allow to express certain properties of some of the types.

#### Uniqueness of mapping keys

See the chapter on encoding of mapping types.

#### Bech32 and Base58 formats

Validity of values of these types can't be expressed as a regular expression, so the implementations MAY validate them separately.

#### Address types

Bech32 strings are not always valid addresses: even if the prefixes are correct, the [binary layout of the payload](https://github.com/IntersectMBO/cardano-ledger/blob/f754084675a1decceed4f309814b09605f443dd5/libs/cardano-ledger-core/src/Cardano/Ledger/Address.hs#L603) must also be valid.

The implementations MAY validate it separately.

#### Byte length limits for strings

In CDDL, the length of a `tstr` value gives the number of bytes, but in `json-schema` there is no way to specify restrictions on byte lengths. So, `maxLength` is not the correct way of specifying the limits, but it is still useful, because no string longer than 64 *characters* satisfies the 64-byte limit.

#### Auxiliary Data encoding

`auxiliary_data` CDDL type is handled specially.

```cddl
auxiliary_data =
  metadata ; Shelley
  / [ transaction_metadata: metadata ; Shelley-ma
    , auxiliary_scripts: [ * native_script ]
    ]
  / #6.259({ ? 0 => metadata         ; Alonzo and beyond
      , ? 1 => [ * native_script ]
      , ? 2 => [ * plutus_v1_script ]
      , ? 3 => [ * plutus_v2_script ]
      })
```

Instead of providing all three variants of encoding, we base the schema on the one that is the most general (the last one):

```json
{
    "AuxiliaryData": {
      "properties": {
        "metadata": {
          "$ref": "cardano-babbage.json#/definitions/TransactionMetadata"
        },
        "native_scripts": {
          "type": "array",
          "items": {
            "$ref": "cardano-babbage.json#/definitions/NativeScript"
          }
        },
        "plutus_scripts": {
          "type": "array",
          "items": {
            "$ref": "cardano-babbage.json#/definitions/PlutusScript"
          }
        }
      },
    }
}
```

It is up to implementors to decide how to serialize the values into CBOR. The property we want to maintain is preserved regardless of the choice: for every block binary there is exactly one JSON encoding.

### Versioning

This CIP should not follow a conventional versioning scheme, rather it should be altered via pull request before a hardforks to add new a JSON schema to align with new ledger ers. Each schema must be standalone and not reuse definitions between eras. Authors MUST follow the [Schema Scope](#scope-of-the-schema), [Schema Design Principles](#schema-design-principles) and [Schema Conventions](#schema-conventions).

Furthermore, for each subsequent schema, the [changelog](./changelog.md) must be updated. Authors must clearly articulate the deltas between schemas.

## Rationale: how does this CIP achieve its goals?

### Scope

We keep the scope of this standard to the data types within Cardano blocks. The rationale for this is that block data is by far the most useful for the majority of Cardano actors. There is also one nice benefit that the definitions can map directly from the provided CDDL file from ledger team.

### Strictness

This CIP lays out strong conventions that future schema authors must follow, along with a large set of design principles. The aim is to minimize the potential for unavoidable deltas between schemas.

By setting sometimes arbitrary conventions we hope to create a single possible interpretation from CBOR to JSON, alleviating any ambiguity.

### Absence of extensibility

The schemas MUST NOT be extensible with additional properties. This may sound counter-intuitive and against the spirit of json-schema, but there are some motivations behind that:

- More safety from typos: object fields that are optional may be specified with slightly incorrect names in dApps' code, leading to inability of the decoders to pick up the values, which may go unnoticed.
- Clear delineation between Cardano domain types and user dApp domain types: forcing the developers to store their dApp domain data separately from Cardano data, or close to it (as opposed to mixing these together in a single object) will indirectly motivate better structured dApp code.

### JSON

JSON was chosen as there is no viable alternative. The majority of Cardano's web tooling is built with Javascript where JSON is the primary object representation format.

Furthermore, even across non-Javascript based stacks, JSON enjoys wide tooling support, this improves the potential for builders to adopt this standard.

### Bech32 for addresses

We choose to use Bech32 as the representation for Cardano addresses. When compared to the alternative of hexademical encoding, Bech32 gives the advantages of an included checksum and a human readable prefix.

## Path to Active

### Acceptance Criteria

- [ ] One future ledger era schema is added
- [ ] This standard is implemented within three separate tools, libraries, etc.

### Implementation Plan

- [x] Complete the specification for the current Babbage era
- [ ] Provide a test suite validating JSON fixtures for all the types against the schema
- [x] Provide an implementation of validating functions that uses this json-schema
- [mlabs-haskell/cip-0116-tests](https://github.com/mlabs-haskell/cip-0116-tests)
- [ ] Collect a list of cardano domain types implementations and negotiate transition to the specified formats with maintainers (if it makes sense and is possible)

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0117/README.md
---

- --
CIP: 117
Title: Explicit script return values
Category: Plutus
Status: Active
Authors:
- Michael Peyton Jones <michael.peyton-jones@iohk.io>
Implementors:
- Michael Peyton Jones <michael.peyton-jones@iohk.io>
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/747
Created: 2024-01-22
License: CC-BY-4.0
- --

## Abstract

Today, Plutus Core scripts signal success or failure exclusively by whether the script terminates normally or abnormally (with an error).
This leads to some false positives, where a script terminates normally, but this was not intended by the author.
We propose to additionally look at what the script evaluates to when checking for success or failure.

## Motivation: why is this CIP necessary?

Consider the following Plutus scripts, intended to be used as minting policy scripts:

```
\redeemer -> \datum -> \script-context -> (builtin error)
```

```
\redeemer -> \script-context -> (con false)
```

Today, both of these will unconditionally succeed!

1. Minting policies only receive two arguments, but this script expects three before it does any work. It therefore evaluates successfully to a lambda.
2. This script evaluates _successfully_ to `(con false)`, but the return value is irrelevant since it terminates successfully.

In both cases the user has made a mistake, but this result is that the script fails _open_, that is, anyone can spend such an output.
A variant of the first mistake is for a script to expect too _few_ arguments, but this will almost always result in an error and so fail closed.[^failing-open]

[^failing-open]: It is not universally clear whether it is good to fail open or closed, but generally for systems like this we tend to fail closed, and it is also easier to detect such failures during testing.

Historically, Plutus Core was going to be a typed language, and at least the first kind of error would have been caught by the typechecker.
However, today there is little stopping people from making such mistakes.

While these mistakes are relatively easily avoidable (any good smart contract toolkit should prevent them), it is nonetheless a potential landmine for users.

## Specification

The specification for checking whether a Plutus Core script accepts a transaction changes as follows (the new part is in brackets):

> A Plutus Core script S with arguments A1...An accepts a transaction if 'eval(S A1 ... An)' succeeds [and evaluates to the builtin constant 'unit'].

This change is not backwards-compatible and will need to go into a new Plutus ledger language.

## Rationale: how does this CIP achieve its goals?

Since the return value of a script will now be significant, a script will only succeed if the whole thing evaluates to 'unit'.
This is very unlikely to happen by accident: mistakes in the number of arguments or in what to return will result in failure.

### Alternatives

- The status quo is not terrible, and we could simply accept it.
- The return value could be a boolean constant, with 'true' indicating success and 'false' indicating failure.
- This is slightly more complicated, and technically we only need a designated success value, since "anything else" indicates failure. We don't need to distinguish between "normal exit indicating rejection of the transaction" and "abnormal exit".
- We could specifically detect when a script returns a lambda, and say that that is a failure.
- This is patching up one particular hole, whereas the proposal here has much more coverage by failing everything that doesn't quite specifically return 'true'.

## Path to Active

### Acceptance Criteria

- [x] The proposal is implemented in the Ledger.
- [x] The ledger changes are released on Mainnnet.

### Implementation Plan

- [x] The Plutus team will implement the changes to the Ledger.

## Copyright

This CIP is licensed under [CC-BY-4.0][].

[CC-BY-4.0]: https://creativecommons.org/licenses/by/4.0/legalcode

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0119/README.md
---

- --
CIP: 119
Title: Governance metadata - DReps
Category: Metadata
Status: Proposed
Authors:
- Thomas Upfield <thomas.upfield@iohk.io>
Implementors: []
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/788
- https://vimeo.com/912374177/0e9299fb5d?share=copy
- https://vimeo.com/915297122/c39f4a739b?share=copy
Created: 2024-02-07
License: CC-BY-4.0
- --

## Abstract
The Conway ledger era ushers in on-chain governance for Cardano via [CIP-1694 | A First Step Towards On-Chain Decentralized Governance](https://github.com/cardano-foundation/CIPs/blob/master/CIP-1694/README.md), with the addition of many new on-chain governance artefacts. Some of these artefacts support linking to off-chain metadata as a way to provide context.

The [CIP-100 | Governance Metadata](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0100) standard provides a base framework for how all off-chain governance metadata should be formed and handled. But this is intentionally limited in scope, so that it can be expanded upon by more specific subsequent CIPs.

This proposal aims to provide a specification for off-chain metadata vocabulary that can be used to give context to CIP-100 for DRep registration and updates. Without a sufficiently detailed standard for DRep metadata we introduce the possibility to undermine the ability of DReps to explain their motivations and therefore people looking for someone to represent them with the best possible information available to make that choice. Furthermore a lack of such standards risks preventing interoperability between tools, to the detriment of user experiences.

#### Thank you
Thank you to [everyone who participated in the CIP workshops](#Acknowledgements), and to @ryun1 for creating the JSON-LD schemas for this CIP and for his excellent technical support and invaluble advice. Thank you also to the other CIP editors and attendees of the CIP Editors' Meetings where this CIP was refined, most notably @rphair and @Crypto2099.

## Motivation: why is this CIP necessary?
CIP-1694 has set forth a model of a blockchain controlled by its community, and in doing so has challenged providers to build apps and tools that will allow users easy access to the governance features currently being built into Cardano.  Minimum viable tools must be ready at the time these governance features are launched.

The motivation for this CIP therefore is to provide these toolmakers with a simple and easy to accommodate standard which, once adopted, will allow them to read and display the metadata created by anyone who follows this standard when creating their DRep registration or update metadata. Tooling designed for DReps so that they can easily create metadata will also be made possible, because toolmakers will not need to individually innovate over the contents or structure of the metadata that their tool creates.

Metadata is needed because blockchains are poor choices to act as content databases. This is why governance metadata anchors were chosen to provide a way to attach long form metadata content to on-chain events. By only supplying a url to the off-chain metadata, and a hash of that metadata to the blockchain we ensure correctness of data whilst minimising the amount of data posted on-chain.

### Benefits
I believed that this CIP would provide a benefit to:

#### Potential delegators
When observing from the chain level, tooling can only see the content and history of DRep registration and update certificates and any associated anchors. These on-chain components do not give any context to the motivation of a DRep, even though this information would likely be the desired context for people who might delegate their voting power. By providing rich contextual metadata we enable people choosing a DRep to delegate their voting power to make well informed decisions.

#### DReps
DReps will be able to use tools that create metadata in a standard format. This in turn will allow their metadata to be read by apps that will render their words on screen for potential delegating Ada Holders to enjoy, this may lead to greater levels of delegation.

#### All participants
By standardising off-chain metadata formats for any tooling which creates and/or renders metadata that is referenced in DRep registration and update transactions we facilitate interoperability. This in turn promotes a rich user experience between tooling. This is good for all governance participants.

## Specification
This CIP explains the structure of any metadata referenced in a metadata anchor optionally included in any DRep registration or update transaction.

### Teams
This CIP has been written for individuals acting in the capacity of DReps, and not for teams of people collaborating as a single DRep, although this does not preclude teams from using metadata in the structure explained by this CIP.

### Witnesses
DRep Metadata will not follow the CIP-100 specification related to signing the metadata, the [`authors` property](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0100/README.md#high-level-description) can be left blank without the need for tooling providers to warn their users that the author has not been validated. Instead the author can be derived from the DRep ID associated with the registration or update. The need for an `authors` field will also be discarded in favour of including `givenName`s and `Identity` inside of the `body` field. For the avoidance of doubt this CIP recommends that the entire authors property be left blank, and that tooling ignore it.

### Extended Vocabulary
Like CIP-108, this CIP also extends the potential vocabulary of CIP-100's `body` property.

Furthermore we extend the Schema.org definition of a [Person](https://schema.org/Person). Any property of Person maybe included within the `body`.

>**Reminder for tooling providers/builders** DRep metadata is user generated content.

The following are a list of properties tooling should expect to encounter:

#### `paymentAddress`
- Optional
- Bech32 encoded payment address, for the same network as the DRep registration is to be submitted to.

DReps may want to receive tokens for a variety of reasons such as:
- donations
- expenses
- any incentive program

Therefore there MAY be a `paymentAddress` field in the metadata where such payments could be sent. This makes such an address public and payments to DReps transparent.

This SHOULD NOT be confused with the `address` property of a [Person](https://schema.org/Person), `address` in the context of a DRep refers to their location and NOT their payment address.

#### `givenName`
- Compulsory
- This is a property inherited from [Person](https://schema.org/Person)
- It is the only compulsory property
- A very short freeform text field. Limited to 80 characters.
- This MUST NOT support markdown text styling.
- It is intended that authors will use this field for their profile name/ username.

#### `image`
- Optional
- This is a property inherited from [Person](https://schema.org/Person)
- This SHOULD be treated as the profile picture of the individual
- This MUST contain a fully described [`imageObject`](https://schema.org/ImageObject) property

##### `imageObject`
- This is to be included in a metadata file as a property of the `image` property, only if the `image` property is included.
- It explains the image to those (inc. tools) who are viewing it.
- `imageObject` MUST take one of the following forms:
1. base64 encoded image
2. URL of image

###### base64 encoded image explained:
`imageObject` contains a base64 encoded image in its [`contentUrl`](https://github.com/schemaorg/schemaorg/issues/2696) property in a [dataURI](https://en.wikipedia.org/wiki/Data_URI_scheme) format:
- i.e. _data:content/type;base64,_ (AND NOT _data:domain.tld_)
- e.g. _contentURL:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAUAAAAFCAYAAACNbyblAAAAHElEQVQI12P4//8/w38GIAXDIBKE0DHxgljNBAAO9TXL0Y4OHwAAAABJRU5ErkJggg==_ (AND NOT _contentURL:https://avatars.githubusercontent.com/u/113025685?v=4_)

###### URL of image explained:
If the `imageObject` DOES NOT contain a base64 encoded image, the `contentUrl` MUST contain the URL where the image can be found and the `sha256` property MUST be populated with the SHA256 hash of the image file contents found at the `contentUrl`. The SHA256 hash is needed in order for readers to verify that the image has not been altered since the metadata anchor was submitted on-chain.

#### `objectives`
- Optional
- A freeform text field with a maximum of 1000 characters
- A short description of what the person believes and what they want to achieve as a DRep

#### `motivations`
- Optional
- A freeform text field with a maximum of 1000 characters
- A short description of why they want to be a DRep, what personal and professional experiences have they had that have driven them to register

#### `qualifications`
- Optional
- A freeform text field with a maximum of 1000 characters
- A space where the registrant can to list the qualifications they hold that are relevant to their role as a DRep
- This is distinct from the properties of a Person listed as `knows` and `knowsAbout` because it encompasses things that the DRep has done as well as things that they know that qualify them for this position.

#### `references`
- Optional
- This CIP extends the `references` property from [CIP-100](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0100#high-level-description)
- `references` contain the following sub-properties `@type`, `label`, and `uri`
- This CIP adds two `@type` identifiers "Identity" and "Link"

##### `@type`: Link
- Optional
- It is expected that these links will be the addresses of the social media/ websites associated with the DRep in order to give a person reviewing this information a fulsome overview of the DRep's online presence.
- The creator of the metadata SHOULD add a `label`, this `label` SHOULD describe the source of the url, e.g. if it is a link to the DRep's X account then the `label` SHOULD say "X". If it is the only personal website provided by the DRep the `label` should say "Personal Website" rather than domain_name.com.
- The `label` of each `Link` SHOULD NOT be left blank
- Each `Link` MUST have exactly one `uri` (as specified in CIP-100) which SHOULD not be blank.

##### `@type`: Identity
- Optional
- The `uri` of a reference with the `@type` "Identity" is a way for DReps to prove that they are who they say they are
- It is expected that the "Identity" of a DRep will be the addresses of their most active social media account twitter, linkedin etc. or personal website.
- The DRep must reference their DRep ID in a prominent place at the location that is specified in the `uri` property of that reference. This will be used by people reviewing this DRep to prove and verify that the person described in the metadata is the same as the person who set up the social media profile.

#### `doNotList`
- Optional
- Is a boolean expression that can be given a single value of either `true` or `false`.
- If not included then the value is assumed to be `false`.
- A `true` value means that the DRep does **not** want to campaign for delegation via tooling.
- A `false` value means that the DRep does want to campaign for delegation via tooling and thus be shown via that tooling.
- e.g. a DRep who does not want to appear in GovTool’s DRep Directory feature creates metadata with `doNotList=true`.

### Application
Only the `givenName` property is listed above as compulsory, DRep metadata must include it to be considered CIP-119 compliant. As this is an extension to CIP-100, all CIP-100 fields can be included within CIP-119 compliant metadata.

### Test Vector
See [test-vector.md](./test-vector.md) for examples.

### Versioning
This proposal should not be versioned, to update this standard a new CIP should be proposed. Although through the JSON-LD mechanism further CIPs can add to the common governance metadata vocabulary.

## Rationale: how does this CIP achieve its goals?
We intentionally have kept this proposal brief and uncomplicated. This was to reduce the time to develop and deploy this standard. This way we enable tooling which depends on this standard to start development.

### Rationale for insisting on a compulsory name
The compulsory nature of this field was controversial because the `givenName`s cannot be made unique and therefore are open to abuse (by e.g. copycats). However this is not a reason to not include a `givenName`, it a reason for people reviewing a DRep's profile to properly check their whole profile before delegating to them. A `givenName` MUST be included because a person must always have a name. Iit is a human readable identifier, it is the property that people reviewing DReps will most likely identify a given DRep by, even in the presence of copycats.

### Rationale for multiple freeform fields
It has been suggested that the `objectives`, `motivations`, and `qualifications` properties ([or at least the latter two](https://github.com/cardano-foundation/CIPs/pull/788#discussion_r1546391918)) could be one freeform property instead of 3. The rationale for having 3 separate properties is to provide structure to DReps so that they have a useful set of prompts about what they can and should write about. The author noticed in research that a single `bio` field in a form typically resulted in lower quality, often single line, responses from respondents than when this `bio` field was split into smaller fields with more highly specified purposes.

It has also been suggested that the format of the input into these three properties could be more tightly specified for example `qualifications` could require a list of qualifications. Whilst this will probably be needed I have left this up to a future CIP to specify what these specifications should be because at this stage (MVP) I have no concrete examples of how people will end up using these fields and I want to leave it up to the community to experiment with this.

### Rationale for the identity solution that is used
PKI cryptographic solutions were also considered in the place of the current solution however they were generally considered too complex to implement for minimum viable tooling. There were also other issues with cryptographic forms of identification for MVPs:
1. solutions that involve a public/private key setup still require people verifying the identity of a DRep to know what their authentic public key is.
2. specifying the use of a verification service such as [Keybase](https://keybase.io/) would lead to centralisation and therefore reliance on a 3rd party for the identity validation of DReps.

### Rationale for extending the schema.org Person property
This CIP is not written to specifically cover the metadata created to describe teams of people collaborating to register as a DRep, but it is written to cover the metadata created by individuals to describe themselves in their capacity as a DRep. Therefore DReps are people.

People who want to extend the use of the DRep metadata can now do so in a way that allows tooling providers to use off the peg solutions. Furthermore there may be SEO benefits to using schema.org templates.

### Rationale for decisions made regarding `imageObject` and b64 encoding
According to schema.org The `image` property inherited from [Person](https://schema.org/Person) can either be a URL to a separate location where an image is stored, or it can be an `imageObject`.

For the following reasons it was originally intended that this CIP would specify the use of an `imageObject` with a b64 encoded image only, because:
1. The data at the location specified by a URL could be subject to change without the hash in the metadata anchor needing to be changed
2. Choosing just one way to write and read image data would to limit the amount of tooling options that need to be created to cater to those wishing to create DRep metadata.

However it was pointed out that this may quickly lead to relatively massive (multi-megabyte) metadata files that are more difficult to fetch and store without providing substantial value. Even IPFS would take a relatively long time to serve these files, and if there was a need to index them by some chain indexer (such as DB-Sync) then this could massively increase the storage space needed to run the indexer.

It is also the case that CIP-100 allows for metadata to be saved within a governance transaction, and including b64 encoded images directly within transactions would be troublesome due to their size. This would not be an issue with including an image file URL.

Therefore it was decided to allow a provision for people to submit `imageObject`'s with a URL only if a hash was included OR with a base64 encoded image, and allow them to make the decision as to which was most appropriate for their use case.

#### Rationale for `doNotList`

This field was intended for DReps who wish to identify themselves via rich metadata but are not seeking to campaign for delegations.
By not being listed via "DRep aggregation/campaign" tools the idea is that these DReps are less likely to attract unwanted delegation from ada holders.
These DReps could be organizations that want to use their ada to vote in a transparent way on-chain but do not wish to vote on the behalf of others.

It is expected that tooling such as block explorers will list DReps using `doNotList=true`. Tooling built specifically for DRep campaign and delegation should respect the intent of this field.

This proposal cannot force tooling to respect this desire from DReps. DReps must be aware that any information anchored on-chain can be found via tooling and may result in delegation.

### A Note on Teams
CIP-1694 allows for DReps to be registered using a native or Plutus script credential, this implies that individuals could organise to form a team that would have a broad range of expertise, and would perhaps therefore be more attractive to some delegating Ada Holders.

Participants at the workshop held to debut the first draft of this CIP strongly advocated in favour of including features that would assist a DRep comprised of a team of individuals to properly describe their endeavour.

This CIP has not included these features, the decision not to include these features was made in order to simplify this CIP so that it became suitable for the minimum viable level of tooling, with the expectation that further CIPs will build on it.

### Open Questions
|QID |Question |Answer |
|----|---------|-------|
|1   |Should we accommodate for profile pictures to be included in metadata? | Yes, furthermore, a future CIP may allow for multiple pictures |
|2   |Do we need to replace the `bio` field with a more structured set of fields? | Yes, adding structure is intended to guide people to add data which is more interesting to the reader |
|3   |Can we include and verify an ADA handle to uniquely identify a DRep |[This](#type-identity) is how we verify the identity of a DRep |
|4   |What do we do about lack of metadata integrity | This is up to the tooling provider|
|5   |Should we split this CIP up into separate transactions or also add the vote transaction metadata |The scope is fine |
|6   |Compulsory vs optional for all fields |See the relevant section for each property |
|7   |types of DRep (script? representing an organisation? want delegations?)| This CIP is optimised for individuals who are DReps, but doesnt exclude teams. It is agnostic as o wherther the DRep is registered via a script or not |
|8   |How can we verify the information that we are displaying(?) |This CIP adopts provisions from CIP-100 about how to display metadata which is in an unexpected format. Otherwise this CIP leaves questions surrounding the display of data to tooling providers. Identity verification is covered in QID 8. |
|9   |Should tooling providers displaying data warn people that none of the information is verified and they should DYOR? |This is not specified in this CIP |
|10  |Should tooling providers making metadata provide some information about best practices such as putting DRep ID in twitter bio? |This is not specified in the CIP |
|11  |What do we care about when someone is retiring? | Retirement transactions no longer include a metadata anchor|
|12  |When someone updates do we want a 1 line summary of what has changed? |Possibly future CIPs could include this feature |
|13  |Should tools inform people of recent changes? |This is up to tooling providers |
|14  |Should there be an incentives address, where DRep incentives are paid? |See [`paymentAddress`](#paymentAddress) |
|15  |PKI Public Key, Fingerprint, or DID? | See [`type`: Identity](#type-identity) |

## Path to Active

### Acceptance Criteria
- Publish JSON-LD schemas & test-vector.md
- Adoption by at least one community tool

### Implementation Plan
<!-- How I plan to meet the acceptance criteria -->

## Acknowledgements
There have been 3 lively public workshops on this subject, and I would like to thank the following people

<details>
  <summary><strong>Workshop 1 - 07/02/2024</strong></summary>

- Abhik Nag
- Adam Dean
- Adam Rusch
- Digital Fortress (Rick McCracken)
- Eduardo Silka
- Jose Miguel De Gamboa
- Leonardo Silka
- Lorenzo Bruno
- Mike Susko
- Nicolas Cerny
- Peter Wolcott
- Ryan Williams
- Sheldon Hunt
- Tyler Wales
- Upstream SPO
- Valeria Devaux
- Дима

[A recording of this meeting can be found here](https://vimeo.com/912374177/0e9299fb5d?share=copy)
</details>

<details>
  <summary><strong>Workshop 2 - 20/03/2024</strong></summary>

- Adam Rusch
- Aleksandar Djuricic (Aleks)
- Christopher Hockaday
- Digital Fortress (Rick McCracken)
- Eduardo Silka
- HD5000
- Igor Velickovic
- Input Endorsers
- Konstantinos Dermentzis
- Leonardo Silka
- Michael Madoff
- Michał Szałowski
- Mike Hornan
- Peter Wolcott
- Ryan
- Ryan (Cerkoryn)
- Ryan Williams
- Steve Lockhart

[A recording of this meeting can be found here](https://vimeo.com/915297122/c39f4a739b?share=copy)
</details>

<details>
  <summary><strong>Australia and APAC Workshop  - 05/03/2024</strong></summary>

- Andreas,
- Linh P,
- Mark Byers,
- Mike Hornan,
- Pedro Lucas,
- Phil Lewis,
</details>

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0120/README.md
---

- --
CIP: 120
Title: Constitution specification
Category: Metadata
Status: Proposed
Authors:
- Ryan Williams <ryan.williams@intersectmbo.org>
- Danielle Stanko <danielle.stanko@iohk.io>
Implementors:
- Danielle Stanko <danielle.stanko@iohk.io>
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/796
Created: 2024-03-19
License: CC-BY-4.0
- --

## Abstract

Cardano's minimum viable governance model as described within
[CIP-1694 | A First Step Towards On-Chain Decentralized Governance](https://github.com/cardano-foundation/CIPs/blob/master/CIP-1694/README.md)
introduces the concept of a Cardano constitution.
Although CIP-1694 gives no definition to the constitution's content or form.

This proposal aims to describe a standardized technical form for the Cardano
Constitution to enhance the accessibility and safety of the document.

> **Note:** This proposal only covers the technical form of the constitution,
> this standard is agnostic to the content of the constitution.

## Motivation: why is this CIP necessary?

CIP-1694 defines the on-chain anchor mechanism used to link the off-chain
Constitution document to on-chain actions.
This mechanism was chosen due to its simplicity and cost effectiveness,
moving the potentially large Cardano constitution off-chain,
leaving only a hash digest and URI on-chain.
This is the extent to which CIP-1694 outlines the Cardano Constitution:
CIP-1694 does not provide suggestions around hashing algorithm,
off-chain storage best practices or use of rich text styling.

By formalizing the form of the constitution and its iterations,
we aim to promote its longevity and accessibility.
This is essential to ensure the effectiveness of the CIP-1694 governance model.

This standard will impact how Ada holders read the Constitution but
the main stakeholders for this are the tool makers who wish to read,
render and write a constitution.

### Safety

Without describing best practices for the form and handling of the constitution,
 we risk the constitution document being stored in an insecure manner.
By storing the constitution on a decentralized platform,
we can ensure its immutability and permissionless access.
This is a step to improve the longevity and accessibility of each constitution
iteration.

### Interoperability

By defining a file extension and formatting rules for the constitution we
ensure that tooling reading and writing the constitution will be interoperable.
Furthermore we aim to make the role of constitution iteration comparison tools
easier, by minimizing formatting and style changes between iterations.
This will reduce compatibility issues between tools,
promoting the accessibility of the constitution.

### Usability

Rich text formatting greatly enhances the readability of text,
especially in large complex documents.
Without the ability to format text, it could easily become cumbersome to read,
negatively effecting the accessability of the Cardano constitution.

## Specification

The following specification SHOULD be applied to all constitution iterations.
This standard could be augmented in the future via a separate CIP which aims to
replace this one.

### Terminology

#### Capitalisation of key terms

To avoid unnecessary edits, and therefore checksum changes,
constitution authors MUST follow the following standard English capitalisation
rules unless a translation language indicates otherwise:

##### `Constitution` vs. `constitution`

- The Constitution in effect — either the "initial" one or any new constitution — is unique and therefore capitalised ("Constitution") as a proper noun.
- Draft or proposed constitutions are not unique & therefore are not capitalised ("constitution") as a common noun.
- "Cardano Constitution" is a very specific proper noun phrase (also a title) and so each word is capitalised.
- The phrase "the Constitution", unless used non-specifically (e.g. "the constitution that voters prefer"), would generally be assumed to be _the_ Constitution and capitalised as a proper noun.

##### `Ada` vs. `ada` vs. `ADA`

- `Ada` is the currency, while `ada` indicates units of that currency (e.g. "Ada holders can accumulate more ada to increase their influence.")
- `ADA` is the trading symbol (e.g. "Fluctuations in ADA might influence decisions about the Treasury.")

### Characters

The constitution text MUST only contain printable text characters in UTF-8
encoding.

### Lines

Each line in the constitution text MUST contain at maximum 80 characters,
including spaces and punctuation.

While 80 characters is a limit, authors don't have to try and always hit 80.
Legibility of the raw (unrendered) document SHOULD be kept in mind.

#### Sentences

The constitution text MUST only contain a maximum of one sentence per line,
with each sentence followed by a newline.
Each new sentence SHOULD start on its own line with a capitalized letter.

Long sentences can be split multiple lines,
when writing the author SHOULD try to split long sentences along natural breaks.

Example:

```md
This is a short sentence on one line.

This is a long sentence and I have valid reasons for it being so long,
such as being an example of a long sentence.
When this sentence is rendered it SHOULD be shown to directly follow the
sentence above.

This sentence is the start of a new paragraph.
```

When rendered,
these newlines between sentences SHOULD NOT be shown as newlines.
Instead they SHOULD be rendered as a space character between sentences.

Paragraphs are shown by leaving a blank line between text.

### File

Constitution files MUST be `.txt` files.

Constitution files SHOULD be named in sequential whole numbers.
Following the pattern `cardano-constitution-{i}.txt`
where `{i}` is the iteration number.

Starting from an interim constitution named `cardano-constitution-0.txt`,
the next constitution SHOULD be named `cardano-constitution-1.txt`.

To prevent misalignment, constitutions governing networks other than Cardano's
mainnet, CAN be prefixed with the network name.
For example, on the preview network the constitution file COULD be named
`preview-cardano-constitution-0.txt`.

### Hashing

When supplying a constitution hash digest to chain, the algorithm used MUST be
Blake2b-256.
Before creating a hash digest the constitution plain text MUST be in its raw
text, including any [Rich Text Formatting](#rich-text-formatting)
related characters.

### Storage

The each ratified constitution MUST be stored,
immutably on a distributed storage mechanism where backups can be easily made
in a permissionless manner by interested parties.
This storage platform SHOULD be easily accessible, with strong tooling support.

When generating a URI for the document, authors SHOULD NOT include centralized
gateways.

We propose using the
[InterPlanetary File System (IPFS)](https://www-ipfs-tech.ipns.dweb.link/).

### Rich Text Formatting

The constitution text MAY include a strict subset of rich text styling as
defined in this specification.
Tooling rendering the constitution SHOULD recognize these and render them
faithfully.

#### Line Breaks / Paragraphs

To create paragraphs, use a blank line to separate one or more lines of text.

Examples:

```md
Here's a line for us to start with.

This line is separated from the one above by two newlines,
so it will be a *separate paragraph*.

This line is also a separate paragraph, but...
This line is only separated by a single newline,
so it's a separate line in the *same paragraph*.
```

#### Headers

Headers are denoted via a hashtag character `#` followed by a space ` `.
There are six levels of headers, with each lower level set via an added `#`.
Headers are ended via a line break.
Headers SHOULD be followed below by a blank line.
Headers SHOULD not be preceded by whitespace.

The lower the number of `#` the larger order the text SHOULD be rendered.

Example:

```md
# H1

## H2

Some non-heading text.

### H3

#### H4

##### H5

###### H6

```

If text is in a header no other formatting can be applied.

An empty line SHOULD be left above and below each heading

#### Emphasis

Emphasis is applied to text between single or double asterisks,
without space between asterisks and text.
Italicized emphasis is shown via single asterisk (`*`).
Bold emphasis is shown via double asterisks (`**`).

Emphasis cannot span multiple lines.

Examples:

```md
Emphasis, aka italics, with single *asterisks*.

Strong emphasis, aka bold, with double **asterisks**.
```

Both italicized and bold cannot be applied to the same text.

#### Code and Syntax Highlighting

Texted can be highlighted as code, when encased without spaces by backticks.
This MUST not contain line breaks.

Example:

```md
Inline `code` has `back-ticks around` it.
```

The text contained within headings or emphasis cannot be highlighted as code.

#### Ordered Lists

To create an ordered list,
add line items with numbers followed by one period and then one space.
Each line item is separated by an empty line.
The numbers MUST be in numerical order,
but the list SHOULD start with the number one.

Ordered lists MUST NOT have indented items.
Ordered lists MUST NOT include headings.

```md
1. This is the first item in my ordered list

2. this is the second item in my list

3. the third item
```

#### Unordered Lists

To create an unordered list, add dashes (`-`) and one space,
in front of line items.
Each line item is separated by an empty line.

Unordered lists MUST NOT have indented items.
Unordered lists MUST NOT start with a number followed by a period.

```md
- this is my list

- I like unordered lists
```

Unordered lists MUST NOT include headings.

### Best Practices

#### Rendering

When rendering the raw document tooling COULD use standard markdown rendering
tools.

#### Hashing

When submitting an update constitution governance action,
tooling SHOULD verify the document hash digest and matches the document.

Tooling reading constitution anchors from chain SHOULD always perform a
correctness check using the hash digest on-chain.
If the hash provided on-chain does not match the hash produced from
the off-chain document, then the tooling SHOULD highlight this in a
very obvious way to users.

#### Conformance

Tools writing constitutions SHOULD strive to follow this specification.
If tooling discovering and rendering constitution documents discovers that
the document does not follow the "MUST"s in this specification then a small
warning SHOULD be given to users.

#### Form

Authors SHOULD aim to keep the document as clean and consistent as possible.

Text SHOULD try to be left aligned, without using unneeded whitespace leading
or trailing lines.

Spaces SHOULD be used over tab characters.

The last line in the document SHOULD be empty.

### Test vectors

See [Test vector file](./test-vector.md).

## Rationale: how does this CIP achieve its goals?

### Line length

We choose to restrict the maximum number of characters per line in aims of
improving readability of the document in plain text and within diff views.

It SHOULD also be considered that the 80-character limit also helps one find
run-on sentences.
If your sentence is much longer than 80, it might need breaking down.
And you SHOULD never want to see a sentence that's over two
lines long in normal text.

### Sentences

By limiting documents to one sentence per line we hope to improve the
experience when comparing documents and commenting on specific sentences.
Conventional document comparison tools such as git diff views, compare
documents on a by line basis.
By spreading text across lines,
it greatly improves tooling's ability to differentiate between documents.

Furthermore, isolating one sentence per line, allows users to more easily
isolate specific lines to comment upon.
This gives each sentence an unambiguous reference point,
which can be very useful for sharing and discussing.

### Versioning

We chose to only allow replacement of this document rather than including a
more conventional versioning scheme.
This was done for simplicity to minimize the amount of effort required to
create tooling which reads and writes constitutions.

The alternative was to add some details of version to the constitution document.
This would make changing hashing algorithm, rich text formatting, etc.
much easier.
But this makes the standard and subsequent, more complex than necessary.
We do not believe the added complexity is justified,
for the expected number of future replacement CIPs to this one.

### File

The text file was chosen, due to its ubiquity across platforms.
By choosing a common format,
we improve the accessibility of the raw document.

We choose to add sequential numbering to constitution document iterations to
improve differentiation between documents.

### Hashing

Blake2b-256 was chosen for its common use across the Cardano ecosystem.
This means that a lot of Cardano tooling already has this algorithm implemented.
This lowers the bar to entry for existing tool makers to add
constitutional support.

For simplicity, we decide not to include an easy mechanism for changing of
hashing algorithms between constitutions.

### Storage

Ensuring the Cardano Constitution and its iterations can be accessed in a
permissionless manor is paramount.
Permissionless networks such as IPFS reduce the ability for parties to
censor the content.
With each interested party able to make copies of constitutions,
this improves the resilience of the documents from deletion.

The primary competing idea to platforms such as IPFS is to store the
constitution text on Cardano itself.
This would philosophically be superior to storing the document off-chain,
keeping the Cardano Constitution on Cardano.
The counter point to this is that,
Cardano is not a general data storage system, rather it is a ledger.
Storing data on Cardano is expensive and difficult, without strong tooling
support.

### Rich Text Formatting

Rich text styling will greatly improve the readability of the
constitution documents, when rendered.

#### Markdown

Markdown styling was chosen due to its ubiquity, with strong tooling support.
Furthermore, markdown has a benefit in that the unrendered documents are
still human readable.
This is in contrast to other solutions such as HTML.

#### Strict subset

We chose a strict subset of markdown text styling for two reasons.
Firstly, markdown contains a very large and varied syntax,
reducing the scope making implementation easier for all tooling.
Secondly, some features of markdown may not want to be used in a formal
constitution document.
Embedded HTML or videos are likely things to be avoided.

## Open Questions

- [x] How can we support multi-languages?
- The Cardano constitution will be in English, but we will add best practice guidelines via [Best Practices](#best-practices).
- [x] Should we specify any standardization for the proposal policy?
- Due to lack of interest in this, we will leave it out of this standard.
- [x] How can we add page breaks?
- We wont, instead we will prioritize a minimum set of rich text formatting. We can provide some guidance via [Best Practices](#best-practices).
- [x] Do we want a mechanism for specifying authors? (similar to CIP-100)
- No, as CIP-100 compliant metadata can be supplied at time of constitution update.
- [x] What SHOULD we name the constitution file? we could embed some nice naming or metadata.
- Naming the file `cardano-constitution` seems specific enough, adding iteration numbers is a nice addition too.

## Path to Active

### Acceptance Criteria

- [x] This standard is followed for the interim Cardano Constitution
- [ ] This standard is utilized by two tools reading constitution data from chain
- [constitution.gov.tools](https://constitution.gov.tools/)

### Implementation Plan

#### Solicitation of feedback

- [x] Answer all [Open Questions](#open-questions)
- [ ] Review from the Civics Committee

#### Test vector

- [x] Author to provide a test vector file with examples.

## Acknowledgements

<details>
  <summary><strong>First draft</strong></summary>

  We would like to thank those who reviewed the first draft of this proposal;
- Danielle Stanko
- Kevin Hammond
- Steven Johnson

</details>

<details>
  <summary><strong>Significant Contributions</strong></summary>

  We would like to thank Robert Phair ([@rphair](https://github.com/rphair)) for
  his expert contributions to this proposal.

</details>

## Copyright

This CIP is licensed under
[CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0121/README.md
---

- --
CIP: 121
Title: Integer-ByteString conversions
Category: Plutus
Status: Active
Authors:
- Koz Ross <koz@mlabs.city>
- Ilia Rodionov <ilia@mlabs.city>
- Jeff Cheah <jeff@mlabs.city>
Implementors:
- Koz Ross <koz@mlabs.city>
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/624
Created: 2023-11-17
License: CC-BY-4.0
- --

## Abstract

Plutus Core primitive operations to convert `BuiltinInteger` to
`BuiltinByteString`, and vice-versa. Furthermore, the `BuiltinInteger`
conversion allows different endianness for the encoding (most-significant-first
and most-significant-last), as well as padding with zeroes based on a requested
length if required.

## Motivation: why is this CIP necessary?

Plutus Core creates a strong abstraction boundary between the concepts of
'number' (represented by `BuiltinInteger`) and 'blob of bytes' (represented by
`BuiltinByteString`), defining different sets of (largely non-overlapping)
operations for each. This is, in principle, a good practice, as these concepts
are distinct in (most of) the operations that make sense on them. However,
sometimes, being able to 'move between' these two 'worlds' is important: namely,
the ability to represent a given `BuiltinInteger` as a `BuiltinByteString`, as
well as to convert between this representation and the `BuiltinInteger` it
represents. Currently, no such capability exists: while [CIP-0058][cip-0058]
proposed such a capability (among others), to date, this has not been
implemented into Plutus Core.

To see why such a capability would be beneficial, we give two motivating use
cases.

### Case 1: signing bids

Consider the following code snippet:

```haskell
validBidTerms :: AuctionTerms -> CurrencySymbol -> BidTerms -> Bool
validBidTerms AuctionTerms {..} auctionID BidTerms {..}
  | BidderInfo {..} <- bt'Bidder =
  validBidderInfo bt'Bidder &&
- - The bidder pubkey hash corresponds to the bidder verification key.
  verifyEd25519Signature at'SellerVK
    (sellerSignatureMessage auctionID bi'BidderVK)
    bt'SellerSignature &&
- - The seller authorized the bidder to participate
  verifyEd25519Signature bi'BidderVK
    (bidderSignatureMessage auctionID bt'BidPrice bi'bidderPKH)
    bt'BidderSignature
- - The bidder authorized the bid

bidderSignatureMessage
  :: CurrencySymbol
- > Integer
- > PubKeyHash
- > BuiltinByteString
bidderSignatureMessage auctionID bidPrice bidderPKH =
  toByteString auctionID <>
  toByteString bidPrice <>
  toByteString bidderPKH

sellerSignatureMessage
  :: CurrencySymbol
- > BuiltinByteString
- > BuiltinByteString
sellerSignatureMessage auctionID bidderVK =
  toByteString auctionID <>
  bidderVK
```

Here, we attempt to verify (using the Curve25519) that a bid at an auction was
signed by a particular bidder. The message to verify must include the bid
placed, represented using `Integer` here (which translates to `BuiltinInteger`
onchain). However, the `verifyEd25519Signature` primitive can only accept
`BuiltinByteString`s as messages to verify. Thus, we have a problem: how to
include the placed bid into the bid message to be verified?

More generally, constructing messages to sign usually consists of
concatenating together some primitives represented as `BuiltinByteString`s.
We currently have a way to do this for (some) strings using `encodeUtf8`,
but no way to do this for `BuiltinInteger`s.

### Case 2: finite fields

[Finite fields][finite-field], also known as Galois fields, are a common
algebraic structure in cryptographic constructions. Many, if not most, common
constructions in cryptography use finite fields as their basis, including
[Curve25519][curve-25519], [Curve448][curve-448] and the [Pasta
curves][pasta-curves], to name but a few. Elements in a finite field are
naturally representable as `BuiltinInteger`s of bounded size onchain, but for
applications like the constructions specified above (and indeed, anything built
atop such constructions), we need to be able to perform the following tasks
efficiently:

- Verify that a particular value belongs to the field; and
- Perform bitwise (that is, non-numerical) operations on such values, possibly
  together with numerical ones.

Furthermore, Case 2 presents two further challenges: _endianness_ and
_padding_. Due to many cryptographic algorithms being designed for use over the
network, their specifications assume a big-endian byte ordering in their
implementations. Likewise, due to the finiteness of a finite field's elements,
they can be encoded in a fixed-length form, which implementations make use of,
both for convenience and efficiency.

[cip-0058]: https://github.com/cardano-foundation/CIPs/tree/master/CIP-0058
[finite-field]: https://en.wikipedia.org/wiki/Finite_field
[curve-25519]: https://en.wikipedia.org/wiki/Curve25519
[curve-448]: https://en.wikipedia.org/wiki/Curve448
[pasta-curves]: https://electriccoin.co/blog/the-pasta-curves-for-halo-2-and-beyond/
## 


While it is not outright impossible to perform conversions from `BuiltinInteger`
to `BuiltinByteString` currently, it is unreasonably difficult and
resource-intensive: `BuiltinInteger` to `BuiltinByteString` involves a repeated
combination of division-with-remainder in a loop, while `BuiltinByteString` to
`BuiltinInteger` involves repeated multiplications by large constants and
accumulations. Aside from these both requiring looping (with the overheads this
imposes), both of these are effectively quadratic operations with current
primitives: the only means we have to accumulate `BuiltinByteString`s is by
consing or appending (which are both quadratic due to `BuiltinByteString` being
a counted array), and any `BuiltinInteger` operation is linear in the size of
its arguments. This makes even Case 1 far more effort, both for the developer and
the node, than it should be, and Case 2 ranges from difficult to impossible once
we factor in the limited available primitive operations and the endianness and
padding problems.

We propose that two primitives be added to Plutus Core: one for converting
`BuiltinInteger`s to `BuiltinByteString`s, the other for converting
`BuiltinByteString`s to `BuiltinInteger`s. The first of these primitives would
allow for specifying an endianness for the result, as well as to perform padding
to a required length if necessary; the second primitive is able to operate on
padded or unpadded encodings, in either endianness.

Additionally, we state the following goals that any implementation of such
primitives must have.

### No metadata

The representation produced by the `BuiltinInteger` to `BuiltinByteString`
conversion should be 'minimal', representing only the number being given to it,
and no other information besides. It would be tempting to, for example, encode
the endianness requested into the `BuiltinByteString`, but ultimately, this
information could be added later by users if they want it, while removing it
would be trickier. Additionally, metadata-related concerns would complicate both
the specification and implementation of the primitives, for arguably marginal
benefit.

### Internals-independence

Users of these primitives should not need to know how _exactly_
`BuiltinInteger`s are represented to use them successfully. This is beneficial
to both users (as they now don't have to concern themselves with
platform-specific implementation issues) and Plutus Core maintainers (as changes
in the representation of `BuiltinInteger` aren't going to affect these
primitives).

### No support for negative numbers

While for fixed-size numbers, [two's-complement][twos-complement] is the default
choice for negative number representations, for arbitrary-size numbers, there is
no agreed-upon choice. Furthermore, indicating the 'negativity' of a number
would require making representations larger or more complex regardless of which
representation we chose, while also complicating both the primitives we want to
define, and any user-defined operations on such representations, possibly in
ways that users do not want. Lastly, for our cases, negative values are not
really needed, and if the ability to encode negative numbers was necessary,
users could still define whichever one(s) they needed themselves, with little
effort or computational cost.

[twos-complement]: https://en.wikipedia.org/wiki/Two%27s_complement
## 


This CIP partially supercedes [CIP-0058][cip-0058]: specifically, the
specifications here replace the `integerToByteString` and `byteStringToInteger`
primitives specified in CIP-0058, as improved, and more general, solutions.

## Specification

We describe the specification of two Plutus Core primitives, which will have the
following signatures:

- `builtinIntegerToByteString :: BuiltinBool -> BuiltinInteger -> BuiltinInteger
- > BuiltinByteString`
- `builtinByteStringToInteger :: BuiltinBool -> BuiltinByteString ->
  BuiltinInteger`

To describe the semantics of these primitives, we first specify how we represent
a `BuiltinInteger` as a `BuiltinByteString`; after that, we describe the two
primitives, as well as giving some properties they must follow.

### Representation

Our `BuiltinByteString` representations of non-negative `BuiltinInteger`s treat
the `BuiltinInteger` being represented as a sequence of digits in base-256.
Thus, any byte in the `BuiltinByteString` representation corresponds to a single
base-256 digit, whose digit value is equal to its value as an 8-bit unsigned
integer. For example, the byte `0x80` would have digit value 128, while the byte
`0x03` would have digit value 3.

To determine place value, we define two possible arrangements of digits in such
a representation: _most-significant-first_, and _most-significant-last_. In the
most-significant-first representation, the first digit (that is, the byte at
index 0) has the highest place value; in the most-significant-last
representation, the first digit instead has the _lowest_ place value. These
correspond to the notions of [big-endian and little-endian][endianness]
respectively.

For any positive `BuiltinInteger` `i`, let

$$i_0 \times 256^0 + i_1 \times 256^1 + \ldots + i_k \times 256^k$$

be its [base-256 form][base-256-form]. Then, for the most-significant-first
representation, the `BuiltinByteString` encoding for `i` is the
`BuiltinByteString` `b` such that $\texttt{indexByteString bs j} = i_{k - j}$. For
the most-significant-last encoding, we instead have
$\texttt{indexByteString bs j} = i_j$.

[base-256-form]: https://en.wikipedia.org/wiki/Numeral_system#Positional_systems_in_detail

For example, consider the number `123_456`. Its base-256 form is

```
64 * 256 ^ 0 + 226 * 256 ^ 1 + 1 * 256 ^ 2
```

Therefore, its most-significant-first representation would be

```
[ 0x01, 0xE2, 0x40 ]
```

while its most-significant-last representation would be

```
[ 0x40, 0xE2, 0x01 ]
```

For `0`, in line with the above definition, both its most-significant-first and
most-significant-last representation is `[]` (that is, the empty
`BuiltinByteString`).

To represent any given non-negative `BuiltinInteger` `i` as above, we require
a minimum number of base-256 digits. For positive `i`, this is
$\max \\{1, \lceil \log_{256}(\texttt{i}) \rceil \\}$; for `i = 0`, we define this to be
$0$. We can choose to represent `i` with more digits than this minimum, by the
use of _padding_. Let $k$ be the minimum number of digits to represent `i`, and
let $j$ be a positive number: to represent `i` using $k + j$ digits in the
most-significant-first encoding, we set the first $j$ bytes of the encoding as
`0x0`; for the most-significant-last encoding, we set the _last_ $j$ bytes of
the encoding as `0x0` instead.

[endianness]: https://en.wikipedia.org/wiki/Endianness

To extend our previous example, a five-digit, most-significant-first
representation of `123_456` is

```
[ 0x00, 0x00, 0x01, 0xC2, 0x80 ]
```

while the most-significant-last representation would be

```
[ 0x80, 0xC2, 0x01, 0x00, 0x00 ]
```

We observe that these extra digits do not change what exact `BuiltinInteger` is
represented, as any zero digit has zero place value.

### `builtinIntegerToByteString`

We can now describe the semantics of the `builtinIntegerToByteString`
primitive. The `builtinIntegerToByteString` function takes three arguments; we
specify (and name) them below:

1. Whether the most-significant-first encoding should be used. This is the
   _endianness argument_, which has type `BuiltinBool`.
2. The number of bytes required in the output (if such a requirement exists).
   This is the _length argument_, which has type `BuiltinInteger`.
3. The `BuiltinInteger` to convert. This is the _input_.

If the input is negative, `builtinIntegerToByteString` fails. In this case, the
resulting error message must specify _at least_ the following information:

- That `builtinIntegerToByteString` failed due to a negative conversion attempt;
  and
- What negative `BuiltinInteger` was passed as the input.

If the length argument is outside the closed interval $(0, 2^{29} - 1)$,
`builtinIntegerToByteString` fails. In this case, the resulting error message
must specify _at least_ the following information:

- That `builtinIntegerToByteString` failed due to an invalid length argument;
  and
- What `BuiltinInteger` was passed as the length argument.

If the input is `0`, `builtinIntegerToByteString` returns the
`BuiltinByteString` consisting of a number of zero bytes equal to the length
argument.

If the input is positive, and the length argument is also positive, let `d` be
the minimum number of digits required to represent the input (as per the
representation described above). If `d` is greater than the length argument,
`builtinIntegerToByteString` fails. In this case, the resulting error message
must specify _at least_ the following information:

- That `builtinIntegerToByteString` failed due to the requested length being
  insufficient for the input;
- What `BuiltinInteger` was passed as the length argument; and
- What `BuiltinInteger` was passed as the input.

If `d` is equal to, or greater, than the length argument,
`builtinIntegerToByteString` returns the `BuiltinByteString` encoding the input.
This will be the most-significant-first encoding if the endianness argument is
`True`, or the most-significant-last encoding if the endianness argument is
`False`. The resulting `BuiltinByteString` will be padded to the length
specified by the padding argument if necessary.

If the input is positive, and the length argument is zero,
`builtinIntegerToByteString` returns the `BuiltinByteString` encoding the input.
Its length will be minimal (that is, no padding will be done). If the endianness
argument is `True`, the result will use the most-significant-first encoding, and
if the endianness argument is `False`, the result will use the
most-significant-last encoding.

We give some examples of the intended behaviour of `builtinIntegerToByteString`
below:

```haskell
- - fails due to negative input
builtinIntegerToByteString False 0 (-1) -- => ERROR
- - endianness argument doesn't affect this case
builtinIntegerToByteString True 0 (-1) -- => ERROR
- - length argument doesn't affect this case
builtinIntegerToByteString False 100 (-1) -- => ERROR
- - zero case, no padding
builtinIntegerToByteString False 0 0 -- => []
- - endianness argument doesn't affect this case
builtinIntegerToByteString True 0 0 -- => []
- - length argument adds more zeroes, but endianness doesn't matter
builtinIntegerToByteString False 5 0 -- => [ 0x00, 0x00, 0x00, 0x00, 0x00 ]
builtinIntegerToByteString True 5 0 -- => [ 0x00, 0x00, 0x00, 0x00, 0x00 ]
- - length argument too large (2^29)
builtinIntegerToByteString False 536870912 0 -- => ERROR
- - endianness doesn't affect this case
builtinIntegerToByteString True 536870912 0 -- => ERROR
- - fails due to insufficient digits (404 needs 2)
builtinIntegerToByteString False 1 404 -- => ERROR
- - endianness argument doesn't affect this case
builtinIntegerToByteString True 1 404 -- => ERROR
- - zero length argument is exactly the same as requesting exactly the right
- - digit count
builtinIntegerToByteString False 2 404 -- => [ 0x94, 0x01 ]
builtinIntegerToByteString False 0 404 -- => [ 0x94, 0x01 ]
- - switching endianness argument reverses the result
builtinIntegerToByteString True 2 404 -- => [ 0x01, 0x94 ]
builtinIntegerToByteString True 0 404 -- => [ 0x01, 0x94 ]
- - padding for most-significant-last goes at the end
builtinIntegerToByteString False 5 404 -- => [ 0x94, 0x01, 0x00, 0x00, 0x00 ]
- - padding for most-significant-first goes at the start
builtinIntegerToByteString True 5 404 -- => [ 0x00, 0x00, 0x00, 0x01, 0x94 ]
```

We also describe properties that any implementation of `builtinIntegerToByteString` must
have. Throughout, `q` is not negative, `p` is positive, `d` is in the closed
interval $(0, 2^{29} - 1)$, `k` is in the closed interval $(1, 2^{29} - 1)$,
`0 <= j < k`, and `1 <= r <= 255`. We also define `singleton x = consByteString
x emptyByteString`.

1. `lengthOfByteString (builtinIntegerToByteString e d 0) = d`
2. `indexByteString (builtinIntegerToByteString e k 0) j = 0`
3. `lengthOfByteString (builtinIntegerToByteString e 0 p) > 0`
4. `builtinIntegerToByteString False 0 (multiplyInteger p 256) = consByteString 0
   (builtinIntegerToByteString False 0 p)`
5. `builtinIntegerToByteString True 0 (multiplyInteger p 256) = appendByteString
   (builtinIntegerToByteString True 0 p) (singleton 0)`
6. `builtinIntegerToByteString False 0 (plusInteger (multiplyInteger q 256) r) =
   appendByteString (builtinIntegerToByteString False 0 r)`
   (builtinIntegerToByteString False 0 q)`
7. `builtinIntegerToByteString True 0 (plusInteger (multiplyInteger q 256) r) =
   appendByteString (builtinIntegerToByteString False 0 q)
   (builtinIntegerToByteString False 0 r)`

### `builtinByteStringToInteger`

The `builtinByteStringToInteger` primitive takes two arguments. We specify, and
name, these below:

1. Whether the input uses the most-significant-first encoding. This is the
   _stated endianness argument_, which has type `BuiltinBool`.
2. The `BuiltinByteString` to convert. This is the _input_.

If the input is the empty `BuiltinByteString`, `builtinByteStringToInteger`
returns 0. If the input is non-empty, `builtinByteStringToInteger` produces the
`BuiltinInteger` encoded by the input. The encoding is treated as
most-significant-first if the stated endianness argument is `True`, and
most-significant-last if the stated endianness argument is `False`. The input
encoding may be padded or not.

We give some examples of the intended behaviour of `builtinByteStringToInteger`
below:

```haskell
- - empty input gives zero
builtinByteStringToInteger False emptyByteString => 0
- - stated endianness argument doesn't affect this case
builtinByteStringToInteger True emptyByteString => 0
- - if all the bytes are the same, stated endianness argument doesn't matter
builtinByteStringToInteger False (consByteString 0x01 (consByteString 0x01
emptyByteString) -- => 257
builtinByteStringToInteger True (consByteString 0x01 (consByteString 0x01
emptyByteString) -- => 257
- - most-significant-first padding is at the start
builtinByteStringToInteger True (consByteString 0x00 (consByteString 0x01
(consByteString 0x01 emptyByteString))) -- => 257
builtinByteStringToInteger False (consByteString 0x00 (consByteString 0x01
(consByteString 0x01 emptyByteString))) -- => 65792
- - most-significant-last padding is at the end
builtinByteStringToInteger False (consByteString 0x01 (consByteString 0x01
(consByteString 0x00 emptyByteString) -- => 257
builtinByteStringToInteger True (consByteString 0x01 (consByteString 0x01
(consByteString 0x00 emptyByteString) -- => 65792
```

We also describe properties that any `builtinByteStringToInteger` implementation
must have. Throughout, `q` is not negative and `0 <= w8 <= 255`.

1. `builtinByteStringToInteger b (builtinIntegerToByteString b 0 q) = q`
2. `builtinByteStringToInteger b (consByteString w8 emptyByteString) = w8`
3. `builtinIntegerToByteString b (lengthOfByteString bs) (builtinByteStringToInteger b bs) =
   bs`

## Rationale: how does this CIP achieve its goals?

We believe that these operations address both of the described cases well, while
also meeting the goals stated at the start of this CIP. Our specified primitives
address both the problems of endianness and padding specified in Case 2, while
also ensuring that use cases like Case 1 (where bounding length isn't important)
are not made more difficult than necessary. The representation we have chosen is
metadata-free, doesn't depend on any representation choices (current or future)
of `BuiltinInteger`, while also being flexible enough to satisfy both cases
where endianness and padding matter, and when they don't.

### Alternative possibilities

As part of this proposal, we considered two alternative possibilities:

1. Use the [CIP-0058][cip-0058] versions of these operations; and
2. Have a uniform treatment of the length argument for
   `builtinIntegerToByteString` (always minimum or always maximum).

[CIP-0058][cip-0058] defines a sizeable collection of bitwise primitive
operations for Plutus Core, mostly for use over `BuiltinByteString`s. As part of
these, it also defines conversion functions similar to
`builtinIntegerToByteString` and `builtinByteStringToInteger`, which are named
`integerToByteString` and `byteStringToInteger` respectively. Unlike the
operations specified in this CIP, the CIP-0058 operations do not address the
problems of either padding or endianness: more precisely, the representations
constructed are always minimally-sized, and use a big-endian encoding. While in
the context they are being presented in, these choices are defensible, they do
not adequately address Case 2, and in
particular, many cryptographic constructions used with finite fields. Users of
the CIP-0058 primitives who needed to ensure a minimum length of a converted
`BuiltinInteger` would have to pad manually, which CIP-0058 gives no additional
support for; additionally, if a little-endian representation was required, the
`BuiltinByteString` result would have to be reversed, which has quadratic cost
if using only Plutus Core primitives. Thus, we consider these implementations to
be a good attempt, but not suited to even their intended use, much less more
general applications.

An alternative possibility for the length argument of
`builtinIntegerToByteString` would be to treat the argument as either a minimum,
or a maximum, rather than our more hybrid approach. Specifically, for any input
`i`, let `d` be the minimum number of digits required to represent `i` as per
the description of our representation, and let `k` be the length argument.
Then:

- The _minimum length argument_ approach would produce a result of size
  $\min \\{ \texttt{d}, \texttt{k} \\}$; that is, if the length argument is
  smaller than the minimum required digits, the minimum would be used instead.
- The _maximum length argument_ approach would produce a result of size `k`,
  and would error if $\texttt{d} > \texttt{k}$.

Both the minimum length argument, and the maximum length argument, approaches
have merits. The maximum length argument approach in particular is useful for Case 2:
in such a setting, we already know the maximum size of
any element's representation, and if we somehow ended up with a larger
representation than this, it would be a mistake, which the maximum length
argument would catch immediately. For the minimum length argument approach, the
advantage would be more for Case 1: where the length of the
representation is not known (and the user isn't particularly concerned anyway).
In such a situation, the user could pass any argument and know that the
conversion would still work.

However, both of these approaches have disadvantages as well. The minimum
length argument approach would be more tedious to use with Case 2, as each
conversion would require a size check of the resulting
`BuiltinByteString`. While this is not expensive, it is annoying, and given the
complexity of the constructions that would be built atop of any finite field
implementations, it feels unreasonable to require this from users. Likewise, the
maximum length argument approach is unreasonable for situations like Case 1: the only
way to establish how many digits would be required involves performing an
integer logarithm in base 256, which is inefficient and error-prone. Our hybrid
approach gives the benefits of both the minimum length argument and maximum
length argument approaches, without the downsides of either: we observe that,
for situations like Case 1, the length argument would be `0` in practically all
cases, which is a value that would not be useful in any situation where the maximum
length argument approach would be used. This observation allows our approach to work
equally well for both Case 1 and 2, with minimal friction.

## Path to Active

### Acceptance Criteria

We consider the following criteria to be essential for acceptance:

- A proof-of-concept implementation of the operation specified here must exist,
  outside of the Plutus source tree. The implementation must be in Haskell.
- The proof-of-concept implementation must have tests, demonstrating that it
  behaves as the specification requires, and that the representations it
  produces match the described representation in this CIP.
- The proof-of-concept implementation must demonstrate that it will successfully
  build, and pass its tests, using all GHC versions currently usable to build
  Plutus (8.10, 9.2 and 9.6 at the time of writing), across all [Tier 1][tier-1]
  platforms.

Ideally, the implementation should also demonstrate its performance
characteristics by well-designed benchmarks.

[tier-1]: https://gitlab.haskell.org/ghc/ghc/-/wikis/platforms#tier-1-platforms

### Implementation Plan

MLabs have completed the implementation of the proof of concept as required
(located [here](https://github.com/mlabs-haskell/plutus-integer-bytestring).
This implementation has been merged into Plutus Core, and will be released in
the upcoming V3 release.

## Copyright

This CIP is licensed under the [Apache-2.0][Apache-2.0] license.

[CC-BY-4.0]: https://creativecommons.org/licenses/by/4.0/legalcode
[Apache-2.0]: http://www.apache.org/licenses/LICENSE-2.0

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0122/README.md
---

- --
CIP: 122
Title: Logical operations over BuiltinByteString
Category: Plutus
Status: Active
Authors:
- Koz Ross <koz@mlabs.city>
Implementors:
- Koz Ross <koz@mlabs.city>
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/806
Created: 2024-05-03
License: Apache-2.0
- --

## Abstract

We describe the semantics of a set of logical operations for Plutus
`BuiltinByteString`s. Specifically, we provide descriptions for:

- Bitwise logical AND, OR, XOR and complement;
- Reading a bit value at a given index;
- Setting bits value at given indices; and
- Replicating a byte a given number of times.

As part of this, we also describe the bit ordering within a `BuiltinByteString`,
and provide some laws these operations should obey.

## Motivation: why is this CIP necessary?

Bitwise operations, both over fixed-width and variable-width blocks of bits,
have a range of uses, including data structures (especially
[succinct][succinct-data-structures] ones) and cryptography. Currently,
operations on individual bits in Plutus Core are difficult, or outright
impossible, while also keeping within the tight constraints required onchain.
While it is possible to some degree to work with individual _bytes_ over
`BuiltinByteString`s, this isn't sufficient, or efficient, when bit
maniputations are required.

To demonstrate where bitwise operations would allow onchain possibilities that
are currently either impractical or impossible, we give the following use cases.

### Case 1: integer set

An _integer set_ (also known as a bit set, bitmap, or bitvector) is a
[succinct][succinct-data-structures] data structure for representing a set of
numbers in a pre-defined range $[0, n)$ for some $n \in \mathbb{N}$. The
structure supports the following operations:

- Construction given a fixed number of elements, as well as the bound $n$.
- Construction of the empty set (contains no elements) and the universe
  (contains all elements).
- Set union, intersection, complement and difference (symmetric and asymmetric).
- Membership testing for a specific element.
- Inserting or removing elements.

These structures have a range of uses. In addition to being used as sets of
bounded natural numbers, an integer set could also represent an array of Boolean
values. These have [a range of applications][bitvector-apps], mostly as
'backends' for other, more complex structures. Furthermore, by using some index
arithmetic, integer sets can also be used to represent
[binary matrices][binary-matrix] (in any number of
dimensions), which have an even wider range of uses:

- Representations of graphs in [adjacency-matrix][adjacency-matrix] form
- [Checking the rules for a game of Go][go-binary-matrix]
- [FSM representation][finite-state-machine-4vl]
- Representation of an arbitrary binary relation between finite sets

The succinctness of the integer set (and the other succinct data structures it
enables) is particularly valuable on-chain, due to the limited transaction size
and memory available.

Typically, such a structure would be represented as a packed array of bytes
(similar to the Haskell `ByteString`). Essentially, given a bound $n$, the
packed array has a length in bytes large enough to contain at least $n$ bits,
with a bit at position $i$ corresponding to the value $i \in \mathbb{N}$. This
representation ensures the succinctness of the structure (at most 7 bits of
overhead are required if $n = 8k + 1$ for some $k \in \mathbb{N}$), and
also allows all the above operations to be implemented efficiently:

- Construction given a fixed number of elements and the bound $n$ involves
  allocating the packed array, then modifying some bits to be set.
- Construction of the empty set is a packed array where every byte is `0x00`,
  while the universe is a packed array where every byte is `0xFF`.
- Set union is bitwise OR over both arguments.
- Set intersection is bitwise AND over both arguments.
- Set complement is bitwise complement over the entire packed array.
- Symmetric set difference is bitwise XOR over both arguments; asymmetric set
  difference can be defined using a combination of bitwise complement and
  bitwise OR.
- Membership testing is checking whether a bit is set.
- Inserting an element is setting the corresponding bit.
- Removing an element is clearing the corresponding bit.

Given that this is a packed representation, these operations can be implemented
very efficiently by relying on the cache-friendly properties of packed array
traversals, as well as making use of optimized routines available in many
languages. Thus, this structure can be used to efficiently represent sets of
numbers in any bounded range (as ranges not starting from $0$ can be represented
by storing an offset), while also being minimal in space usage.

Currently, such a structure cannot be easily implemented in Plutus Core while
preserving the properties described above. The two options using existing
primitives are either to use `[BuiltinInteger]`, or to mimic the above
operations over `BuiltinByteString`. The first of these is not space _or_
time-efficient: each `BuiltinInteger` takes up multiple machine words of space,
and the list overheads introduced are linear in the number of items stored,
destroying succinctness; membership testing, insertion and removal require
either maintaining an ordered list or forcing linear scans for at least some
operations, which are inefficient over lists; and 'bulk' operations like union,
intersection and complement become very difficult and time-consuming. The second
is not much better: while we preserve succinctness, there is no easy way to
access individual bits, only bytes, which would require a division-remainder
loop for each such operation, with all the overheads this imposes; intersection,
union and symmetric difference would have to be simulated byte-by-byte,
requiring large lookup tables or complex conditional logic; and construction
would require immense amounts of copying and tricky byte construction logic.
While it is not outright impossible to make such a structure using current
primitives, it would be so impractical that it could never see real use.

Furthermore, for sparse (or dense) integer sets (that is, where either most
elements in the range are absent or present respectively), a range of
[compression techniques][bitmap-index-compression] have been developed. All of
these rely on bitwise operations to achieve their goals, and can potentially
yield significant space savings in many cases. Given the limitations onchain
that we have to work within, having such techniques available to implementers
would be a huge potential advantage.

### Case 2: hashing

[Hashing][hashing], that is, computing a fixed-length 'fingerprint' or 'digest'
of a variable-length input (typically viewed as binary) is a common task
required in a range of applications. Most notably, hashing is a key tool in
cryptographic protocols and applications, either in its own right, or as part of
a larger task. The value of such functionality is such that Plutus Core already
contains primitives for certain hash functions, specifically two variants of
[SHA256][sha256] and [BLAKE2b][blake2b]. At the same time, hash functions
choices are often determined by protocol or use case, and providing individual
primitives for every possible hash function is not a scalable choice. It is much
preferrable to give necessary tools to implement such functionality to users of
Plutus (Core), allowing them to use whichever hash function(s) their
applications require.

As an example, we consider the [Argon2][argon2] family of hash functions. In
order to implement any variant of this family requires the following operations:

1. Conversion of numbers to bytes
2. Bytestring concatenation
3. BLAKE2b hashing
4. Floor division
5. Indexing bytes in a bytestring
6. Logical XOR

Operations 1 to 5 are already provided by Plutus Core (with 1 being included [in
CIP-121][conversion-cip]); however, without logical XOR, no function in the
Argon2 family could be implemented. While in theory, it could be simulated with
what operations already exist, much as with Case 1, this would be impractical at
best, and outright impossible at worst, due to the severe limits imposed
on-chain. This is particularly the case here, as all Argon2 variants call
logical XOR in a loop, whose step count is defined by _multiple_ user-specified
(or protocol-specified) parameters.

We observe that this requirement for logical XOR is not unique to the Argon2
family of hash functions. Indeed, logical XOR is widely used for [a variety of
cryptographic applications][xor-crypto], as it is a low-cost mixing
function that happens to be self-inverting, as well as preserving randomness
(that is, a random bit XORed with a non-random bit will give a random bit).

## Specification

We describe the proposed operations in several stages. First, we specify a
scheme for indexing individual bits (rather than whole bytes) in a
`BuiltinByteString`. We then specify the semantics of each operation, as well as
giving costing expectations and some examples. Lastly, we provide some laws that
any implementation of these operations is expected to obey.

### Bit indexing scheme

We begin by observing that a `BuiltinByteString` is a packed array of bytes
(that is, `BuiltinInteger`s in the range $[0, 255]$) according to the API
provided by existing Plutus Core primitives. In particular, we have the ability
to access individual bytes by index as a primitive operation. Thus, we can view
a `BuiltinByteString` as an indexed collection of bytes; for any
`BuiltinByteString` $b$ of length $n$, and any $i \in 0, 1, \ldots, n - 1$, we
define $b\\{i\\}$ as the byte at index $i$ in $b$, as defined by the
`indexByteString` primitive. In essence, for any `BuiltinByteString` of
length `n`, we have _byte_ indexes as follows:

```
| Index | 0  | 1  | ... | n - 1    |
|-------|----|----| ... |----------|
| Byte  | w0 | w1 | ... | w(n - 1) |
```

To view a `BuiltinByteString` as an indexed collection of _bits_, we must first
consider the bit ordering within a byte. Suppose $i \in 0, 1, \ldots, 7$ is an
index into a byte $w$. We say that the bit at $i$ in $w$ is _set_ when

$$
\left \lfloor \frac{w}{2^{i}} \right \rfloor \mod 2 \equiv 1
$$

Otherwise, the bit at $i$ in $w$ is _clear_. We define $w[i]$ to be $1$ when
the bit at $i$ in $w$ is set, and $0$ otherwise; this is the _value_ at index
$i$ in $w$.

For example, consider the byte represented by the `BuiltinInteger` 42. By the
above scheme, we have the following:

| Bit index | Set or clear? |
|-----------|---------------|
| $0$       | Clear         |
| $1$       | Set           |
| $2$       | Clear         |
| $3$       | Set           |
| $4$       | Clear         |
| $5$       | Set           |
| $6$       | Clear         |
| $7$       | Clear         |

Put another way, we can view $w[i] = 1$ to mean that the $(i + 1)$ th least significant
digit in $w$'s binary representation is $1$, and likewise, $w[i] = 0$ would mean
that the $i$th least significant digit in $w$'s binary representation is $0$.
Continuing with the above example, $42$ is represented in binary as `00101010`;
we can see that the second-least-significant, fourth-least-significant, and
sixth-least-significant digits are `1`, and all the others are zero. This
description mirrors the way bytes are represented on machine architectures.

We now extend the above scheme to `BuiltinByteString`s. Let $b$ be a
`BuiltinByteString` whose length is $n$, and let $i \in 0, 1, \ldots, 8 \cdot n - 1$.
For any $j \in 0, 1, \ldots, n - 1$, let $j^{\prime} = n - j - 1$. We say that the bit
at $i$ in $b$ is set if

$$
b\left\\{\left(\left\lfloor \frac{i}{8} \right\rfloor\right)^{\prime}\right\\}[i\mod 8] = 1
$$

We define the bit at $i$ in $b$ being clear analogously. Similarly to bits in a
byte, we define $b[i]$ to be $1$ when the bit at $i$ in $b$ is set, and $0$
otherwise; similarly to bytes, we term this the _value_ at index $i$ in $b$.

As an example, consider the `BuiltinByteString` `[42, 57, 133]`: that is, the
`BuiltinByteString` $b$ such that $b\\{0\\} = 42$, $b\\{1\\} = 57$ and $b\\{2\\}
= 133$. We observe that the range of 'valid' bit indexes $i$ into $b$ is in
$[0, 3 \cdot 8 - 1 = 23]$. Consider $i = 4$; by the definition above, this
corresponds to the _byte_ index 2, as $\left\lfloor\frac{4}{8}\right\rfloor =
0$, and $3 - 0 - 1 = 2$ (as $b$ has length $3$). Within the byte $133$, this
means we have $\left\lfloor\frac{133}{2^4}\right\rfloor \mod 2 \equiv 0$. Thus,
$b[4] = 0$. Consider instead the index $i = 19$; by the definition above, this
corresponds to the _byte_ index 0, as $\left\lfloor\frac{19}{8}\right\rfloor =
2$, and $3 - 2 - 1 = 0$. Within the byte $42$, this means we have
$\left\lfloor\frac{42}{2^3}\right\rfloor\mod 2 \equiv 1$. Thus, $b[19] = 1$.

Put another way, our _byte_ indexes run 'the opposite way' to our _bit_ indexes.
Thus, for any `BuiltinByteString` of length $n$, we have _bit_ indexes relative
_byte_ indexes as follows:

```
| Byte index | 0                              | 1  | ... | n - 1                         |
|------------|--------------------------------|----| ... |-------------------------------|
| Byte       | w0                             | w1 | ... | w(n - 1)                      |
|------------|--------------------------------|----| ... |-------------------------------|
| Bit index  | 8n - 1 | 8n - 2 | ... | 8n - 8 |   ...    | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0 |
```

### Operation semantics

We describe precisely the operations we intend to implement, and their
semantics. These operations will have the following signatures:

- `bitwiseLogicalAnd :: BuiltinBool -> BuiltinByteString -> BuiltinByteString ->
  BuiltinByteString`
- `bitwiseLogicalOr :: BuiltinBool -> BuiltinByteString -> BuiltinByteString ->
  BuiltinByteString`
- `bitwiseLogicalXor :: BuiltinBool -> BuiltinByteString -> BuiltinByteString ->
  BuiltinByteString`
- `bitwiseLogicalComplement :: BuiltinByteString -> BuiltinByteString`
- `readBit :: BuiltinByteString -> BuiltinInteger -> BuiltinBool`
- `writeBits :: BuiltinByteString -> [(BuiltinInteger, BuiltinBool)] ->
  BuiltinByteString`
- `replicateByteString :: BuiltinInteger -> BuiltinInteger -> BuiltinByteString`

We assume the following costing, for both memory and execution time:

| Operation | Cost |
|-----------|------|
| `bitwiseLogicalAnd` | Linear in longest `BuiltinByteString` argument |
| `bitwiseLogicalOr` | Linear in longest `BuiltinByteString` argument |
| `bitwiseLogicalXor` | Linear in longest `BuiltinByteString` argument |
| `bitwiseLogicalComplement` | Linear in `BuiltinByteString` argument |
| `readBit` | Constant |
| `writeBits` | Additively linear in both arguments |
| `replicateByteString` | Linear in the _value_ of the first argument |

#### Padding versus truncation semantics

For the binary logical operations (that is, `bitwiseLogicalAnd`,
`bitwiseLogicalOr` and `bitwiseLogicalXor`), the we have two choices of
semantics when handling `BuiltinByteString` arguments of different lengths. We
can either produce a result whose length is the _minimum_ of the two arguments
(which we call _truncation semantics_), or produce a result whose length is the
_maximum_ of the two arguments (which we call _padding semantics_). As these can
both be useful depending on context, we allow both, controlled by a
`BuiltinBool` flag, on all the operations listed above.

In cases where we have arguments of different lengths, in order to produce a
result of the appropriate lengths, one of the arguments needs to be either
padded or truncated. Let `short` and `long` refer to the `BuiltinByteString`
argument of shorter length, and of longer length, respectively. The following
table describes what happens to the arguments before the operation:

| Semantics | `short` | `long` |
|-----------|---------|--------|
| Padding   | Pad at high _byte_ indexes | Unchanged |
| Truncation | Unchanged | Truncate high _byte_ indexes |

We pad with different bytes depending on operation: for `bitwiseLogicalAnd`, we
pad with `0xFF`, while for `bitwiseLogicalOr` and `bitwiseLogicalXor` we pad
with `0x00` instead. We refer to arguments so changed as
_semantics-modified_ arguments.

For example, consider the `BuiltinByteString`s `x = [0x00, 0xF0, 0xFF]` and `y =
[0xFF, 0xF0]`. The following table describes what the semantics-modified
versions of these arguments would become for each operation and each semantics:

| Operation | Semantics | `x` | `y` |
|-----------|-----------|-----|-----|
| `bitwiseLogicalAnd` | Padding | `[0x00, 0xF0, 0xFF]` | `[0xFF, 0xF0, 0xFF]` |
| `bitwiseLogicalAnd` | Truncation | `[0x00, 0xF0]` | `[0xFF, 0xF0]` |
| `bitwiseLogicalOr` | Padding | `[0x00, 0xF0, 0xFF]` | `[0xFF, 0xF0, 0x00]` |
| `bitwiseLogicalor` | Truncation | `[0x00, 0xF0]` | `[0xFF, 0xF0]` |
| `bitwiseLogicalXor` | Padding | `[0x00, 0xF0, 0xFF]` | `[0xFF, 0xF0, 0x00]` |
| `bitwiseLogicalXor` | Truncation | `[0x00, 0xF0]` | `[0xFF, 0xF0]` |

Based on the above, we observe that under padding semantics, the result of any
of the listed operations would have a byte length of 3, while under truncation
semantics, the result would have a byte length of 2 instead.

#### `bitwiseLogicalAnd`

`bitwiseLogicalAnd` takes three arguments; we name and describe them below.

1. Whether padding semantics should be used. If this argument is `False`,
   truncation semantics are used instead. This is the _padding semantics
   argument_, and has type `BuiltinBool`.
2. The first input `BuiltinByteString`. This is the _first data argument_.
3. The second input `BuiltinByteString`. This is the _second data argument_.

Let $b_1, b_2$ refer to the semantics-modified first data argument and
semantics-modified second data argument respectively, and let $n$ be either of
their lengths in bytes; see the
[section on padding versus truncation semantics](#padding-versus-truncation-semantics)
for the exact specification of this. Let the result of `bitwiseLogicalAnd`, given
$b_1, b_2$ and some padding semantics argument, be $b_r$, also of length $n$
in bytes. We use $b_1\\{i\\}$ to refer to the byte at index $i$ in $b_1$ (and
analogously for $b_2$, $b_r$); see the [section on the bit indexing
scheme](#bit-indexing-scheme) for the exact specification of this.

For all $i \in 0, 1, \ldots, n - 1$, we have
$b_r\\{i\\} = b_0\\{i\\} \text{ }\\& \text{ } b_1\\{i\\}$, where $\\&$ refers to a
[bitwise AND][bitwise-and].

Some examples of the intended behaviour of `bitwiseLogicalAnd` follow. For
brevity, we write `BuiltinByteString` literals as lists of hexadecimal values.

```
- - truncation semantics
bitwiseLogicalAnd False [] [0xFF] => []

bitwiseLogicalAnd False [0xFF] [] => []

bitwiseLogicalAnd False [0xFF] [0x00] => [0x00]

bitwiseLogicalAnd False [0x00] [0xFF] => [0x00]

bitwiseLogicalAnd False [0x4F, 0x00] [0xF4] => [0x44]

- - padding semantics
bitwiseLogicalAnd True [] [0xFF] => [0xFF]

bitwiseLogicalAnd True [0xFF] [] => [0xFF]

bitwiseLogicalAnd True [0xFF] [0x00] => [0x00]

bitwiseLogicalAnd True [0x00] [0xFF] => [0x00]

bitwiseLogicalAnd True [0x4F, 0x00] [0xF4] => [0x44, 0x00]
```

#### `bitwiseLogicalOr`

`bitwiseLogicalOr` takes three arguments; we name and describe them below.

1. Whether padding semantics should be used. If this argument is `False`,
   truncation semantics are used instead. This is the _padding semantics
   argument_, and has type `BuiltinBool`.
2. The first input `BuiltinByteString`. This is the _first data argument_.
3. The second input `BuiltinByteString`. This is the _second data argument_.

Let $b_1, b_2$ refer to the semantics-modified first data argument and
semantics-modified second data argument respectively, and let $n$ be either of
their lengths in bytes; see the
[section on padding versus truncation semantics](#padding-versus-truncation-semantics)
for the exact specification of this. Let the result of `bitwiseLogicalOr`, given
$b_1, b_2$ and some padding semantics argument, be $b_r$, also of length $n$
in bytes. We use $b_1\\{i\\}$ to refer to the byte at index $i$ in $b_1$ (and
analogously for $b_2$, $b_r$); see the [section on the bit indexing
scheme](#bit-indexing-scheme) for the exact specification of this.

For all $i \in 0, 1, \ldots, n - 1$, we have
$b_r\\{i\\} = b_0\\{i\\} \text{ } \| \text{ } b_1\\{i\\}$, where $\|$ refers to
a [bitwise OR][bitwise-or].

```
- - truncation semantics
bitwiseLogicalOr False [] [0xFF] => []

bitwiseLogicalOr False [0xFF] [] => []

bitwiseLogicalOr False [0xFF] [0x00] => [0xFF]

bitwiseLogicalOr False [0x00] [0xFF] => [0xFF]

bitwiseLogicalOr False [0x4F, 0x00] [0xF4] => [0xFF]

- - padding semantics
bitwiseLogicalOr True [] [0xFF] => [0xFF]

bitwiseLogicalOr True [0xFF] [] => [0xFF]

bitwiseLogicalOr True [0xFF] [0x00] => [0xFF]

bitwiseLogicalOr True [0x00] [0xFF] => [0xFF]

bitwiseLogicalOr True [0x4F, 0x00] [0xF4] => [0xFF, 0x00]
```

#### `bitwiseLogicalXor`

`bitwiseLogicalXor` takes three arguments; we name and describe them below.

1. Whether padding semantics should be used. If this argument is `False`,
   truncation semantics are used instead. This is the _padding semantics
   argument_, and has type `BuiltinBool`.
2. The first input `BuiltinByteString`. This is the _first data argument_.
3. The second input `BuiltinByteString`. This is the _second data argument_.

Let $b_1, b_2$ refer to the semantics-modified first data argument and
semantics-modified second data argument respectively, and let $n$ be either of
their lengths in bytes; see the
[section on padding versus truncation semantics](#padding-versus-truncation-semantics)
for the exact specification of this. Let the result of `bitwiseLogicalXor`, given
$b_1, b_2$ and some padding semantics argument, be $b_r$, also of length $n$
in bytes. We use $b_1\\{i\\}$ to refer to the byte at index $i$ in $b_1$ (and
analogously for $b_2$, $b_r$); see the [section on the bit indexing
scheme](#bit-indexing-scheme) for the exact specification of this.

For all $i \in 0, 1, \ldots, n - 1$, we have
$b_r\\{i\\} = b_0\\{i\\} \text{ } \wedge \text{ } b_1\\{i\\}$, where $\wedge$ refers to
a [bitwise XOR][bitwise-xor].

Some examples of the intended behaviour of `bitwiseLogicalXor` follow. For
brevity, we write `BuiltinByteString` literals as lists of hexadecimal values.

```
- - truncation semantics
bitwiseLogicalXor False [] [0xFF] => []

bitwiseLogicalXor False [0xFF] [] => []

bitwiseLogicalXor False [0xFF] [0x00] => [0xFF]

bitwiseLogicalXor False [0x00] [0xFF] => [0xFF]

bitwiseLogicalXor False [0x4F, 0x00] [0xF4] => [0xBB]

- - padding semantics
bitwiseLogicalOr True [] [0xFF] => [0xFF]

bitwiseLogicalOr True [0xFF] [] => [0xFF]

bitwiseLogicalOr True [0xFF] [0x00] => [0xFF]

bitwiseLogicalOr True [0x00] [0xFF] => [0xFF]

bitwiseLogicalOr True [0x4F, 0x00] [0xF4] => [0xBB, 0x00]
```

#### `bitwiseLogicalComplement`

`bitwiseLogicalComplement` takes a single argument, of type `BuiltinByteString`;
let $b$ refer to that argument, and $n$ its length in bytes. Let $b_r$ be
the result of `bitwiseLogicalComplement`; its length in bytes is also $n$. We
use $b[i]$ to refer to the value at index $i$ of $b$ (and analogously for $b_r$);
see the [section on the bit indexing scheme](#bit-indexing-scheme) for the exact
specification of this.

For all $i \in 0, 1, \ldots , 8 \cdot n - 1$, we have

$$
b_r[i] = \begin{cases}
        0 & \text{if } b[i] = 1\\
        1 & \text{otherwise}\\
        \end{cases}
$$

Some examples of the intended behaviour of `bitwiseLogicalComplement` follow. For
brevity, we write `BuiltinByteString` literals as lists of hexadecimal values.

```
bitwiseLogicalComplement [] => []

bitwiseLogicalComplement [0x0F] => [0xF0]

bitwiseLogicalComplement [0x4F, 0xF4] => [0xB0, 0x0B]
```

#### `readBit`

`readBit` takes two arguments; we name and describe them below.

1. The `BuiltinByteString` in which the bit we want to read can be found. This
   is the _data argument_.
2. A bit index into the data argument, of type `BuiltinInteger`. This is the
   _index argument_.

Let $b$ refer to the data argument, of length $n$ in bytes, and let $i$ refer to
the index argument. We use $b[i]$ to refer to the value at index $i$ of $b$; see
the [section on the bit indexing scheme](#bit-indexing-scheme) for the exact
specification of this.

If $i < 0$ or $i \geq 8 \cdot n$, then `readBit`
fails. In this case, the resulting error message must specify _at least_ the
following information:

- That `readBit` failed due to an out-of-bounds index argument; and
- What `BuiltinInteger` was passed as an index argument.

Otherwise, if $b[i] = 0$, `readBit` returns `False`, and if $b[i] = 1$,
`readBit` returns `True`.

Some examples of the intended behaviour of `readBit` follow. For
brevity, we write `BuiltinByteString` literals as lists of hexadecimal values.

```
- - Indexing an empty BuiltinByteString fails
readBit [] 0 => error

readBit [] 345 => error

- - Negative indexes fail
readBit [] (-1) => error

readBit [0xFF] (-1) => error

- - Indexing reads 'from the end'
readBit [0xF4] 0 => False

readBit [0xF4] 1 => False

readBit [0xF4] 2 => True

readBit [0xF4] 3 => False

readBit [0xF4] 4 => True

readBit [0xF4] 5 => True

readBit [0xF4] 6 => True

readBit [0xF4] 7 => True

- - Out-of-bounds indexes fail
readBit [0xF4] 8 => error

readBit [0xFF, 0xF4] 16 => error

- - Larger indexes read backwards into the bytes from the end
readBit [0xF4, 0xFF] 10 => False
```

#### `writeBits`

`writeBits` takes two arguments: we name and describe them below.

1. The `BuiltinByteString` in which we want to change some bits. This is the
   _data argument_.
2. A list of index-value pairs, indicating which positions in the data argument
   should be changed to which value. This is the _change list argument_. Each
   index has type `BuiltinInteger`, while each value has type `BuiltinBool`.

Let $b$ refer to the data argument of length $n$ in bytes. We define `writeBits`
recursively over the structure of the change list argument. Throughout, we use
$b_r$ to refer to the result of `writeBits`, whose length is also $n$. We use
$b[i]$ to refer to the value at index $i$ of $b$ (and analogously, $b_r$); see
the [section on the bit indexing scheme](#bit-indexing-scheme) for the exact
specification of this.

If the change list argument is empty, we return the data argument unchanged.
Otherwise, let $(i, v)$ be the head of the change list argument, and $\ell$ its
tail. If $i < 0$ or $i \geq 8 \cdot n$, then `writeBits` fails. In this case,
the resulting error message must specify at _least_ the following information:

- That `writeBits` failed due to an out-of-bounds index argument; and
- What `BuiltinInteger` was passed as $i$.

Otherwise, for all $j \in 0, 1, \ldots 8 \cdot n - 1$, we have

$$
b_r[j] = \begin{cases}
         0 & \text{if } j = i \text{ and } v = \texttt{False}\\
         1 & \text{if } j = i \text{ and } v = \texttt{True}\\
         b[j] & \text{otherwise}\\
         \end{cases}
$$

Then, if we did not fail as described above, we repeat the `writeBits`
operation, but with $b_r$ as the data argument and $\ell$ as the change list
argument.

Some examples of the intended behaviour of `writeBits` follow. For
brevity, we write `BuiltinByteString` literals as lists of hexadecimal values.

```
- - Writing an empty BuiltinByteString fails
writeBits [] [(0, False)] => error

- - Irrespective of index
writeBits [] [(15, False)] => error

- - And value
writeBits [] [(0, True)] => error

- - And multiplicity
writeBits [] [(0, False), (1, False)] => error

- - Negative indexes fail
writeBits [0xFF] [((-1), False)] => error

- - Even when mixed with valid ones
writeBits [0xFF] [(0, False), ((-1), True)] => error

- - In any position
writeBits [0xFF] [((-1), True), (0, False)] => error

- - Out-of-bounds indexes fail
writeBits [0xFF] [(8, False)] => error

- - Even when mixed with valid ones
writeBits [0xFF] [(1, False), (8, False)] => error

- - In any position
writeBits [0xFF] [(8, False), (1, False)] => error

- - Bits are written 'from the end'
writeBits [0xFF] [(0, False)] => [0xFE]

writeBits [0xFF] [(1, False)] => [0xFD]

writeBits [0xFF] [(2, False)] => [0xFB]

writeBits [0xFF] [(3, False)] => [0xF7]

writeBits [0xFF] [(4, False)] => [0xEF]

writeBits [0xFF] [(5, False)] => [0xDF]

writeBits [0xFF] [(6, False)] => [0xBF]

writeBits [0xFF] [(7, False)] => [0x7F]

- - True value sets the bit
writeBits [0x00] [(5, True)] => [0x20]

- - False value clears the bit
writeBits [0xFF] [(5, False)] => [0xDF]

- - Larger indexes write backwards into the bytes from the end
writeBits [0xF4, 0xFF] [(10, False)] => [0xF0, 0xFF]

- - Multiple items in a change list apply cumulatively
writeBits [0xF4, 0xFF] [(10, False), (1, False)] => [0xF0, 0xFD]

writeBits (writeBits [0xF4, 0xFF] [(10, False)]) [(1, False)] => [0xF0, 0xFD]

- - Order within a change list is unimportant among unique indexes
writeBits [0xF4, 0xFF] [(1, False), (10, False)] => [0xF0, 0xFD]

- - But _is_ important for identical indexes
writeBits [0x00, 0xFF] [(10, True), (10, False)] => [0x00, 0xFF]

writeBits [0x00, 0xFF] [(10, False), (10, True)] => [0x04, 0xFF]

- - Setting an already set bit does nothing
writeBits [0xFF] [(0, True)] => [0xFF]

- - Clearing an already clear bit does nothing
writeBits [0x00] [(0, False)] => [0x00]
```

#### `replicateByteString`

`replicateByteString` takes two arguments; we name and describe them below.

1. The desired result length, of type `BuiltinInteger`. This is the _length
   argument_.
2. The byte to place at each position in the result, represented as a
   `BuiltinInteger` (corresponding to the unsigned integer this byte encodes).
   This is the _byte argument_.

Let $n$ be the length argument, and $w$ the byte argument. If $n < 0$, then
`replicateByteString` fails. In this case, the resulting error message must specify
_at least_ the following information:

- That `replicateByteString` failed due to a negative length argument; and
- What `BuiltinInteger` was passed as the length argument.

If $n \geq 0$, and $w < 0$ or $w > 255$, then `replicateByteString` fails. In this
case, the resulting error message must specify _at least_ the following
information:

- That `replicateByteString` failed due to the byte argument not being a valid
  byte; and
- What `BuiltinInteger` was passed as the byte argument.

Otherwise, let $b$ be the result of `replicateByteString`, and let $b\\{i\\}$ be the
byte at position $i$ of $b$, as per [the section describing the bit indexing
scheme](#bit-indexing-scheme). We have:

- The length (in bytes) of $b$ is $n$; and
- For all $i \in 0, 1, \ldots, n - 1$, $b\\{i\\} = w$.

Some examples of the intended behaviour of `replicateByteString` follow. For
brevity, we write `BuiltinByteString` literals as lists of hexadecimal values.

```
- - Replicating a negative number of times fails
replicateByteString (-1) 0 => error

- - Irrespective of byte argument
replicateByteString (-1) 3 => error

- - Out-of-bounds byte arguments fail
replicateByteString 1 (-1) => error

replicateByteString 1 256 => error

- - Irrespective of length argument
replicateByteString 4 (-1) => error

replicateByteString 4 256 => error

- - Length of result matches length argument, and all bytes are the same
replicateByteString 0 0xFF => []

replicateByteString 4 0xFF => [0xFF, 0xFF, 0xFF, 0xFF]
```

### Laws

#### Binary operations

We describe laws for all three operations that work over two
`BuiltinByteStrings`, that is, `bitwiseLogicalAnd`, `bitwiseLogicalOr` and
`bitwiseLogicalXor`, together, as many of them are similar (and related). We
describe padding semantics and truncation semantics laws, as they are slightly
different.

All three operations above, under both padding and truncation semantics, are
[commutative semigroups][special-semigroups]. Thus, we have:

```haskell
bitwiseLogicalAnd s x y = bitwiseLogicalAnd s y x

bitwiseLogicalAnd s x (bitwiseLogicalAnd s y z) = bitwiseLogicalAnd s
(bitwiseLogicalAnd s x y) z

- - and the same for bitwiseLogicalOr and bitwiseLogicalXor
```

Note that the semantics (designated as `s` above) must be consistent in order
for these laws to hold. Furthermore, under padding semantics, all the above
operations are [commutative monoids][commutative-monoid]:

```haskell
bitwiseLogicalAnd True x "" = bitwiseLogicalAnd True "" x = x

- - and the same for bitwiseLogicalOr and bitwiseLogicalXor
```

Under truncation semantics, `""` (that is, the empty `BuiltinByteString`) acts
instead as an [absorbing element][absorbing-element]:

```haskell
bitwiseLogicalAnd False x "" = bitwiseLogicalAnd False "" x = ""

- - and the same for bitwiseLogicalOr and bitwiseLogicalXor
```

`bitwiseLogicalAnd` and `bitwiseLogicalOr` are also [semilattices][semilattice],
due to their idempotence:

```haskell
bitwiseLogicalAnd s x x = x

- - and the same for bitwiseLogicalOr
```

`bitwiseLogicalXor` is instead involute:

```haskell
bitwiseLogicalXor s x (bitwiseLogicalXor s x x) = bitwiseLogicalXor s
(bitwiseLogicalXor s x x) x = x
```

Additionally, under padding semantics, `bitwiseLogicalAnd` and
`bitwiseLogicalOr` are [self-distributive][distributive]:

```haskell
bitwiseLogicalAnd True x (bitwiseLogicalAnd True y z) = bitwiseLogicalAnd True
(bitwiseLogicalAnd True x y) (bitwiseLogicalAnd True x z)

bitwiseLogicalAnd True (bitwiseLogicalAnd True x y) z = bitwiseLogicalAnd True
(bitwiseLogicalAnd True x z) (bitwiseLogicalAnd True y z)

- - and the same for bitwiseLogicalOr
```

Under truncation semantics, `bitwiseLogicalAnd` is only left-distributive over
itself, `bitwiseLogicalOr` and `bitwiseLogicalXor`:

```haskell
bitwiseLogicalAnd False x (bitwiseLogicalAnd False y z) = bitwiseLogicalAnd
False (bitwiseLogicalAnd False x y) (bitwiseLogicalAnd False x z)

bitwiseLogicalAnd False x (bitwiseLogicalOr False y z) = bitwiseLogicalOr False
(bitwiseLogicalAnd False x y) (bitwiseLogicalAnd False x z)

bitwiseLogicalAnd False x (bitwiseLogicalXor False y z) = bitwiseLogicalXor
False (bitwiseLogicalAnd False x y) (bitwiseLogicalAnd False x z)
```

`bitwiseLogicalOr` under truncation semantics is left-distributive over itself
and `bitwiseLogicalAnd`:

```haskell
bitwiseLogicalOr False x (bitwiseLogicalOr False y z) = bitwiseLogicalOr False
(bitwiseLogicalOr False x y) (bitwiseLogicalOr False x z)

bitwiseLogicalOr False x (bitwiseLogicalAnd False y z) = bitwiseLogicalAnd False
(bitwiseLogicalOr False x y) (bitwiseLogicalOr False x z)
```

If the first and second data arguments to these operations have the same length,
these operations satisfy several additional laws. We describe these briefly
below, with the added note that, in this case, padding and truncation semantics
coincide:

- `bitwiseLogicalAnd` and `bitwiseLogicalOr` form a [bounded lattice][lattice]
- `bitwiseLogicalAnd` is [distributive][distributive] over itself, `bitwiseLogicalOr` and
  `bitwiseLogicalXor`
- `bitwiseLogicalOr` is [distributive][distributive] over itself and `bitwiseLogicalAnd`

We do not specify these laws here, as they do not hold in general. At the same
time, we expect that any implementation of these operations will be subject to
these laws.

#### `bitwiseLogicalComplement`

The main law of `bitwiseLogicalComplement` is involution:

```haskell
bitwiseLogicalComplement (bitwiseLogicalComplement x) = x
```

In combination with `bitwiseLogicalAnd` and `bitwiseLogicalOr`,
`bitwiseLogicalComplement` gives rise to the famous [De Morgan laws][de-morgan], irrespective of semantics:

```haskell
bitwiseLogicalComplement (bitwiseLogicalAnd s x y) = bitwiseLogicalOr s
(bitwiseLogicalComplement x) (bitwiseLogicalComplement y)

bitwiseLogicalComplement (bitwiseLogicalOr s x y) = bitwiseLogicalAnd s
(bitwiseLogicalComplement x) (bitwiseLogicalComplement y)
```

For `bitwiseLogicalXor`, we instead have (again, irrespective of semantics):

```haskell
bitwiseLogicalXor s x (bitwiseLogicalComplement x) = x
```

#### Bit reading and modification

Throughout, we assume any index arguments to be 'in-bounds'; that is, all the
index arguments used in the statements of any law are such that the operation
they are applied to wouldn't produce an error.

The first law of `writeBits` is similar to the [set-twice law of
lenses][lens-laws]:

```haskell
writeBits bs [(i, b1), (i, b2)] = writeBits bs [(i, b2)]
```

Together with `readBit`, we obtain the remaining two analogues to the lens
laws:

```haskell
- - writing to an index, then reading from that index, gets you what you wrote
readBit (writeBits bs [(i, b)]) i = b

- - if you read from an index, then write that value to that same index, nothing
- - happens
writeBits bs [(i, readBit bs i)] = bs
```

Furthermore, given a fixed data argument, `writeBits` acts as a [monoid
homomorphism][monoid-homomorphism] lists under concatenation to functions:

```haskell
writeBits bs [] = bs

writeBits bs (is <> js) = writeBits (writeBits bs is) js
```

#### `replicateByteString`

Given a fixed byte argument, `replicateByteString` acts as a [monoid
homomorphism][monoid-homomorphism] from natural numbers under addition to
`BuiltinByteString`s under concatenation:

```haskell
replicateByteString 0 w = ""

replicateByteString (n + m) w = replicateByteString n w <> replicateByteString m w
```

Additionally, for any 'in-bounds' index (that is, any index for which
`indexByteString` won't error) `i`, we have

```haskell
indexByteString (replicateByteString n w) i = w
```

Lastly, we have

```haskell
lengthByteString (replicateByteString n w) = n
```

## Rationale: how does this CIP achieve its goals?

The operations, and semantics, described in this CIP provide a set of
well-defined bitwise logical operations, as well as bitwise access and
modification, to allow cases similar to Case 1 to be performed efficiently and
conveniently. Furthermore, the semantics we describe would be reasonably
familiar to users of other programming languages (including Haskell) which have
provisions for bitwise logical operations of this kind, as well as some way of
extending these operations to operate on packed byte vectors. At the same time,
there are several choices we have made that are somewhat unusual, or could
potentially have been implemented differently based on existing work: most
notably, our choice of bit indexing scheme, the padding-versus-truncation
semantics, and the multiplicitous definition of bit modification. Among existing
work, a particularly important example is [CIP-58][cip-58], which makes
provisions for operations similar to the ones described here, and from which we
differ in several important ways. We clarify the reasoning behind our choices,
and how they differ from existing work, below.

Aside from the issues we list below, we don't consider other operations
controversial. Indeed, `bitwiseLogicalComplement` has a direct parallel to the
implementation in [CIP-58][cip-58], and `replicateByteString` is a direct wrapper
around the `replicate` function in `ByteString`. Thus, we do not discuss them
further here.

### Relationship to CIP-58 and CIP-121

Our work relates to both [CIP-58][cip-58] and [CIP-121][cip-121]. Essentially,
our goal with both this CIP and CIP-121 is to both break CIP-58 into more
manageable (and reviewable) parts, and also address some of the design choices
in CIP-58 that were not as good (or as clear) as they could have been. In this
regard, this CIP is a direct continuation of CIP-121; CIP-121 dealt with
conversions between `BuiltinByteString` and `BuiltinInteger`, while this CIP
handles bit indexing more generally, as well as 'parallel' logical operations
that operate on all the bits of a `BuiltinByteString` in bulk.

We describe how our work in this CIP relates to (and in some cases, supercedes)
CIP-58, as well as how it follows on from CIP-121, in more detail below.

### Bit indexing scheme

The bit indexing scheme we describe here is designed around two
considerations. Firstly, we want operations on these bits, as well as those
results, to be as consistent and as predictable as possible: any individual
familiar with such operations on variable-length bitvectors from another
language shouldn't be surprised by the semantics. Secondly, we want to
anticipate future bitwise operation extensions, such as shifts and rotations,
and have the indexing scheme support efficient implementations (and predictable
semantics) for these.

While prior art for bit access (and modification) exists
in almost any programming language, these are typically over types of fixed
width (usually bytes, machine words, or something similar); for variable-width
types, these typically are either not implemented at all, or if they are
implemented, this is done in an external library, with varying support for
certain operations. An example of the first is Haskell's `ByteString`, which has
no way to even access, much less modify, individual bits; an example of the
second is the [CRoaring][croaring] library for C, which supports all the
operations we describe in this CIP, along with multiple others. In the second
case, the _exact_ arrangement of bits inside the representation is not something
users are exposed to directly: instead, the bitvector type is opaque, and the
library only guarantees consistency of API. In our case, this is not a viable
choice, as we require bit access _and_ byte access to both work on
`BuiltinByteString`, and thus, some consistency of representation is required.

The scheme for indexing bits within a byte that we describe in [the relevant
section](#bit-indexing-scheme) is the same as the one used by the `Data.Bits`
API in Haskell for `Word8` bit indexing, and mirrors the decisions of most
languages that provide such an API at all, as well as the conventional
definition of such operations as `(w >> i) & 1` for access, `w | (1 << i)` for
setting, and `w & ~(1 << i)` for clearing. We could choose to 'flip' this
indexing, by using a similar operation for 'index flipping' as we currently use
for bytes: essentially, instead of

$$
\left \lfloor \frac{w}{2^{i}} \right \rfloor \mod 2 \equiv 1
$$

we would instead use

$$
\left \lfloor \frac{w}{2^{8 - i - 1}} \right \rfloor \mod 2 \equiv 1
$$

to designate bit $i$ as set (and analogously for clear). Together with the
ability to choose _not_ to flip the _byte_ index, we get four possibilities,
which have [been described previously][too-many-ways-1]. For clarity, we name,
and describe, them below. Throughout, we use `n` as the length of a given
`BuiltinByteString` in bytes.

The first possibility is that we 'flip' neither bit, nor byte, indexes. We call
this the _no-flip variant_:

```
| Byte index | 0                             | 1                 | ... | n - 1                          |
|------------|-------------------------------|-------------------| ... |--------------------------------|
| Byte       | w0                            | w1                | ... | w(n - 1)                       |
|------------|-------------------------------|-------------------| ... |--------------------------------|
| Bit index  | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0 | 15 | 14 | ... | 8 | ... | 8n - 1 | 8n - 2 | ... | 8n - 8 |
```

The second possibility is that we 'flip' _both_ bit and byte indexes. We call
this the _both-flip variant_:

```
| Byte index | 0                              | ... | n - 2            | n - 1                         |
|------------|--------------------------------| ... |------------------|-------------------------------|
| Byte       | w0                             | ... | w (n - 2)        | w(n - 1)                      |
|------------|--------------------------------| ... |------------------|-------------------------------|
| Bit index  | 8n - 8 | 8n - 7 | ... | 8n - 1 | ... | 8 | 9 | ... | 15 | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |
```

The third possibility is that we 'flip' _bit_ indexes, but not _byte_ indexes.
We call this the _bit-flip variant_:

```
| Byte index | 0                             | 1            | ... | n - 1                          |
|------------|-------------------------------|--------------| ... |--------------------------------|
| Byte       | w0                            | w1           | ... | w(n - 1)                       |
|------------|-------------------------------|--------------| ... |--------------------------------|
| Bit index  | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | ... | 15 | ... | 8n - 8 | 8n - 7 | ... | 8n - 1 |
```

The fourth possibility is the one we describe in the [bit indexing scheme
section](#bit-indexing-scheme), which is also the scheme chosen by CIP-58. We
repeat it below for clarity:

```
| Byte index | 0                              | 1  | ... | n - 1                         |
|------------|--------------------------------|----| ... |-------------------------------|
| Byte       | w0                             | w1 | ... | w(n - 1)                      |
|------------|--------------------------------|----| ... |-------------------------------|
| Bit index  | 8n - 1 | 8n - 2 | ... | 8n - 8 |   ...    | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0 |
```

On the face of it, these schemes appear equivalent: they are all consistent, and
all have formal descriptions, and quite similar ones at that. However, we
believe that only the one we chose is the correct one. To explain this, we
introduce two notions that we consider to be both intuitive and important,
then specify why our choice of indexing scheme fits those notions better than
any other.

The first notion is _index locality_. Intuitively, this states that if
two indexes are 'close' (in that their absolute difference is small), the values
at those indexes should be 'close' (in that their positioning in memory should
be separated less). We believe this notion to be reasonable, as this is an
expectation from array indexing (and indeed, `BuiltinByteString` indexing), as
well as the reason why packed array data is efficient on modern memory
hierarchies. Extending this notion to bits, we can observe that the both-flip
and no-flip variants of the bit indexing scheme do not preserve index locality:
the separation between a bit at index $0$ and index $1$ is _significantly_
different to the separation between a bit at index $7$ and index $8$ in both
representations, despite their absolute difference being identical. Thus, we
believe that these two variants are not viable, as they are not only confusing
from the point of view of behaviour, they would also make implementation of
future operations (such as shifts or rotations) significantly harder to both do,
and also reason about. Thus, only the bit-flip variant, as well as our choice,
remain contenders.

The second notion is _most-significant-first conversion agreement_. This notion
refers to the [CIP-121 concept of the same name][cip-121-big-endian], and
ensures that (at least for the most-significant-first arrangement), the
following workflow doesn't produce unexpected results:

1. Convert a `BuiltinInteger` to `BuiltinByteString` using
   `builtinIntegerToByteString` with the most-significant-first endianness
   argument.
2. Manipulate the bits of the result of step 1 using the operations specified
   here.
3. Convert the result of step 2 back to a `BuiltinInteger` using
   `builtinByteStringToInteger` with the most-significant-first endianness
   argument.

This workflow is directly relevant to Case 2. The Argon2 family of hashes use
certain inputs (which happen to be numbers) both as numbers (meaning, for
arithmetic operatons) and also as blocks of binary (specifically for XOR). This
is not unique to Argon2, or even hashing, as a range of operations (especially
in cryptographic applications) use similar approaches, whether for performance,
semantics or both. In such cases, users of our primitives (both logical and
conversion) must be confident that their changes 'translate' in the way they
expect between these two 'views' of the data.

The choice of most-significant-first as the arrangement that we must agree with
seems somewhat arbitrary at a glance, for two reasons: firstly, it's not clear
why we must pick a single arrangement to be consistent with; secondly, the
reasoning for the choice of most-significant-first over most-significant-last
as the arrangement to agree with isn't immediately apparent. To see why this is
the only choice that we consider reasonable, we first observe that, according
to the definition of the bit indexing scheme given [in
the corresponding section](#bit-indexing-scheme), as well as the corresponding
definition for the bit-flip variant, we view a `BuiltinByteString` of length $n$
as a binary natural number with exactly $8n$ digits, and the value at index $i$
corresponds to the digits whose place value is either $2^i$ (for the bit-flip
variant), or $2^{8n - i - 1}$ (for our chosen method). Put another way, under
the specification for the bit-flip variant, the least significant binary digit
is first, whereas in our chosen specification, the least significant binary
digit is last. CIP-121's conversion primitives mirror this reasoning: the
most-significant-first arrangement corresponds to our chosen method, while the
most-significant-last arrangement corresponds to the bit-flip variant instead.
The difference is the digit value: for us, the digit value is (effectively) 2,
while for CIP-121's conversion primitives, it is 256 instead.

We also observe that, when we index a `BuiltinByteString`'s _bytes_, we get back
a `BuiltinInteger`, whic has a numerical value as a natural number in the range
$[0, 255]$. Putting these two observations together, we consider it sensible
that, given a non-empty `BuiltinByteString`, if we were to get the values at bit
indexes $0$ through $7$, then sum their corresponding place values (treating
clear bits as $0$ and set bits as the appropriate place value), we should get
the same result as indexing whichever byte those bits came from.

Consider the `BuiltinByteString` whose only byte is $42$, whose representation
is as follows:

```
| Byte index | 0        |
|------------|----------|
| Byte       | 00101010 |
```

We note that, if we index this `BuiltinByteString` at byte position $0$, we get
back the answer $42$. Furthermore, if we use `builtinByteStringToInteger` from
CIP-121 with such a `BuiltinByteString`, we get the result $42$ as well,
regardless of the endianness argument we choose.

Under the bit-flip variant, the bit indexes of this `BuiltinByteString` would be
as follows:

```
| Byte index | 0                             |
|------------|-------------------------------|
| Byte       | 0 | 0 | 1 | 0 | 1 | 0 | 1 | 0 |
|------------|-------------------------------|
| Bit index  | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |
```

However, we immediately see a problem: under this indexing scheme, the $2^2 = 4$
place value is $1$, which would suggest that in the binary representation of
$42$, the corresponding digit is also $1$. However, this is not the case. Under
our scheme of choice however, we get the correct answer:

```
| Byte index | 0                             |
|------------|-------------------------------|
| Byte       | 0 | 0 | 1 | 0 | 1 | 0 | 1 | 0 |
|------------|-------------------------------|
| Bit index  | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0 |
```

Here, the $4$ place value is correctly $0$. This demonstrates that of the two
indexing scheme possibilities that preserve index locality, only one can be
consistent with _any_ choice of byte arrangement, whether most-significant-first
or most-significant-last: the one we chose. This implies that we cannot be
consistent with both arrangements while also preserving index locality.

Let us now consider a larger example `BuiltinByteString`:

```
| Byte index | 0        | 1        |
|------------|----------|----------|
| Byte       | 00101010 | 11011011 |
```

This would produce two different results when converted with
`builtinByteStringToInteger`, depending on the choice of endianness argument:

- For the most-significant-first arrangement, the result is $42 * 256 + 223 =
  10975$.
- For the most-significant-last arrangement, the result is $223 * 256 + 42 =
  57130$.

These have the following 'breakdowns' in base-2:

- $10975 = 8096 + 2048 + 512 + 256 + 32 + 16 + 8 + 4 + 2 + 1 = 2^13 + 2^11 + 2^9 + 2^8 + 2^5 + 2^4 + 2^3 + 2^2 + 2^1 + 2^0$
- $57130 = 32768 + 16386 + 4096 + 2048 + 1024 + 512 + 256 + 32 + 8 + 2 = 2^15 + 2^14 + 2^12 + 2^11 + 2^10 + 2^9 + 2^8 + 2^5 + 2^3 + 2^1$

Under the bit-flip variant, the bit indexes of this `BuiltinByteString` would be
as follows:

```
| Byte index | 0                             | 1                                   |
|------------|-------------------------------|-------------------------------------|
| Byte       | 0 | 0 | 1 | 0 | 1 | 0 | 1 | 0 | 1 | 1 | 0  | 1  | 1  | 0  | 1  | 1  |
|------------|-------------------------------|-------------------------------------|
| Bit index  | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 |
```

We immediately see a problem, as in this representation, it suggests that the
$2^1 = 2$ place value has zero digit value. This is true of _neither_ $10975$
nor $57130$'s base-2 forms, which have the $2$ place value with a $1$ digit
value. This suggests that the bit-flip variant cannot agree with _either_ choice
of arrangement in general.

However, if we view the bit indexes using our chosen scheme:

```
| Byte index | 0                                   | 1                             |
|------------|-------------------------------------|-------------------------------|
| Byte       | 0  | 0  | 1  | 0  | 1  | 0  | 1 | 0 | 1 | 1 | 0 | 1 | 1 | 0 | 1 | 1 |
|------------|-------------------------------------|-------------------------------|
| Bit index  | 15 | 14 | 13 | 12 | 11 | 10 | 9 | 8 | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0 |
```

the $2$ place value is correctly shown as having a digit value of 1.

Combining these observations, we note that, assuming we value index locality,
choosing our scheme gives us consistency with the most-significant-first
arrangement, as well as consistency with byte indexing digit values, but
choosing the bit-flip variant gives us _neither_. As we need both index locality
_and_ consistency with at least one arrangement, our choice is the correct one.
The fact that we also get byte indexing digit values being consistent is another
reason for our choice.

### Padding versus truncation

For the operations defined in this CIP taking two `BuiltinByteString` arguments
(that is, `bitwiseLogicalAnd`, `bitwiseLogicalOr`, and `bitwiseLogicalXor`),
when the two arguments have identical lengths, the semantics are natural,
mirroring the corresponding operations on the
[Boolean algebra][boolean-algebra-2] $\textbf{2}^{8n}$, where $n$ is the length
of either argument in bytes. When the arguments do _not_ have matching lengths,
however, the situation becomes more complex, as there are several ways in which
we could define these operations. The most natural possibilities are as follows;
we repeat some of the definitions used [in the corresponding
section](#padding-versus-truncation-semantics).

- Extend the shorter argument with the identity element (all-1s for
  `bitwiseLogicalAnd`, all-0s otherwise) to match the length of the longer argument,
  then perform the operation as if on matching-length arguments. We call this
  _padding semantics_.
- Ignore the bytes of the longer argument whose indexes would not be valid for
  the shorter argument, then perform the operation as if on matching-length
  arguments. We call this _truncation semantics_.
- Fail with an error whenever argument lengths don't match. We call this
  _match semantics_.

Furthermore, for both padding and truncation semantics, we can choose to pad (or
truncate) _low_ index bytes or _high_ index bytes. To illustrate the difference,
consider the two `BuiltinByteString`s (written as arrays of bytes for
simplicity) `[0xFF, 0x0F, 0x00]` and `[0x8F, 0x99]`. Under padding semantics,
padding low index bytes would give us `[0x00, 0x8F, 0x99]` (or `[0xFF, 0x8F,
0x99]` depending on operation), while padding high index bytes would give us
`[0x8F, 0x99, 0x00]` (or `[0x8F, 0x99, 0xFF]` depending on operation). Under
truncation semantics, truncating low index bytes would give us `[0x0F, 0x00]`,
while truncating high index bytes would give us `[0xFF, 0x0F]`.

It is not a priori clear which of these we should choose: they are subject to
different laws (as evidenced by the [corresponding
section](#laws-and-examples)), none of which are strict supersets of each other
(at least not for _all_ inputs possible). While [CIP-58][cip-58] chose match
semantics, we believe this was not the correct decision: we use Case 1 to
justify the benefit of having other semantics described above available.

Consider the following operation: given a bound $k$, a 'direction' (larger or
smaller), and an integer set, remove all elements indicates by the direction and
$k$ (that is, either smaller than $k$ or larger than $k$, as indicated by the
direction). This could be done using a `bitwiseLogicalAnd` and a mask. However,
under match semantics, this mask would have to have a length equal to the
integer set representation; under padding semantics, the mask would potentially
only need $\Theta(k)$ length, depending on direction. This is noteworthy, as
padding the mask would require an additional copy operation, only to produce a
value that would be discarded immediately.

Consider instead the following operation: given two integer sets with different
(upper) bounds, take their intersection, producing an integer set whose size is
the minimum of the two. This can once again be done using `bitwiseLogicalAnd`,
but under match semantics (or padding semantics for that matter), we would first
have to slice the longer argument, while under truncation semantics, we wouldn't
need to.

Match semantics can be useful for Case 1 as well. Consider the case that a
representation of an integer set is supplied as an
input datum (in its `Data` encoding). In order to deserialize it, we need to
verify at least whether it has the right length in bytes to represent an integer
set with a given bound. Under padding or truncation semantics, we would have to
check this at deserialization time; under exact match semantics, provided we
were sure that at least one argument is of a known size, we could simply perform
the necessary operations and let the match semantics error if given something
inappropriate.

It is also worth noting that truncation semantics are well-established in the
Haskell ecosystem. Viewed another way, all of the operations under discussion in
this sections are specialized versions of the `zipWith` operation; Haskell
libraries provide this type of operation for a range of linear collections,
including lists, `Vector`s, and mostly notably, `ByteString`s. In all of these
cases, truncation semantics are what is implemented; it would be surprising to
developers coming from Haskell to find that they have to do additional work to
replicate them in Plutus. While we don't anticipate direct use of Plutus Core
primitives by developers (although this is not an unheard-of case), we should
enable library authors to build familiar APIs _on top of_ Plutus Core
primitives, which suggests truncation semantics should be available, at least as
an option.

All the above suggests that no _single_ choice of semantics will satisfy all
reasonable needs, if only from the point of view of efficiency. This suggests,
much as for [CIP-121 primitives][conversion-cip] and endianness issues, that
the primitive should allow a choice in what semantics get used for any given
call. Ideally, we would allow a choice of any of the three options described
above (along with a choice of low or high index padding or truncation);
however, this is awkward to do in Plutus Core. While the choice between
_two_ options is straightforward (pass a `BuiltinBool`), the choice between
more than this number would require something like a `BuiltinInteger` argument
with 'designated values' ('0 means match', '1 means low-index padding', etc).
This is not ideal, as they involve additional checks, argument redundancy, or
both. In light of this, we made the following decisions:

1. We would choose only two of the three semantics, and have this choice
   controlled for any given call be controlled by a `BuiltinBool` flag; and
2. For padding or truncation semantics, we would _always_ use either low or high
   index padding (or truncation).

This leads naturally to two questions: which of the three semantics above we can
afford to exclude, and whether low or high index padding should be chosen. We
believe that the correct choices are to exclude match semantics, and to use
high index padding and truncation, for several reasons.

Firstly, we can simulate
match semantics with either padding or truncation semantics, together with a
length check. While we could also simulate padding semantics via match semantics
similarly, the amount of effort (both developer and computational) required is
significantly more in that case: a length check is a constant-time operation,
while manually padding is linear at best (and even then, it requires operations
only this CIP provides, as it would be quadratic otherwise), and on top of that,
manual padding is much fiddlier and easier to get wrong.

Secondly, truncation semantics are common enough in Haskell that we believe
excluding them as an option is both surprising and wrong. Any developer familiar
with Haskell has interacted with various `zipWith` operations, and having our
primitives behave differently to this at minimum creates friction for
implementers of higher-level abstractions atop the primitives in this CIP. While
Haskellers are not exclusive users of Plutus primitives (directly or not), there
are definitely enough of them that _not_ having truncation semantics available
would create a lot of unnecessary friction.

Thirdly, outside of error checking, match semantics give few benefits,
performance or otherwise. The examples above demonstrate cases where padding and
truncation semantics lead to better performance, less fiddly implementations, or
both: finding such a case for match semantics outside of error checking is
difficult at best.

This combination of reasoning leads us to consider padding and truncation as the
two semantics we should retain, and this guided our implementation choices
accordingly. With regard to padding (or truncating) low or high indexes, given
that we pad (or truncate) whole bytes by necessity, it makes the corresponding
operations (effectively) operate over bytes, or rather, they view
`BuiltinByteString`s as linear collections of bytes, rather than bits. When
viewed this way, the `zipWith` analogy with Haskell suggests that truncating
high is the correct choice: truncating low would be quite surprising to a
Haskeller familiar with how `zipWith`-style operations behave. Furthermore, as
having padding low and truncating high would be confusing (and arguably quite
strange), padding high seems like the correct choice. Thus, we decided to both
pad and truncate high in light of this.

### Bit setting

`writeBits` in our description takes a change list argument, allowing
changing multiple bits at once. This is an added complexity, and an argument can
be made that something similar to the following operation would be sufficient:

```haskell
writeBit :: BuiltinByteString -> BuiltinInteger -> BuiltinBool ->
BuiltinByteString
```

Essentially, `writeBit bs i v` would be equivalent to `writeBits bs
[(i, v)]` as currently defined. This was the choice made by [CIP-58][cip-58],
with the consideration of simplicity in mind.

At the same time, due to the immutability semantics of Plutus Core, each time
`writeBit` would be called, we would have to copy its `BuiltinByteString`
argument. Thus, a sequence of $k$ `setBit` calls in a fold over a
`BuiltinByteString` of length $n$ would require $\Theta(nk)$ time and
$\Theta(nk)$ space. Meanwhile, if we instead used `writeBits`, the time
drops to $\Theta(n + k)$ and the space to $\Theta(n)$, which is a non-trivial
improvement. While we cannot avoid the worst-case copying behaviour of
`setBit` (if we have a critical path of read-write dependencies of length
$k$, for example), and 'list packing' carries some cost, we have
[benchmarks][benchmarks-bits] that show not only that this 'packing cost' is
essentially zero, but that for `BuiltinByteString`s of 30 bytes or fewer,
copying completely overwhelms the work required to modify the bits specified in
the change list argument. This alone is good evidence for having `writeBits` instead;
indeed, there is prior art for doing this [in the `vector` library][vector], for
the exact reasons we give here.

The argument could also be made whether this design should be extended to other
primitive operations in this CIP which both take `BuiltinByteString` arguments
and also produce `BuiltinByteString` results. We believe that this is not as
justified as in the `writeBits` case, for several reasons. Firstly, for
`bitwiseLogicalComplement`, it's not clear what benefit this would have at
all: the only possible signature such an operation would have is
`[BuiltinByteString] -> [BuiltinByteString]`, which in effect would be a
specialized form of mapping. While an argument could be made for a _general_
form of mapping as a Plutus Core primitive, it wouldn't be reasonable for an
operation like this to be considered for such.

Secondly, the performance benefits of such an operation aren't nearly as
significant in theory, and likely wouldn't be in practice either. Consider
this hypothetical operation (with fold semantics):

```haskell
bitwiseLogicalXors :: BuiltinBool -> [BuiltinByteString] -> BuiltinByteString
```

Simulating this operation as a fold using `bitwiseLogicalXor`, in the worst
case, irrespective of padding or truncation semantics, requires $\Theta(nk)$
time and space, where $n$ is the size of each `BuiltinByteString` in the
argument list, and $k$ is the length of the argument list itself. Using
`bitwiseLogicalXors` instead would reduce the space required to $\Theta(n)$,
but would not affect the time complexity at all.

Lastly, it is questionable whether 'bulk' operations like `bitwiseLogicalXors`
above would see as much use as `writeBits`. In the context of Case 1,
`bitwiseLogicalXors` corresponds to taking the symmetric difference of multiple
integer sets; it seems unlikely that the number of sets we'd want to do this
with would frequently be higher than 2. However, in the same context,
`writeBits` corresponds to constructing an integer set given a list of
members (or, for that matter, _non_-members): this is an operation that is both
required by the case description, and also much more likely to be used often.

On the basis of the above, we believe that choosing to implement
`writeBits` as a 'bulk' operation, but to leave others as 'singular' is the
right choice.

## Path to Active

### Acceptance Criteria

We consider the following criteria to be essential for acceptance:

- A proof-of-concept implementation of the operations specified in this
  document, outside of the Plutus source tree. The implementation must be in
  GHC Haskell, without relying on the FFI.
- The proof-of-concept implementation must have tests, demonstrating that it
  behaves as the specification requires.
- The proof-of-concept implementation must demonstrate that it will
  successfully build, and pass its tests, using all GHC versions currently
  usable to build Plutus (8.10, 9.2 and 9.6 at the time of writing), across all
  [Tier 1][tier-1-ghc] platforms.

Ideally, the implementation should also demonstrate its performance
characteristics by well-designed benchmarks.

- [x] Included within a major haskell cardano-node release
- https://github.com/IntersectMBO/cardano-node/releases/tag/10.1.1
- [x] Enabled on Cardano mainnet via a hardfork
- Enabled by Plomin hardfork

### Implementation Plan

MLabs has begun the [implementation of the proof-of-concept][mlabs-impl] as
required in the acceptance criteria. Upon completion, we will send a pull
request to Plutus with the implementation of the primitives for Plutus
Core, mirroring the proof-of-concept.

## Copyright

This CIP is licensed under [Apache-2.0](http://www.apache.org/licenses/LICENSE-2.0).

[mlabs-impl]: https://github.com/mlabs-haskell/plutus-integer-bytestring
[tier-1-ghc]: https://gitlab.haskell.org/ghc/ghc/-/wikis/platforms#tier-1-platforms
[special-semigroups]: https://en.wikipedia.org/wiki/Special_classes_of_semigroups
[commutative-monoid]: https://en.wikipedia.org/wiki/Monoid#Commutative_monoid
[absorbing-element]: https://en.wikipedia.org/wiki/Zero_element#Absorbing_elements
[semilattice]: https://en.wikipedia.org/wiki/Semilattice
[distributive]: https://en.wikipedia.org/wiki/Distributive_property
[lattice]: https://en.wikipedia.org/wiki/Lattice_(order)
[de-morgan]: https://en.wikipedia.org/wiki/De_Morgan%27s_laws
[lens-laws]: https://oleg.fi/gists/posts/2017-04-18-glassery.html#laws:lens
[monoid-homomorphism]: https://en.wikipedia.org/wiki/Monoid#Monoid_homomorphisms
[succinct-data-structures]: https://en.wikipedia.org/wiki/Succinct_data_structure
[adjacency-matrix]: https://en.wikipedia.org/wiki/Adjacency_matrix
[binary-matrix]: https://en.wikipedia.org/wiki/Logical_matrix
[go-binary-matrix]: https://senseis.xmp.net/?BinMatrix
[finite-state-machine-4vl]: https://en.wikipedia.org/wiki/Four-valued_logic#Matrix_machine
[bitvector-apps]: https://en.wikipedia.org/wiki/Bit_array#Applications
[bitmap-index-compression]: https://en.wikipedia.org/wiki/Bitmap_index#Compression
[cip-58]: https://github.com/cardano-foundation/CIPs/tree/master/CIP-0058
[croaring]: https://github.com/RoaringBitmap/CRoaring
[too-many-ways-1]: https://fgiesen.wordpress.com/2018/02/19/reading-bits-in-far-too-many-ways-part-1
[conversion-cip]: https://github.com/cardano-foundation/CIPs/blob/master/CIP-0121/README.md
[benchmarks-bits]: https://github.com/mlabs-haskell/plutus-integer-bytestring/blob/main/bench/naive/Main.hs#L74-L83
[vector]: https://hackage.haskell.org/package/vector-0.13.1.0/docs/Data-Vector.html#v:-47--47-
[boolean-algebra-2]: https://en.wikipedia.org/wiki/Two-element_Boolean_algebra
[hashing]: https://en.wikipedia.org/wiki/Hash_function
[sha256]: https://en.wikipedia.org/wiki/Secure_Hash_Algorithms
[blake2b]: https://en.wikipedia.org/wiki/BLAKE_(hash_function)
[argon2]: https://en.wikipedia.org/wiki/Argon2
[xor-crypto]: https://en.wikipedia.org/wiki/Exclusive_or#Bitwise_operation
[cip-121]: https://github.com/cardano-foundation/CIPs/blob/master/CIP-0121/README.md
[cip-121-big-endian]:https://github.com/cardano-foundation/CIPs/blob/master/CIP-0121/README.md/#representation
[bitwise-and]: https://en.wikipedia.org/wiki/Bitwise_operation#AND
[bitwise-or]: https://en.wikipedia.org/wiki/Bitwise_operation#OR
[bitwise-xor]: https://en.wikipedia.org/wiki/Bitwise_operation#XOR

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0123/README.md
---

- --
CIP: 123
Title: Bitwise operations over BuiltinByteString
Category: Plutus
Status: Active
Authors:
- Koz Ross <koz@mlabs.city>
Implementors:
- Koz Ross <koz@mlabs.city>
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/825
Created: 2024-05-16
License: Apache-2.0
- --

## Abstract

We describe the semantics of a set of bitwise operations for Plutus
`BuiltinByteString`s. Specifically, we provide descriptions for:

- Bit shifts and rotations
- Counting the number of set bits (`popcount`)
- Finding the first set bit

We base our work on similar operations described in [CIP-58][cip-58], but use
the bit indexing scheme from [the logical operations cip][logic-cip] for the
semantics. This is intended as follow-on work from both of these.

## Motivation: why is this CIP necessary?

Bitwise operations, both over fixed-width and variable-width blocks of bits,
have a range of uses. Indeed, we have already proposed [CIP-122][logic-cip],
with some example cases, and a range of primitive operations on
`BuiltinByteString`s designed to allow bitwise operations in the service of
those example cases, as well as many others. These operations form a core of
functionality, which is important and necessary, but not complete. We believe
that the operations we describe in this CIP form a useful 'completion' of the
work in CIP-122, based on similar work done in the [earlier CIP-58][cip-58].

To demonstrate why our proposed operations are useful, we re-use the cases
provided in the [CIP-122][logic-cip], and show why the operations
we describe would be beneficial.

### Case 1: integer set

For integer sets, the [previous description][integer-set] lacks two important,
and useful, operations:

- Given an integer set, return its cardinality; and
- Given an integer set, return its minimal member (or specify it is empty).

These operations have a range of uses. The first corresponds to the notion of
[Hamming weight][hamming-weight], which can be used for operations ranging from
representing boards in chess games to exponentiation by squaring to succinct
data structures. Together with bitwise XOR, it can also compute the [Hamming
distance][hamming-distance]. The second operation also has a [range of
uses][ffs-uses], ranging from succinct priority queues to integer normalization.
It is also useful for [rank-select dictionaries][rank-select-dictionary], a
succinct structure that can act as the basis of a range of others, such as
dictionaries, multisets and trees of different arity.

In all of the above, these operations need to be implemented efficiently to be
useful. While we could use only bit reading to perform all of these, it is
extremely inefficient: given an input of length $n$, assuming that any bit
distribution is equally probable, we need $\Theta(8 \cdot n)$ time in the
average case. While it is
impossible to do both of these operations in sub-linear time in general, the
large constant factors this imposes (as well as the overhead of looping over
_bit_ indexes) is a cost we can ill afford on-chain, especially if the goal is
to use these operations as 'building blocks' for something like a data
structure.

### Case 2: hashing

In our [previously-described][hashing] case, we stated what operations we would
need for the Argon2 family of hashes specifically. However, Argon2 has a
specific advantage in that the number of operations it requires are both
relatively few, and the most complex of which (BLAKE2b hashing) already exists
in Plutus Core as a primitive. However, other hash functions (and indeed, many
other cryptographic primitives) rely on two other important instructions: bit
shifts and bit rotations. As an example, consider SHA512, which is an important
component in several cryptographic protocols (including Ed25519 signature
verification): its implementation requires both shifts and rotations to work.

Like with Case 1, we can theoretically simulate both rotations and shifts using
a combination of bit reads and bit writes to an empty `BuiltinByteString`.
However, the cost of this is extreme: we would need to produce a list of
index-value pairs of length equal to the Hamming weight of the input, only to
then immediately discard it! To put this into some perspective, for an 8-byte
input, performing a rotation involves allocating an expected 32 index-value
pairs, using _significantly_ more memory than the result. On-chain, we can't
really afford this cost, especially in an operation intended to be used as part
of larger constructions (as would be necessary here).

## Specification

We describe the proposed operations in several stages. First, we give an
overview of the proposed operations' signatures and costings; second, we
describe the semantics of each proposed operation in detail, as well as some
examples. Lastly, we provide laws that any implementation of the proposed
operations should obey.

Throughout, we make use of the [bit indexing scheme][bit-indexing-scheme]
described in a CIP-122. We also re-use the notation $x[i]$ to refer to the value
of at bit index $i$ of $x$, and the notation $x\\{i\\}$ to refer to the byte at
byte index $i$ of $x$: both are specified in CIP-122.

### Operation semantics

Our proposed operations will have the following signatures:

- ``bitwiseShift :: BuiltinByteString -> BuiltinInteger -> BuiltinByteString``
- ``bitwiseRotate :: BuiltinByteString -> BuiltinInteger -> BuiltinByteString``
- ``countSetBits :: BuiltinByteString -> BuiltinInteger``
- ``findFirstSetBit :: BuiltinByteString -> BuiltinInteger``

We assume the following costing, for both memory and execution time:

| Operation | Execution time cost | Memory cost |
|-----------|---------------------|-------------|
|`bitwiseShift`| Linear in the `BuiltinByteString` argument | As execution time|
|`bitwiseRotate` | Linear in the `BuiltinByteString` argument | As execution time |
|`countSetBits` | Linear in the argument | Constant |
|`findFirstSetBit` | Linear in the argument | Constant |

#### `bitwiseShift`

`bitwiseShift` takes two arguments; we name and describe them below.

1. The `BuiltinByteString` to be shifted. This is the _data argument_.
2. The shift, whose sign indicates direction and whose magnitude indicates the
   size of the shift. This is the _shift argument_, and has type
   `BuiltinInteger`.

Let $b$ refer to the data argument, whose length in bytes is $n$, and let $i$
refer to the shift argument. Let the result of `bitwiseShift` called with $b$
and $i$ be $b_r$, also of length $n$.

For all $j \in 0, 1, \ldots 8 \cdot n - 1$, we have

$$
b_r[j] = \begin{cases}
         b[j - i] & \text{if } j - i \in 0, 1, \ldots 8 \cdot n - 1\\
         0 & \text{otherwise} \\
         \end{cases}
$$

Some examples of the intended behaviour of `bitwiseShift` follow. For
brevity, we write `BuiltinByteString` literals as lists of hexadecimal values.

```
- - Shifting the empty bytestring does nothing
bitwiseShift [] 3 => []
- - Regardless of direction
bitwiseShift [] (-3) => []
- - Positive shifts move bits to higher indexes, cutting off high indexes and
- - filling low ones with zeroes
bitwiseShift [0xEB, 0xFC] 5 => [0x7F, 0x80]
- - Negative shifts move bits to lower indexes, cutting off low indexes and
- - filling high ones with zeroes
bitwiseShift [0xEB, 0xFC] (-5) => [0x07, 0x5F]
- - Shifting by the total number of bits or more clears all bytes
bitwiseShift [0xEB, 0xFC] 16 => [0x00, 0x00]
- - Regardless of direction
bitwiseShift [0xEB, 0xFC] (-16) => [0x00, 0x00]
```

#### `bitwiseRotate`

`bitwiseRotate` takes two arguments; we name and describe them below.

1. The `BuiltinByteString` to be rotated. This is the _data argument_.
2. The rotation, whose sign indicates direction and whose magnitude indicates
   the size of the rotation. This is the _rotation argument_, and has type
   `BuiltinInteger`.

Let $b$ refer to the data argument, whose length in bytes is $n$, and let $i$
refer to the rotation argument. Let the result of `bitwiseRotate` called with $b$
and $i$ be $b_r$, also of length $n$.

For all $j \in 0, 1, \ldots 8 \cdot n - 1$, we have
$b_r = b[(j - i) \text{ } \mathrm{mod} \text { } (8 \cdot n)]$.

Some examples of the intended behaviour of `bitwiseRotate` follow. For
brevity, we write `BuiltinByteString` literals as lists of hexadecimal values.

```
- - Rotating the empty bytestring does nothing
bitwiseRotate [] 3 => []
- - Regardless of direction
bitwiseRotate [] (-1) => []
- - Positive rotations move bits to higher indexes, 'wrapping around' for high
- - indexes into low indexes
bitwiseRotate [0xEB, 0xFC] 5 => [0x7F, 0x9D]
- - Negative rotations move bits to lower indexes, 'wrapping around' for low
- - indexes into high indexes
bitwiseRotate [0xEB, 0xFC] (-5) => [0xE7, 0x5F]
- - Rotation by the total number of bits does nothing
bitwiseRotate [0xEB, 0xFC] 16 => [0xEB, 0xFC]
- - Regardless of direction
bitwiseRotate [0xEB, 0xFC] (-16) => [0xEB, 0xFC]
- - Rotation by more than the total number of bits is the same as the remainder
- - after division by number of bits
bitwiseRotate [0xEB, 0xFC] 21 =>[0x7F, 0x9D]
- - Regardless of direction, preserving sign
bitwiseRotate [0xEB, 0xFC] (-21) => [0xE7, 0x5F]
```

#### `countSetBits`

Let $b$ refer to `countSetBits`' only argument, whose length in bytes is $n$,
and let $r$ be the result of calling `countSetBits` on $b$. Then we have

$$
r = \sum_{i=0}^{8 \cdot n - 1} b[i]
$$

Some examples of the intended behaviour of `countSetBits` follow. For
brevity, we write `BuiltinByteString` literals as lists of hexadecimal values.

```
- - The empty bytestring has no set bits
countSetBits [] => 0
- - Bytestrings with only zero bytes have no set bits
countSetBits [0x00, 0x00] => 0
- - Set bits are counted regardless of where they are
countSetBits [0x01, 0x00] => 1
countSetBits [0x00, 0x01] => 1
```

#### `findFirstSetBit`

Let $b$ refer to `findFirstSetBit`'s only argument, whose length in bytes is $n$,
and let $r$ be the result of calling `findFirstSetBit` on $b$. Then we have the
following:

1. $r \in -1, 0, 1, \ldots, 8 \cdot n - 1$
2. If for all $i \in 0, 1, \ldots n - 1$, $b\\{i\\} = \texttt{0x00}$, then $r = -1$;
   otherwise, $r > -1$.
3. If $r > -1$, then $b[r] = 1$, and for all $i \in 0, 1, \ldots, r - 1$, $b[i]
   = 0$.

Some examples of the intended behaviour of `findFirstSetBit` follow. For
brevity, we write `BuiltinByteString` literals as lists of hexadecimal values.

```
- - The empty bytestring has no first set bit
findFirstSetBit [] => -1
- - Bytestrings with only zero bytes have no first set bit
findFirstSetBit [0x00, 0x00] => -1
- - Only the first set bit matters, regardless what comes after it
findFirstSetBit [0x00, 0x02] => 1
findFirstSetBit [0xFF, 0xF2] => 1
```

### Laws

Throughout, we use `bitLen bs` to indicate the number of bits in `bs`; that is,
`sizeOfByteString bs * 8`. We also make reference to [logical
operations][logic-cip] from a previous CIP as part of specifying these laws.

#### Shifts and rotations

We describe the laws for `bitwiseShift` and `bitwiseRotate` together, as they
are similar. Firstly, we observe that `bitwiseShift` and `bitwiseRotate` both
form a [monoid homomorphism][monoid-homomorphism] between natural number
addition and function composition:

```haskell
bitwiseShift bs 0 = bitwiseRotate bs 0 = bs

bitwiseShift bs (i + j) = bitwiseShift (bitwiseShift bs i) j

bitwiseRotate bs (i + j) = bitwiseRotate (bitwiseRotate bs i) j
```

However, `bitwiseRotate`'s homomorphism is between _integer_ addition and
function composition: namely, `i` and `j` in the above law are allowed to have
different signs. `bitwiseShift`'s composition law only holds if `i` and `j`
don't have opposite signs: that is, if they're either both non-negative or both
non-positive.

Shifts by more than the number of bits in the data argument produce an empty
`BuiltinByteString`:

```haskell
- - n is non-negative

bitwiseShift bs (bitLen bs + n) =
bitwiseShift bs (- (bitLen bs + n)) =
replicateByteString (sizeOfByteString bs) 0x00
```

Rotations, on the other hand, exhibit 'modular roll-over':

```haskell
- - n is non-negative
bitwiseRotate bs (binLen bs + n) = bitwiseRotate bs n

bitwiseRotate bs (- (bitLen bs + n)) = bitwiseRotate bs (- n)
```

Shifts clear bits at low indexes if the shift argument is positive, and at high
indexes if the shift argument is negative:

```
- - 0 < n < bitLen bs, and 0 <= i < n
readBit (bitwiseShift bs n) i = False

readBit (bitwiseShift bs (- n)) (bitLen bs - i - 1)  = False
```

Rotations instead preserve all set and clear bits, but move them around:

```
- - 0 <= i < bitLen bs
readBit bs i = readBit (bitwiseRotate bs j) (modInteger (i + j) (bitLen bs))
```

#### `countSetBits`

`countSetBits` forms a [monoid homomorphism][monoid-homomorphism] between
`BuiltinByteString` concatenation and natural number addition:

```haskell
countSetBits "" = 0

countSetBits (x <> y) = countSetBits x + countSetBits y
```

`countSetBits` also demonstrates that `bitwiseRotate` indeed preserves the
number of set (and thus clear) bits:

```haskell
countSetBits bs = countSetBits (bitwiseRotate bs i)
```

There is also a relationship between the result of `countSetBits` on a given
argument and its complement:

```haskell
countSetBits bs = bitLen bs - countSetBits (bitwiseLogicalComplement bs)
```

Furthermore, `countSetBits` exhibits (or more precisely, gives evidence for) the
[inclusion-exclusion principle][include-exclude] from combinatorics, but only
under truncation semantics:

```haskell
countSetBits (bitwiseLogicalXor False x y) = countSetBits (bitwiseLogicalOr
False x y) - countSetBits (bitwiseLogicalAnd False x y)
```

Lastly, `countSetBits` has a relationship to bitwise XOR, regardless
of semantics:

```haskell
countSetBits (bitwiseLogicalXor semantics x x) = 0
```

#### `findFirstSetBit`

`BuiltinByteString`s where every byte is the same (provided they are not empty)
have the same first bit as a singleton of that same byte:

```haskell
- - 0 <= w8 <= 255, n >= 1
findFirstSetBit (replicateByteString n w8) =
findFirstSetBit (replicateByteString 1 w8)
```

Additionally, `findFirstSet` has a relationship to bitwise XOR, regardless of
semantics:

```haskell
findFirstSetBit (bitwiseLogicalXor semantics x x) = -1
```

Any result of a `findFirstSetBit` operation that isn't `-1` gives a valid bit
index to a set bit, but any non-negative `BuiltinInteger` less than this will
give an index to a clear bit:

```haskell
- - bs is not all zero bytes or empty
readBit bs (findFirstSetBit bs) = True

- - 0 <= i < findFirstSet bs
readBit bs i = False
```

## Rationale: how does this CIP achieve its goals?

Our four operations, along with their semantics, fulfil the requirements of both
our cases, and are law-abiding, familiar, consistent and straightforward to
implement. Furthermore, they relate directly to operations provided by
[CIP-122][logic-cip], as well as being identical to the equivalent operations in
[CIP-58][cip-58]. At the same time, some alternative choices could have been
made:

- Not implementing these operations at all, instead requiring higher-level APIs
  to implement them atop CIP-122 primitives;
- Providing only some of these four operations;
- Having `findFirstSetBit` return the bit length for an all-zero argument,
  instead of `-1`;
- Including a way of finding the _last_ set bit as well.

We discuss our choices with regards to all the above, and specify why we made
the choices that we did.

While we chose to provide all of these operations as primitives, we could
instead have required higher-level APIs to provide these on the basis
of [CIP-122][logic-cip] bit reads and writes. This is indeed possible:

- `bitwiseShift` and `bitwiseRotate` is a loop over all bits in the data argument,
  which is used to construct a change list that sets appropriate bits (with the
  right offset), which then gets applied to a `BuiltinByteString` of the same
  length as the data argument, but full of zero bytes.
- `countSetBits` is a fold over all bit indexes, incrementing an accumulator
  every time a set bit is found.
- `findFirstSetBit` is a fold over all bit indexes, returning the first index
  with a set bit, or (-1) if none can be found.

However, especially for `bitwiseShift` and `bitwiseRotate`, this comes at
significant cost. For shifts and rotations, we have to construct a change list
argument whose length is proportional to the Hamming weight of the data
argument. Assuming that all inputs are equally likely, this is proportional to
the bit length of the data argument: such a change list is likely significantly
larger than the result of the shift or rotation, making the memory cost of such
operations far higher than it needs to be. Given the constraints on memory and
execution time on-chain, at minimum, these two operations would have to be
primitives to make them viable at all: Case 2-style implementations would have
intolerably large memory costs otherwise, as such algorithms often make heavy
use of both shifts and rotations.

The case for `countSetBits` and `findFirstSetBit` is less clear here, as they
wouldn't require nearly as much of a memory cost if implemented atop CIP-122
primitives using folds. However, the cost would still be significant:
`countSetBits` requires looping over every bit index in the argument, and
`findFirstSetBit` (again, assuming any bit distribution in an argument is
equally probable) requires looping over about half this many. This would make
these operations unreasonable even over smaller inputs, which would make
applications like rank-select dictionaries (which expect these operations to be
fast and low-cost) unworkable. As succinct data structures are foremost in our
minds when considering the current primitives, we believe it is important that
`countSetBits` and `findFirstSetBit` are as efficient as they could be, hence
their inclusion.

When designing our operations, we tried to keep them familiar to Haskellers,
namely by having them behave like the similar operations from the `Bits` and
`FiniteBits` type classes. In particular,
`bitwiseShift` and `bitwiseRotate` act similarly given the same shift (or
rotation) argument, as positive values shift left and negative ones shift
right. The only exception is the choice for `findFirstSetBit` to return `-1`
when no set bits are found, which runs counter to the way `countTrailingZeros`
from `FiniteBits` works, which instead returns the bit length. This is also
different to how this operations [works in hardware][ctz]. Indeed, having
`findFirstSetBit` work this way would not only be more familiar, it would also
provide additional laws:

```haskell
- - Not possible under our current definition
- - bitLen is the bit length of the argument
0 <= popcount bs <= bitLen bs - findFirstSet bs
```

Under both definitions, the intent is the same: if the argument doesn't contain
any set bits, produce an invalid index. The difference is that we choose to
produce a _negative_ invalid index, whereas the consensus is to produce a
_non-negative_ invalid index instead. However, one task that is likely to come
up frequently when using `findFirstSetBit` is checking whether the index we were
given was valid or not (essentially, whether the argument has any nonzero bytes
in it). Under our definition, all that would be required is

```haskell
let found = findFirstSetBit bs
  in if found < 0
     then weMissed
     else validIndex
```

This is a cheap operation in Plutus Core, requiring only comparing against a
constant. However, if we used the more widely-used definition, we would instead
have to do this:

```haskell
let found = findFirstSetBit bs
    bitLen = 8 * sizeOfByteString bs
  in if found >= bitLen
     then weMissed
     else validIndex
```

This requires us to do considerably more work (finding the length of the
argument, multiplying by 8, then compare against that result), and is also much
more prone to error: users have to remember to use a `>=` comparison, as well as
to multiply the argument length by 8. This is less of an issue with
implementations of this operation in other languages, as their equivalent
operations are designed for fixed-width arguments (indeed, `FiniteBits`
_requires_ this), which makes their bit length a constant. In our case, this
isn't as simple, as `BuiltinByteString`s have variable length, which would make
the cost described above unavoidable. Our solution is both more efficient and
less error-prone: all a user needs to remember is that invalid indexes from
`findFirstSetBit` are negative. On this basis, we decided to vary from
conventional approaches.

One notable omission from our operators is the equivalent of counting _leading_
zeroes: namely, an operation that would find the _last_ set bit. Typically, both
a count of leading _and_ trailing zeroes is provided in both hardware and
software: this is the case for `FiniteBits`, as well as [most hardware
implementations][ctz]. To relate this to our cases, specifically Case 1, this
would allow us to efficiently find the _largest_ element in an integer set. The
reason we omit this operation is because, when compared with counting trailing
zeroes, it is far less useful: while it can be used for computing fast integer
square roots, it lacks many other uses. Counting trailing zeroes, on the other
hand, is essential for rank-select dictionaries (specifically for the `select`
operation), and also enables a range of other uses, which we have already
mentioned. In order to limit the number of new primitives, we decided that
counting leading zeroes can be omitted for now. However, our design doesn't
preclude such an operation from being added later if a use case for it is found
to be useful.

## Path to Active

### Acceptance Criteria

We consider the following criteria to be essential for acceptance:

- A proof-of-concept implementation of the operations specified in this
  document, outside of the Plutus source tree. The implementation must be in
  GHC Haskell, without relying on the FFI.
- The proof-of-concept implementation must have tests, demonstrating that it
  behaves as the specification requires.
- The proof-of-concept implementation must demonstrate that it will
  successfully build, and pass its tests, using all GHC versions currently
  usable to build Plutus (8.10, 9.2 and 9.6 at the time of writing), across
  all [Tier 1][tier-1] platforms.

Ideally, the implementation should also demonstrate its performance
characteristics by well-designed benchmarks.

- [x] Included within a major haskell cardano-node release
- https://github.com/IntersectMBO/cardano-node/releases/tag/10.1.1
- [x] Enabled on Cardano mainnet via a hardfork
- Enabled by Plomin hardfork

### Implementation Plan

MLabs has begun the implementation of the [proof-of-concept][impl] as required in
the acceptance criteria. Upon completion, we will send a pull request to
Plutus with the implementation of the primitives for Plutus Core, mirroring
the proof-of-concept.

## Copyright

This CIP is licensed under [Apache-2.0](http://www.apache.org/licenses/LICENSE-2.0).

[tier-1]: https://gitlab.haskell.org/ghc/ghc/-/wikis/platforms#tier-1-platforms
[impl]: https://github.com/mlabs-haskell/plutus-integer-bytestring/tree/koz/milestone-2
[bit-indexing-scheme]: https://github.com/mlabs-haskell/CIPs/blob/koz/logic-ops/CIP-XXX/CIP-XXX.md#bit-indexing-scheme
[monoid-homomorphism]: https://en.wikipedia.org/wiki/Monoid#Monoid_homomorphisms
[logic-cip]: https://github.com/mlabs-haskell/CIPs/blob/koz/logic-ops/CIP-0122/CIP-0122.md
[include-exclude]: https://en.wikipedia.org/wiki/Inclusion%E2%80%93exclusion_principle
[cip-58]: https://github.com/cardano-foundation/CIPs/tree/master/CIP-0058
[integer-set]: https://github.com/mlabs-haskell/CIPs/blob/koz/logic-ops/CIP-XXX/CIP-XXX.md#case-1-integer-set
[hamming-weight]: https://en.wikipedia.org/wiki/Hamming_weight#History_and_usage
[hamming-distance]: https://en.wikipedia.org/wiki/Hamming_distance
[ffs-uses]: https://en.wikipedia.org/wiki/Find_first_set#Applications
[rank-select-dictionary]: https://en.wikipedia.org/wiki/Succinct_data_structure#Succinct_indexable_dictionaries
[hashing]: https://github.com/mlabs-haskell/CIPs/blob/koz/logic-ops/CIP-XXX/CIP-XXX.md#case-2-hashing
[ctz]: https://en.wikipedia.org/wiki/Find_first_set#Hardware_support

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0124/README.md
---

- --
CIP: 124
Title: Extend token metadata for translations
Category: Metadata
Status: Proposed
Authors:
- Vito Melchionna <info@granadapool.com>
- Aaron Schmid <aaron@entropilabs.io>
- Carolina Isler @LaPetiteADA <lapetiteada@granadapool.com>
Implementors: []
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/488
Created: 2023-03-19
License: CC-BY-4.0
- --

## Abstract

This proposal defines an additional property to the metadata standard for tokens (NFTs and FTs) to support text localization.

## Motivation: why is this CIP necessary?

Current token metadata only supports a single hardcoded language (mostly English), which limits the accessibility to a certain culture. To get closer to mass adoption, we need to bring down language barriers by extending the current standard to support translations. This is especially relevant for games, metaverse solutions, and RealFi use cases of NFTs.

## Specification

This proposal follows the same specifications as [CIP-0025](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0025) (all versions).

The name of a culture consists of its [[ISO-639]](https://www.iso.org/standard/4767.html) language code with small letters and its [[ISO-3166]](https://www.iso.org/standard/63545.html) country/region code with capital letter separated by a dash "-". For instance, this proposal was written in "en-US": English with the US culture.

This convention is compatible with most operative systems (Linux and Windows) and widely used translation software.

### General structure

The extended JSON metadata standard (CIP-25) allows flexible off- and on-chain string tranlations:

```
{
  "721": {
      "<policy_id>": {
        "<asset_name>": {
          "name": <string>,
          "image": <uri | array>,
          "mediaType": image/<mime_sub_type>,
          "description": <string | array>,
          "files": [{
            "name": <string>,
            "mediaType": <mime_type>,
            "src": "<uri | array>"
          }],

          <other properties>

          "strings": {
              "de-CH": {
                "name": <string in Swiss German>,
                "image": <localized uri for Swiss German | array>,
                "description": <string in Swiss German | array>
                 <other localized properties>
              },
              "it-IT": {
                "name": <string in Italian>,
                "image": <localized uri for Italian | array>,
                "description": <string in Italian | array>
                <other localized properties>
              },

              <other languages and cultures>
          }
        },
      },
      "version": <version_id>,

      <information about collection>

      "strings": {
              "de-CH": {
                 <localized information about collection in de-CH>
              },
              "it-IT": {
                 <localized information about collection in it-IT>
              },

              <other languages and cultures>
      }
  }
}
```

### CDDL

Extended versions of CIP-25

[Version 1](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0124/cddl/version_1.cddl)

[Version 2](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0124/cddl/version_2.cddl)

- The new `strings` properties are optional, but if included, they must be valid JSON objects.
- The JSON object's keys and structure should match the same keys defined on the `policy_id` level for collection-specific information, or on the `asset_name` level for asset-specific properties, depending which attributes have translations. By doing so, the developer experience to access the localized strings significantly improves.

## Rationale: how does this CIP achieve its goals?

### Access valid localized properties of a token's metadata

After fetching the policy's metadata, the procedure stays the same when looking up CIP-25 properties. However, to find culture-based translations, developers have to access the `strings` property located at the same level of the wanted information.

In case of the ´policy_id´ level (collections):

1. Lookup the 721 key
2. Lookup the Policy Id of the token
3. Lookup the strings property
4. Lookup for the culture
5. You end up with the translated metadata for the policy

In case of the ´asset_name´ level (specific assets):

1. Lookup the 721 key
2. Lookup the Policy Id of the token
3. Lookup the Asset name of the token
4. Lookup the strings property
5. Lookup for the culture
6. You end up with the translated metadata for the specific asset

> **Note**
> The metadata's size is a constraint that should be considered as it could eventually push the boundaries of Cardano's transaction limits and scalability in terms of memory resources. The translations under the "strings" keys can be stored off-chain on an IPFS server using the proposed JSON structure. Then, the localized texts will be accessible through an URL, similarly to the "image" property.

### Code example (JavaScript/TypeScript)

To access the localized strings from the fetched metadata for a native asset, we can simply access the JSON properties from the front end by using the user's selected culture:

```
const response = await fetch(`${<BASE_URL>}/policyId/${<policyId>}`);

const metadata = response.json();
const policy = metadata["721"][<policy_id>];

// This check determines if the translations are stored off-chain on an IPFS url
function isValidURL(url) {
    try {
        new URL(url);
        return true;
    } catch (e) {
        return false;
    }
}

function fetchTranslationStrings(url) {
  const response = await fetch(url);
  const translations = await response.json();
  return translations || null;
}

function getPolicyString(policy, key, culture="en") {
    // translations are stored off-chain
    if(isValidURL(policy["strings"])) {
      const translations = fetchTranslationStrings(policy["strings"]);
      if(translations) {
        return translations[culture][key] || policy[key]; // default value (not localized)
      }
    }

    // translations are stored on-chain
    return policy["strings"][culture][key]
        ? policy["strings"][culture][key]
        : policy[key]; // default value (not localized)
}

function getAssetString(policy, asset, key, culture="en") {
    // translations are stored off-chain
    if(isValidURL(policy[asset]["strings"])) {
      const translations = fetchTranslationStrings(policy[asset]["strings"]);
      if(translations) {
        return translations[culture][key] || policy[asset][key]; // default value (not localized)
      }
    }

    // translations are stored on-chain
    return policy[asset]["strings"][culture][key]
        ? policy[asset]["strings"][culture][key]
        : policy[asset][key]; // default value (not localized)
}

console.log(`Default policy property: ${getPolicyString(policy, <policy_property>)}`);
console.log(`Localized policy property: ${getPolicyString(policy, <policy_property>, "it-IT")}`);
console.log(`Default asset name: ${getAssetString(policy, <asset_name>, "name")}`);
console.log(`Localized asset name: ${getAssetString(policy, <asset_name>, "name", "de-CH")}`);
```

### Update metadata and translations

Following the specifications stated on CIP-25, the strings can be only changed if the policy allows it.

### Backward Compatibility

This metadata standard extension is backward compatible and it doesn't affect applications using the current standard. Dapps implementing the proposed extended standard can also default on the legacy values if the localized strings are not available on an asset.

## Path to Active

### Acceptance Criteria

- [x] Implementation is compliant with JSON conventions.
- [x] Implementation is compliant with the [[ISO-639]](https://www.iso.org/standard/4767.html) standard for language code, and the [[ISO-3166]](https://www.iso.org/standard/63545.html) standard for country/region code.
- [ ] Implementations and peer review verify that:
- [ ] NFT metadata standard extension covers all existing localization needs and use cases on web2.
- [ ] Access to localized strings is easy and logical from a coding perspective.

### Implementation Plan

- [ ] Propose this method in documentation and references for web3 developers.
- [x] NMKR has supported this CIP with peer feedback.
- [ ] NMKR has provided a pilot implementation of this localization method.

## References

- CIP about Media Token Metadata Standard [CIP-0025](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0025).
- [[ISO-639]](https://www.iso.org/standard/4767.html) language code.
- [[ISO-3166]](https://www.iso.org/standard/63545.html) country/region code.

## Copyright

This CIP is licensed under [CC-BY-4.0].

[CC-BY-4.0]: https://creativecommons.org/licenses/by/4.0/legalcode
[Apache-2.0]: http://www.apache.org/licenses/LICENSE-2.0

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0127/README.md
---

- --
CIP: 127
Title: ripemd-160 hashing in Plutus Core
Status: Active
Category: Plutus
Authors:
- Tomasz Rybarczy <tomasz.rybarczyk@iohk.io>
Implementors: []
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/826
Created: 2024-05-22
License: Apache-2.0
- --

## Abstract
This CIP follows closely the (CIP-0101)[^1] and proposes an extension of the current Plutus functions with another hashing primitive [`RIPEMD-160`](https://en.bitcoin.it/wiki/RIPEMD-160). Primary goal is to introduce compatibility with Bitcoin's cryptographic infrastructure.

## Motivation: why is this CIP necessary?

The [integration](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0049/README.md) of the ECDSA and Schnorr signatures over the secp256k1 curve into Plutus was a significant step towards interoperability with Ethereum and Bitcoin ecosystems. However, full compatibility is still impossible due to the absence of the `RIPEMD-160` hashing algorithm in Plutus interpreter, which is a fundamental component of Bitcoin's cryptographic framework.
Most of common addresses in Bitcoin are derived from double [hashing procedure](https://learnmeabitcoin.com/technical/cryptography/hash-function/#hash160) involving `SHA-256` followed by `RIPEMD-160` function:
- [P2KH](https://learnmeabitcoin.com/technical/script/p2pkh/)
- [P2WPKH](https://learnmeabitcoin.com/technical/script/p2wpkh/)
- [P2SH](https://learnmeabitcoin.com/technical/script/p2sh/)
- [P2WSH](https://learnmeabitcoin.com/technical/script/p2wsh/)

Adding `RIPEMD-160` to Plutus would enhance the potential for cross-chain solutions between Cardano and Bitcoin blockchains and complements the set of primitives which we already have in that regard. It would allow for the verification of Bitcoin addresses and transactions on-chain. This addition enables also the verification of signed messages that identify the signer by the public key hash, which has not yet been witnessed on the Bitcoin blockchain.

The RIPEMD-160 is not only relevant to Bitcoin - other chains like [Cosmos](https://docs.cosmos.network/main/build/architecture/adr-028-public-key-addresses#legacy-public-key-addresses-dont-change) or [BNB](https://docs.bnbchain.org/docs/beaconchain/learn/accounts/#address) also use it for address generation.

## Specification
This proposal aims to introduce a new built-in hash function `RIPEMD-160`.

This function will be developed following the [`RIP-160`](https://homes.esat.kuleuven.be/~bosselae/ripemd160/pdf/AB-9601/AB-9601.pdf) specification and will utilize the [`cryptonite`](https://github.com/haskell-crypto/cryptonite/blob/master/Crypto/Hash/RIPEMD160.hs)

Since `cardano-base` already relies on `cryptonite` in the context of [`keccak-256`](https://github.com/input-output-hk/cardano-base/blob/master/cardano-crypto-class/src/Cardano/Crypto/Hash/Keccak256.hs) we would like to expose `RIPEMD-160` through the same library, to facilitate its integration into Plutus.

More specifically, Plutus will gain the following primitive operation:

- `ripemd_160` :: ByteString -> ByteString

The input to this function can be a `ByteString` of arbitrary size, and the output will be a `ByteString` of 20 bytes.
Note that this function aligns with the format of existing hash functions in Plutus, such as [blake2b_256](https://github.com/input-output-hk/plutus/blob/75267027f157f1312964e7126280920d1245c52d/plutus-core/plutus-core/src/Data/ByteString/Hash.hs#L25)

## Rationale: how does this CIP achieve its goals?
While the `RIPEMD-160` function might be implemented in on-chain scripts, doing so would be computationally unfeasible.

The library, cryptonite, is not implemented by and under control of the Plutus team. However,
- It is a library already used in the Plutus stack to expose KECCAK-256, and can be considered as a trustworthy implementation.
- Its behaviour is predictable and computationally efficient. The cost of the function is linear with respect to the size of the message provided as input. This is the same behaviour that other hash functions exposed in plutus (blake, sha3, keccak-256) have.

### Acceptance Criteria
- [X] A `cardano-base` binding is created for the `ripemd-160` function and included in a new version of the library.
- [X] A Plutus binding is created for the `ripemd_160` function and included in a new version of Plutus.
- [X] Integration tests, similar to those of the existing Plutus hash functions, are added to the testing infrastructure.
- [X] The function is benchmarked to assess its cost. As for other hash functions available in Plutus (blake2b, sha256 and keccak_256), we expect the cost of `ripemd_160` to be linear with respect to the size of the message. The Plutus team determines the exact costing functions empirically.
- [x] The ledger is updated to include new protocol parameters to control costing of the new builtins.
- Included within the haskell cardano-node implementation from [10.1.1](https://github.com/IntersectMBO/cardano-node/releases/tag/10.1.1).
- [x] This CIP may transition to active status once the Plutus version containing the `ripemd_160` function is introduced in a node release and becomes available on Mainnet.
- Enabled by Plomin hardfork

### Implementation Plan
The Plutus team will develop the binding, integration tests, and benchmarks. The E2E tests will be designed and implemented collaboratively by the testing team, the Plutus team, and community members planning to use this primitive.

## Copyright
This CIP is licensed under [Apache-2.0][https://www.apache.org/licenses/LICENSE-2.0].

[^1]: I did not hesitate to reuse parts of the original text of (CIP-0101)[../CIP-0101/README.md) without explicit quotations. This approach was approved by the original authors.

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0128/README.md
---

- --
CIP: 128
Title: Preserving Order of Transaction Inputs
Category: Ledger
Status: Proposed
Authors:
- Jonathan Rodriguez <info@anastasialabs.com>
Implementors: []
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/758
Created: 2024-02-01
License: CC-BY-4.0
- --


## Abstract

We propose the introduction of a new structure for transaction inputs aimed at enhancing the execution efficiency of Plutus contracts.

This CIP facilitates the preservation of input ordering from a submitted transaction, diverging from the current state where the ledger reorders transaction inputs lexicographically. Preserving the input ordering from a submitted transaction enables a validator to efficiently identify the required inputs in accordance with its validation logic, thus eliminating the need to use unnecessary computation by traversing all transaction inputs or performing sorting within the validator itself.

Furthermore, this implementation offers the potential to enhance existing design patterns already used in production, specifically by improving the ability to group and match inputs and outputs located at the specific index positions.

## Motivation: why is this CIP necessary?

According to the Babbage CDDL [transaction_body](https://github.com/IntersectMBO/cardano-ledger/blob/master/eras/babbage/impl/cddl-files/babbage.cddl) , the inputs and reference inputs of a transaction body are represented as a set, which indicates the non-duplicate inputs. However, the ledger not only process the [transaction_inputs](https://github.com/IntersectMBO/cardano-ledger/blob/0274cf65dbb79773122b69dfd36a8299eec2783f/eras/babbage/impl/cddl-files/babbage.cddl#L75-L77) as a set , but also orders them lexicographically, first by `transaction_id` and then by `index`.

The primary issue with the ledger ordering inputs lexicographically is that validators must traverse all the inputs or sort them within the validator to find the required inputs for the validation logic. This process can lead to inefficiencies and increases the risk of exceeding the execution budget specially when processing a large set of inputs.

For instance, consider a spending validator that needs to access its own input and output. Each function described below necessitates traversing all inputs and outputs to fulfill such validation:

 ```haskell
findOwnInput :: ScriptContext -> Maybe TxInInfo
findOwnInput ScriptContext{scriptContextTxInfo=TxInfo{txInfoInputs}, scriptContextPurpose=Spending txOutRef} =
    find (\TxInInfo{txInInfoOutRef} -> txInInfoOutRef == txOutRef) txInfoInputs
findOwnInput _ = Nothing
 ```

 ```haskell
getContinuingOutputs :: ScriptContext -> [TxOut]
getContinuingOutputs ctx
    | Just TxInInfo{txInInfoResolved=TxOut{txOutAddress}} <- findOwnInput ctx =
        filter (f txOutAddress) (txInfoOutputs $ scriptContextTxInfo ctx)
    where
        f addr TxOut{txOutAddress=otherAddress} = addr == otherAddress
getContinuingOutputs _ = traceError "Lf" -- "Can't get any continuing outputs"
```

```haskell
validatorA :: BuiltinData -> BuiltinData -> ScriptContext -> Bool
validatorA _datum _redeemer context =
  validateWithInputOutput input output
  where
    Just (TxInInfo {txInInfoResolved = input}) = findOwnInput context
    [output] = getContinuingOutputs context
    validateWithInputOutput _ _ = True
```

Furthermore, preserving the order of inputs from the submitted transaction will facilitate the utilization of index redeemer patterns that many projects are transitioning to, as illustrated below.

```haskell
validatorIndex :: BuiltinData -> Integer -> ScriptContext -> Bool
validatorIndex _datum index ScriptContext {scriptContextTxInfo = txInfo, scriptContextPurpose = Spending txOutRef} =
  validateInputOutput input output && txOutRef == intxOutRef
  where
    inputs = txInfoInputs txInfo
    outputs = txInfoOutputs txInfo
    TxInInfo {txInInfoOutRef = intxOutRef, txInInfoResolved = input} = inputs P.!! index
    output = outputs P.!! index
    validateWithInputOutput _ _ = True
```

Given the deterministic nature of the extended UTXO model, the determination of inputs and outputs can already be achieved through the off-chain code, eliminating the necessity to find or traverse inputs and outputs within the validator.

## Specification
As per protocol specifications, the transaction body is structured as follows:

```
transaction_body =
  { 0 : set<transaction_input>    ; inputs
  , 1 : [* transaction_output]
  , 2 : coin                      ; fee
  , ? 3 : uint                    ; time to live
  , ? 4 : [* certificate]
  , ? 5 : withdrawals
  , ? 6 : update
  , ? 7 : auxiliary_data_hash
  , ? 8 : uint                    ; validity interval start
  , ? 9 : mint
  , ? 11 : script_data_hash
  , ? 13 : set<transaction_input> ; collateral inputs
  , ? 14 : required_signers
  , ? 15 : network_id
  , ? 16 : transaction_output     ; collateral return; New
  , ? 17 : coin                   ; total collateral; New
  , ? 18 : set<transaction_input> ; reference inputs; New
  }
```

Specifically, the inputs and reference inputs are currently represented as a set:
```
0 : set<transaction_input>    ; inputs
```
```
18 : set<transaction_input> ; reference inputs; New
```

The proposed solution suggests modifying the inputs representation to an `oset` type:
> An `oset` type behaves much like a `set` but remembers the order in which the elements were originally inserted.
```
0 : oset<transaction_input>    ; inputs
```
```
18 : oset<transaction_input> ; reference inputs; New
```


## Rationale: how does this CIP achieve its goals?
The motivation behind this CIP stems from observed limitations and inefficiencies associated with the current lexicographical ordering of transaction inputs.

The strict lexicographical ordering mandated by the ledger requires traversing inputs to locate the appropriate inputs for the validation logic, which can lead to execution inefficiencies.

To address these issues, the proposed solution suggests transitioning from an `set` based representation of transaction inputs and reference inputs to an `oset` type, which preserves the order of elements.

This CIP tries to revive the original draft [CIP-0051](https://github.com/cardano-foundation/CIPs/pull/231)

### Alternatives
#### 1. Retain the existing set-based representation:

This approach involves maintaining the current set-based representation of transaction inputs and reference inputs.

## Path to Active

### Acceptance Criteria
- [ ] Included within a Plutus version within a Cardano mainnet hardfork.

### Implementation Plan
- [ ] Passes all requirements of both Plutus and Ledger teams as agreed to improve Plutus script efficiency.


## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0129/README.md
---

- --
CIP: 129
Title: Governance Identifiers
Status: Proposed
Category: Tools
Authors:
- Ashish Prajapati <ashish@strica.io>
Implementors: N/A
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/845
- https://github.com/cardano-foundation/CIPs/pull/857
Created: 2024-07-15
License: CC-BY-4.0
- --

## Abstract

This Cardano Improvement Proposal (CIP) defines a standardized structure for encoding and representing various governance and credential identifiers, specifically designed for DRep, Constitutional Committee (CC) keys, and Governance Actions within the Conway era. This specification introduces a single-byte header that encapsulates metadata related to the key type and credential type, allowing identifiers to retain critical metadata even when stored as byte arrays. By encoding this metadata directly into the bech32 format, we enhance both usability and interoperability across Cardano infrastructure and tools.

## Motivation: why is this CIP necessary?

The Conway era on Cardano introduces new governance features, requiring unique and identifiable credentials for roles such as DReps, Constitutional Committee members, and distinct governance actions. Existing infrastructure and tools that process bech32 identifiers often decode and store the raw byte data for efficiency, unintentionally stripping away the metadata embedded in the bech32 prefix. This CIP addresses that limitation by embedding metadata into a structured single-byte header, allowing credentials to be stored in byte form without losing essential metadata. This standardization facilitates seamless linkage, sharing, and compatibility of governance identifiers across the ecosystem, supporting a robust and interoperable governance framework in Cardano.

## Specification

### Introduction
We define a bytes representation for the various credentials and identifiers along with the their bech32 prefixes in this CIP. Taking inspiration from the Cardano addresses bytes format, we define an 8 bit header and a payload to define the key, which look similar to the reward address byte format but with a new specification and using the governance credentials.

In this CIP, We also define a simple bech32 prefix for gov actions, which does not have a credential. Gov actions only contain transaction ID bytes and an index, defined in the Gov Action Section below. The chosen prefixes for each identifier align with Cardano's established naming convention used in ledger specification, ensuring easy recognition and minimizing confusion within the ecosystem.

### Binary format

In the header-byte, bits [7;4] indicate the type of gov key being used. The remaining four bits [3;0] are used to define the credential type. There are currently 3 types of credentials defined in the Cardano Conway era, this specification will allow us to define a maximum of 16 different types of keys in the future.

```
  1 byte     variable length
 <------> <------------------->
┌────────┬─────────────────────┐
│ header │        key      │
└────────┴─────────────────────┘
    🔎
    ╎          7 6 5 4 3 2 1 0
    ╎         ┌─┬─┬─┬─┬─┬─┬─┬─┐
    ╰╌╌╌╌╌╌╌╌ |t│t│t│t│c│c│c│c│
              └─┴─┴─┴─┴─┴─┴─┴─┘
```

#### Key Type
There are currently 3 types of governance keys, and it is defined using the first half (bits [7;4]) of the header. The different types are summarized below,

Key Type (`t t t t . . . .`)      | Key
- --                               | ---
`0000....`                        | CC Hot
`0001....`                        | CC Cold
`0010....`                        | DRep

#### Credential Type

The second half of the header (bits [3;0]) refers to the credential type which can have the values and types as summarized in the table below,

We *reserve* the values 0 and 1 to prevent accidental conflicts with Cardano Address Network tags, ensuring that governance identifiers remain distinct and are not inadvertently processed as addresses.

Credential Type (`. . . . c c c c`)   | Semantic
- --                                   | ---
`....0010`                            | Key Hash
`....0011`                            | Script Hash

### Governance Action Identifiers

Cardano's Conway era introduces proposal procedures to submit governance actions. Governance actions are voted on by different kinds of credentials, and as such it is necessary to be able to share governance action identifiers across communication channels.

Governance action identifiers are defined via CIP-1694 as combination of a transaction ID it was submitted in and an index within the transaction.

We define a byte format to combine the transaction ID and the index to form a single valid byte string, as such it can be converted into a hex format and have its own Bech32 prefix.

Transaction ID is always a 32 byte length, hence we can append the index bytes to the transaction id, please see examples below:

#### Example 1

Original standard definition - `0000000000000000000000000000000000000000000000000000000000000000#17`

Transaction ID in Hex - `0000000000000000000000000000000000000000000000000000000000000000`

Governance Action index in Hex - `11` (number 17)

(CIP-129) Governance action ID in Hex - `000000000000000000000000000000000000000000000000000000000000000011`

(CIP-129) Governance action ID in Bech32 - `gov_action1qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqpzklpgpf`

#### Example 2

Original standard definition - `1111111111111111111111111111111111111111111111111111111111111111#0`

Transaction ID in Hex - `1111111111111111111111111111111111111111111111111111111111111111`

Governance Action index in Hex - `00` (number 0)

(CIP-129) Governance action ID in Hex - `111111111111111111111111111111111111111111111111111111111111111100`

(CIP-129) Governance action ID in Bech32 - `gov_action1zyg3zyg3zyg3zyg3zyg3zyg3zyg3zyg3zyg3zyg3zyg3zyg3zygsq6dmejn`

### Bech32 Encoding

| Prefix        | Meaning                                                 | Contents                                                          |
| ------------- | --------------------------------------------------------| ----------------------------------------------------------------- |
| `drep`        | DRep identifier                                         | DRep Credential                                                   |
| `cc_hot`      | CC Hot identifier                                       | CC Hot Credential                                                 |
| `cc_cold`     | CC Cold identifier                                      | CC Cold Credential                                                |
| `gov_action`  | gov action identifier                                   | gov action tx id concatenated with index                          |


### Identifier Test Vector

We can define a complete identifier as per the spec above by combining the header and the key, see below

#### Constitutional Committee Hot Credential

Key - `00000000000000000000000000000000000000000000000000000000`
Type - `Key Hash`

Identifier - `0200000000000000000000000000000000000000000000000000000000`

Bech32 - `cc_hot1qgqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqvcdjk7`

#### Constitutional Committee Cold Credential

Key - `00000000000000000000000000000000000000000000000000000000`
Type - `Script Hash`

Identifier - `1300000000000000000000000000000000000000000000000000000000`

Bech32 - `cc_cold1zvqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq6kflvs`

#### DRep Credential

Key - `00000000000000000000000000000000000000000000000000000000`
Type - `Key Hash`

Identifier - `2200000000000000000000000000000000000000000000000000000000`

Bech32 - `drep1ygqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq7vlc9n`

## Rationale: how does this CIP achieve its goals?

This CIP achieves its objectives by introducing a unified header and payload structure for governance-related keys, allowing for metadata to be directly embedded within the byte-level representation of each identifier. By defining a single-byte header that includes both key type and credential type, the proposal provides a consistent, compact format that retains crucial metadata even when stored or transmitted as raw byte arrays. This specification is designed to be forward-compatible, with a capacity to support up to 16 key types, allowing it to evolve with Cardano’s governance and credential requirements.

This approach aligns with existing Cardano address encoding practices while adding specificity for governance keys in the Conway era. By also defining distinct bech32 prefixes for each identifier type, the CIP enhances user-friendliness and makes it easier for tooling and infrastructure to recognize, validate, and link these identifiers within the ecosystem. This design ensures governance identifiers are not only interoperable across platforms but also intuitive and accessible, paving the way for streamlined governance interactions within Cardano’s tooling and community.


## Path to Active

### Acceptance Criteria
Tools, Wallets, and Explorers to utilize the identifiers and bech32 prefixes defined in this CIP for communication and view purposes.

- [ ] Requires updating Ledger nano app, and Trezor. The changes can be proposed once the CIP is merged.
- [ ] Tooling
- [x] CNTools
- [x] SPO Scripts
- [x] typhonjs
- [x] Gov Tools
- [x] cardano-signer
- [ ] APIs
- [x] Koios
- [x] Cardanoscan API
- [ ] Blockfrost
- [x] Explorers
- [x] Cardanoscan.io
- [x] AdaStat.net
- [ ] Wallets
- [x] Eternl
- [x] Typhon Wallet
- [ ] Lace

### Implementation Plan
- This CIP uses some bech32 prefixes which are already defined by CIP105 and ref by CIP005, This PR includes updates to both the CIPs with updated vector spec. The suggested changes align with the design of the original CIP005.

- For key generation tools - To move faster to this CIP, current tools which implement CIP105 based bech32 prefixes do not have to implement this CIP specification. They can change the prefix according to the updates suggested in this CIP, updating bech32 prefix will be a find and replace sort of an updated and it will instantly become compatible with this CIP.

- Tools like explorers and wallets - These tools can potentially support both formats to start with for the purpose of allowing users to search for drep, cold keys, hot keys as these 3 are impacted by this CIP upgrade. And can continue to show only this CIP specification, having an easy backward compatibility as well moving to this CIP standard.

- This CIP does not require a hard fork for implementation, the goal is to use the identifies specified in this CIP for the UI, and as a medium of communication for sharing such keys and IDs.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0132/README.md
---

- --
CIP: 132
Title: New Plutus Builtin dropList
Status: Proposed
Category: Plutus
Authors:
- Philip DiSarro <info@anastasialabs.com>
Implementors: []
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/767
Created: 2024-02-25
License: CC-BY-4.0
- --

## Abstract
This document describes the addition of a new Plutus builtin `dropList` with the signature `Integer -> List a -> List a` that drops a given number of elements the list. This drastically increases the efficiency of `elemAt` which is currently a huge throughput bottleneck for many DApps.

## Motivation: why is this CIP necessary?
The deterministic script evaluation property of the ledger (also stated as "script interpreter arguments are fixed") is a unique characteristic of the Cardano ledger that allows us to perform powerful optimizations that are not possible in systems with indeterminstic script evaluation. For instance, searching for elements in a data structure can
be done entirely off-chain, and then we simply provide the onchain code with the index (via redeemer) to where the element we want to find is supposed to be, and then check (onchain) that it is indeed the element we were expecting. This design pattern of passing the indices of elements required for validation logic in the redeemer is commonly referred to as redeemer-indexing.
Even though it is still a very powerful optimization in its current state, it is currently bottlenecked by the lack of a builtin that applies tail a given number of times to a list. Currently, any implementation of `elemAt :: Integer -> List a -> a` or `drop` requires the use of the fixed point combinator (Y combinator) which has a significant cost in onchain code.

Consider the naive approach:
```haskell
{- | Fixpoint recursion. Used to encode recursive functions.
     Hopefully this illustrates the overhead that this incurs.
- }
pfix :: Term s (((a :--> b) :--> a :--> b) :--> a :--> b)
pfix = phoistAcyclic $
  punsafeCoerce $
    plam' $ \f ->
      plam' (\(x :: Term s POpaque) -> f # plam' (\(v :: Term s POpaque) -> punsafeCoerce x # x # v))
        # punsafeCoerce (plam' $ \(x :: Term s POpaque) -> f # plam' (\(v :: Term s POpaque) -> punsafeCoerce x # x # v))

- - | Lazy if-then-else
- - Two forces + two delays + builtinIfThenElse
pif :: Term s PBool -> Term s a -> Term s a -> Term s a
pif b case_true case_false = pforce $ (pforce $ punsafeBuiltin PLC.IfThenElse) # b # pdelay case_true # pdelay case_false

pelemAt' :: PIsListLike l a => Term s (PInteger :--> l a :--> a)
pelemAt' = phoistAcyclic $
  pfix #$ plam $ \self n xs ->
    pif
      (n #== 0)
      (phead # xs)
      (self # (n - 1) #$ ptail # xs)

pelemAt' # 5 # (pconstant [1,2,3,4,5])
- - the function `self` must be passed as an argument to each recursive call.
- - each recursive call results in:
- -   uplc Apply operations to apply the arguments `n` and `xs` to `self`
- -   lazy ifThenElse (two forces + two delays + builtinIfThenElse)
- -   builtinEqualsInteger
- -   builtinSubtractInteger
- -   uplc Apply operations to apply the arguments (including `self`, `n` and `xs`) to the fixed-point recursive function
```
As you can see, the naive `elemAt` implementation is quite inefficient. This is a huge efficiency bottleneck for many DApps which use `elemAt` many times to locate elements at indices specified in the redeemer. In an attempt to address this, many protocols use the following heuristic optimization (where the number of skips is determined through trial and error based on the DApps throughput in testing):
```
pelemAtFast :: PIsListLike l a => Term s (PInteger :--> l a :--> a)
pelemAtFast = phoistAcyclic $
  pfix #$ plam $ \self n xs ->
    pif
      (n #> 10)
      (self # (n - 1) #$ ptail #$ ptail #$ ptail #$ ptail #$ ptail #$ ptail #$ ptail #$ ptail #$ ptail #$ ptail # xs)
      (pelemAtFast2 # n # xs)

pelemAtFast2 :: PIsListLike l a => Term s (PInteger :--> l a :--> a)
pelemAtFast2 = phoistAcyclic $
  pfix #$ plam $ \self n xs ->
      (pif
         (n #> 5)
         (self # (n - 5) #$ ptail #$ ptail #$ ptail #$ ptail #$ ptail # xs)
         (pif (n #== 0) (phead # xs) (pelemAt' # (n - 1) # (ptail # xs))))
```
This drastically reduces the amount of recursion we have to do which greatly increases the efficiency of this function in practice. However, it should be clear to see that there is still a huge degree of inefficiency in this implementation. Also it is difficulty to determine the correct magic numbers to skip, and the performance
varies drastically depending on the cut-off values chosen for `n` as-well as the number of different skip-cases (in this case we have skip cases for both `n > 10` and `n > 5`).

## Specification

### Function definition
We define a new Plutus built-in function with the following type signature:
```haskell
builtinDropList :: BuiltinInteger -> BuiltinList a -> BuiltinList a
```

Similar to the behavior of the `indexOfByteString` builtin, this new builtin will simply error if the provided index is out of bounds for the list.


### Cost Model
Although the `BuiltinList` type is a recursive data-type, costing should be relatively straightforward.
We propose to define a cost model linear in the size of `n`, the number of elements to drop. What remains is to find a proper coefficient and offset for that linear model, which should be quite easy.


## Rationale: how does this CIP achieve its goals?
- Easy to implement as it reuses existing code of the Plutus codebase;
- The built-in is generic enough to cover a wider set of use-cases;
- The built-in is still relevant even if we get constant lookup index data-structures since there are occasions where BuiltinList would be preferred;
- This directly addresses the big performance bottleneck that the fixed-point recursion implementation of `elemAt` and `drop` impose on many DApps;

### Alternatives

- We could decide to accept the heuristic `elemAtFast` implementation as  as an adequate solution.
- We could provide a more generic builtin that applies a function recursively `n` times (seems complicated and bad idea).
- We could try to reduce the overhead introduced by aspects of the `elemAt` by making the language / compiler more performant (still can't imagine we would be able to get anywhere near the performance of this builtin).

## Path to Active

### Acceptance Criteria
- [ ] Fully implemented in Cardano with hard fork that includes Plutus V4.

### Implementation Plan
- [x] Passes all requirements of both Plutus and Ledger teams as agreed to improve Plutus script efficiency and usability.

## Copyright
This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

[CC-BY-4.0]: https://creativecommons.org/licenses/by/4.0/legalcode
[Apache-2.0]: http://www.apache.org/licenses/LICENSE-2.0

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0133/README.md
---

- --
CIP: 133
Title: Plutus support for Multi-Scalar Multiplication over BLS12-381
Status: Proposed
Category: Plutus
Authors:
- Dmytro Kaidalov <dmytro.kaidalov@iohk.io>
- Adam Smolarek <adam.smolarek@iohk.io>
- Thomas Vellekoop <thomas.vellekoop@iohk.io>
Implementors:
- IOG Plutus team
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/884
Created: 2024-08-22
License: CC-BY-4.0
- --
## Abstract
The CIP proposes an extension of the current Plutus functions to provide support for the efficient computation of the multi-scalar multiplication over the BLS12-381 curve. This operation is crucial in a number of cryptographic protocols that can enhance the capabilities of the Cardano blockchain.

## Motivation: why is this CIP necessary?
Multi-scalar multiplication (MSM) is an algebraic group operation of the following form. Let $G$ be a group of prime order $p$. Let $g_0, g_1, ..., g_{N-1}$ be elements of $G$ and let $e_0, e_1, ..., e_{N-1}$ be elements of $Z_p$. Then, the multi-scalar multiplication $M$ is calculated as $M=\sum_{i=0}^{N-1} e_i \cdot g_i$.

This operation appears in many cryptographic protocols. Its naive implementation requires $N$ scalar multiplications and $N$ group additions. However, the performance can be significantly improved by employing advanced algorithms, such as the [Pippenger Approach](https://hackmd.io/@tazAymRSQCGXTUKkbh1BAg/Sk27liTW9). Moreover, it can be further optimized for a particular group type (e.g., for elliptic curve groups [[BH22](https://eprint.iacr.org/2022/1400.pdf), [L23](https://dspacemainprd01.lib.uwaterloo.ca/server/api/core/bitstreams/3b15ca4c-9125-4e45-9378-c5474eec6a07/content)]).

Multi-scalar multiplication appears in various cryptographic protocols, such as cryptographic signatures, zero-knowledge proofs, SNARK systems, and others. It is especially important in elliptic-curve-based SNARK proof systems, where large-scale MSMs can become a bottleneck in both proving and verification algorithms.

The recent Chang upgrade in Cardano included [CIP-0381](https://cips.cardano.org/cip/CIP-0381), which introduced built-in support for operations over the BLS12-381 elliptic curve (the implementation uses [blst](https://github.com/supranational/blst/tree/master) library). It made it feasible to implement various SNARK systems on-chain in Cardano. However, MSM still remains a bottleneck for many use cases. Implementing MSM naively, even using built-in functions for BLS12-381, consumes a significant portion of the computational budget of a transaction. It hinders implementation of such SNARK systems as KZG-based PLONK or Groth16, which require computations of MSM.

Our [benchmarks](https://github.com/input-output-hk/plutus-msm-bench) show that MSM of 10 $G1$ points over BLS12-381 curve consumes 7.74% of the computational budget of a transaction, while MSM of size more than 129 cannot fit into a single transaction at all. It impedes verification of complex circuits which might require much larger MSM.

We also did preliminary [benchmarks](https://github.com/dkaidalov/bench-blst-msm/) to compare an [optimized blst](https://github.com/supranational/blst/blob/e99f7db0db413e2efefcfd077a4e335766f39c27/src/multi_scalar.c) implementation of MSM with naive implementation using just blst group operations. We used Rust bindings to do these benchmarks (the underlying bindings are the same as used in [cardano-base for bslt](https://github.com/IntersectMBO/cardano-base/blob/master/cardano-crypto-class/src/Cardano/Crypto/EllipticCurve/BLS12_381/Internal.hs#L353), which means we can expect similar behavior for the Haskell stack). The results for G1 group are the following (more results in the [repository](https://github.com/dkaidalov/bench-blst-msm/)):

| MSM Size | MSM Optimized Average Time | MSM Naive Average Time | Ratio Naive/Optimized |
|---------:|---------------------------:|-----------------------:|----------------------:|
|       15 |                  728.565µs |                1.263ms |                  1.73 |
|       20 |                  816.508µs |                1.499ms |                  1.83 |
|       25 |                  956.735µs |                1.837ms |                  1.92 |
|       30 |                    1.022ms |                2.055ms |                  2.01 |
|       35 |                 428.765µs* |                2.715ms |                 6.13* |
|       40 |                   469.49µs |                2.882ms |                  6.33 |
|       45 |                  445.014µs |                3.288ms |                  7.37 |
|       50 |                  533.669µs |                3.933ms |                  7.38 |
|      100 |                   770.71µs |                7.307ms |                  9.48 |
|      200 |                    1.065ms |                14.07ms |                  13.2 |
|      300 |                    1.177ms |               21.605ms |                 18.34 |
|      400 |                    1.312ms |               27.405ms |                 20.87 |
|     1000 |                    2.584ms |               65.725ms |                 25.43 |
|     2000 |                    4.108ms |              131.453ms |                 31.99 |
|     3000 |                    5.510ms |              195.986ms |                 35.56 |
|     4000 |                     7.00ms |              263.722ms |                 37.67 |

\* _the sudden time improvement after 32 points is attributed to the inner workings of the blst library, which may operate differently for the MSM of size less than 32 (this should be carefully analyzed when establishing costing function for MSM built-in)._

As it can be seen the performance improvement rises quickly with the size of the MSM. Note that the current threshold for Plutus naive implementation is 129 points per transaction. Our [blst benchmarks](https://github.com/dkaidalov/bench-blst-msm/) show that naive MSM of size 129 takes approximately the same time as optimized MSM for more than 4000 points, which gives a hint to what improvements we can expect with a Plutus built-in for MSM. Moreover, these benchmarks do not account for the Plutus overhead to call many built-in BLS12 functions while implementing the naive MSM, so the final improvement may be even larger.
On the other hand, it is important to mention that if those points are brought into the script as input, their number would be constrained by the size of the script and by the computational complexity of points decompression. The benchmarks of points decompression in [CIP-0381](https://cips.cardano.org/cip/CIP-0381) shows that up to 300 G1 points can be passed as input to a 16kb script. However, in real cryptographic protocols typically only a part of the points involved in MSM is passed as input, while another part is computed during the execution (e.g., in PLONK-based SNARKs).

The availability of MSM built-in function in Plutus language will provide more efficient and reliable means to perform this important computation. Implementing an optimized MSM manually in Plutus deemed to be infeasible because of its complexity and because it operates at the level of basic curve operations.

A nonexclusive list of cryptographic protocols that would benefit from having MSM built-in for BLS12-381 curve:

1. The verification of pairing-based SNARKs over BLS12-381. For instance, the size of the MSM in Groth16 verifier depends on the number of public inputs. The size of the MSM in KZG-based PLONK verifier depends on the arithmetization structure.
2. Public key aggregation in [BLS multi-signature aggregation scheme](https://crypto.stanford.edu/~dabo/pubs/papers/BLSmultisig.html). It is a popular scheme that allows to aggregate many signatures into a common message, so that verifying the short multi-signature is fast.
3. A [cryptographic accumulator](https://github.com/perturbing/plutus-accumulator). It is a cryptographic primitive that allows a prover to succinctly commit to a set of values while being able to provide proofs of (non-)membership. Accumulators have found numerous applications including signature schemes, anonymous credentials, zero-knowledge proof systems, and others.

The above mentioned cryptographic protocols are used in many Cardano products, for instance:

- **Partner chains** - a crucial component for the scalability of Cardano. Its interoperability with Cardano relies on the ability to construct a secure bridge for message passing. A reliable trustless bridge requires SNARK proofs for efficient proving of the partner chain state.
- **Hydra** - a prominent layer-2 solution for scalability of Cardano. Hydra relies on a multisignature scheme, where all participants of the side channel need to agree on the new state. Moreover, Hydra tails could benefit from SNARKs for proving correct spending of a set of transactions.
- **Mithril** - a protocol for helping to scale the adoption of Cardano and its accessibility for users. It creates certified snapshots of the Cardano blockchain allowing to obtain a verified version of the current state without having to download and verify the full history of the blockchain. Mithril utilizes a stake-based threshold multisignature scheme based on elliptic curve pairings. Even though at the moment most use cases of Mithril relies on off-chain computations, eventually the Mithril certificates might also be verified in Plutus smart contracts.
- **Atala Prism** - a decentralized identification mechanism. One of the properties it can provide is anonymity: users can selectively disclose attributes of their certificate or prove statements without disclosing their identity. Up to date, the most efficient solutions for doing that use pairing-based zero-knowledge protocols.

In conclusion, integrating multi-scalar multiplication as a core built-in within Plutus is not only essential for enhancing cryptographic capabilities, but also for optimizing on-chain computations. The current approach of naive manual implementation in Plutus is inefficient for large-scale MSMs. Incorporating this function directly will streamline these operations, reduce transaction costs, and maintain the integrity of existing tools, thereby significantly advancing the Plutus ecosystem's functionality and user experience.

## Specification
The MSM for BLS12-381 is implemented in [blst](https://github.com/supranational/blst/blob/e99f7db0db413e2efefcfd077a4e335766f39c27/bindings/blst.h#L241) library, which is already a dependency for the [cardano-base](https://github.com/IntersectMBO/cardano-base/blob/master/cardano-crypto-class/src/Cardano/Crypto/EllipticCurve/). This library provides efficient and formally verified implementation of operations over the BLS12-381 elliptic curve. It has been used for implementing [CIP-0381](https://cips.cardano.org/cip/CIP-0381). Basically, we would like to expose several additional functions from this library in Plutus API.

### Function definition
We propose to define two new Plutus functions **bls12_381_G1_multiScalarMul** and **bls12_381_G2_multiScalarMul** as follows:

```hs
bls12_381_G1_multiScalarMul :: [Integer] -> [bls12_381_G1_element] -> bls12_381_G1_element
bls12_381_G2_multiScalarMul :: [Integer] -> [bls12_381_G2_element] -> bls12_381_G2_element
```

The types **bls12_381_G1_element** and **bls12_381_G2_element** are already introduced by [CIP-0381](https://cips.cardano.org/cip/CIP-0381). Given two arrays of scalars and group elements the functions compute multi-scalar multiplication for the corresponding subgroup. The arrays of scalars and group elements must be non-empty and of equal size. If the input arrays are empty or not equal, the functions must fail. These new functions naturally extend a set of operations over BLS12-381 defined by [CIP-0381](https://cips.cardano.org/cip/CIP-0381).

### Cost model
The computational impact of multi-scalar multiplication is complicated by it having dynamic-size arguments. Preliminary [benchmarks](https://github.com/dkaidalov/bench-blst-msm/) show that the computational complexity grows linearly with the size of the MSM. This should be reflected in the costing function.
It should also be taken into account that the efficiency of the MSM algorithm may vary [depending on the blst setup](https://github.com/supranational/blst/blob/master/src/multi_scalar.c#L61).

There may be an extra complication in the costing procedure because all scalars have to be reduced modulo the order of the group before being passed to the blst functions (this happens [here](https://github.com/IntersectMBO/cardano-base/blob/6f9c20abdd3010e5a25356580cc968ba430101ad/cardano-crypto-class/src/Cardano/Crypto/EllipticCurve/BLS12_381/Internal.hs#L521) for the existing BLS12-381 scalar multiplication function in `cardano-crypto-class`). Presumably this is almost zero-cost for scalars already in the correct range, but if we pass in a very long list of very large scalars, the aggregated reduction time might be quite significant, and this must be taken into account in the costing function to guard against the possibility of a large amount of computation being done too cheaply.

## Rationale: how does this CIP achieve its goals?
Integrating these functions directly into Plutus will streamline cryptographic operations, reduce transaction costs, and uphold the integrity of existing cryptographic interfaces. It addresses current inefficiencies and enhances the cryptographic capabilities of the Plutus platform.

It will allow the implementation of complex cryptographic protocols on-chain in Plutus smart contracts, significantly expanding the capabilities of the Cardano blockchain.

## Path to Active

### Acceptance Criteria

We consider the following criteria to be essential for acceptance:

- [ ] The PR for this functionality is merged in the Plutus repository.
- [ ] This PR must include tests, demonstrating that it behaves as the specification requires in this CIP.
- [ ] A benchmarked use case is implemented in the Plutus repository, demonstrating that realistic use of this primitive does, in fact, provide major cost savings.

### Implementation Plan

- [x] IOG Plutus team consulted and accept the proposal.
- [x] Authors to provide preliminary benchmarks of naive MSM implementation in Plutus and blst MSM.
- https://github.com/input-output-hk/plutus-msm-bench
- https://github.com/dkaidalov/bench-blst-msm/

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0134/README.md
---

- --
CIP: 134
Title: Cardano URIs - Address Representation
Category: Wallets
Status: Proposed
Authors:
- Steven Johnson <steven.johnson@iohk.io>
Implementors: []
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/888
Created: 2024-08-23
License: CC-BY-4.0
- --

## Abstract

This CIP proposes an extension to [CIP-0013] to allow easy and unambiguous encoding of
[CIP-0019]/[CIP-0105] Addresses into URL's.

## Motivation: why is this CIP necessary?

[CIP-0013] defines encoding for payment addresses and stake pool, however [CIP-0019]
and [CIP-0105] define numerous other address types.

These addresses cannot currently be encoded into URIs unambiguously.
This extension proposes a simple and forward compatible method of encoding such addresses into the URI scheme defined by [CIP-0013].

[x509] certificates can contain name or alternative name information related to either the issuer of
the certificate or its subject.
It is desirable to distinguish an Issuer or Subject of a certificate by one or more on-chain keys.
This, for example, can facilitate the ability to link off-chain actions authorized with a x509 certificate,
with an on-chain identity.

Currently, there is no standard way to embed a Cardano address, such as a stake address,
or a dRep address as distinguishing name within a [x509] certificate.

A URI is one of the possible names that can be associated with the Issuer or Subject of a certificate.
This URI can be referenced in the alternative name for both the Issuer and Subject of a certificate
by using the uniformResourceIdentifier in the general name.
[CIP-0013] does not define a method for stake or dRep addresses, etc., to be encoded in the URI scheme it defines.

Allowing these addresses to be easily encoded as URIs allows them to be 100% interoperable
with existing public key infrastructure and certificate creation tools.

## Specification

We extend [CIP-0013] with a single new authority for referencing Cardano addresses in [CIP-0019] format.

### Grammar & interpretation

We extend the grammar from [CIP-0013] with the new authority:

```
authorityref = (... | addr )

addr = "//addr/" cip19-addr
cip19-addr = *cip19-char
cip19-char = ALPHA / DIGIT / "_"
```

Effectively, any address string specified by [CIP-0019], [CIP-0105] or future extension to either
of these specifications can be embedded directly within the URI.

### Examples

#### CIP-19 Addresses

```uri
web+cardano://addr/addr1qx2fxv2umyhttkxyxp8x0dlpdt3k6cwng5pxj3jhsydzer3n0d3vllmyqwsx5wktcd8cc3sq835lu7drv2xwl2wywfgse35a3x
web+cardano://addr/addr_test1gz2fxv2umyhttkxyxp8x0dlpdt3k6cwng5pxj3jhsydzer5pnz75xxcrdw5vky
web+cardano://addr/stake1uyehkck0lajq8gr28t9uxnuvgcqrc6070x3k9r8048z8y5gh6ffgw
web+cardano://addr/stake_test1uqehkck0lajq8gr28t9uxnuvgcqrc6070x3k9r8048z8y5gssrtvn
```

#### CIP-105 Addresses

```uri
web+cardano://addr/drep_vk17axh4sc9zwkpsft3tlgpjemfwc0u5mnld80r85zw7zdqcst6w54sdv4a4e
web+cardano://addr/drep15k6929drl7xt0spvudgcxndryn4kmlzpk4meed0xhqe25nle07s
web+cardano://addr/drep_script16pjhzfkm7rqntfezfkgu5p50t0mkntmdruwlp089zu8v29l95rg
web+cardano://addr/cc_cold_vk149up407pvp9p36lldlp4qckqqzn6vm7u5yerwy8d8rqalse3t04q7qsvwl
web+cardano://addr/cc_cold1lmaet9hdvu9d9jvh34u0un4ndw3yewaq5ch6fnwsctw02xxwylj
web+cardano://addr/cc_cold_script14ehj5f64f40xju0086fnunctulkh46mq7munm7upe4hpcwpcat
web+cardano://addr/cc_cold_script14ehj5f64f40xju0086fnunctulkh46mq7munm7upe4hpcwpcatv
web+cardano://addr/cc_hot_vk10y48lq72hypxraew74lwjjn9e2dscuwphckglh2nrrpkgweqk5hschnzv5
web+cardano://addr/cc_hot17mffcrm3vnfhvyxt7ea3y65e804jfgrk6pjn78aqd9vg7xpq8dv
web+cardano://addr/cc_hot_script16fayy2wf9myfvxmtl5e2suuqmnhy5zx80vxkezen7xqwskncf40
```

## Rationale: how does this CIP achieve its goals?

By extending [CIP-0013] to allow a [CIP-0019] encoded address to be simply embedded in the URI scheme,
we enable existing certificate creation tools and public key infrastructure to be used to easily
create certificates that reference Cardano addresses.

It is envisioned that this extension could have use cases beyond the one presented here.

## Path to Active

### Acceptance Criteria

- [ ] Community Feedback and Review Integrated.
- [ ] Demonstration of Cardano addresses being embedded in x509 certificates using existing tools.
- [ ] At least one project utilizing this standard.

### Implementation Plan

Project Catalyst intends to use this standard to facilitate linking of on-chain and off-chain identity
with x509 certificates.
This specification does not deal with the processes or proofs required, simply the URI scheme that is
required to embed a Cardano address in a x509 certificate.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

[CIP-0013]:https://github.com/cardano-foundation/CIPs/blob/master/CIP-0013/
[CIP-0019]:https://github.com/cardano-foundation/CIPs/blob/master/CIP-0019/
[CIP-0105]:https://github.com/cardano-foundation/CIPs/blob/master/CIP-0105/
[x509]:https://datatracker.ietf.org/doc/html/rfc5280

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0135/README.md
---

- --
CIP: 135
Title: Disaster Recovery Plan for Cardano networks
Category: Tools
Status: Active
Authors:
- Kevin Hammond <kevin.hammond@iohk.io>
- Sam Leathers <samuel.leathers@iohk.io>
- Alex Moser <alex.moser@cardanofoundation.org>
- Steve Wagendorp <steve.wagendorp@cardanofoundation.org>
- Andrew Westberg <andrewwestberg@gmail.com>
- Nicholas Clarke <nicholas.clarke@tweag.io>
Implementors: N/A
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/893
Created: 2024-06-17
License: CC-BY-4.0
- --

## Abstract

While the Cardano mainnet and other networks have proven to be highly resilient, it is necessary to proactively
consider the possible recovery mechanisms and procedures that may be required in the unlikely
event of a major failure where the network is unable to recover itself.

This CIP considers three representative scenarios and addresses specific considerations relevant
in each case:

Scenario 1 - __Long-Lived Network Partition__
Scenario 2 - __Failure to Make Blocks for an Extended Period of Time__
Scenario 3 - __Bad Blocks Minted on Chain__

To ensure successful recovery in the event of a chain failure, it's crucial to establish effective
communication channels and exercise recovery procedures in advance to familiarize the community and
stake pool operators (SPOs) with the process.

This CIP is based on an earlier IOHK technical report that is referenced below, supplemented by internal
documentation and discussions that have not been publicly released. It should be considered to be a living
document that is reviewed and revised on a regular basis.

Note that although the focus of disaster recovery is on Cardano mainnet, since this is the greatest risk
of loss of funds, the recovery procedures are generic and apply to other Cardano
networks, including SanchoNet, Preview, PreProd or private networks.
Appropriate adjustments may need to be made to reflect differences in timing or other concerns.


## Motivation: why is this CIP necessary?

This CIP is needed to familiarize stakeholders with the processes and procedures that should be
followed in the unlikely event that the Cardano mainnet, or another Cardano network, encounters
a situation where the built-in on-chain recovery mechanisms fail.

## Specification

While the exact recovery process will depend on the unique nature of the failure, there are three main scenarios we can consider.

### Scenario 1: Long-Lived Network Partition

Ouroboros Praos is designed to cope with real-world networking
conditions, in which some nodes may temporarily be disconnected from
the network.  In this case, the network will continue to make blocks,
perhaps at some lower chain density (reflecting the temporary loss of
stake to the network as a whole).  As nodes rejoin the network, they
will then participate in normal block production once again. In this
way, the network remains resilient to changes in connectivity.

If many nodes become disconnected, the network could divide into two
or more completely disconnected parts.  Each part of the network could
then form its own chain, backed by the stake that is participating in
its own partition.  Under normal conditions, Praos will also deal with
this situation.  When the partitioned group of nodes reconnects, the
longest chain will dominate, and the shorter chain will be discarded.
The nodes on the shorter chain will automatically rollback to the
point where the fork occurred, and then rejoin the main chain.  This
is perfectly normal.  Such forks will typically last only a few
blocks.

However, in an extreme situation, the partition may persist beyond the
Praos rollback limit of *k* blocks (currently 2,160 blocks on mainnet).
In this case, the nodes will not be able to rollback to rejoin the main chain, since this
would violate the required Praos guarantees.


#### Remediations

Disconnected nodes must be reconnected to the main chain by their operators. This can be done
by truncating the local block database to a point before the chain fork and then resyncing
against the main network, using the `db-truncator` tool, for example.

Full node wallets can also be recovered in the same way, though this may require technical
skills that the end users do not possess. It may be easier, if slower, for them to simply
resynchronize their nodes from the start of the chain (i.e. from the genesis block).

Ouroboros Genesis provides additional resilience when recovering from long lived network partitions.
In Praos nodes resyncing from a point before the chain fork could still in some cases follow the
alternative chain (if it is the first one seen) and extra mechanisms may be needed to avoid this
possibility. In Praos, for example, this may require that all participants on the alternative chain
truncate the local block database prior to the partition being resolved. In Ouroboros Genesis
when resyncing from a point before the chain fork, the chain selection rules will ensure
selection of the correct path for the main chain assuming the partition has been resolved.

Alternative methods to resynchronise the node to the main chain might
include the use of Mithril or other signed snapshots.  These would
allow faster recovery.  However, in this case, care needs to be taken
to achieve the correct balance of trust against speed of recovery.

#### Additional Effects on Cardano Users

Although block producing nodes will rejoin the main network following the remediation
described above, the blocks that they have
minted while they were disconnected will not be included in the main
chain.  This may have real world effects that will not be
automatically remedied when the nodes rejoin the main chain.  For
example, transactions may have been processed that have significant
real world value, or assumptions may have been made about chains of
evidence/validity, or the timing of transactions. End users should be
aware of the possibility and include provisions in their contracts to
cover this eventuality.  It may be necessary to resubmit some or all of the
transactions that were processed on the minority chain onto the main chain.
To avoid unexpected effects, this should be done by the end users/applications, and not
by block producers acting on their behalf.

If they are not observant, stake pools, full node wallets and
other node users (e.g. explorers) could continue indefinitely on the minority
chain.  Such users should take care to be aware of this situation and
take steps to rejoin the main chain as quickly as possible.
A reliable and trusted public warning system should be considered that can alert users
and advise them on how to rejoin the main chain.


#### Timing Considerations

On Cardano mainnet, partitions of less than 2,160 blocks will automatically rejoin the main chain.  With current Cardano mainnet settings, this represents
a period of up to 12 hours during which automatic rollback will occur.  If the partition exceeds 2,160 blocks, then the
procedure described above will be necessary to allow nodes to rejoin the main chain.  Other Cardano networks may have different
timing characteristics.


### Scenario 2: Failure to Make Blocks for an Extended Period of Time

Ouroboros Praos requires *at least* one block to be produced every *3k/f* slots.  With the current Cardano mainnet
settings, that is a 36 hour period.  Such an event is extremely unlikely, but if it were to happen then the network
would be unable to make any further blocks.

#### Mitigation

It is recommended to monitor the chain for block production.  If a low density period is observed, then block producers
should be notified, and efforts made to mint new blocks prior to the expiry of the *3k/f* window.  If this is not possible
then the remediation procedures should be followed.

#### Remediation

Identify a small group of block producing nodes that will be used to recover the chain.  For Cardano mainnet, this group should have
sufficient delegated stake to be capable of generating at least 9 blocks in a 36 hour window.
It should be isolated from the rest of the network.
The chain can then be recovered by resetting the wall clocks on the group of block producing nodes,
restarting them from the last good block on the Cardano network, playing forward the chain production
at high speed (10x usual speed is recommended), while inserting new empty blocks at the slots which
are allocated to the block producers. The recovery nodes can then be restarted with normal settings, including
connections to the network.  Ouroboros Genesis then allows other nodes in the network to rapidly resynchronize
with the newly restored chain.  This would leave one or more gaps in the chain, interspersed with empty blocks.

##### Rewards Donation by Recovery Block Producers

In order to avoid allegations of unfair behaviour, block producing nodes that are used to recover the network should
donate any rewards that they receive during recovery to the treasury.


#### Additional Effects on Cardano Users

Unlike Scenario 1, no transactions will be submitted that need to be resubmitted on the chain.
Users will, however, experience an extended period during which the chain is unavailable.
Cardano applications and contracts should be designed with this possibility in mind.
Full node wallets and other node users should recover quickly once the network is restarted
but there may be a period of instability while network connections are re-established
and the Ouroboros Genesis snapshot is distributed across all nodes.


#### Timing Considerations

The chain will tolerate a gap of up to *3k/f* slots (36 hours with current Cardano mainnet settings).
A period of low chain density could have security implications that affect dynamic availability
and leave open the possibility for future long range attacks. This may be particularly
relevant should chain recovery be performed as described above (using less stake than is required
for an honest majority). To mitigate the presence of an extended period of low chain density we may
need to make use of the lightweight checkpointing mechanism in Ouroborus Genesis. Alternatively, Mithril
could also be used to provide certified snapshots to stake pools as a means to verify the correct state of the ledger.

The adoption of Mithril for fast bootstrapping by light clients and edge nodes should help to mitigate risks
for the types of users on the network that do not participate in consensus.

As described below, Ouroboros Genesis snapshots may also be useful as part of the recovery process.


### Scenario 3: Bad Blocks Minted on Chain

In the event that a bad block was to be minted on-chain, then some or all validators might be unable to process the block.
They would therefore stop, and be unable to restart.  Wallet and other nodes might be unable to synchronise beyond the
point of the bad block.

#### Remediation

Depending on the cause of the issue and its severity, alternative remediations might be possible.

- *Scenario 3.1**: if some existing node versions were able to process the block, but others were not, then
the chain would continue to grow at a lower chain density.  SPOs would need to be persuaded to upgrade (or downgrade)
to a suitable node version that would allow the chain to continue.  The chain density would then gradually recover to its normal level.
Other users would need to upgrade (or downgrade) to a version of the node that could follow the full chain.

- *Scenario 3.2**: if no node version was able to process the block and a
gap of less than *3k/f* slots existed, then the chain could be rolled
back immediately before the bad block was created, and nodes
restarted from this point.  The chain would then grow as normal, with a small gap around the bad block.
In this case, care would need to be taken that the rogue transaction was not accidentally reinserted into the chain.
This might involve clearing node mempools, applying filters on the transaction, or developing and deploying a new node version that
rejected the bad block.

- *Scenario 3.3**: an alternative to rolling back would be to develop and deploy a "hot-fix" node that could
accept the bad block, either as an exception, or as new acceptable behaviour.
Nodes would then be able to incorporate the bad block as part of the chain,
minting new blocks as usual, or following the chain.
In this case, the bad block would persist on-chain indefinitely and future nodes
would also need to accept the bad block.  Such an approach is best used when the rejected block has behaviour
that was unanticipated, but which is benign in nature.  This will leave no abnormal gaps in the chain.

- *Scenario 3.4**: if more than *3k/f* slots have passed since the bad block was minted, then it will be necessary to roll back the chain immediately
prior to the bad block as in Scenario 3.2, and then proceed as described for Scenario 2.  As with Scenario 2, this will leave
a series of gaps in the chain that are interspersed with empty blocks.

#### Timing Considerations

If more than *3k/f* slots have passed since the bad block was minted on-chain (36 hours with current Cardano mainnet settings),
then a mix of recovery techniques will be needed, as described in Scenario 3.4.  When deciding on the correct recovery
technique for Scenarios 3.1-3.3, consideration should be given as to whether the recovery can be successfully completed before *3k/f* slots
have elapsed.  In case of doubt, the procedure for Scenario 3.4 should be followed.

### Using Ouroboros Genesis Snapshots

Any of the above conditions may result in a period of lower chain density. The
updated consensus mechanism introduced in Ouroboros Genesis relies on making
chain density comparisons to assist a node when catching up with the network,
in order to reduce the reliance on having trusted peers when syncing. As
such, low-density periods pose a potential security risk for the future; they
are periods where a motivated adversary could perform a long-range attack by
building a higher density chain.

In order to mitigate this, Genesis introduces the concepts of lightweight
checkpoints. A lightweight checkpoint is effectively a block point - a
combination of block number and hash - which can be distributed along with the
node. Unlike Mithril Snapshots (see below), Genesis lightweight snapshots are not assured by any committee - rather, they form part of the trusted codebase distributed with the node, or by other parties.

When syncing, a Genesis node will refuse to validate past the block number of any lightweight checkpoint if the chain does not contain the correct block at that point.

Genesis snapshots play two potential roles in disaster recovery:

1. In scenarios where the network is split, a lightweight snapshot could guide
   a node from the abandoned partition in connecting to the main partition. In
   general this should not be needed, however, since the main partition should win
   out in any Genesis density comparisons. This usage also falls closer to
   scenario 2, in that it relies on an external source imposing a chain selection,
   which must then be trusted by all parties.
2. Following a disaster recovery procedure, a sufficient number of blocks
   covering the low density period should be added to the list of lightweight
   checkpoints. These would serve the purpose of preventing a subsequent
   long-range attack.

Note that, in this second scenario, concerns about the legitimacy of the
checkpoint are much less salient. The checkpoint can be issued post disaster
recovery, at such a time where the points it contains are in the past, and are
both agreed upon and easy to verify for all honest parties.


### Using Mithril Snapshots

Mithril is a stake-based threshold multi-signatures scheme. One of the applications of this protocol in Cardano
is to create certified snapshots of the Cardano blockchain. Mithril snapshots allow nodes or applications
to obtain a verified copy of the current state of the blockchain without having to download and verify the full history.

SPOs that participate in the Mithril network provide signed snapshots to a Mithril aggregator that
is responsible for collecting individual signatures from Mithril signers and aggregating them into a multi-signature.
Using this capability, the Mithril aggregator can then provide certified snapshots of the Cardano blockchain that
can potentially be used as a trusted source for recovery purposes.

Provided that it gains sufficient adoption on the Cardano network and that
snapshots continue to be signed by an honest majority of stake pools
following a chain recovery event, Mithril may therefore provide an
alternative solution to Ouroboros Genesis checkpoints as a way to
verify the correct state of the ledger


### Recommended Actions for Cardano mainnet

1. Monitor Cardano mainnet for periods of low density and take early action if an extended period is observed.
2. Identify a collection of block producer nodes that has sufficient stake to mint at least 9 blocks in any 36 hour window.
3. Set up emergency communication channels with stake pool operators and other community members.
4. Practice disaster recovery procedures on a regular basis.
5. Provide signed Mithril snapshots and a way for full node wallet users and others to recover from this snapshot.
6. Determine how to employ Ouroboros Genesis snapshots as part of the disaster recovery process

#### Community Engagement

One of the key requirements for successful disaster recovery will be proper engagement with the community.

1. Identify stake pool operators (SPOs) who can assist with disaster recovery
2. Discuss disaster recovery requirements with Intersect's Technical Working Groups and Security Council
3. Identify and establish the right communications channels with the community, including Intersect
4. Set up regular disaster recovery practice sessions


## Rationale: how does this CIP achieve its goals?

This CIP outlines key disaster recovery scenarios that the Cardano community should understand to mitigate
potential network outages. As a living document, it will be regularly reviewed and updated to inform
stakeholders and encourage more detailed contingency planning. The CIP aims to facilitate discussions,
establish recovery procedures, and encourage regular recovery practice exercises to ensure preparedness
and validation of recovery actions in the event of an outage.

## Path to Active

### Acceptance criteria

- [x] The proposal has been reviewed by the community and sufficiently advertised on various channels.
- [x] Intersect Technical Groups
- [x] Intersect Discord Channels
- [x] Cardano Forum

- [x] All major concerns or feedback have been addressed.

### Implementation Plan

N/A

## Change Log

| Version | Date | Description |
| -------- | -------- | ------- |
| 0.1 | 2024-08-30 | Initial submitted version |
| 0.2 | 2024-09-10 | Revised version to emphasize genericity of recovery techniques |
| 0.3 | 2024-09-18 | Revised version following CIP editors meeting |

## References

[Cardano Disaster Recovery Plan (May 2021)](https://iohk.io/en/research/library/papers/cardano-disaster-recovery-plan/)

[Cardano Incident Reports](https://updates.cardano.intersectmbo.org/tags/incident)

[January 2023 Block Production Temporary Outage](https://updates.cardano.intersectmbo.org/2023-04-17-ledger)

[DB Truncator Tool](https://github.com/IntersectMBO/ouroboros-consensus/tree/486753d0b7d6b0d09621d1ef8be85e5117ff3d1e/ouroboros-consensus-cardano/app)

[DB Synthesizer Tool](https://github.com/IntersectMBO/ouroboros-consensus/tree/486753d0b7d6b0d09621d1ef8be85e5117ff3d1e/ouroboros-consensus-cardano/app)

[Ouroboros Genesis](https://iohk.io/en/research/library/papers/ouroboros-genesis-composable-proof-of-stake-blockchains-with-dynamic-availability/)

[Mithril](https://github.com/input-output-hk/mithril)


## Copyright

 This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0136/README.md
---

- --
CIP: 136
Title: Governance metadata - Constitutional Committee votes
Category: Metadata
Status: Proposed
Authors:
- Ryan Williams <ryan.williams@intersectmbo.org>
- Eystein Magnus Hansen <eysteinsofus@gmail.com>
Implementors: []
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/878
Created: 2024-07-17
License: CC-BY-4.0
- --

## Abstract

The Conway ledger era ushers in on-chain governance for Cardano via [CIP-1694 | A First Step Towards On-Chain Decentralized Governance](https://github.com/cardano-foundation/CIPs/blob/master/CIP-1694/README.md), with the addition of many new on-chain governance artifacts.
Some of these artifacts support the linking of off-chain metadata, as a way to provide context to on-chain actions.

The [CIP-100 | Governance Metadata](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0100) standard provides a base framework for how all off-chain governance metadata can be formed and handled.
This standard was intentionally limited in scope, so that it can be expanded upon by more specific subsequent CIPs.

This proposal aims to provide a specification for the off-chain metadata vocabulary that can be used to give context to Constitutional Committee (CC) votes.

## Motivation: why is this CIP necessary?

The high-level motivation for this proposal is to provide a standard which improves legitimacy of Cardano's governance system.

### Clarity for governance action authors

Governance action authors are likely to have dedicated a significant amount of time to making their action meaningful and effective (as well as locking a significant deposit).
If this action is not able to be ratified by the CC, it is fair for the author to expect a reasonable explanation from the CC.

Without reasonable context being provided by the CC votes, authors may struggle to iterate upon their actions, until they are deemed constitutional.
This situation could decrease perceived legitimacy in Cardano's governance.

### Context for other voting bodies

By producing a standard we hope to encourage all CC members to attach rich contextual metadata to their votes.
This context should show CC member's decision making is fair and reasonable.

This context allows the other voting bodies to adequately check the power of the CC.

### CC votes are different to other types of vote

The CC and their votes are fundamentally very different from the other voting bodies.
This makes reusing standards from these voting bodies problematic.

### Inclusion within interim constitution

Cardano's [Interim Constitution Article VI Section 4](https://github.com/IntersectMBO/interim-constitution/blob/75155526ce850118898bd5eacf460f5d68ceb083/cardano-constitution-0.txt#L330) states:

```txt
Constitutional Committee processes shall be transparent.
The Constitutional Committee shall publish each decision.
When voting no on a proposal, the Committee shall set forth the basis
for its decision with reference to specific Articles of this Constitution
that are in conflict with a given proposal.
```

This section suggests that the CC shall provide rationale documents.
Specifying a standard structure and common vocabulary for these documents aids the creation of supporting tooling.

### Tooling

By creating and implementing these metadata standards we facilitate the creation of tooling that can read and write this data.
Such tooling greatly expands the reach and effectiveness of rationales as it allows for rich user interfaces to be created.
i.e. translation tools, rationale comparison tools.

## Specification

We define a specification for fields which can be added to CC votes.

### Extended Body Vocabulary

The following properties extend the potential vocabulary of [CIP-100](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0100)'s `body` property.

#### `summary`

- A short text field. Limited to `300` characters.
- Authors SHOULD use this field to clearly state their stance on the issue.
- Authors SHOULD use this field to succinctly describe their rationale.
- Authors SHOULD give a brief overview of the main arguments will support your position.
- This SHOULD NOT support markdown text styling.
- Compulsory.

#### `rationaleStatement`

- A long text field.
- Authors SHOULD use this field to fully describe their rationale.
- Authors SHOULD discuss their arguments in full detail.
- This field SHOULD support markdown text styling.
- Compulsory.

#### `precedentDiscussion`

- A long text field.
- The author SHOULD use this field to discuss what they feel is relevant precedent.
- This field SHOULD support markdown text styling.
- Optional.

#### `counterargumentDiscussion`

- A long text field.
- The author SHOULD use this field to discuss significant counter arguments to the position taken.
- This field SHOULD support markdown text styling.
- Optional.

#### `conclusion`

- A long text field.
- The author SHOULD use this field to conclude their rationale.
- This SHOULD NOT support markdown text styling.
- Optional.

#### `internalVote`

- A custom object field.
- This field SHOULD be used to reflect any internal voting decisions within CC member.
- This field SHOULD be used by members who are constructed from organizations or consortiums.
- Optional.

##### `constitutional`

- A positive integer.
- The author SHOULD use this field to represent a number of internal votes for the constitutionality of the action.
- Optional.

##### `unconstitutional`

- A positive integer.
- The author SHOULD use this field to represent a number of internal votes against the constitutionality of the action.
- Optional.

##### `abstain`

- A positive integer.
- The author SHOULD use this field to represent a number of internal abstain votes for the action.
- Optional.

##### `didNotVote`

- A positive integer.
- The author SHOULD use this field to represent a number of unused internal votes.
- Optional.

##### `againstVote`

- A positive integer.
- The author SHOULD use this field to represent a number of internal votes to not vote on the action.
- Optional.

### Extended `references` Vocabulary

Here we extend CIP-100's `references` field.

#### `RelevantArticles`

- We add to CIP-100's `@type`s, with a type of `RelevantArticles`.
- Authors SHOULD use this field to list the relevant constitution articles to their argument.

### Application

CC must include all compulsory fields to be considered CIP-136 compliant.
As this is an extension to CIP-100, all CIP-100 fields can be included within CIP-136 compliant metadata.

### Test Vector

See [test-vector.md](./test-vector.md) for examples.

### Versioning

This proposal should not be versioned, to update this standard a new CIP should be proposed.
Although through the JSON-LD mechanism further CIPs can add to the common governance metadata vocabulary.

## Rationale: how does this CIP achieve its goals?

By providing a peer reviewed structure for CC vote rationale, we encourage detailed voting rationales increasing the legitimacy of CC votes within the governance system.

### `summary`

We include compulsory summary with limited size to allow for the creation of tooling which layers of inspection to vote rationale.
This allows readers to get a summary of a rationale at a high level before reading all the details.

### `rationaleStatement`

This field allows for a very long-form discussion of their rationale.
This is compulsory because it forms the core of their rationale.

By setting some fields to compulsory we ensure a minimum amount of data for downstream tools to expect to render.

### `precedentDiscussion`

This is a dedicated field to be able to discuss specific precedent of votes.
By separating this from `rationaleStatement` we encourage specific discussion of precedence as well as clear separation in tooling.

### `counterargumentDiscussion`

This is a dedicated field to be able to discuss counterarguments from those proposed in the other fields.
By separating this from `rationaleStatement` we encourage specific discussion of counterarguments as well as clear separation in tooling.

### `internalVote`

This field, gives the ability for CC members who are operated by multiple individuals to share insights on specific voting choice of the individuals.
This could add additional context to the workings and opinions of the individuals who operate the CC member.

### `relevantArticles`

By providing a new type to CIP-100 `References` we encourage tooling to differentiate clearly `References` to the constitution from other types of `Reference`.

## Path to Active

### Acceptance Criteria

- [ ] This standard is supported by two separate tools, which create and submit CC votes.
- [ ] This standard is supported by two different chain indexing tools, used to read and render metadata.

### Implementation Plan

- [x] Seek feedback from individuals who are members of current Interim Constitutional Committee.
- [x] Author to provide test vectors, examples, and schema files.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0137/README.md
---

- --
CIP: 137
Title: Decentralized Message Queue
Category: Network
Status: Proposed
Authors:
- Jean-Philippe Raynaud <jp.raynaud@gmail.com>
- Arnaud Bailly <arnaud.bailly@iohk.io>
- Marcin Szamotulski <marcin.szamotulski@iohk.io>
- Armando Santos <armando.santos@iohk.io>
- Neil Davies <neil.davies@iohk.io>
- Sebastian Nagel <sebastian.nagel@ncoding.at>
Implementors:
- Cardano Scaling team <https://github.com/cardano-scaling>
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/876
Created: 2024-08-02
License: Apache-2.0
- --

## Abstract

We propose to create a decentralized message diffusion protocol leveraging the Cardano network layer. This protocol allows to follow a topic based diffusion of messages from publishers to subscribers in a decentralized way.

The messages can be sent and received by nodes running in a non intrusive way side by side to the Cardano node in order to enable inter-nodes communications.

In this way, we can significantly reduce the cost and effort required to build a decentralised network for message diffusion by using Cardano's established infrastructure, with limited impact on the performance and no impact on the security of the Cardano network.

## Motivation: why is this CIP necessary?

Many protocols in the Cardano ecosystem need the capability to diffuse messages in a decentralized manner. However, it is not possible to diffuse any type of message from Cardano block producers to a limited subset of subscribed peers. Nonetheless, the Cardano network has a proven efficient, reliable and secure infrastructure which is used to diffuse a transaction from one peer to all the other peers in the network. This infrastructure can be leveraged to achieve the goal of diffusing other types of messages with the guarantees offered by the Cardano network and a reduced development overhead.

Mithril is a protocol based on a [Stake-based Threshold Multi-signature scheme](https://iohk.io/en/research/library/papers/mithril-stake-based-threshold-multisignatures/) which leverages the Cardano SPOs to certify data from the Cardano chain in a trustless way. Mithril is currently used in the Cardano ecosystem in order to enable fast bootstrapping of full nodes and enabling secure light wallets. The Mithril protocol coordinates the collection of individual signatures originating from the signers (run by SPOs) by the aggregators which combine them into Mithril multi-signatures and certificates. In order to be fully decentralized, the protocol needs to rely on a decentralized peer to peer network which, if built from the ground up, would require significant efforts and investment. Furthermore, the majority of SPO's, as the representatives of Cardano's active stake, will have to adopt and operate Mithril nodes alongside their Cardano node. Thus a natural solution is to use the Cardano network layer to significantly facilitate the development of the Mithril protocol without a significant impact on the Cardano network or high maintenance efforts for the SPOs. Mithril will be a fundamental first user of the proposed Decentralized Message Queue and it will be used as an illustrative example throughout this document.

Other protocols in the Cardano ecosystem, such as Leios and Peras (and probably other protocols in the future), also need the capability to diffuse messages originating from block producers in a decentralized fashion. However, in the Leios and Peras cases, the Cardano node itself is a producer and consumer of these messages. We have taken into consideration this need for a generic solution in the design proposed.

The proposed solution is described in detail below.

## Specification

### Overview

![Overview](./img/overview.jpg)

This specification proposes to create `3` new mini-protocols in the Cardano network layer:

- `node-2-node`:
- [**Message Submission mini-protocol**](#Message-Submission-mini-protocol): Diffusion of the messages on the Cardano network.
- `node-2-client`:
- [**Local Message Submission mini-protocol**](#Local-Message-Submission-mini-protocol): Local submission of a message to be diffused by the Cardano network.
- [**Local Message Notification mini-protocol**](#Local-Message-Notification-mini-protocol): Local notification of a message received from the Cardano network.

> [!NOTE]
> The terms **Message producer**, **Message consumer** and **Network node** may represent different entities depending on the concrete implementation made for a specific protocol:
>
> - the **Network node** could be either the **Cardano node** itself or a **Decentralized Message Queue node** or **DMQ node** implementing the mini-protocols described in this document. Opting in for one of these possibilities will depend on a careful analysis of the impact on the security of the Cardano node, impact on the load of the Cardano network, the specific network topology needed by the protocol and the needed level of coupling with the Cardano node itself (access to ledger, consensus, ...). It's worth mentioning that each protocol will implement its own version of the **Network node** by leveraging a common implementation of the mini-protocols.
> - the message **Message producer** and **Message consumer** could be either the **Cardano node** itself or **another node** able to interact with the **Network node** through the node-to-client mini-protocols detailed in this document.
>
> Here is a summary of the meanings of these terms depending on the protocol:
> | Protocol | Message producer | Message consumer | Network node |
> |------|------|------|------|
> | **Mithril** | [Mithril signer](https://mithril.network/doc/mithril/mithril-network/signer) | [Mithril aggregator](https://mithril.network/doc/mithril/mithril-network/architecture) | [DMQ node](#information-diffusion-architecture) |

### Message Submission mini-protocol

#### Description

The node to node message submission protocol is used to transfer messages between full nodes. It follows a pull-based strategy where the inbound side asks for new messages and the outbound side returns them back. This protocol is designed to guard both sides against resource consumption attacks from the other side in a trustless setting.

> [!NOTE]
> There exists a local message submission protocol which is used when the server trusts a local client as described in the [following section](#Local-Message-Submission-mini-protocol).

#### State machine

| Agency                   |                                                                   |
| ------------------------ | ----------------------------------------------------------------- |
| Outbound side has Agency | StInit, StMessageIdsNonBlocking, StMessageIdsBlocking, StMessages |
| Inbound side has Agency  | StIdle, StDone                                                    |

```mermaid
stateDiagram-v2

  classDef White fill:white,stroke:white
  classDef Black fill:white,stroke:black
  classDef Blue fill:white,stroke:blue
  classDef Green fill:white,stroke:green

  start:::White --> StInit:::Green
  StInit:::Green --> StIdle:::Blue : MsgInit
  StIdle:::Blue --> StMessageIdsBlocking:::Green : MsgRequestMessageIdsBlocking
  StMessageIdsBlocking:::Green --> StIdle:::Blue : MsgReplyMessageIds
  StIdle:::Blue --> StMessageIdsNonBlocking:::Green : MsgRequestMessageIdsNonBlocking
  StMessageIdsNonBlocking:::Green --> StIdle:::Blue : MsgReplyMessageIds
  StMessageIdsBlocking:::Green --> StDone:::Black : MsgDone
  StIdle:::Blue --> StMessages:::Green : MsgRequestMessages
  StMessages:::Green --> StIdle:::Blue : MsgReplyMessages

```

##### Protocol messages

- **MsgInit**: Initial message of the protocol.
- **MsgRequestMessageIdsNonBlocking(ack,req)**: The inbound side asks for new message ids and acknowledges old ids. The outbound side immediately replies (possible with an empty list).
- **MsgRequestMessageIdsBlocking(ack,req)**: The inbound side asks for new messages ids and acknowledges old ids. The outbound side will block until new messages are available.
- **MsgReplyMessageIds([(id,size)])**: The outbound side replies with a list of available messages. The list contains pairs of message ids and the corresponding size of the message in bytes. In the blocking case the reply is guaranteed to contain at least one message. In the non-blocking case, the reply may contain an empty list.
- **MsgRequestMessages([id])**: The inbound side requests messages by sending a list of message-ids.
- **MsgReplyMessages([messages])**: The outbound side replies with a list messages.
- **MsgDone**: The outbound side terminates the mini-protocol.

##### Transition table

| From state              | Message                         | Parameters  | To State                |
| ----------------------- | ------------------------------- | ----------- | ----------------------- |
| StInit                  | MsgInit                         |             | StIdle                  |
| StIdle                  | MsgRequestMessageIdsBlocking    | ack,req     | StMessageIdsBlocking    |
| StMessageIdsBlocking    | MsgReplyMessageIds              | [(id,size)] | StIdle                  |
| StIdle                  | MsgRequestMessageIdsNonBlocking | ack,req     | StMessageIdsNonBlocking |
| StMessageIdsNonBlocking | MsgReplyMessageIds              | [(id,size)] | StIdle                  |
| StIdle                  | MsgRequestMessages              | [id]        | StMessages              |
| StMessages              | MsgReplyMessages                | [messages]  | StIdle                  |
| StMessageIdsBlocking    | MsgDone                         |             | StDone                  |

> [!NOTE]
> The `StInit` state is needed as it allows to start all outbound sides on the same side of the connection, which is needed as the information flows in the opposite direction with this special **message submission mini-protocol**. This is also the case with the **tx-submission mini-protocol** because information flows in the other direction than for headers with **chain-sync mini-protocol** or blocks with **block-fetch mini-protocol**.

##### CDDL encoding specification

```cddl
messageSubmissionMessage
  = msgInit
  / msgRequestMessageIds
  / msgReplyMessageIds
  / msgRequestMessages
  / msgReplyMessages
  / msgDone

msgInit              = [0]
msgRequestMessageIds = [1, isBlocking, messageCount, messageCount]
msgReplyMessageIds   = [2, [ *messageIdAndSize ] ]
msgRequestMessages   = [3, messageIdList ]
msgReplyMessages     = [4, ]
msgDone              = [5, ]

isBlocking = false / true
messageCount = word16
messageId = bstr
messageBody = bstr
messageIdAndSize = [ messageId, messageSizeInBytes ]
messageIdList = [ * messageId ]
messageList = [ * message ]
messageSizeInBytes = word32
kesSignature = bstr
kesPeriod = word32
operationalCertificate = [ bstr .size 32, word64, word64, bstr .size 64 ]
coldVerificationKey = bstr .size 32
expiresAt = word32

messagePayload = [
  messageId
  , messageBody
  , kesPeriod
  , expiresAt
]
message = [
  messagePayload
  , kesSignature
  , operationalCertificate
  , coldVerificationKey
]
```

#### Inbound side and outbound side implementation

This mini-protocol is designed with two goals in mind:

- diffuse messages with high efficiency
- protect from asymmetric resource attacks from the message consumer against the message provider

The mini-protocol is based on two pull-based operations:

- the message consumer asks for message ids,
- and uses these ids to request a batch of messages (which it has not received yet)

The outbound side is responsible for initiating the mini-protocol with a peer node, but the inbound side (i.e. the other node) is the one who asks for information.

The outbound side maintains a limited size FIFO queue of outstanding messages for each of the inbound sides it is connected to, so does the inbound side with a mirror FIFO queue of message ids:

- the inbound side asks for the next message ids and acknowledges for the previous message ids received (and removed from its queue).
- the outbound side removes the acknowledged ids from the FIFO queue it maintains for the inbound side.
- the inbound side can download the content of the messages by giving an unordered list of ids to the outbound side.
- the outbound side reply omits any message that may have become invalid in the meantime.

The protocol supports blocking and non-blocking requests:

- the outbound side must reply immediately to a non-blocking request.
- the inbound side must wait until the outbound side has at least one message available.
- if the current queue of the inbound side is empty, it must use a blocking request and a non-blocking request otherwise.

#### Protocol invariants

##### Outbound side

- blocking request must be done if and only if the buffer of unacknowledged ids is empty (this also means that one cannot do a non-blocking request if the unacknowledged ids buffer is empty).
- one cannot request `0` ids either through a blocking or a non-blocking request.

##### Inbound side

- it is a protocol error to send a message which id wasn't requested.

#### Message invalidation

##### Message invalidation mechanism

In order to bound the resource requirements needed to store the messages in a network node, their lifetime should be limited. Thus, they carry an expiration date (formatted as a Unix timestamp) which must be checked before processing the message:

- the message will be invalidated when the local clock of the processing node exceeds the expiration date
- an expiration date which expires too far in the future will be considered a protocol violation (the maximum allowed time to live is a protocol parameter for each topic which may be negotiated).

##### Cost of valid message storage

> [!NOTE]
> Computations are based on the assumption of a **30 minutes** TTL for messages and are assuming that the messages are stored once in the memory of the network node (i.e. the aforementioned FIFO queues store reference to the messages).

For a total of **3,100** Cardano SPOs on the `mainnet`, on an average **50%** of them will be eligible to send signatures (i.e. will win at least one lottery in the Mithril protocol). This means that if the full Cardano stake distribution is involved in the Mithril protocol, only **1,550** signers will send signatures at each round:

- the maximum number of valid messages stored by a node at any given time is:

| Send period | Messages in memory |
| ----------- | ------------------ |
| 1 min       | 45 k               |
| 2 min       | 23 k               |
| 5 min       | 9 k                |
| 10 min      | 5 k                |

- the maximum extra memory for the valid messages stored by a node at any given time is:

| Send period | Lower bound | Upper bound |
| ----------- | ----------- | ----------- |
| 1 min       | 51 MB       | 124 MB      |
| 2 min       | 26 MB       | 62 MB       |
| 5 min       | 11 MB       | 25 MB       |
| 10 min      | 6 MB        | 13 MB       |

#### Protocol authentication

##### Message authentication mechanism

The payload part of the message (message id, message body, KES period and expiration timestamp fields) is signed with the KES key of the SPO (the message signed is the CBOR encoding of the payload: `bstr .cbor messagePayload`). The message is composed of the aforementioned payload (encoded as an array), the KES signature (raw bytes), the operational certificate (the KES public key, the issue number of the operational certificate, the KES period at the time of creation of the operational certificate and their cold signing key signature, encoded as an array) and the cold verification key (raw bytes) are appended to the message.

Before being diffused to other peers, an incoming message must be verified by the receiving node. This is done with the following steps:

- Verify that the operational certificate is valid by checking that the KES verification key is signed by cold secret key.
- Verify the KES signature of the message body with the KES verification key from the operational certificate.
- Compute the SPO pool id by hashing the cold verification key from the operational certificate. Make sure that this pool id is part of the stake distribution (the network layer will need to have access to this information).
- Verify that the announced id of the message is verified upon reception.

If any of these step fails, the message is considered as invalid, which is a protocol violation.

> [!WARNING]
> We also probably need to make sure that the KES key used to sign is from the latest rotation:
>
> - either the last seen opcert number in the block headers of the chain.
> - or the last seen opcert number from a previous message diffused.
> - or the last opcert number recorded in the Mithril signer registration.
>
> If the opcert number received is strictly lower than the previous one which has been seen, it should be considered as a protocol violation.

##### Cost of authentication

> [!NOTE]
> Computations are based on the assumption of a **2 ms** KES signature verification time on a virtual CPU, which may vary depending on the infrastructure.

For a total of **3,100** Cardano SPOs on the `mainnet`, on an average **50%** of them will be eligible to send signatures (i.e. will win at least one lottery in the Mithril protocol). This means that if the full Cardano stake distribution is involved in the Mithril protocol, only **1,550** signers will send signatures at each round:

- the number of messages received by a node which need to be verified is:

| Send period | Messages sent |
| ----------- | ------------- |
| 1 min       | 64 M/month    |
| 2 min       | 32 M/month    |
| 5 min       | 13 M/month    |
| 10 min      | 7 M/month     |

- the extra CPU time for the verification of messages based on the aforementioned volume of messages received is:

| Send period | CPU core usage |
| ----------- | -------------- |
| 1 min       | 5%             |
| 2 min       | 2.5%           |
| 5 min       | 1.0%           |
| 10 min      | 0.5%           |

#### Network load

##### Mithril extra network usage

> [!NOTE]
> The below computations of the network throughput and volume apply a multiplicative factor of **2** to the number of messages transmitted to reflect the redundancy of the diffusion mechanism.

> [!WARNING]
> Some compression can be applied to the Mithril signatures which allows them to always be on the lower bound size, but it is not implemented yet.

The following tables gather figures about expected network load in the case of **Mithril** using the mini-protocol to diffuse the individual signatures:

| Message part           | Lower bound | Upper bound |
| ---------------------- | ----------- | ----------- |
| messageId              | 32 B        | 32 B        |
| messageBody            | 360 B       | 2,000 B     |
| kesSignature           | 448 B       | 448 B       |
| kesPeriod              | 8 B         | 8 B         |
| operationalCertificate | 304 B       | 304 B       |
| coldVerificationKey    | 4 B         | 4 B         |
| expiresAt              | 4 B         | 4 B         |

| Message | Lower bound | Upper bound |
| ------- | ----------- | ----------- |
| total   | 1,160 B     | 2,800 B     |

For a total of **3,100** Cardano SPOs on the `mainnet`, on an average **50%** of them will be eligible to send signatures (i.e. will win at least one lottery in the Mithril protocol). This means that if the full Cardano stake distribution is involved in the Mithril protocol, only **1,550** signers will send signatures at each round:

- the network outbound throughput of a peer is:

| Send period | Lower bound | Upper bound |
| ----------- | ----------- | ----------- |
| 1 min       | 57 kB/s     | 138 kB/s    |
| 2 min       | 29 kB/s     | 69 kB/s     |
| 5 min       | 12 kB/s     | 28 kB/s     |
| 10 min      | 6 kB/s      | 14 kB/s     |

- the network outbound volume of a peer is:

| Send period | Lower bound  | Upper bound  |
| ----------- | ------------ | ------------ |
| 1 min       | 147 GB/month | 356 GB/month |
| 2 min       | 74 GB/month  | 178 GB/month |
| 5 min       | 30 GB/month  | 72 GB/month  |
| 10 min      | 15 GB/month  | 36 GB/month  |

#### Infrastructure extra operating costs

##### Networking traffic cost

> [!NOTE]
>
> - These data apply to cloud providers which bill the traffic on the volume, not the bandwidth.
> - Some cloud providers offer a free tier for the first **100GB** of traffic which is not taken into consideration here for simplicity.

| Cloud Provider | Inbound Traffic | Outbound Traffic |
| -------------- | --------------- | ---------------- |
| AWS            | 0 $/GB          | 0.09 $/GB        |
| GCP            | 0 $/GB          | 0.11 $/GB        |
| Azure          | 0 $/GB          | 0.09 $/GB        |
| Average        | 0 $/GB          | 0.1 $/GB         |

##### Mithril message diffusion extra networking cost

For a total of `3,000` SPOs sending messages, the extra networking cost incurred for a Cardano full node is:

| Send period | Lower bound | Upper bound |
| ----------- | ----------- | ----------- |
| 1 min       | 15 $/month  | 36 $/month  |
| 2 min       | 8 $/month   | 18 $/month  |
| 5 min       | 3 $/month   | 8 $/month   |
| 10 min      | 2 $/month   | 4 $/month   |

#### Possible attacks

##### Sybil attack

In this attack, a malicious sender would attempt to create multiple identities impersonating SPOs. This attack is completely mitigated by the [Message authentication mechanism](#Message-authentication-mechanism) as only active SPO on the Cardano chain can be authenticated and send messages. This would be considered as a protocol violation and the malicous peer would be disconnected.

##### Equivocation

In this attack, a malicious SPO would send different messages to different peers. This attack needs to be handled by the receiver of the message as the network layer does not verify the content of the message body by design.

In the specific case of Mithril, the individual signature is unique so there will be two cases:

- the message embeds a valid signature and it will be accepted by the receiving Mithril aggregator.
- the message embeds an invalid signature and it will be rejected by the receiving Mithril aggregator.

##### DoS attack

In this attack, a malicous SPO would try to flood the network by sending many messages at once. In that case, the network layer could detect that the throughput of messages originating from a SPO is above a threshold and consider it as a protocol violation, thus disconnecting the malicous peer. If a peer asks for N messages and receives more than N messages, then it would also be considered as a protocol violation. Also, the way mini-protocols are implemented allows to set a maximum message size.

#### Network node handshaking

A standalone network node will use its own `handshake`. It can introduce its own protocol parameters, but quite likely it will start with `NodeToNodeVersionData`:

```hs
data NodeToNodeVersionData = NodeToNodeVersionData
  { networkMagic  :: !NetworkMagic
  , diffusionMode :: !DiffusionMode
  , peerSharing   :: !PeerSharing
  , query         :: !Bool
  }
  deriving (Show, Typeable, Eq)
```

- `networkMagic`: this is used for debugging purpose and to make sure the network node runs on the right network (it should be different than the existing `networkMagic`s).
- `diffusionMode`: this would be useful if there are initiator-only nodes, e.g. a network node running next to an edge node (a wallet).
- `peerSharing`: this will be useful to implement peer sharing in the side network if this is needed.
- `query`: this is useful for tools like `cardano-cli ping`.

### Local Message Submission mini-protocol

#### Description

The local message submission mini-protocol is used by local clients to submit message to a local network node. This mini-protocol is **not** used to diffuse messages from a network node to another.

The protocol follows a simple request-response pattern:

1. The client sends a request with a single message.
2. The server either accepts the message (and returns a confirmation) or rejects it (and returns the reason)

#### State machine

| Agency            |                |
| ----------------- | -------------- |
| Client has Agency | StIdle         |
| Server has Agency | StBusy, StDone |

```mermaid
stateDiagram-v2

  classDef White fill:white,stroke:white
  classDef Black fill:white,stroke:black
  classDef Blue fill:white,stroke:blue
  classDef Green fill:white,stroke:green

  start:::White --> StIdle:::Blue;
  StIdle:::Blue --> StBusy:::Green : MsgSubmitMessage
  StBusy:::Green --> StIdle:::Blue : MsgAcceptMessage
  StBusy:::Green --> StIdle:::Blue : MsgRejectMessage
  StIdle:::Blue --> StDone:::Black : MsgDone

```

##### Protocol messages

- **MsgSubmitMessage(message)**: The client submits a message.
- **MsgAcceptMessage**: The server accepts the message.
- **MsgRejectMessage(reason)**: The server rejects the message and replies with a _reason_.
- **MsgDone**: The client terminates the mini-protocol.

##### Transition table

| From state | Message          | Parameters | To State |
| ---------- | ---------------- | ---------- | -------- |
| StIdle     | MsgSubmitMessage | message    | StBusy   |
| StBusy     | MsgAcceptMessage |            | StIdle   |
| StBusy     | MsgRejectMessage | reason     | StIdle   |
| StIdle     | MsgDone          |            | StDone   |

##### CDDL Encoding Specification

```cddl
localMessageSubmissionMessage
  = msgSubmitMessage
  / msgAcceptMessage
  / msgRejectMessage
  / msgDone

msgSubmitMessage = [0, message]
msgAcceptMessage = [1]
msgRejectMessage = [2, reason]
msgDone          = [3]

reason = invalid
       / alreadyReceived
       / expired
       / other

invalid         = [0, tstr]
alreadyReceived = [1]
expired         = [2]
other           = [3, tstr]

messageId    = bstr
messageBody  = bstr
kesSignature = bstr
kesPeriod    = word64
operationalCertificate = [ bstr .size 32, word64, word64, bstr .size 64 ]
coldVerificationKey = bstr .size 32
expiresAt = word32

messagePayload = [
  messageId
  , messageBody
  , kesPeriod
  , expiresAt
]
message = [
  messagePayload
  , kesSignature
  , operationalCertificate
  , coldVerificationKey
]
```

### Local Message Notification mini-protocol

#### Description

The local message notification mini-protocol is used by local clients to be notified about new message received by the network node.

The protocol follows a simple request-response pattern:

1. The client sends a request with a single message.

2. The server either has messages that it can provide, returning the list of all available messages; or doesn't have any, in which case it returns a suitable response saying exactly that.

#### State machine

| Agency            |                                          |
| ----------------- | ---------------------------------------- |
| Client has Agency | StIdle                                   |
| Server has Agency | StBusyNonBlocking,StBusyBlocking, StDone |

```mermaid
stateDiagram-v2

  classDef White fill:white,stroke:white
  classDef Black fill:white,stroke:black
  classDef Blue fill:white,stroke:blue
  classDef Green fill:white,stroke:green

  start:::White --> StIdle:::Blue;
  StIdle:::Blue --> StBusyNonBlocking:::Green : MsgRequestMessagesNonBlocking
  StBusyNonBlocking:::Green --> StIdle:::Blue : MsgReplyMessagesNonBlocking
  StIdle:::Blue --> StBusyBlocking:::Green : MsgRequestMessagesBlocking
  StBusyBlocking:::Green --> StIdle:::Blue : MsgReplyMessagesBlocking
  StIdle:::Blue --> StDone:::Black : MsgClientDone

```

##### Protocol messages

- **MsgRequestMessagesNonBlocking**: The client asks for available messages and acknowledges old message ids. The server side immediately replies (possible without available message).
- **MsgReplyMessagesNonBlocking([message], hasMore)**: The server has received new messages and indicates if further message are available. In the non-blocking case, the reply may contain an empty list.
- **MsgRequestMessagesBlocking**: The client asks for available messages and acknowledges old message ids. The server will only reply once there are available messages.
- **MsgReplyMessagesBlocking([message])**: The server has received new messages and indicates if further message are available. In the blocking case, the reply is guaranteed to contain at least one message.
- **MsgClientDone**: The client terminates the mini-protocol.

#### Transition table

| From state        | Message                       | Parameters           | To State          |
| ----------------- | ----------------------------- | -------------------- | ----------------- |
| StIdle            | MsgRequestMessagesNonBlocking |                      | StBusyNonBlocking |
| StBusyNonBlocking | MsgReplyMessagesNonBlocking   | ([message], hasMore) | StIdle            |
| StIdle            | MsgRequestMessagesBlocking    |                      | StBusyBlocking    |
| StBusyBlocking    | MsgReplyMessagesBlocking      | [message]            | StIdle            |
| StIdle            | MsgClientDone                 |                      | StDone            |

##### CDDL Encoding Specification

```cddl
localMessageNotificationMessage
  =
  ; corresponds to either MsgRequestMessagesBlocking or
  ; MsgRequestMessagesNonBlocking in the spec
    msgRequestMessages
  / MsgReplyMessagesNonBlocking
  / msgReplyMessagesBlocking
  / msgClientDone

msgRequestMessages          = [0, isBlocking]
msgReplyMessagesNonBlocking = [1, [* message], hasMore]
msgReplyMessagesBlocking    = [2, [+ message]]
msgClientDone               = [3]

messageId    = bstr
messageBody  = bstr
kesSignature = bstr
kesPeriod    = word64
operationalCertificate = [ bstr .size 32, word64, word64, bstr .size 64 ]
coldVerificationKey = bstr .size 32
expiresAt = word32

messagePayload = [
  messageId
  , messageBody
  , kesPeriod
  , expiresAt
]
message = [
  messagePayload
  , kesSignature
  , operationalCertificate
  , coldVerificationKey
]

hasMore = false / true
isBlocking = false / true
```

## Rationale: how does this CIP achieve its goals?

### Why are we proposing this CIP?

#### For Mithril

- Mithril requires strong network foundations to support interactions between its various nodes:

- Mithril needs to exist in a decentralized context where multiple aggregators can operate seamlessly and independently.
- Mithril needs participation of all or nearly all of the Cardano network SPOs to provide maximal security to the multi-signatures embedded in the certificates.
- Creating a separate network would entail significant costs and efforts (there are more than 3,000 SPOs which would need to be connected with resilient and secure network, and much more passive nodes).
- Cardano SPOs need to be vigilant about what other applications run on their block producers and relay nodes. While a separate p2p network could be created, incoming connections must be treated carefully and all the same DoS considerations as with Cardano would need to apply. By standardizing the message diffusion of Mithril in the same way as the Cardano protocol stack, the additional risk of operating Mithril is greatly reduced.
- The Cardano network is very efficient for diffusion (e.g. broadcasting) which is precisely what is needed for Mithril.
- Mithril signer node needs to run on the same machine as the Cardano block producing node (to access the KES keys). It makes sense to use the same network layer, which will also facilitate a high level of participation.

#### For Cardano

- Why would it be great for Cardano to support a decentralized message queue with its network?

- This is a required feature to make the Cardano ecosystem scalable.
- The design is versatile enough to support present and future use cases.

### Information Diffusion Architecture

In this section, we propose an architecture for `Cardano` and `Mithril`. Note,
in this section, `Mithril` is used as a placeholder for a possible `Mithril`
application: we've seen many ideas about how `Mithril` can be used in `Cardano`
and all of them can follow this design.

Any software included in `cardano-node` needs to undergo a rigorous development
and review process to avoid catastrophic events. We think that merging
`Cardano`, a mature software, with new technologies, should be a process, and
thus we propose first to develop `Decentralized Message Queue (DMQ)` nodes as standalone processes which
communicate with `cardano-node` via its local interface (`node-to-client`
protocol). As we will see, this approach has advantages for the `Mithril`
network and SPOs.

`Ouroboros-Network` (ON) is, to a large extent, an agnostic network stack,
which can be adapted to be used by both `Cardano` and `DMQ` nodes. To construct
an overlay network, the stack needs to access stake pool information registered
on chain. This can be done via the `node-to-client` protocol over a Unix
socket. Since `DMQ` nodes will have their own end-points, we'll either need
to propose a modification to the SPO registration process, which includes
`Mithril` instantiated `DMQ` nodes, or we could pass incoming `Mithril` connections from
`cardano-node` to the `DMQ` node via
[CMSG_DATA](https://man.openbsd.org/CMSG_DATA.3).

`Ouroboros-Network` outbound governor component, which is responsible for
creating the overlay network has a built-in mechanism which churns connections
based on a defined metric. By developing a standalone `Mithril network` node, we can
design metrics specific for the purpose. This way, the `Mithril` network will
optimise for its benefits rather than being stuck in a suboptimal solution from
its perspective - if `Mithril` and `Cardano` were tied more strongly (e.g. as
part `cardano-node`), then we wouldn't have that opportunity because `Cardano`
must meet its deadlines, or the security of the system as a whole must be at
stake.

Eventually, there will be many `Mithril` applications, and all of them might
have different network requirements and thus might require slightly different
configurations. We SHOULD aim to build something that can be used as
a scaffolding for such applications. This may also open future avenues for
delivering new functionalities that fit together well with existing
infrastructure while still mitigating the risk of catastrophic events at the
core of the system.

Please also note that any protocols and their instances that will be built as
part of the standalone `DMQ` node could be reused in future for `Peras` and
`Leios`. It will give us even more confidence that the future core system will
be built from components that are used in production.

`Cardano` and `DMQ` nodes, as separate executables, can still be packaged
together, lowering the barrier of participation. When run separately, the SPO
is also in better control of the resources dedicated to each process - this is
important for the health of both systems.

Another benefit of such a design is that `DMQ` node can be developed on its own
pace, not affected by significant changes in other parts of the system like
ledger - the Cardano Core team restrains itself to not publishing new `cardano-node`
versions with significant changes across many of its subsystems - just for the
sake of clarity of where to look if a bug is found.

In this design, a `DMQ` node runs alongside a `cardano-node`, which is connected to
it via a UNIX socket (`node-to-client`) protocol. This means an SPO will run
a `DMQ` node instantiated for `Mithril` per `cardano-node`. Future protocols will run
their custom instantiated new `DMQ` node instance. The `DMQ` node connected to BP can
also have access to necessary keys for signing purposes. `DMQ` node might also
use the KES agent, as `cardano-node` will in the near future, to securely access
the KES key.

## Path to Active

### Acceptance Criteria

1. A Cardano node implementing the previously described mini-protocols is released for production.
1. A message producer node (e.g. a Mithril signer) publishing messages to the local DMQ node through mini-protocols is released.
1. A message subscriber node (e.g. a Mithril aggregator) receiving messages from the local DMQ node is released.

### Implementation Plan

> [!WARNING]
> A hard-fork of the Cardano chain may be required if some information, like peer ports for an overlay network, are to be registered by the SPOs on-chain.

- [x] Write a "formal" specification of the protocols along with vectors/conformance checker for protocol's structure and state machine logic.
- [x] Write an architecture document extending this CIP with more technical details about the implementation.
- See [here](<https://github.com/IntersectMBO/ouroboros-network/wiki/Decentralized-Message-Queue-(DMQ)-Implementation-Overview>)
- [x] Validate protocol behaviour with all relevant parties (Network and Node teams).
- [x] Make the current Cardano Network Diffusion Layer general and reusable so a new, separate Mithril Diffusion Layer can be instantiated.
- See [here](https://github.com/IntersectMBO/ouroboros-network/wiki/Reusable-Diffusion-Investigation) and [here](https://github.com/IntersectMBO/ouroboros-network/pull/5016)
- [x] Implement DMQ Node that is able to run general diffusion (i.e. without the mini-protocols).
- See [here](https://github.com/IntersectMBO/ouroboros-network/pull/5109)
- [x] Implement the n2n and n2c mini-protocols:
- [x] Haskell DMQ Node:
- [x] n2c mini-protocols
- [x] n2n mini-protocols
- [x] Pallas Library (TxPipe):
- [x] n2c mini-protocols
- [x] ~~n2n mini-protocols~~ (will be done in a separate stream of work)
- [x] Implement the n2c mini-protocols in Mithril nodes:
- [x] Mithril signer
- [x] Mithril aggregator

## References

### Cardano network

- **The Shelley Networking Protocol**: https://ouroboros-network.cardano.intersectmbo.org/pdfs/network-spec/network-spec.pdf
- **Introduction to the design of the Data Diffusion and
  Networking for Cardano Shelley**: https://ouroboros-network.cardano.intersectmbo.org/pdfs/network-design/network-design.pdf

### Mithril

- **Mithril: Stake-based Threshold Multisignatures**: https://iohk.io/en/research/library/papers/mithril-stake-based-threshold-multisignatures/
- **Mithril Network Architecture**: https://mithril.network/doc/mithril/mithril-network/architecture
- **Mithril Protocol in depth**: https://mithril.network/doc/mithril/mithril-protocol/protocol
- **Mithril Certificate Chain in depth**: https://mithril.network/doc/mithril/mithril-protocol/certificates
- **Fast Bootstrap a Cardano node**: https://mithril.network/doc/manual/getting-started/bootstrap-cardano-node
- **Run a Mithril Signer node (SPO)**: https://mithril.network/doc/manual/getting-started/run-signer-node/
- **Mithril Threat Model**: https://mithril.network/doc/mithril/threat-model

## Copyright

This CIP is licensed under [Apache-2.0](http://www.apache.org/licenses/LICENSE-2.0)

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0138/README.md
---

- --
CIP: 138
Title: Plutus Core Builtin Type - `Array`
Category: Plutus
Status: Proposed
Authors:
- Michael Peyton Jones <michael.peyton-jones@iohk.io>
- Ana Pantilie <ana.pantilie@iohk.io>
Implementors: []
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/915
Created: 2024-09-18
License: CC-BY-4.0
- --

## Abstract

We propose an array builtin type for Plutus Core. This type will have constant-time lookup,
which is a useful feature that is otherwise not possible to achieve.

## Motivation: why is this CIP necessary?

The first part of [CPS-0013][1] outlines in great detail the motivation for introducing this
new builtin type.

To summarize, it is currently not possible to write a data structure with constant-time
lookup in Plutus Core. We propose to solve this problem by introducing an array type into
Plutus Core's builtin language, as it is the standard example of a data structure
with this property, and it is a key component of many classical algorithms and data
structures.

Access to an array type would provide significant performance improvement opportunities to
users of Plutus Core, since currently they must rely on suboptimal data structures such as
lists for looking up elements inside a collection.

## Specification

We add the following builtin type: `Array` of kind `Type -> Type` representing
a one-dimensional array type with indices of type `Integer`. Elements are indexed
consecutively starting from `0`.

- **Note***: Here `Type` is the universe of all _builtin_ types, since we do not consider
types formed out of applying builtin types to arbitrary types to be inhabited.

The `Array` builtin type should be implemented with a fixed-size immutable array structure
that has constant-time lookup.

We add the following builtin functions:

1. `indexArray` of type `forall a . Integer -> Array a -> a`.
- It returns the element at the given index in the array, or fails with an error if the
    index is outside the bounds of the array.
- It uses constant time and constant memory.
2. `lengthOfArray` of type `forall a . Array a -> Integer`
- It returns the length of the array.
- It uses constant time and constant memory.
3. `listToArray` of type `forall a . List a -> Array a`
- It converts the argument builtin list into a builtin array.
- It uses linear time and linear memory.

### Binary serialisation and deserialisation

As with all Plutus Core builtin types, arrays must have a fixed binary representation.

For arrays, this representation will be based on the one currently implemented in the [flat][8] encoding
for the [Haskell `List` type][9].  Plutus Core arrays must be converted to lists before being serialised,
and deserialisation is performed by using the flat decoder for lists and converting the result to an array.

## Rationale: how does this CIP achieve its goals?

### Choice of builtin functions and their specification

The following section presents the reasoning behind the above specification of a Plutus
core builtin array type.

It is important to mention that we based our decisions on the desire to keep the builtin
language as small as possible, i.e. to not introduce types or functions
which are not essential for definitional purposes or essential for providing a practical
interface to users.

It also discusses some alternatives or additions which should be considered as part of the
[preliminary investigation](#implementation-plan).

### Providing safe lookups

The `indexArray` builtin function is necessary, since access to constant-time lookup is the main
requirement outlined in this CIP. However, there remains the question of how users should
deal with the function's partiality.
We considered the following options:

1. Introduce another new builtin type, one which implements `Maybe` semantics. The type
signature for `indexArray` would become `forall a . Integer -> Array a -> Maybe a` and it
would return `Nothing` for out-of-bounds lookups.
- The obvious disadvantage is the necessity of adding another new builtin type (there is
    no `Maybe` builtin type in Plutus Core), which would further increase the complexity of
    the builtin language.
- Another disadvantage would be that this solution is the most costly: users will incur
    additional costs in deconstructing the returned `Maybe`.

2. Failed lookups return a default value provided by the caller:
`indexArray :: forall a . Integer -> a -> Array a -> a`.
- This solution is problematic whenever there is no sensible default value and
    the user wants the function to fail. Since Plutus Core is strict, it is not possible
    to pass `error` as the default value without it getting evaluated before the call and
    terminating execution immediately.
- As of the time of writing, builtin functions cannot be higher-order. However, that is
    subject to change in the near future when pattern matching builtins will be supported by
    Plutus Core. This feature would allow a safe version of `indexArray` with type `forall a .
    Integer -> (() -> a) -> Array a -> a` to be expressible in the builtins language.

3. Include `lengthOfArray` and require the user to perform appropriate length guards before
calling `indexArray`.
- This is a familiar option from other languages.
- By definition arrays are fixed-size and their length is available in constant time.
- It also allows users to omit bounds checks when they know a priori that the
    index is in bounds.

After considering these three options, we concluded that the addition of
the `lengthOfArray` builtin function is both necessary and sufficient for introducing a
well-defined array type into the builtin language.

### Constructing arrays

The last required functionality for having a practical interface is the ability to
construct arrays.

Due to our decision of providing immutable arrays, it is difficult (or very expensive) to build up arrays incrementally.
A naive approach would require repeated copying and potentially the usage of quadratic
space and time.

A more appropriate approach would be to construct the array in bulk,
however that would require an intermediate representation of a collection of elements. Fortunately this already exists in the builtin language in the form of builtin lists.
We can then naturally introduce a function which transforms lists into arrays: `listToArray`.


### Slices

Many array-like data structures support cheap _slices_, e.g. a function with the following
type signature: `slice :: Integer -> Integer -> Array a -> Array a`, which produces a _view_
of the original array between the two indices (similarly, the same can be achieved using an
`indexArray` and a `lengthOfArray`).

We do not propose to add `slice` to our builtin set, since it is very easy to build a data
structure that supports slices on top of `array`, simply by tracking some additional
integers to track the subset of the array that is in view.

### Arrays in `Data`

In [CPS-0013][1] the idea of representing the arguments to the `Data` constructor
`Constr` as an `Array` in Plutus Core was presented as being more appropriate than
the current list representation.

A significant requirement in implementing this modification is maintaining backwards
compatibility. Therefore, we cannot simply modify the internal representation of `Data`.

One idea would be to add a new builtin such as the following:
`unConstrDataArray :: Data -> (Integer, Array Data)`. However, this builtin will
inevitably have linear time complexity since it is based on a list traversal. So
it does not actually solve the original problem, unless it can be shown experimentally
that, in practice, these lists are usually small enough for the transformation to be negligible.

## Path to Active

### Acceptance Criteria

- [ ] The feature is implemented according to the implementation plan and merged into
the master branch of the [plutus repository][5].
- [ ] [cardano-ledger][6] is updated to include new protocol parameters to control costing of
the new builtins.
- [ ] The feature is integrated into [cardano-node][7] and released as part of a hard fork.

### Implementation Plan

The implementation of this CIP should not proceed without an empirical assessment of the effectiveness of the new primitives, as per the following plan:

1. Implement the new primitives according to the specification, including the experimental
versions discussed in the CIP.
2. Assign a preliminary cost to the new builtin functions. Consider similar operations and their
current costs.
3. Create variants of the [existing benchmarks][4] and potentially add some more.
4. From the total set of newly implemented builtins, find a minimal but practical set of
primitives which are indeed significantly faster in both real-time performance and modelled costs.
5. If such a set does not exist, find out why. This means that the preliminary investigation
was not successful. If it does, revise the specification to include
the final set of primitives.

If the preliminary performance investigation was not successful, this CIP should be revised
according to the findings of the experiment. Otherwise, the implementation can proceed:

6. Determine the most appropriate costing functions for modelling the builtin's performance
and assign costs accordingly.
6. Add the new builtin type and functions to the appropriate sections in the [Plutus Core
Specification][2].
7. Formalize the new builtin type and functions in the [plutus-metatheory][3].
8. The final version of the feature is ready to be merged into [plutus][5] and accepted by
the Plutus Core team.

## Copyright
This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

[1]: https://cips.cardano.org/cps/CPS-0013 "CPS-0013"
[2]: https://plutus.cardano.intersectmbo.org/resources/plutus-core-spec.pdf "Formal Specification of the Plutus Core Language"
[3]: https://github.com/IntersectMBO/plutus/tree/master/plutus-metatheory "plutus-metatheory"
[4]: https://github.com/IntersectMBO/plutus/tree/master/plutus-benchmark/list "List benchmarks"
[5]: https://github.com/IntersectMBO/plutus "plutus"
[6]: https://github.com/IntersectMBO/cardano-ledger "cardano-ledger"
[7]: https://github.com/IntersectMBO/cardano-node "cardano-node"
[8]: https://hackage.haskell.org/package/flat "flat"
[9]: https://hackage.haskell.org/package/base-4.21.0.0/docs/Data-List.html "Haskell List"

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0139/README.md
---

- --
CIP: 139
Title: Universal Query Layer
Category: Tools
Status: Proposed
Authors:
- Vladimir Kalnitsky <klntsky@gmail.com>
- Giovanni Garufi <giovanni@mlabs.city>
Implementors: []
Discussions:
- https://discord.gg/MU8vHAgmGy
- https://github.com/cardano-foundation/CIPs/pull/869
Solution-To: CPS-0012
Created: 2024-05-14
License: CC-BY-4.0
- --

## Abstract

A transport-agnostic query layer specification for use in dApps and wallets.

## Motivation: why is this CIP necessary?

See [CPS-12 | Query Layer Standardization](https://github.com/cardano-foundation/CIPs/blob/master/CPS-0012/README.md) for motivation.

## Specification

The goal of this proposal is to define a standard, JSON-based, transport-agnostic query layer for wallets to implement which covers enough functionalities to be useful to a wide set of dApps.

We will start by discussing existing query layer designs for dApps, so we can properly define the use case for this CIP.
Next, we will define the query layer API.
We attempt to define the API in a transport-agnostic way: the request and return types for each endpoint are as defined in CIP-0116, with a few notable exception where the corresponding CDDL type lacked some useful information (more on this can be found in the `Query Layer API` section). Finally, we end this section with some notes on rollbacks and pagination.

#### Existing Query Layer designs

There are two approaches to Cardano dApp development:

1. **Using customized chain followers**. A chain follower is a program that interacts with cardano-node and processes all incoming transactions, as well as rollbacks, to maintain consistent dApp-specific state. Example: [Carp](https://dcspark.github.io/carp/docs/intro/).

2. **Using general-purpose query layers**. General-purpose query layers allow to query blockchain data using a wide set of APIs that are not built with a particular dApp domain in mind. dApp state has to be constructed based on data returned from the queries. Examples: Blockfrost, Maestro.

The first approach allows for lower runtime resource consumption, but a general-purpose query layer has an advantage of being more easily reusable between dApps.

In this proposal, we are focusing on general-purpose querying only.

### Query Layer API

This section contains descriptions for methods & their parameter lists.

The scope of this section is loosely based on a [comparison table for existing Cardano query layers](./Query_Layer_API_Comparison.md).
The goal is to make it so that the API could be implemented via simple adapters that transform requests and responses to the appropriate formats.

The payload formats used below are either references to [CIP-0116 - Standard JSON encoding for Domain Types](https://cips.cardano.org/cip/CIP-0116), which specifies cardano domain types via a JSON schema, or references to the [Query Layer JSON schema](./query-layer.json) which we defined in this CIP to define some types that are not present in the CDDL spec.

[View the list of endpoints here](./endpoints.md)

### Transports

The API can be implemented across several transports. The goal is to allow several different clients, possibly written in different languages, to interact with wallets.
For this reason we provide an [Openapi schema](./open-api.json), a [JSON-RPC](./json-rpc.json) schema, and an interface for an [injected Javascript](./ts-api.md) object in Typescript.
We also generate a [specification](./cip-0144.md) of this API that is compatible with [CIP-0144 | Wallet Connector API]. This defines the API as a wallet extension that can be enabled by the user, enabling a full-data wallet
We generate these interfaces from a high level specification of the endpoints [source](https://github.com/mlabs-haskell/query-layer-impl), ensuring that the information is consistent and easily updatable for different choices of transport layer.

### Pagination

In CIP-30, pagination is not reliable: because there is no guarantee that the set of UTxOs does not change between calls. On the other hand, there are good reasons to want to paginate responses, especially when designing an universal query layer. There are, generally, no bounds on the number of results that will be returned by many of the queries we want the API to cover (e.g. there is no way to control how many UTxOs might ever be at a given script address).
Even if we remove pagination from this API, we still have the issue that the underlying provider being used to fetch the data, could be using pagination itself. While this is somewhat of an "implementation detail", it can still lead to issues for end-users interacting with this API.
In this CIP, we have decided to remove pagination from the API. While very useful to have, it introduces potential issues about consistency of the results that affect both dApp developers and end users.
We hope to revisit this topic in a future CIP, to come up with a solution that does not force us to pick between consistency and efficiency.

### Handling of rollbacks

Transaction rollbacks are essential to blockchains: local node's view of the chain may be different from other nodes'. During conflict resolution, the node may issue a rollback event, that should be handled by dApps.

Customized chain followers, at least in principle, allow for "live" rollback handling: that is, a user-facing dApp can subscribe to a local view of a part of the UTxO set.

General purpose query layers can also handle rollbacks just fine, but they don't propagate rollback events to dApps, because they do not possess any dApp-specific info to determine if a dApp *needs* to handle a particular rollback. dApps that work with general-purpose query layers follow pull-based architecture, rather than event subscription-based, which means they just request data as needed, instead of reacting to blockchain events.

In the context of this API, rollbacks should be acknowledged as a source of potential inconsistency between data pieces returned by different queries.

#### Error handling

Errors should be divided in two categories:

- domain errors
- transport errors (404, 500, etc)

Here we will only specify the domain errors. Users should also handle transport specific errors that can occur when interacting with the API.

##### Error Types

###### APIError

```
APIErrorCode {
	InvalidRequest: -1,
	InternalError: -2,
	Refused: -3,
	AccountChange: -4,
}
APIError {
	code: APIErrorCode,
	info: string
}
```

- InvalidRequest - Inputs do not conform to this spec or are otherwise invalid.
- InternalError - An error occurred during execution of this API call.
- Refused - The request was refused due to lack of access - e.g. wallet disconnects.
- AccountChange - The account has changed. The dApp should call wallet.enable() to reestablish connection to the new account. The wallet should not ask for confirmation as the user was the one who initiated the account change in the first place.

Note that the error codes and their meaning are copied from [CIP-30](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0030#apierror). The reason is that this API will most likely live "alongside" the CIP-30 API, so unifying the error types reduces burden on the users.

### Versioning

The API has an endpoint that must return the current version of the API implemented. While the CIP is in preparation, the version shall be set to 0.0.0. The moment this CIP is merged the version should be set to 1.0.0, and all implementations should return that as the current version. Any changes to the API should come in form of PRs to this CIP. Every change must update the version in accordance to SemVer.

## Rationale: how does this CIP achieve its goals?

This CIP originates from the work layed out in the wallet working group, and specifically to address [CPS-012](https://github.com/cardano-foundation/CIPs/blob/master/CPS-0012/README.md)

This CPS initiative originated in the discussion about [Extensive Wallet Standard CIP](https://github.com/cardano-foundation/CIPs/pull/620) on the CIP Discord server ([invite](https://discord.gg/P59aNVN8zu))
in the [`#general`](https://discord.com/channels/971785110770831360/992011119872970762/1176567729017327737) channel, continuing in a dedicated [`#query-layer-standard`](https://discord.com/channels/971785110770831360/1178763938389823598) channel.

This CIP attempts to solve the issues raised and discussed in these previous discussions and CPSs. At every step we have tried to get feedback for this CIP from the community, this includes: dApp and wallet developers, query layer providers, end users, etc.

We have attempted to define a minimal API to cover many use cases, but we expect this to evolve over time: either to fill in some gap we missed, or simply to keep up with the continuing evolution of Cardano itself. We encourage future authors to follow the versioning scheme defined above.

## Path to Active

### Acceptance Criteria

- [ ] There are at least two protocol adapter for any of the existing query layers that implements this spec, that can be run.
- [ ] There are at least two offchain library that implements a provider interface for this CIP, effectively making it usable with the protocol adapter in production.

### Implementation Plan

- [ ] Build at least one protocol adapter for any of the existing query layers that implements this spec
- [ ] Build at least one offchain library integration

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0140/README.md
---

- --
CIP: 140
Title: Ouroboros Peras - Faster Settlement
Category: Consensus
Status: Proposed
Authors:
- Arnaud Bailly <arnaud.bailly@iohk.io>
- Brian W. Bush <brian.bush@iohk.io>
- Sandro Coretti-Drayton <sandro.coretti@iohk.io>
- Yves Hauser <yves.hauser@iohk.io>
- Hans Lahe <hans.lahe@iohk.io>
Implementors:
- Intersect
Solution-To:
- CPS-0017
- CPS-0021
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/872
Created: 2024-08-15
License: Apache-2.0
- --


## Abstract

We propose Ouroboros Peras, an enhancement to the Ouroboros Praos protocol that introduces a voting layer for fast settlement. It is adaptively secure, supports dynamic participation, and integrates self healing. Voting provides a “boost” to blocks that receive a quorum of votes, and this dramatically reduces the roll-back probability of the boosted block and its predecessors. Fast settlement occurs in the presence of adversaries with up to one-quarter of the stake, but Praos-like safety is maintained when adversaries control more than that amount of stake. In fact, the protocol enters a “cool-down period” of Praos-like behavior when adversaries prevent voting quorums; that cool-down period is exited only when the chain has healed, achieves chain quality, and reaches a common prefix. For realistic settings of the Peras protocol parameters, blocks can be identified (with overwhelming probability) *ex post facto* as being settled versus rolled-back after as little as two minutes. This enables use cases like partner-chains and bridges where high certainty for the status of a transaction is required in a brief time. The protocol requires the implementation of a vote-diffusion layer, certificates that aggregate votes, and one minor addition to the contents of a block.

<details>
  <summary><h2>Table of contents</h2></summary>

- [Motivation: why is this CIP necessary](#motivation-why-is-this-cip-necessary)
- [Specification](#specification)
- [Non-normative overview of the Peras protocol](#non-normative-overview-of-the-peras-protocol)
- [Normative Peras specification in Agda](#normative-peras-specification-in-agda)
- [Protocol parameters](#protocol-parameters)
- [Network representation](#network-representation)
- [Slots and rounds](#slots-and-rounds)
- [Hashing](#hashing)
- [Parties](#parties)
- [Signatures](#signatures)
- [Slot leadership and committee membership](#slot-leadership-and-committee-membership)
- [Votes](#votes)
- [Certificates](#certificates)
- [Block bodies](#block-bodies)
- [Blocks](#blocks)
- [Chains](#chains)
- [Messages and their envelopes](#messages-and-their-envelopes)
- [Block trees](#block-trees)
- [Parameterization of the semantics](#parameterization-of-the-semantics)
- [Block-tree update](#block-tree-update)
- [Block selection](#block-selection)
- [Rules for voting in a round](#rules-for-voting-in-a-round)
- [State](#state)
- [Progress](#progress)
- [Advancing the clock](#advancing-the-clock)
- [Updating the global state](#updating-the-global-state)
- [Fetching](#fetching)
- [Voting](#voting)
- [Block creation](#block-creation)
- [Small-step semantics](#small-step-semantics)
- [Constraints on Peras parameters](#constraints-on-peras-parameters)
- [Specification of votes and certificates](#specification-of-votes-and-certificates)
- [CDDL schema for the ledger](#cddl-schema-for-the-ledger)
- [Rationale: how does this CIP achieve its goals?](#rationale-how-does-this-cip-achieve-its-goals)
- [How Peras settles faster](#how-peras-settles-faster)
- [Evidence for fast settlement](#evidence-for-fast-settlement)
- [Why Peras is practical to implement](#why-peras-is-practical-to-implement)
- [Use cases](#use-cases)
- [Feasible values for Peras protocol parameters](#feasible-values-for-peras-protocol-parameters)
- [Attack and mitigation](#feasible-values-for-peras-protocol-parameters)
- [Resource requirements](#resource-requirements)
- [Network](#network)
- [Cost](#cost)
- [Persistent storage](#persistent-storage)
- [CPU](#cpu)
- [Memory](memory)
- [Path to active](#path-to-active)
- [Acceptance criteria](#acceptance-criteria)
- [Implementation plan](#implementation-plan)
- [Versioning](#versioning)
- [References](#references)
- [Appendix](#appendix)
- [Typechecking this specification](#typechecking-this-specification)
- [Copyright](#copyright)

</details>

## Motivation: why is this CIP necessary?

Under Ouroboros Praos, settlement occurs probabilistically on the Cardano blockchain, where the probability that a block will be rolled back from the preferred chain decreases exponentially as the chain grows beyond the block and where that rate of decrease is slower when adversarial activity is stronger[^1]. Some use cases require high assurance that a block (and the transactions within it) will not be rolled back, necessitating a potentially lengthy wait before a transaction is considered "settled" or "finalize". Some major centralized exchanges, for example, require fifteen confirmations (i.e., blocks) before a transaction is considered settled[^2]: this amounts to waiting ten minutes before a consumer has their transacted funds or tokens available for subsequent use. This situation is not unique to Cardano: centralized exchanges generally require at least five minutes wait for most of the common blockchains[^2]. Partner chains and bridges may have stringent requirements for fast and highly certain settlement.

There are several definitions of "settlement" or "finality", and precision is important when discussing these. Two noteworthy scenarios can be defined precisely.

- *Ex ante* settlement probability: "What is the probability that a transaction that I just submitted will ever be rolled back?"
- *Ex post facto* settlement probability: "Given that I submitted my transaction $x$ seconds ago and it has not yet been rolled back, what is the probability that it will ever be rolled back?"

If one is unwilling or unable to re-submit a rolled-back transaction, then the *ex ante* probability might be of most interest. This matches use cases where there is no opportunities for the parties involved in a transaction to resubmit it: for example, one party might have purchased physical goods and left the vendor's premises, leaving no chance to resubmit a rolled-back transaction. Other use cases are better suited for *ex post facto* settlement: for example, a partner chain, bridge, or decentralized exchange can monitor the ledger for a fixed time and see that the transaction either is not or is rolled back, knowing that there is only a vanishingly small chance of that status ever changing once they have watched the chain for the fixed amount of time, giving them an opportunity to re-submit the transaction if it was rolled back. Both opportunity and back-end infrastructure distinguish these use cases. Protocols like Peras optimize *ex post facto* certainty after predefined waiting times.

Ouroboros Praos optimizes the worst-case scenario of highly adversarial conditions, but the Cardano blockchain typically operates in the absence of such a challenge. Optimistic protocols like Peras can take advantage of the "average case" of lower or no adversarial activity by settling transactions faster than Praos, but without sacrificing any of the security guarantees of Praos if the protocol (such as Peras) falls back to Praos-like behavior for a "safety and repair period" after adversarial conditions occur. Furthermore, protocol modification like Peras should not require radical or costly changes to the current `cardano-node` implementations. It is also desirable that settlement-related protocol changes do not interfere with other pending protocol changes like Genesis (security enhancement)[^3] and Leios (maximal throughput)[^4]. Fast settlement is a critical part of Cardano scaling, as described in [*Scaling blockchain protocols: a research-based approach*](https://www.youtube.com/watch?v=Czmg9WmSCcI).

This CIP responds to the Cardano Problem Statement ["Faster Settlement"](https://github.com/cardano-foundation/CIPs/pull/922).

[^1]: https://eprint.iacr.org/2022/1571.pdf

[^2]: https://support.kraken.com/hc/en-us/articles/203325283-Cryptocurrency-deposit-processing-times

[^3]: https://iohk.io/en/blog/posts/2024/05/08/ouroboros-genesis-design-update/

[^4]: https://iohk.io/en/research/library/papers/high-throughput-blockchain-consensus-under-realistic-network-assumptions/


## Specification


### Non-normative overview of the Peras protocol

The following informal, non-normative, pseudo-imperative summary of the Peras protocol is provided as an index into the [formal, normative specification in Agda](#normative-peras-specification-in-agda). Peras relies on a few key concepts:

- The progression of the blockchain's [*slots*](#slots-and-rounds) is partitioned in [*rounds*](#slots-and-rounds) of equal length.
- In each round a [*committee of voters*](#slot-leadership-and-committee-membership) is selected via a sortition algorithm.
- Members of the committee may [*vote*](#votes) for a block in the history of their preferred chain.
- A [*quorum*](#quorum) of votes during the same round for the same block is memorialized as a [*certificate*](#certificates).
- A quorum of votes for a block gives that block's weight a [*boost*](#boost).
- The [*weight*](#weight) of a [*chain*](#chains) is its length plus the total of the boosts its blocks have received.
- The lack of a quorum in a round typically triggers a *cool-down period* where no voting occurs.
- Relevant vote certificates are typically *recorded* in a [*block*](#blocks) near the start and finish of a cool-down period.
- Certificates [*expire*](#expiration) after a specified number of slots if they have not been included in a block.

The protocol keeps track of the following [variables](#block-trees), initialized to the values below:

- $C_\text{pref} \gets C_\text{genesis}$: preferred chain
- $\mathcal{C} \gets \{C_\text{genesis}\}$: set of chains
- $\mathcal{V} \gets \emptyset$: set of votes
- $\mathsf{Certs} \gets \{\mathsf{cert}_\text{genesis}\}$: set of certificates
- $\mathsf{cert}^\prime \gets \mathsf{cert}_\text{genesis}$: the latest certificate seen
- $\mathsf{cert}^* \gets \mathsf{cert}_\text{genesis}$: the latest certificate on chain

A [*fetching*](#fetching) operation occurs at the beginning of each slot:

- Fetch new chains $\mathcal{C}\_\text{new}$ and votes $\mathcal{V}\_\text{new}$.
- Add any new chains in $\mathcal{C}\_\text{new}$ to $\mathcal{C}$, add any new certificates contained in chains in $\mathcal{C}\_\text{new}$ to $\mathsf{Certs}$.
- Add $\mathcal{V}\_\text{new}$ to $\mathcal{V}$ and turn any new quorum in $\mathcal{V}$ into a certificate $\mathsf{cert}$ and add $\mathsf{cert}$ to $\mathsf{Certs}$.
- Ignore all but the first equivocating vote: i.e., do not add them to $\mathcal{V}$.

       This requires diffusing certificates in cases where different honest nodes received different equivocating votes.
- Set $C_\text{pref}$ to the heaviest (w.r.t. $\mathsf{Wt}\_\mathsf{P}(\cdot)$ ) valid chain in $\mathcal{C}$.
- Each party $\mathsf{P}$ assigns a certain weight to every chain $C$, based on $C$'s length and all certificates that vote for blocks in $C$ that $\mathsf{P}$ has seen so far (and thus stored in a local list $\mathsf{Certs}$).
- Let $\mathsf{certCount}\_\mathsf{P}(C)$ denote the number of such certificates: i.e., $\mathsf{certCount}\_\mathsf{P}(C) := \left| \left\\{ \mathsf{cert} \in \mathsf{Certs} : \mathsf{cert} \text{ votes for a block on } C \right\\} \right|$.
- Then the weight of the chain $C$ in $\mathsf{P}$'s view is $\mathsf{Wt}\_\mathsf{P}(C) := \mathsf{len}(C) + B \cdot \mathsf{certCount}\_\mathsf{P}(C)$ for a protocol parameter $B$.
- Set $\mathsf{cert}^\prime$ to the certificate with the highest round number in $\mathsf{Certs}$.
- Set $\mathsf{cert}^*$ to the certificate with the highest round number present in $C_\text{pref}$.

[*Block creation*](#block-creation) occurs whenever party $\mathsf{P}$ is slot leader in a slot $s$, belonging to some round $r$:

- Create a new block $\mathsf{block} = (s, \mathsf{P}, h, \mathsf{cert}, ...)$, where
- $h$ is the hash of the last block in $C_\text{pref}$,
- $\mathsf{cert}$ is set to $\mathsf{cert}^\prime$ if
- there is no round $(r-2)$ certificate in $\mathsf{Certs}$, and
- $r - \mathsf{round}(\mathsf{cert}^\prime) \leq A$, and
- $\mathsf{round}(\mathsf{cert}^\prime) > \mathsf{round}(\mathsf{cert}^*)$,
- and to $\bot$ otherwise,
- Extend $C_\text{pref}$ by $\mathsf{block}$, add the new $C_\text{pref}$ to $\mathcal{C}$ and diffuse it.

During [*voting*](#voting), each party $\mathsf{P}$ does the following at the beginning of each voting round $r$:

- Let $\mathsf{block}$ be the youngest block at least $L$ slots old on $C_\text{pref}$.
- If party $\mathsf{P}$ is (voting) committee member in a round $r$,
- either
- : $\mathsf{round}(\mathsf{cert}^\prime) = r-1$ and $\mathsf{cert}^\prime$ was received before the end of round $r-1$, and
- : $\mathsf{block}$ extends (i.e., has the ancestor or is identical to) the block certified by $\mathsf{cert}^\prime$,
- or
- : $r \geq \mathsf{round}(\mathsf{cert}^\prime) + R$, and
- : $r = \mathsf{round}(\mathsf{cert}^*) + c \cdot K$ for some $c > 0$,
- then create a vote $v = (r, \mathsf{P}, h,...)$,
- Add $v$ to $\mathcal{V}$ and diffuse it.

The diagram below illustrates the key concepts and entities in Peras. In addition to the rhythm of block production, voting is attempted regularly at the start of each round. Certificates record quorums of votes, but most certificates are not recorded in the blocks. The left side of the diagram shows the tail end of a cool-down period, where voting is not allowed. The chain leaves the cool-down period once voting resumes. The recording of a certificate reference in two blocks memorializes the exit of the cool-down period. Nodes track the tip of their preferred chain, the most recent certificate seen, and the most recent certificate on their preferred chain. Each successful vote boosts the weight of that chain.

![Blockchain diagram illustrating key concepts and entities in Peras.](images/simvis-blocktree.png)

| Icon                                                      | Meaning                                                                                                                                                                                                                                                                               |
| --------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ![Block](images/block.png)                              | A block created by a node, linked to its predecessor. Contains the block hash (truncated to 8 characters), the creator's id, the creation time, and three icons representing truth value of the three conditions presiding to inclusion of a certificate within a block logic.        |
| ![Block-certificate](images/block-with-certificate.png) | A block containing a certificate on-chain. Its content is identical to a normal block but color differs to ease spotting when a certificate is included on-chain.                                                                                                                     |
| ![Certificate](images/certificate.png)                  | A certificate created by a node, linked to the block it certifies. A certificate is only identified by its round number, as by construction there cannot be more than one certificate each round.                                                                                     |
| ![Vote](images/vote.png)                                | A vote cast by a node, linked to the block it votes for. Contains the round number in which the vote is cast, the voter's id, and the truth values of the four different rules for casting a vote.                                                                                    |
| ![Cooldown](images/cooldown.png)                        | Record a node's decision to enter cooldown period, linked to the block that triggered it. Contains the round number in which the cooldown is started, the node's id, and the truth values of the conditions that lead to the node not casting a vote and entering cooldown.           |
| ![Node](images/node.png)                                | A node in the network, identified simply by a number. This is a marker representing the state of a node: What's the tip of its best chain, the latest "live" certificate it knows, and the latest on-chain certificate it knows. |

An [online simulator for Peras](https://peras-simulation.cardano-scaling.org/) is available.


### Normative Peras specification in Agda

The following formal, relational specification for Peras type utilizes [Agda 2.6.4.3](https://github.com/agda/agda/tree/v2.6.4.3). See [the Appendix](#typechecking-this-specification) for instruction on type-checking this specification with the Agda compiler and see [github:input-output-hk/peras-design](https://github.com/input-output-hk/peras-design/) for proofs and other modules related to this specification.

```agda
module README where
```

Most of the imports come from the [Agda Standard Library 2.0](https://github.com/agda/agda-stdlib/tree/v2.0).

```agda
open import Data.Bool using (Bool; if_then_else_; not; _∧_)
open import Data.Empty using (⊥)
open import Data.Fin using (Fin; pred)
                     renaming (zero to fzero; suc to fsuc)
open import Data.List using (List; any; concat; dropWhile; filter; head; map; mapMaybe; sum; []; _∷_; _++_)
                      renaming (length to ∣_∣)
open import Data.List.Membership.Propositional using (_∈_)
open import Data.List.Relation.Unary.All using (All)
open import Data.List.Relation.Unary.Any using (Any; any?; _─_)
                                         renaming (_∷=_ to _∷ˡ=_)
open import Data.Maybe using (Maybe; just; nothing)
open import Data.Nat using (NonZero; Ordering; suc; ℕ; _≤_; _≥_; _>_; _≤?_; _<ᵇ_; _≤ᵇ_; _+_; _∸_; _*_; _/_; _%_)
open import Data.Nat.Properties using (_≟_)
open import Data.Product using (proj₁; proj₂; _×_; _,_)
open import Data.Sum using (_⊎_)
open import Data.Unit using (⊤)
open import Function.Base using (_∘_)
open import Relation.Binary using (DecidableEquality)
open import Relation.Binary.PropositionalEquality using (_≡_; _≢_; cong)
open import Relation.Nullary using (Dec; no; yes; ¬_; ⌊_⌋)
open import Relation.Nullary.Decidable using (_×-dec_; ¬?)
```

Several additional imports come from the [IOG Agda Prelude v0.1.0.0](https://github.com/input-output-hk/iog-agda-prelude/releases/tag/v0.1.0.0).

```agda
open import Prelude.AssocList using (AssocList; set; _⁉_)
open import Prelude.DecEq using (DecEq; _==_)
open import Prelude.Default using (Default)
open import Prelude.InferenceRules
```

#### Protocol parameters

The seven *protocol parameters* are natural numbers.

```agda
record Params : Set where
  field
```

The $U$ parameter is the duration of each voting round, measured in slots.

```agda
        U : ℕ
```

The $L$ parameter is the minimum age of a candidate block for being voted upon, measured in slots.

```agda
        L : ℕ
```

<span id="#expiration"/>The $A$ parameter is the maximum age for a certificate to be included in a block, measured in rounds.

```agda
        A : ℕ
```

The $R$ parameter is the number of rounds for which to ignore certificates after entering a cool-down period.

```agda
        R : ℕ
```

The $K$ parameter is the minimum number of rounds to wait before voting again after a cool-down period starts.

```agda
        K : ℕ
```

<span id="#boost"/>The $B$ parameter is the extra chain weight that a certificate (a quorum of votes) imparts to a block.

```agda
        B : ℕ
```

<span id="#quorum"/>The $\tau$ parameter is the number of votes (the quorum) required to create a certificate.

```agda
        τ : ℕ
```

Note that neither the round length nor the cool-down duration may be zero.

```agda
        ⦃ U-nonZero ⦄ : NonZero U
        ⦃ K-nonZero ⦄ : NonZero K
```

#### Network representation

At the protocol level, the only *network parameter* of interest is the diffusion time $\Delta$,  which is the upper limit on the number of slots needed to honestly diffuse a message to all nodes.

```agda
record Network : Set₁ where
  field
    Δ : ℕ
```

#### Slots and rounds

As in Praos, time is measured in *slots*.

```agda
record SlotNumber : Set where
  constructor MkSlotNumber
  field getSlotNumber : ℕ

  next : SlotNumber
  next = record {getSlotNumber = suc getSlotNumber}

open SlotNumber using (getSlotNumber)
```

Each Peras voting *round* consists of $U$ consecutive slots.

```agda
record RoundNumber : Set where
  constructor MkRoundNumber
  field getRoundNumber : ℕ

open RoundNumber using (getRoundNumber)

module _ ⦃ _ : Params ⦄ where
  open Params ⦃...⦄

  StartOfRound : SlotNumber → RoundNumber → Set
  StartOfRound (MkSlotNumber sl) (MkRoundNumber r) = sl ≡ r * U

  rnd : ℕ → ⦃ _ : NonZero U ⦄ → ℕ
  rnd s = s / U

  v-round : SlotNumber → RoundNumber
  v-round (MkSlotNumber s) = MkRoundNumber (rnd s)
```

#### Hashing

The protocol requires a type for the result of hashing data, an empty value for that type, and an equality test for that type.

```agda
postulate
  ByteString : Set
  emptyBS : ByteString
  _≟-BS_ : DecidableEquality ByteString
```

Hashes are represented by a byte string, and most of the protocol's primary data types can be hashed.

```agda
record Hash (a : Set) : Set where
  constructor MkHash
  field hashBytes : ByteString

record Hashable (a : Set) : Set where
  field hash : a → Hash a

open Hashable ⦃...⦄
```

#### Parties

A *party* operates a node and controls its cryptographic keys. Parties are, of course, distinguishable for one another.

```agda
postulate
  Party : Set
  _≟-party_ : DecidableEquality Party

instance
  iDecEqParty : DecEq Party
  iDecEqParty .DecEq._≟_ = _≟-party_
```

```agda
Parties = List Party
```

#### Signatures

The protocol uses standard KES *signatures* (Ed25519) for signing blocks or votes.

```agda
postulate
  Signature : Set
```

#### Slot leadership and committee membership

A *leadership proof* attests a party's slot leadership exactly as it does in Praos. The function `IsSlotLeader` verifies a party's leadership for a particular slot and the function `IsBlockSignature` verifies the validity of a block's signature.

```agda
record Block : Set  -- Blocks will be defined later in this specification.

postulate
  LeadershipProof : Set
  IsSlotLeader : Party → SlotNumber → LeadershipProof → Set
  IsBlockSignature : Block → Signature → Set
```

The voting scheme used by Peras is specified in the proposed CIP [*Votes & Certificates on Cardano*](https://github.com/cardano-foundation/CIPs/pull/870). It involves a *proof of membership* in a round's voting committee. The function `IsCommitteeMember` verifies a party's membership in a round's voting committee and the weight of their vote. The function `IsVoteSignature` verifies that validity of a vote's signature.

```agda
record Vote : Set  -- Votes will be defined later in this specification.

record VotingWeight : Set where
  field votes : ℕ

postulate
  MembershipProof : Set
  IsCommitteeMember : Party → RoundNumber → VotingWeight → MembershipProof → Set
  IsVoteSignature : Vote → Signature → Set
```

#### Votes

- Votes* have a creator, a weight, a proof of the creator's membership in the round's voting committee, and a reference to the block being voted for.

```agda
record Vote where
  constructor MkVote
  field votingRound : RoundNumber
        creatorId   : Party
        weight      : VotingWeight
        proofM      : MembershipProof
        blockHash   : Hash Block
        signature   : Signature

  votingWeight : ℕ
  votingWeight = VotingWeight.votes weight

  votingRound' : ℕ
  votingRound' = getRoundNumber votingRound
```

Votes are valid if the party and weight are correct for the round and if the vote is properly signed.

```agda
ValidVote : Vote → Set
ValidVote v =
  IsCommitteeMember
    (Vote.creatorId v)
    (Vote.votingRound v)
    (Vote.weight v)
    (Vote.proofM v)
  × IsVoteSignature v (Vote.signature v)
```

- Equivocated votes* are ones that duplicate votes by the same party in the same round. The protocol will reject such equivocated votes.

```agda
data _∻_ : Vote → Vote → Set where
  Equivocation : ∀ {v₁ v₂}
    → Vote.creatorId v₁ ≡ Vote.creatorId v₂
    → Vote.votingRound v₁ ≡ Vote.votingRound v₂
    → v₁ ≢ v₂
    → v₁ ∻ v₂
```

#### Certificates

A *certificate* attests that a quorum of votes for the same block were cast during a round.

```agda
record Certificate : Set where
  constructor MkCertificate
  field round : RoundNumber
        blockRef : Hash Block

  roundNumber : ℕ
  roundNumber = getRoundNumber round

postulate
  _≟-certificate_ : DecidableEquality Certificate
```

The protocol places special emphasis on the most recent certificate among a set of certificates.

```agda
latestCert : Certificate → List Certificate → Certificate
latestCert c = maximumBy c Certificate.roundNumber
  where maximumBy : {a : Set} → a → (a → ℕ) → List a → a
        maximumBy candidate _ [] = candidate
        maximumBy candidate f (x ∷ xs) =
          if f candidate ≤ᵇ f x
            then maximumBy x f xs
            else maximumBy candidate f xs
```

#### Block bodies

- Block bodies* are identical to those in Praos. They consist of a payload of transactions and are identified by their unique hash. The detailed contents are irrelevant for Peras, so we represent them in a slightly simplified manner.

```agda
postulate
  Tx : Set
  hashTxs : List Tx → Hash (List Tx)

Payload = List Tx

instance
  iHashablePayload : Hashable Payload
  iHashablePayload .hash = hashTxs

record BlockBody : Set where
  constructor MkBlockBody
  field blockHash : Hash Payload
        payload : Payload
```

#### Blocks

- Blocks* are identical to those in Praos, except for the rare inclusion of a certificate, which may happen near the beginning or ending of a cool-down period. The other detailed contents are irrelevant for Peras, so we represent them in a slightly simplified manner.

```agda
record Block where
  constructor MkBlock
  field slotNumber : SlotNumber
        creatorId : Party
        parentBlock : Hash Block
        certificate : Maybe Certificate  -- NB: New in Peras and not present in Praos.
        leadershipProof : LeadershipProof
        signature : Signature
        bodyHash : Hash Payload

  slotNumber' : ℕ
  slotNumber' = getSlotNumber slotNumber

postulate
  hashBlock : Block → Hash Block

instance
  iHashableBlock : Hashable Block
  iHashableBlock .hash = hashBlock

_≟-BlockHash_ : DecidableEquality (Hash Block)
(MkHash b₁) ≟-BlockHash (MkHash b₂) with b₁ ≟-BS b₂
... | yes p = yes (cong MkHash p)
... | no ¬p =  no (¬p ∘ cong Hash.hashBytes)

genesisHash : Hash Block
genesisHash = MkHash emptyBS

cert₀ : Certificate
cert₀ = MkCertificate (MkRoundNumber 0) genesisHash
```
#### Chains

The linking of blocks into a *chain* is identical to Praos.

```agda
Chain = List Block
```

The genesis chain is the empty list.
```agda
genesis : Chain
genesis = []
```

The protocol scrutinizes any certificates recorded on the chain.
```agda
certsFromChain : Chain → List Certificate
certsFromChain = mapMaybe Block.certificate
```

It also needs to test whether a certificate (quorum of votes) refers to a block found on a particular chain.

```agda
_PointsInto_ : Certificate → Chain → Set
_PointsInto_ c = Any ((Certificate.blockRef c ≡_) ∘ hash)

_PointsInto?_ : ∀ (c : Certificate) → (ch : Chain) → Dec (c PointsInto ch)
_PointsInto?_ c = any? ((Certificate.blockRef c ≟-BlockHash_) ∘ hash)
```

Peras differs from Praos in that the <span id="#weight"/>weight of a chain is its length plus the boost parameter $B$ times the number of vote quorums (certificates) its blocks have received.

```agda
module _ ⦃ _ : Params ⦄ where
  open Params ⦃...⦄

  ∥_∥_ : Chain → List Certificate → ℕ
  ∥ ch ∥ cts = ∣ ch ∣ + ∣ filter (_PointsInto? ch) cts ∣ * B
```

The protocol can identify a chain by the hash of its most recent block (its tip).

```agda
tipHash : Chain → Hash Block
tipHash [] = genesisHash
tipHash (b ∷ _) = hash b
```

A chain is valid if its blocks are signed and their creators were slot leaders. The chain's genesis is always valid.

```agda
data ValidChain : Chain → Set where
  Genesis : ValidChain genesis
  Cons : ∀ {c : Chain} {b : Block}
    → IsBlockSignature b (Block.signature b)
    → IsSlotLeader (Block.creatorId b) (Block.slotNumber b) (Block.leadershipProof b)
    → Block.parentBlock b ≡ tipHash c
    → ValidChain c
    → ValidChain (b ∷ c)
```

A block is said to extend a certificate on a chain if the certified block is an ancestor of or identical to the block and on the chain.

```agda
ChainExtends : Hash Block → Certificate → Chain → Set
ChainExtends h c =
  Any (λ block → (hash block ≡ Certificate.blockRef c))
    ∘ dropWhile (λ block' → ¬? (hash block' ≟-BlockHash h))

Extends : Hash Block → Certificate → List Chain → Set
Extends h c
  with c ≟-certificate cert₀
Extends h c | yes _ = λ _ → ⊤
Extends h c | no _ = Any (ChainExtends h c)
```

#### Messages and their envelopes

In addition to the chain *messages* already diffused among nodes in Praos, the Peras protocol also diffuses votes between nodes. (Note that Peras implementations might choose also to diffuse certificates in lieu of sets of votes that meet the quorum condition.)

```agda
data Message : Set where
  ChainMsg : {c : Chain} → ValidChain c → Message
  VoteMsg : {v : Vote} → ValidVote v → Message
```

Diffusion of votes or blocks over the network may involve delays of a slot or more.

```agda
module _ ⦃ _ : Params ⦄ ⦃ _ : Network ⦄ where
  open Params ⦃...⦄
  open Network ⦃...⦄

  Delay = Fin (suc (suc Δ))
  pattern 𝟘 = fzero
  pattern 𝟙 = fsuc fzero
```

Messages are put into an *envelope* and assigned to a party. Such messages can be delayed.

```agda
  record Envelope : Set where
    constructor ⦅_,_,_⦆
    field
      partyId : Party
      message : Message
      delay : Delay
```

#### Block trees

- Block trees* are defined by functions and properties: any implementation of the block tree has to possess the required functions.

```agda
module _ ⦃ _ : Params ⦄ where
  open Params ⦃...⦄

  record IsTreeType {T : Set}
                    (tree₀ : T)
                    (addChain : T → {c : Chain} → ValidChain c → T)
                    (allChains : T → List Chain)
                    (preferredChain : T → Chain)
                    (addVote : T → {v : Vote} → ValidVote v → T)
                    (votes : T → List Vote)
                    (certs : T → List Certificate)
                    (cert₀ : Certificate)
         : Set₁ where

    field
```

It must also conform to properties that must hold with respect to chains, certificates and votes. First, the genesis tree must prefer the genesis chain, have an empty set of certificates, and have an empty set of votes.

```agda
      instantiated :
        preferredChain tree₀ ≡ genesis

      instantiated-certs :
        certs tree₀ ≡ cert₀ ∷ []

      instantiated-votes :
        votes tree₀ ≡ []
```

The certificates in a chain newly incorporated into the block tree must equate to the certificates on the chain itself and the block tree's record of certificates.

```agda
      extendable-chain : ∀ (t : T) {c : Chain} (vc : ValidChain c)
        → certs (addChain t vc) ≡ certsFromChain c ++ certs t
```

A valid block tree must have a valid preferred chain.

```agda
      valid : ∀ (t : T)
        → ValidChain (preferredChain t)
```

The preferred chain must be at least as weighty as any other chain present in the block tree.

```agda
      optimal : ∀ (c : Chain) (t : T)
        → let b = preferredChain t
              cts = certs t
          in ValidChain c
        → c ∈ allChains t
        → ∥ c ∥ cts ≤ ∥ b ∥ cts
```

The preferred chain must be present in the list of all chains seen.

```agda
      self-contained : ∀ (t : T)
        → preferredChain t ∈ allChains t
```

Duplicate or equivocated votes must not be present in the block tree.

```agda
      unique-votes : ∀ (t : T) {v : Vote} (vv : ValidVote v)
        → let vs = votes t
          in v ∈ vs
        → vs ≡ votes (addVote t vv)

      no-equivocations : ∀ (t : T) {v : Vote} (vv : ValidVote v)
        → let vs = votes t
          in Any (v ∻_) vs
        → vs ≡ votes (addVote t vv)
```

Every certificate must represent a quorum of recorded votes.

```agda
      quorum-cert : ∀ (t : T) (b : Block) (r : ℕ)
        →  (sum ∘ map Vote.votingWeight) (filter (λ {v →
                     (getRoundNumber (Vote.votingRound v) ≟ r)
               ×-dec (Vote.blockHash v ≟-BlockHash hash b)}
             ) (votes t)) ≥ τ
        → Any (λ {c →
            (getRoundNumber (Certificate.round c) ≡ r)
          × (Certificate.blockRef c ≡ hash b) }) (certs t)
```

The concrete block tree type (`TreeType`) manages chains, certificates, and votes.

```agda
  record TreeType (T : Set) : Set₁ where

    field
      tree₀ : T
      addChain : T → {c : Chain} → ValidChain c → T
      allChains : T → List Chain
      preferredChain : T → Chain
      addVote : T → {v : Vote} → ValidVote v → T
      votes : T → List Vote
      certs : T → List Certificate
```

It conforms to the `IsTreeType` requirements.

```agda
      is-TreeType : IsTreeType
                      tree₀ addChain allChains preferredChain
                      addVote votes certs cert₀
```

Several convenience functions are provided for extracting information about certificates and votes.

```agda
    latestCertOnChain : T → Certificate
    latestCertOnChain = latestCert cert₀ ∘ mapMaybe Block.certificate ∘ preferredChain

    latestCertSeen : T → Certificate
    latestCertSeen = latestCert cert₀ ∘ certs

    hasVote : RoundNumber → T → Set
    hasVote (MkRoundNumber r) = Any ((r ≡_) ∘ Vote.votingRound') ∘ votes
```

#### Parameterization of the semantics

In order to define the semantics the following parameters are required.

- The type of the block-tree
- A function that mimics the node's memory pool by selecting the transactions available to a particular party in a particular slot
- A list of the parties participating in the protocol

```agda
module Semantics
           ⦃ _ : Params ⦄
           ⦃ _ : Network ⦄
           {T : Set} {blockTree : TreeType T}
           {txSelection : SlotNumber → Party → List Tx}
           {parties : Parties}
           where
    open Params ⦃...⦄
    open TreeType blockTree
```

The protocol starts from the genesis block tree.

```agda
    instance
      Default-T : Default T
      Default-T .Default.def = tree₀
```

#### Block-tree update

Updating the block tree involves recording the votes and chains received via messages.

```agda
    data _[_]→_ : T → Message → T → Set where

      VoteReceived : ∀ {v vv t} →
        ──────────────────────────────────
        t [ VoteMsg {v} vv ]→ addVote t vv

      ChainReceived : ∀ {c vc t} →
        ────────────────────────────────────
        t [ ChainMsg {c} vc ]→ addChain t vc
```

#### Block selection

The block selected for voting is the most recent one on the preferred chain that is at least $L$ slots old.

```agda
    BlockSelection : SlotNumber → T → Hash Block
    BlockSelection (MkSlotNumber s) = tipHash ∘ filter (λ {b → (Block.slotNumber' b) + L ≤? s}) ∘ preferredChain
```

#### Rules for voting in a round

Voting is allowed in a round if voting has proceeded regularly in preceding rounds or if a sufficient number of slots have lapsed since the protocol entered a cool-down period. Specifically, either of two pairs of conditions must be met.

- `VR-1A`: The vote has seen the certificate for the previous round.

```agda
    VotingRule-1A : RoundNumber → T → Set
    VotingRule-1A (MkRoundNumber r) t = r ≡ Certificate.roundNumber (latestCertSeen t) + 1
```

- `VR-1B`: The block being voted upon extends the most recently certified block

```agda
    VotingRule-1B : SlotNumber → T → Set
    VotingRule-1B s t =
      Extends (BlockSelection s t) (latestCertSeen t) (allChains t)
```

- `VR-1`: Both `VR-1A` and `VR-1B` hold, which is the situation typically occurring when the voting has regularly occurred in preceding rounds.

```agda
    VotingRule-1 : SlotNumber → T → Set
    VotingRule-1 s t =
        VotingRule-1A (v-round s) t
      × VotingRule-1B s t
```

- `VR-2A`: The last certificate a party has seen is from a round at least $R$ rounds previously. This enforces the chain-healing period that must occur before leaving a cool-down period.

```agda
    VotingRule-2A : RoundNumber → T → Set
    VotingRule-2A (MkRoundNumber r) t =
      r ≥ Certificate.roundNumber (latestCertSeen t) + R
```

- `VR-2B`: The last certificate included in a party's current chain is from a round exactly $c \cdot K$ rounds ago for some $c \in ℕ$ with $c ≥ 0$. This enforces chain quality and common prefix before leaving a cool-down period.

```agda
    VotingRule-2B : RoundNumber → T → Set
    VotingRule-2B (MkRoundNumber r) t =
        r > Certificate.roundNumber (latestCertOnChain t)
      × r mod K ≡ (Certificate.roundNumber (latestCertOnChain t)) mod K
      where
        _mod_ : ℕ → (n : ℕ) → ⦃ NonZero n ⦄ → ℕ
        _mod_ a b ⦃ prf ⦄ = _%_ a b ⦃ prf ⦄
```

- `VR-2`: Both `VR-2A` and `VR-2B` hold, which is the situation typically occurring when the chain is about to exit a cool-down period.

```agda
    VotingRule-2 : RoundNumber → T → Set
    VotingRule-2 r t =
        VotingRule-2A r t
      × VotingRule-2B r t
```

If either `VR-1A` and `VR-1B` hold, or `VR-2A` and `VR-2B` hold, then voting is allowed.

```agda
    VotingRule : SlotNumber → T → Set
    VotingRule s t =
        VotingRule-1 s t
      ⊎ VotingRule-2 (v-round s) t
```

#### State

The small-step semantics rely on a global state, which consists of several pieces of information.

- Current slot of the system
- Map with local state per party
- All the messages that have been sent but not yet been delivered
- All the messages that have been sent

```agda
    record State : Set where
      constructor ⟦_,_,_,_,_⟧
      field
        clock : SlotNumber
        blockTrees : AssocList Party T
        messages : List Envelope
        history : List Message
```

#### Progress

Rather than keeping track of progress, we introduce a predicate stating that all messages that are not delayed have been delivered. This is a precondition that must hold before transitioning to the next slot.

```agda
    Fetched : State → Set
    Fetched = All (λ { z → Envelope.delay z ≢ 𝟘 }) ∘ messages
      where open State
```

#### Advancing the clock

Ticking the global clock increments the slot number and decrements the delay of all the messages in the message buffer.

```agda
    tick : State → State
    tick M =
      record M
        { clock = SlotNumber.next clock
        ; messages =
            map (λ where e → record e { delay = pred (Envelope.delay e) })
              messages
        }
      where open State M
```

#### Updating the global state

New messages are buffered, recorded in the global history, and will update a party's portion of the global state.`
```agda
    _,_⇑_ : Message → (Party → Delay) → State → State
    m , fᵈ ⇑ M =
      record M
        { messages =
            map (λ { p → ⦅ p , m , fᵈ p ⦆}) parties
- + messages
        ; history = m ∷ history
        }
      where open State M
```
This occurs when a message diffuses to new parties.
```agda
    delay_by_update_ : Message → (Party → Delay) → State → State
    delay m@(ChainMsg x) by fᵈ update M = m , fᵈ ⇑ M
    delay m@(VoteMsg x) by fᵈ update M = m , fᵈ ⇑ M
```

#### Fetching

A party receives messages from the global state by fetching messages assigned to the party, updating the local block tree, and putting the local state back into the global state.

```agda
    data _⊢_[_]⇀_ : Party → State → Message → State → Set
      where
```

An honest party consumes a message from the global message buffer and updates their local state.

```agda
      honest : ∀ {p} {t t′} {m} {N}
        → let open State N
          in
          (m∈ms : ⦅ p , m , 𝟘 ⦆ ∈ messages) →
        ∙ blockTrees ⁉ p ≡ just t
        ∙ t [ m ]→ t′
          ─────────────────────────────────────
          p ⊢
          N [ m ]⇀ record N
            { blockTrees = set p t′ blockTrees
            ; messages = messages ─ m∈ms
            }
```

#### Voting

Votes are created with the required information about committee membership and the block being voted for.

```agda
    createVote : SlotNumber → Party → VotingWeight → MembershipProof → Signature → Hash Block → Vote
    createVote s p w prf sig hb =
      record
        { votingRound = v-round s
        ; creatorId = p
        ; weight = w
        ; proofM = prf
        ; blockHash = hb
        ; signature = sig
        }
```

A party can consider voting for a block, if

- the current slot is the first slot in a voting round
- the party is a member of the voting committee
- the chain is not in a cool-down phase

Voting updates the party's local state and for all other parties a message is ready to be consumed immediately.

```agda
    infix 2 _⊢_⇉_
    data _⊢_⇉_ : Party → State → State → Set where

      honest : ∀ {p} {t} {M} {w} {π} {σ} {b}
        → let
            open State
            s = clock M
            r = v-round s
            v = createVote s p w π σ b
          in
          (fᵈ : Party → Delay)
          (mem : IsCommitteeMember p r w π)
          (sig : IsVoteSignature v σ) →
        ∙ BlockSelection s t ≡ b
        ∙ blockTrees M ⁉ p ≡ just t
        ∙ StartOfRound s r
        ∙ VotingRule s t
          ────────────────────────────────────────────
          p ⊢
            M ⇉ delay VoteMsg (mem , sig) by fᵈ
                 update M
```

#### Block creation

Certificates are conditionally added to a block, typically near the beginning or ending of a cool-down period. Such recording occurs if . . .

1. There is no certificate seen (recorded) from two rounds ago,
2. The last seen certificate is not expired, and
3. The last seen certificate is from a later round than the last certificate on chain.

```agda
    needCert : RoundNumber → T → Maybe Certificate
    needCert (MkRoundNumber r) t =
      let
        cert⋆ = latestCertOnChain t
        cert′ = latestCertSeen t
      in
        if not (any (λ {c → ⌊ Certificate.roundNumber c + 2 ≟ r ⌋}) (certs t))  -- (1)
           ∧ (r ≤ᵇ A + Certificate.roundNumber cert′)                           -- (2)
           ∧ (Certificate.roundNumber cert⋆ <ᵇ Certificate.roundNumber cert′)   -- (3)
        then just cert′
        else nothing
```

Blocks are created with the required information.

```agda
    createBlock : SlotNumber → Party → LeadershipProof → Signature → T → Block
    createBlock s p π σ t =
      record
        { slotNumber = s
        ; creatorId = p
        ; parentBlock = tipHash (preferredChain t)
        ; certificate = needCert (v-round s) t
        ; leadershipProof = π
        ; bodyHash = hash (txSelection s p)
        ; signature = σ
        }
```

A party can create a new block by adding it to the local block tree and diffuse the block creation messages to the other parties. Block creation is possible, if as in Praos, . . .

- the block signature is correct, and
- the party is the slot leader.

Block creation updates the party's local state, but for all other parties a message is added to the message buffer

```agda
    infix 2 _⊢_↷_
    data _⊢_↷_ : Party → State → State → Set where

      honest : ∀ {p} {t} {M} {π} {σ} →
        let open State M
            b = createBlock clock p π σ t
            pref = preferredChain t
          in
          (fᵈ : Party → Delay)
          (vc : ValidChain (b ∷ pref)) →
        ∙ blockTrees ⁉ p ≡ just t
          ──────────────────────────────
          p ⊢
            M ↷ delay ChainMsg vc by fᵈ
                update M
```

#### Small-step semantics

The small-step semantics describe the evolution of the global state.

```agda
    variable
      M N O : State
      p : Party
```

The relation allows

- Fetching messages at the beginning of each slot
- Block creation
- Voting
- Transitioning to next slot in the same voting round
- Transitioning to next slot in a new voting round

Note that when transitioning to the next slot we need to distinguish whether the next slot is in the same or a new voting round. This is necessary in order to detect adversarial behavior with respect to voting (adversarially not voting in a voting round).

```agda
    data _↝_ : State → State → Set where

      Fetch : ∀ {m} →
        ∙ p ⊢ M [ m ]⇀ N
          ──────────────
          M ↝ N

      CreateVote :
        ∙ Fetched M
        ∙ p ⊢ M ⇉ N
          ─────────
          M ↝ N

      CreateBlock :
        ∙ Fetched M
        ∙ p ⊢ M ↷ N
          ─────────
          M ↝ N

      NextSlot :
        ∙ Fetched M
          ─────────────────────
          M ↝ tick M
```

This completes for formal specification of Peras. The repository [github:input-output-hk/peras-design](https://github.com/input-output-hk/peras-design/tree/main) leverages this specification by providing the following Agda code.

- Proofs of properties of the Peras protocol.
- An executable specification (since the above specification is *relational* and not *executable*)
- Proofs of the soundness of the executable specification with respect to this relational one
- Scaffolding for generating dynamic, property-based conformance tests using the Haskell [`quickcheck-dynamic`](https://hackage.haskell.org/package/quickcheck-dynamic) package.


### Constraints on Peras parameters

The structure of the Peras protocol imposes the following constraints on its [parameters](#protocol-parameters). These arise from both theoretical and practical considerations.

| Parameter               | Symbol          | Units   | Description                                                                               | Constraints                                                      | Rationale                                                                                 |
|-------------------------|-----------------|---------|-------------------------------------------------------------------------------------------|------------------------------------------------------------------|-------------------------------------------------------------------------------------------|
| Round length            | $U$             | slots   | The duration of each voting round.                                                        | $U \geq \Delta$                                                  | All of a round's votes must be received before the end of the round.                      |
| Block selection offset  | $L$             | slots   | The minimum age of a candidate block for being voted upon.                                | $\Delta < L                                                      | Blocks must propagate before they are voted upon.                                         |
| Certificate expiration  | $A$             | rounds  | The maximum age for a certificate to be included in a block.                              | $A = \left\lceil\frac{T_\text{heal}+T_\text{CQ}}{U}\right\rceil$ | After a quorum failure, the chain must heal and achieve quality.                          |
| Chain ignorance period  | $R$             | rounds  | The number of rounds for which to ignore certificates after entering a cool-down period.  | $R = A$                                                          | Ensure chain-ignorance period lasts long enough to include a certificate on the chain.    |
| Cool-down period        | $K$             | rounds  | The minimum number of rounds to wait before voting again after a cool-down period starts. | $K = \left\lceil \frac{A + T_\text{CP}}{U} \right\rceil$         | After a quorum failure, the chain must heal, achieve quality, and attain a common prefix. |
| Certification boost     | $B$             | blocks  | The extra chain weight that a certificate gives to a block.                               | $B > 0$                                                          | Peras requires that some blocks be boosted.                                               |
| Quorum size             | $\tau$          | parties | The number of votes required to create a certificate.                                     | $\tau > 3 n / 4$                                                 | Guard against a minority (< 50%) of adversarial voters.                                   |
| Committee size          | $n$             | parties | The number of members on the voting committee.                                            | $n > 0$                                                          | Peras requires a voting committee.                                                        |
| Network diffusion time  | $\Delta$        | slots   | Upper limit on the time needed to diffuse a message to all nodes.                         | $\Delta > 0$                                                     | Messages have a finite delay.                                                             |
| Active slot coefficient | $f$             | 1/slots | The probability that a party will be the slot leader for a particular slot.               | $0 < f \leq 1$                                                   | Blocks must be produced.                                                                  |
| Healing time            | $T_\text{heal}$ | slots   | Healing period to mitigate a strong (25-50%) adversary.                                   | $T_\text{heal} = \mathcal{O}\left( B / f \right)$                | Sufficient blocks must be produced to overcome an adversarially boosted block.            |
| Chain-quality time      | $T_\text{CQ}$   | slots   | Ensure the presence of at least one honest block on the chain.                            | $T_\text{CQ} = \mathcal{O} (k/f)$                                | A least one honest block must be produced.                                                |
| Common-prefix time      | $T_\text{CP}$   | slots   | Achieve settlement.                                                                       | $T_\text{CP} = \mathcal{O} (k/f)$                                | The Ouroboros Praos security parameter defines the time for having a common prefix.       |
| Security parameter      | $k$             | blocks  | The Ouroboros Praos security parameter.                                                   | n/a                                                              | Value for the Cardano mainnet.                                                            |


### Specification of votes and certificates

The stake-proportional voting in Peras mimics the _sortition_ algorithm used in Praos: specifically it is based on the use of a *verifiable random function* (VRF) by each stake-pool operator guaranteeing the following properties:

- The probability for each voter to cast their vote in a given round is correlated to their share of total stake.
- It should be computationally impossible to predict a given SPO's schedule without access to their secret key VRF key.
- Verification of a voter's right to vote in a round should be efficiently computable.
- A vote should be unique and non-malleable, which is a requirement for the use of efficient certificates aggregation.

Additionally one would like the following property to be provided by the voting scheme:

- Voting should require minimal additional configuration (e.g., key management) for SPOs.
- Voting and certificate construction should be fast in order to ensure it does not interfere with other operations happening in the node.

The precise scheme and format for votes and certificates is immaterial to the protocol itself and is deferred to another proposed CIP [*Votes & Certificates on Cardano*](https://github.com/cardano-foundation/CIPs/pull/870) or to [the scheme documented in the Peras repository](https://github.com/input-output-hk/peras-design/blob/main/analytics/certificates-jan2025.md). Presumably, voting and certificates will be handled uniformly across Mithril, Peras, Leios, and partner chains.

### CDDL schema for the ledger

Peras requires a single addition, `peras_cert`, the [block](#blocks) data on the ledger.

```diff
 block =
   [ header
   , transaction_bodies         : [* transaction_body]
   , transaction_witness_sets   : [* transaction_witness_set]
   , auxiliary_data_set         : {* transaction_index => auxiliary_data }
   , invalid_transactions       : [* transaction_index ]
- , ? peras_cert               : votes_certificate
   ]
```

[Votes](https://github.com/input-output-hk/peras-design/blob/main/analytics/certificates-jan2025.md) are serialized in the following CDDL.

```cddl
votes_certificate =
  [ voter_id         : hash32
  , voting_round     : round_no
  , block_hash       : hash32
  , voting_proof     : vrf_cert
  , voting_weight    : voting_weight
  , kes_period       : kes_period
  , kes_vkey         : kes_vkey
  , kes_signature    : kes_signature
  ]
```

This definition relies on the following primitive types (drawn from Ledger definitions in [crypto.cddl](https://github.com/input-output-hk/cardano-ledger/blob/e2aaf98b5ff2f0983059dc6ea9b1378c2112101a/eras/conway/impl/cddl-files/crypto.cddl#L1)).

```cddl
round_no = uint .size 8
voting_weight = uint .size 8
vrf_cert = [bytes, bytes .size 80]
hash32 = bytes .size 32
kes_vkey = bytes .size 32
kes_signature = bytes .size 448
kes_period = uint .size 8
```

As already mentioned, the vote serialization mimics the block header's structure, which allows Cardano nodes to reuse their existing VRF and KES keys. Also note the following.

- The total size of each vote is 710 bytes, according to the above definition.
- Unless explicitly mentioned, the `hash` function exclusively uses 32-bytes Blake2b-256 hashes.
- The `voter_id` is it's pool identifier (i.e., the hash of the node's cold key).

The CDDL for the [certificates](#certificates) that aggregate votes is specified in the proposed CIP [*Votes & Certificates on Cardano*](https://github.com/cardano-foundation/CIPs/pull/870).


## Rationale: how does this CIP achieve its goals?

The Ouroboros Peras protocol achieves the goal of fast *ex post facto* settlement by periodically voting upon blocks of the preferred chain and giving such blocks a boost in weight if a quorum of voters vote for them in the same round. With overwhelming probability, the boost effectively "cements" the block forever unto the preferred chain, thus guarding it and prior blocks from rollbacks. The protocol operates under conditions of up to 25% adversarial stake, but reverts to the familiar Praos protocol under stronger adversarial conditions; after adversarial conditions abate, it remains safely in "Praos mode" for long enough to achieve chain healing, chain quality, and a common prefix. Thus, it does not weaken the worst-case security guarantees provided by Praos, though it does significantly speed settlement under "normal" non-adversarial conditions and under weakly adversarial conditions.

### How Peras settles faster

The diagram below illustrates why Peras provides fast settlement. If an adversary begins building a private fork, they would reveal it publicly if it ever becomes longer than the public, honestly preferred chain: once it is revealed to be longer, honest parties would build upon it, causing the rollback of the honest blocks that were built subsequent to the divergence of the honest and private chains. Peras's boosted blocks protect against rollbacks because it can be made extremely difficult for an adversary to cause a rollback of a boosted block. Thus adversaries only have an opportunity to roll back blocks after the last boosted block and before voting occurs to boost another block. Mathematically, the window of adversarial opportunity lasts for $U + L$ slots: blocks are voted upon for boosting every $U$ slots, but there is an $L$ slot delay between a block being produced and it being voted upon. Successful voting to boost a block effectively protects that block and its ancestors. Hence a transaction is at risk of rollback for typically no more that $U+L$ slots  and that risk abates once its block or a descendant block is boosted.

![Adversarial scenario against settlement](images/block-progression-adversarial.svg)

### Evidence for fast settlement

The following plot quantifies the settlement-time benefits of Peras[^5] using an approximate adversarial model and probabilistic computations. Using the [example Peras protocol parameters](#feasible-values=for-peras-protocol-parameters), one has *ex post facto* settlement within two minutes of a transaction's being included in a block. This means that if the transaction's block is still on the preferred chain after two minutes, then it will remain there with essentially no chance of being rolled back unless the adversarial stake is stronger than approximately 25%. The solid curves in the plot represent Peras and the dashed ones represent Praos. (The Praos probabilities are consistent with the model of Gaži, Ren, and Russell[^1].) The protocol parameters are those listed in the section [Feasible values for Peras protocol parameters](#feasible-values-for-peras-protocol-parameters), but the [Markov-chain simulation of an adversary building and then strategically revealing it a private chain](https://github.com/input-output-hk/peras-design/tree/main/peras-markov) used to make the plot simplifies the protocol in a few inessential aspects (network diffusion and the $L$ parameter) and does not model the memory pool (which mitigates short honest forks). The red curve shows the *ex ante* probability that a block included in the preferred chain remains on the preferred chain in the future, never being rolled back. The green curve shows the *ex post facto* probability that a block which has remained on the preferred chain for 120 slots (two minutes) remains on the preferred chain in the future.

![Comparison of settlement times for Peras and Praos](images/rollback-posterior.svg)

[^5]: https://peras.cardano-scaling.org/docs/reports/tech-report-2#settlement-probabilities

Even for adversarial stake less of 50%, there is only a vanishingly small probability of rolling back a block if it has "survived" long enough to have a descendant that received a Peras boost.

![Probability of rolling back a transaction covered by a boosted block](images/rollback-boosted.svg)

 Strong adversarial activity prior to the two minutes might cause the block to be rolled back before then, as show in the plot below, but adversarial activity after that time will not roll it back. If the transaction was rolled back in the first two minutes, then it would have to be resubmitted. So, the Peras protocol provides certainty about the fate of a transaction within a brief, fixed amount of time.

![Probability of a block being rolled back in Peras and Praos](images/rollback-prior.svg)

The figure below shows the race between honest parties and a modestly powerful adversary building a private chain. If the adversary's private chain ever becomes longer than the honest public one, then the adversary can reveal their chain publicly, shortly after which time the honest parties will adopt it as their preferred chain, with the consequence of rolling back all of the blocks on the honest chain from the time when the adversary started building privately. The Peras round length is 150 slots in this simulation, and one can see jump to the right every 150 slots the probability distribution of the honest chain's weight advantage over the private adversarial chain's weight. Each jump is a boost of 10 blocks' worth of chain weight. Even after one boost of the honest chain, the adversary has essentially no chance of ever overtaking the honest chain. The "shoulder" on the left side of the probability distribution is associated with the chain entering a cool-down period because the adversary thwarted voting, so no boost occurs.

![Animation of Markov-chain simulation of honest vs adversarial behavior](images/markov.gif)

### Why Peras is practical to implement

Peras is compatible with many stake-based voting schemes, which means it has synergies with protocol enhancements like Ouroboros Genesis and Leios. Because Peras only modifies Praos's chain-weight computation and adds voting, its effects are mostly orthogonal to other existing and proposed aspects of Ouroboros.

Peras is straightforward to implement, as it requires the following additions to the node, which have minimal or modest impact on node resources[^6].
- Chain selection that includes the boosting from certificates
- Building and verifying votes and certificate
- Mini-protocols for diffusing votes and certificates
- Permanent storage of certificates
- Temporary storage of unexpired votes

[^6]: https://peras.cardano-scaling.org/docs/reports/tech-report-2#resources-impact-of-peras

The impact of Peras upon nodes falls into four categories: [network](#network), [CPU](#cpu), [memory](#memory), and [storage](#persistent-storage). [Evidence](#votes--certificates) is provided that the CPU time required to construct and verify votes and certificates is much smaller than the duration of a voting round. Similarly, the [memory](#memory) needed to cache votes and certificates and the [disk space](#persistent-storage) needed to persist certificates are trivial compared to that needed for the UTXO set and the disk needed for the blocks.

On the networking side, [the ΔQ studies](#vote-diffusion) demonstrate that diffusion of Peras votes and certificates consumes minimal bandwidth and would not interfere with other node operations such as memory-pool and block diffusion. However, [diffusion of votes and certificates](#network-traffic) across a network will still have a noticeable impact on the _volume_ of data transfer, on the order of 20%, which might translate to increased operating costs for nodes deployed in cloud providers.

The remainder of this section outlines the use cases for Peras, discusses its mitigation of attacks, and summarizes it resource requirements.

### Use cases

Peras primarily benefits use cases where a party needs certainty, after a fixed amount of time, about the settlement status of a transaction. The generic use case follows.

1. The party submits a transaction to the memory pool.
2. A block producer includes the transaction in a newly forged block.
3. The party waits a fixed amount of time, $U + L$, which is predetermined by the protocol.
4. The party examines the preferred chain to see whether it contains the block and whether the block or one of its descendants has received a Peras boost.
1. It the block is on the preferred chain and guarded by a boost, then the transaction is essentially final/settled.
2. If the block is not on the preferred chain, then it has been rolled back and needs to be resubmitted.
3. If the block is on the preferred chain but there is no boost, then the chain has entered a cool-down period and the party want to wait for more confirmation, as one does currently in Praos.

Note that the third ("cool down") case would only occur if voting was prevented by a substantial disruption such as widespread loss of network connectivity or an attack. Reasonable values for $U$ and $L$ are 90 and 30 seconds, respectively, which means that settled versus rolled-back would be certain in two minutes.

Specific use cases involving time-constrained, high-value transactions conform to this generic pattern. When the value at risk is low, a one-in-a-million chance of a rollback might not be as concerning as it would be for a large transaction. Examples follow:

- Centralized exchanges, where fast settlement improves the user convenience and experience
- Partner chains and bridges, where certainty about synchronization between two chains is essential
- Dapps where fixed-horizon certainty is needed to orchestrate transactions
- Ordinary transactions where a brief wait is acceptable but a roll-back is not

For example, the partner-chain use case might leverage Peras as follows.

1. Funds or tokens need to be transferred from the partner chain to the Cardano chain.
2. A smart-contract transaction escrows the funds/tokens on the partner chain.
3. Simultaneously, a mirror of that smart-contract transaction is submitted on Cardano.
4. After a short amount of time, the Cardano transaction has been incorporated into a newly-formed block.
5. Wait $U + L$ slots on Cardano.
6. With high probability the transaction will be protected by a boosted block, so there would only be an infinitesimally small chance of it ever being rolled back.
- If it was rolled back, go back to step 3 above.
- In the unlikely event that a cool-down period has been entered, wait for more confirmations.
7. Complete the escrow contract on the partner chain.

<span id="#confirmations"/>Taking Kraken as an example of a centralized exchange, we see in the following table[^7] the significant delay required for transactions to be treated as final. A technology like Peras would put Cardano in the cluster of faster-settling blockchains.

| Blockchain | Confirmations Required | Approximate time (minutes) |
| ---------- | ---------------------: | -------------------------: |
| Algorand   |                     10 |                          1 |
| Aptos      |                     50 |                          5 |
| Avalance   |                     20 |                          1 |
| Bitcoin    |                      3 |                         30 |
| Cardano    |                     15 |                         10 |
| Dogecoin   |                     40 |                         40 |
| Ethereum   |                     70 |                         14 |
| Polkadot   |                    n/a |                          5 |
| Ripple     |                    n/a |                          0 |
| Solana     |                    n/a |                          0 |
| Tezos      |                      6 |                          3 |

[^7]: Data extracted from https://support.kraken.com/hc/en-us/articles/203325283-Cryptocurrency-deposit-processing-times on 7 August 2024.


### Feasible values for Peras protocol parameters

Based on the analyses in the [Peras Technical Report #2](https://peras.cardano-scaling.org/docs/reports/tech-report-2#defining-protocol-parameters-values), a reasonable set of default [protocol parameters](#protocol-parameters) for further study, simulation, and discussion is show in the table below. The optimal values for a real-life blockchain would depend somewhat upon external requirements such as balancing settlement time against resisting adversarial behavior at high values of adversarial stake. This set of parameters is focused on the use case of knowing soon whether a block is settled or rolled back; other sets of parameters would be optimal for use cases that reduce the probability of roll-back at the expense of waiting longer for settlement.

| Parameter              | Symbol           | Units   | Value | Rationale                                                            |
| ---------------------- | ---------------- | ------- | ----: | -------------------------------------------------------------------- |
| Round length           | $U$              | slots   |    90 | Settlement/non-settlement in under two minutes.                      |
| Block-selection offset | $L$              | slots   |  > 30 | Several multiples of $\Delta$ to ensure block diffusion.             |
| Certification boost    | $B$              | blocks  |    15 | Negligible probability to roll back boosted block.                   |
| Security parameter     | $k_\text{peras}$ | blocks  |  3150 | Determined by the Praos security parameter and the boost.            |
| Certificate expiration | $A$              | rounds  |   300 | Determined by the Praos security parameter and boost.                |
| Chain-ignorance period | $R$              | rounds  |   300 | Determined by the Praos security parameter, round length, and boost. |
| Cool-down period       | $K$              | rounds  |   780 | Determined by the Praos security parameter, round length and boost.  |
| Committee size         | $n$              | parties |   900 | 1 ppm probability of no honest quorum at 10% adversarial stake.      |
| Quorum size            | $\tau$           | parties |   675 | Three-quarters of committee size.                                    |

In pre-alpha Peras, there is a fundamental trade-off between low latency until a block can receive a boost (favoring a small $L$), and resilience against weak adversaries (<25% stake) trying to disable Peras by forcing into into a cooldown (favoring a high $L$). A *block-selection offset* of $L = 30 \text{\,slots}$ allows plenty of time for blocks to diffuse to voters before a vote occurs, but it provides only little resilience against weak attackers. Future iterations of Peras (involving pre-agreement) allow to resolve this trade-off (at the cost of additional complexity and network bandwidth overhead).

Combining this with a *round length* of $U = 90 \text{\, slots}$ ensures that there is certainty in $U + L = 120 \text{\,slots}$ as to whether a block has been cemented onto the preferred chain by the presence of a certificate for a subsequent block. That certainty of not rolling back certified blocks is provided by a *certification boost* of $B = 15 \text{\,blocks}$ because of the infinitesimal probability of forging that many blocks on a non-preferred fork within the time $U$. Thus, anyone seeing a transaction appearing in a block need wait no more than two minutes to be certain whether the transaction is on the preferred chain (effectively permanently, less than a one in a trillion probability even at 45% adversarial stake) versus having been discarded because of a roll back. Unless the transaction has a stringent time-to-live (TTL) constraint, it can be resubmitted in the first $U - L = 60 \text{\,slots}$ of the current round, or in a subsequent round.

The Praos security parameter $k_\text{praos} = 2160 \text{\,blocks} \approx 43200 \text{\,slots} = 12 \text{\,hours}$ implies a ~17% probability of a longer private adversarial chain at 49% adversarial stake. At that same probability, having to overcome a $B = 15 \text{\,blocks}$ adversarial boost would require $k_\text{peras} \approx 70200 \text{\,slots} = 3510 \text{\,blocks} = 19.5 \text{\,hours}$. This determines the *certificate-expiration time* as $A = \left\lceil (k_\text{peras} - k_\text{praos}) / U \right\rceil = 300 \text{\,rounds}$, the *chain-ignorance period* as $R = A = 300 \text{\,rounds}$, and the *cool-down period* as $K = \left\lceil k_\text{peras} / U \right\rceil = 780 \text{\,rounds}$.

The *committee size* of $n = 900 \text{\,parties}$ corresponds to a one in a million chance of not reaching a quorum if 10% of the parties do not vote for the majority block (either because they are adversarial, offline, didn't receive the block, or chose to vote for a block on a non-preferred fork). This "no quorum" probability is equivalent to one missed quorum in every 1.2 years. The *quorum size* of $\tau = \left\lceil 3 n / 4 \right\rceil = 675 \text{\,parties}$ is computed from this.

At dashboard for computing the probabilistic implications of Peras protocol parameters is a available at https://peras.cardano-scaling.org/dashboard/.


### Attack and mitigation

Three major attack vectors for Peras are (1) adversarial stake, (2) equivocation of votes or blocks, and (3) manipulation of diffusion[^8].

[^8]: https://peras.cardano-scaling.org/docs/reports/tech-report-2#analyses-of-adversarial-scenarios

An adversary with a significant amount of stake has an appreciable likelihood of becoming a member of the Peras voting committees. Unless they possess nearly at least 50% of the total stake, they would not have sufficient adversarial strength to dominate the committee and vote for the block of their choice. (Note that Praos itself would be weakened by an adversary with 50% of the total stake anyway.) If they possessed approximately at least 25% of the stake, they could choose not to vote, with the result that no quorum would be reached and the protocol would enter a cool-down period. Therefore, an adversary with that much stake can negate the benefits of Peras by repeatedly forcing into Praos-like cool downs. The plot below indicates that for modest amounts of adversarial stake and a committee size over 500, it would be extremely difficult for an adversary to force the protocol into a cool-down period.

![Plot of the probability of not having an honest quorum as a function of the adversarial fraction of stake, for various mean sizes of the voting committee.](images/no-honest-quorum.plot.svg)

A malicious party with less than 25% adversarial stake can (as in Praos) create private forks and then reveal them publicly whenever they see fit. In the context of Peras, they might try to build a fork that is longer than the public, preferred fork and then reveal it just $L$ slots before the end of the current round. Since it is then the longest chain, all parties (honest and adversarial) will likely extended it with newly forged blocks. When voting occurs at the start of the next round, that formerly adversarial chain will receive the boost and the honest fork will be abandoned essentially forever. Any transactions on that honest fork will be rolled back. Essentially, a strong enough adversary has some probability of rolling back a public, honest chain's blocks between $L + U$ and $L$ in the past.

![An adversary builds their chain privately and then reveals it in order to receive a boost](images/adversarial-chain-receives-boost-variant.diagram.svg)

The plot below illustrates how shorter rounds and stronger adversaries make such attacks more likely. It is important to note that this attack cannot roll back transactions further than the last previously recorded boost (near $U + L$ in the past). This is why Peras provides an effective *ex post facto* finality: once a boost appears, all of the transactions prior to that are safe. Until a boost appears, blocks are at risk in Peras just as they are in Praos.

![Plot of the probability of the dishonest boost as a function of the adversarial fraction of stake and the round length.](images/adversarial-chain-receives-boost-variant.plot.svg)

Decentralized, stake-based block production and voting systems may be subject to equivocations, where a slot leader or a voting-committee member creates more than one block or casts duplicate votes for different blocks. Protocols' no-equivocation rules ensure that only the first block or vote is acted upon by the node. In the case of Peras, an adversary does not gain power from equivocating votes unless they have near 50% or more of the stake. A scenario where an adversary sends one version of a vote to some honest nodes and a different version to the other honest nodes can be handled by diffusing certificates in this case, such that no permanent disagreements on whether a round was successful arise. Equivocated votes burden the nodes slightly by creating extra network traffic.

Natural events or adversaries might interfere with the diffusion of votes over the network. Peras voting is not affected so long as the network diffuses at least the 75% threshold for reaching a quorum. One quarter of the votes could be lost, dishonest, or withheld. Furthermore, the Peras $L$ parameter ensures that there is plenty of time for honest blocks to diffuse and for them to be in a common prefix of the active forks before voting begins. The Peras $R$ parameter, the number of slots for which certificates (votes) are ignored once a cool-down period starts, guards against an adversary holding onto votes and then releasing them to try to revert an already-begun cool-down period.

In no way does Peras weaken any of the security guarantees provided by Praos or Genesis. Under strongly adversarial conditions, where an adversary can trigger a Peras voting cool-down period, the protocol in essence reverts to the Praos (or Genesis) protocol, but for a duration somewhat longer than the Praos security parameter. Otherwise, settlement occurs at the blocks that Peras voting has boosted. The Peras protocol parameters can be tuned to adjust the settlement time or the non-settlement probabilities. Some stakeholder use cases might prefer shorter settlement times but with a higher probability of retries, or vice versa.


### Resource requirements

The impact of Peras upon nodes falls into four categories: [network](#network), [CPU](#cpu), [memory](#memory), and [storage](#persistent-storage). The [Peras Technical Report #2](https://peras.cardano-scaling.org/docs/reports/tech-report-2#resources-impact-of-peras) provides supporting data, simulations, and discussion. The discussion in the following sub-sections summarizes evidence that the CPU time required to construct and verify votes and certificates is much smaller than the duration of a voting round. Similarly, the memory needed to cache votes and certificates and the disk space needed to persist certificates is trivial compared to the memory needed for the UTXO set and the disk needed for the blocks. On the networking side, ΔQ studies demonstrate that diffusion of Peras votes and certificates consumes minimal bandwidth and would not interfere with other node operations such as memory-pool and block diffusion. However, diffusion of votes and certificates across a network will still have a noticeable impact on the volume of data transfer, in the order of 20%, which might translate to increased operating costs for nodes deployed in cloud providers.

#### Network

For a fully synced nodes, the impact of Peras on network traffic is modest:

- For votes, assuming $U \approx 100$, a committee size of 2000 SPOs, a single vote size of 700 bytes, means we will be adding an average of 14 kB/s to the expected traffic to each node,
- For certificates, assuming an average of 50 kB size consistent with the size of current Mithril certificates, means an negligible increase of 0.5 kB/s on average. Note that a node will download either votes or certificate for a given round, but never both so these numbers are not cumulative. An exception to this is the needless adversarial inclusion of certificates into their blocks when Peras is *not* in a cooldown period.

A fully non-synced node will have to catch-up with the _tip_ of the chain and therefore download all relevant blocks _and_ certificates. At 50% load (current monthly load is 34% as of this writing[^9]), the chain produces a 45 kB block every 20 s on average. Below are rough estimates of the amount of data a node would have to download (and store) for synchronizing, depending on how long it has been offline:

| Time offline | Blocks (GB) | Certificates (GB) |
| ------------ | ----------- | ----------------- |
| 1 month      | 5.56        | 1.23              |
| 3 months     | 16.68       | 3.69              |
| 6 months     | 33.36       | 7.38              |

The Peras [Technical Report #1](https://peras.cardano-scaling.org/docs/reports/tech-report-1#network-performance-analysis) and [Technical Report #2](https://peras.cardano-scaling.org/docs/reports/tech-report-2#vote-diffusion) document network performance analysis for vote diffusion in Peras, using a ΔQ model[^10] to evaluate the expected delay to reach _quorum_. The plot below shows the diffusion of a single vote (red) and the receipt and verification of a quorum of votes (green for parallel verification of votes and blue for sequential verification of votes). The graph demonstrates that vote diffusion should be non-problematic, with a quorum expected to be reached in under 1 second most of the time to compare with a round length of about 2 minutes.

![Vote diffusion](images/vote-diffusion.svg)

#### Cost

Network usage and the infrastructure for the Cardano node translates to monthly costs. This can be estimated for Peras from published network pricing for a few major Cloud and well-known VPS providers, based on the share of stakes each provider is reported to support[^11], and some typical traffic pattern as exemplified by the following plot (data courtesy of Markus Gufler).

![Typical node inbound & outbound traffic](images/node-average-traffic.svg)

The next table compares the cost (in US\$/month) for different outgoing data transfer volumes expressed as bytes/seconds, on similar VMs tailored to cardano-node's hardware requirements[^12] (32 GB RAM, 4+ Cores, 500 GB+ SSD disk). The base cost of the VM is added to the network cost to yield total costs depending on transfer rate[^13]. For an AWS hosted SPO, which represent about 20% of the SPOs, a 14 kB/s increase in traffic would lead to a cost increase of **\$3.8/mo** (34 GB times \$0.11/GB). This represents an average across the whole network: depending on the source of the vote and its diffusion pattern, some nodes might need to send a vote to more than one downstream peer which will increase their traffic, while other nodes might end up not needing to send a single vote to their own peers. Any single node in the network is expected to download each vote _at most_ once.

| Provider     |     VM | 50 kB/s | 125 kB/s | 250 kB/s |
| ------------ | -----: | ------: | -------: | -------: |
| DigitalOcean |   $188 |    $188 |     $188 |     $188 |
| Google Cloud |   $200 |    $214 |     $234 |     $268 |
| AWS          | ? $150 |    $161 |     $178 |     $206 |
| Azure        |   $175 |    $186 |     $202 |     $230 |
| OVH          |    $70 |     $70 |      $70 |      $70 |
| Hetzner      |    $32 |     $32 |      $32 |      $32 |

At present, this CIP does not include a design for a Peras-specific rewards system to e.g. directly incentivize voting participation. Any such design (where rewards are distributed on-chain) would require including certificates on-chain, therefore itself increasing the resource impact of Peras.

[^9]: https://cexplorer.io/usage

[^10]: https://iohk.io/en/research/library/papers/mind-your-outcomes-the-dqsd-paradigm-for-quality-centric-systems-development-and-its-application-to-a-blockchain-case-study/

[^11]: https://pooltool.io/networkhealth

[^12]: https://developers.cardano.org/docs/operate-a-stake-pool/hardware-requirements/

[^13]: The AWS cost is quite hard to estimate up-front. The $150 base price is a rough average of various instances options in the target range. Google, AWS and Azure prices are based on 100% uptime and at least 1-year reservation for discounts. Cloud providers only charge _outgoing_ network traffic. The actual cost per GB depends on the destination, this table assumes all outbound traffic will be targeted outside of the provider which obviously won't be true, so it should be treated as an upper bound.

#### Persistent storage

Under similar assumptions, we can estimate the storage requirements entailed by Peras: ignoring the impact of cool-down periods, which last for a period at least as long as $k$ blocks, the requirement to store certificates for every round increases node's storage by about **20%**. Votes are expected to be kept in memory so their impact on storage will be null.

Additionally, again ignoring cooldown periods, adversaries can include certificates in blocks they forge. Implementations could implement optimizations to avoid storing certificates twice in that case.

#### CPU

The [Peras Technical Report #2](https://peras.cardano-scaling.org/docs/reports/tech-report-2#votes--certificates) provides some models and benchmarks for votes generation, votes verification, certificates proving and certificates verification, and votes diffusion. The CPU requirements will of course be dependent on the details of the Certificates' scheme used but should be negligible compared to the duration of a voting round. Moreover, as already noted, votes and certificates construction are kept out of the critical path of block forging and diffusion hence they should not impact the node's performance.

#### Memory

A node is expected to need to keep the following data in memory:

- *Votes for the latest voting round:* For a committee size of 1000 and individual vote size of 700 bytes, that amounts to 700 kB.
- *Cached certificates for voting rounds up to settlement depth, for fast delivery to downstream nodes:* With a boost of 10/certificate, settlement depth would be in the order of 216 blocks, or 4320 seconds, which represent about 10 rounds of 400 slots. Each certificate weighing 50 kB, that is another 500 kB of data a node would need to cache in memory.

Thus, Peras should not have any significant impact on the memory requirements of a node.


## Path to active

- [ ] Clear evidence of stakeholder use cases that require the fast *ex post facto* settlement that Peras provides.


### Acceptance criteria

- [ ] The revised `cardano-node` implementations pass the node-level conformance test suites.
- [ ] Audit.
- [ ] Successful operation in testnet environments.
- [ ] Community agreement on the settings for the Peras protocol parameters.
- [ ] The upcoming CIP that establishes a *Consensus* category for CIPs may define additional acceptance criteria.


### Implementation plan

- [ ] Detailed node-level (as opposed to this protocol-level) specification.
- [ ] Develop node-level conformance test suite.
- Consider developing a "quick and dirty" implementation for large scale experiments.
- Coordinate with related activities on other protocol enhancements.
- Compatibility between Peras, Leios, and Genesis.
- Common design and implementation for certificates, voting, and related key registration: Mithril, Peras, Leios, and partner chains.
- Triage by intersect Core Infrastructure and Consensus functions.

The following diagram summarizes a possible architecture for Peras highlighting its interactions with other components of the node.

![Peras High-level Architecture](images/peras-architecture.png)

Peras has no impact in the existing block diffusion process, and imposes no changes to the block header structure. The block body structure needs to accommodate for a certificate, but only on the rare occasions when the chain enters or leaves a cool-down period.

The consensus *preferred chain* selection algorithm must be modified to be aware of the existence of a _quorum_ when computing the _weight_ of a possible chain, which is manifested by a _certificate_ from the Peras component. Consensus will need to maintain or query a list of valid certificates (e.g., similar to _volatile_ blocks) as they are received or produced. The chain selection and headers diffusion is not dependent on individual votes.

The Peras component can be treated as another *chain follower* to which new blocks and rollbacks are reported. The Peras component will also need to be able to retrieve current *stake distribution*. Furthermore, it needs to access to VRF and KES keys for voting: see [_Votes & Certificates on Cardano_](https://github.com/cardano-foundation/CIPs/pull/870) for details. Dedicated long term storage will be needed for certificates.

The networking layer will need to accommodate two new mini-protocols for vote and certificate diffusion. In general, there are opportunities for shared voting, certificate, and diffusion components that handle Peras, Leios, and Mithril uniformly.

Prior to full-scale Peras implementation, it would be possible to develop a standalone Peras prototype built upon the existing node, where the node is modified to receive chain-weight information from Peras, which acts as a chain follower and communicates with the node via an ad-hoc mechanism. Such a prototype would run on a testnet where the measurements and experiments could be conducted.


## Versioning

This document describes the *pre-alpha* version of the Peras protocol. We anticipate a subsequent, separate CIP for an *alpha* or *beta* version of the protocol. That version will add strong guarantees for block selection prior to the voting process and will constitute a layer built upon this pre-alpha version.

This machine-readable specification is pinned to [Agda 2.6.4.3](https://github.com/agda/agda/tree/v2.6.4.3), the [Agda Standard Library 2.0](https://github.com/agda/agda-stdlib/tree/v2.0), and the [IOG Agda Prelude v0.1.0.0](https://github.com/input-output-hk/iog-agda-prelude/releases/tag/v0.1.0.0) via the file [flake.lock](./flake.lock). Thus it is fully and deterministically reproducible. Future revisions of the Agda code in this specification must be type checked against either those pinned versions or an upgraded future piinning of Agda and those libraries.


## References

- [Peras web site](https://peras.cardano-scaling.org/)
- [Discord channel for Peras](https://discord.gg/9EgySPJk)
- [Peras Technical Report #1](https://peras.cardano-scaling.org/docs/reports/tech-report-1)
- [Peras Technical Report #2](https://peras.cardano-scaling.org/docs/reports/tech-report-2)
- [Software repository for Peras design](https://github.com/input-output-hk/peras-design/)
- [Online simulator for the Peras protocol](https://peras-simulation.cardano-scaling.org/)
- [Scaling blockchain protocols: a research-based approach](https://www.youtube.com/watch?v=Czmg9WmSCcI)
- [Consensus Redux: Distributed Ledgers in the Face of Adversarial Supremacy](https://eprint.iacr.org/2020/1021.pdf)
- [Practical Settlement Bounds for Longest-Chain Consensus](https://eprint.iacr.org/2022/1571.pdf)


## Appendix


### Typechecking this specification

With [Nix](https://nix.dev/install-nix.html) installed and [Nix flakes](https://nixos.wiki/wiki/Flakes) enabled, one just needs to execute the following command to run the Agda typechecker on this file:

```bash
nix build --no-link
```

Additionally, one can also open a Nix development shell and view, edit, or compile this specification in Emacs.

```bash
nix develop

emacs README.lagda.md
```

The first time one runs `agda-mode` in Emacs[^14], one might have to execute `agda-mode setup`, which adds lines like the following to the local configuration file `$HOME/.emacs`:

```lisp
(load-file (let ((coding-system-for-read 'utf-8))
                (shell-command-to-string "agda-mode locate")))

;; auto-load agda-mode for .agda and .lagda.md
(setq auto-mode-alist
   (append
     '(("\\.agda\\'" . agda2-mode)
       ("\\.lagda.md\\'" . agda2-mode))
     auto-mode-alist))
```

Finally, the Nix flake [flake.nix](./flake.nix) contains the derivation for building this specification, and its lock file [flake.lock](./flake.lock) records the commit hashes of all dependencies, thus enabling a fully reproducible set of dependencies.

[^14]: https://agda.readthedocs.io/en/v2.6.4.3/tools/emacs-mode.html


## Copyright

This CIP is licensed under [Apache-2.0](http://www.apache.org/licenses/LICENSE-2.0).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0141/README.md
---

- --
CIP: 141
Title: Web-Wallet Bridge - Plutus wallets
Status: Proposed
Category: Wallets
Authors:
- Leo
Implementors: BroClan
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/798
Created: 2023-10-12
License: CC-BY-4.0
- --

## Abstract

This document describes a CIP-30 extension allowing webpages (i.e., DApps) to interface with Cardano Plutus wallets. This interface is a work in progress, and new versions are expected to be included as the ecosystem evolves.

## Motivation: why is this CIP necessary?

Plutus wallets is a class of wallets where the holding address does not correspond to a public key, but to a script. This enables more complex validation logic to be implemented in the wallet, allowing for more complex spending conditions to be enforced.

Examples of Plutus wallets include :
- Updatable Multi-signature wallets
- Subscription wallets (wallet that allow for periodic payments to be made)
- Maltifactor authetication wallets
- Semi-custodial wallets
- Tokenized wallets
- EstatePlanning wallets
- wallets controlled by non-cardano addresses

In order to facilitate future DApp development, we will need a way for DApps to communicate with Plutus wallets. Given the unique complexities of Plutus script-based addresses, special provisions need to be made to make the connector compatible with them.

### Rationale for the required data

- Script descriptor: Any transaction consuming a UTXO from a Plutus-based address must attach the corresponding script.
- `ScriptRequirement` list:
- DApps need to know the scriptContext under which the transaction will be validated.
- DApps need to know the collateral donor to attach the collateral to the transaction.
- DApps need to know the `Datum` of the UTXOs that it will be consuming.
- DApps need to know the `Redeemers` that will be used in the transaction.
- Change datum
- DApps need to know the `Datum` that will be used as the change output for the transaction. This is mandatory for wallets based on Plutus v2 and before, as the change output must contain a datum to be valid and spendable.

## Specification

### Versioning

To facilitate future updates, the API will be versioned. Newer version of the API have to be compatible with the previous versions.

The current version of the API is 2.0, the major version number is derived from the version of Plutus that the API implements, and the minor version number is used for incremental updates within the same version of Plutus.

### Data Types

#### KeyHash

A hex-encoded string of the corresponding bytes. This represents the hash of the public key used to sign transactions.

```ts
type KeyHash = String
```

#### ScriptRequirements

Script requirements encapsulate all the possible requirements for a transaction to be valid. It includes the following fields:

- collateral: The input that can be used as collateral for the transaction.
- inputs: The list of inputs that must be used as inputs for the transaction.
- reference_inputs: The list of inputs that must be used as reference inputs for the transaction.
- outputs: The list of outputs that must be included in the transaction.
- mint: The amount of tokens that must be minted/burned in the transaction.
- certificates: The list of certificates that must be included in the transaction.
- withdrawals: The list of withdrawals that must be included in the transaction.
- validity_range: The validity range that the transaction must be valid in.
- signatories: The list of signatories that must sign the transaction.
- redeemers: The list of redeemers required for the transaction, if the type is secret, the secretId will be provided to retrieve the secret from the wallet, if the type is signature, the redeemer will be a pair of [DataSpec, Signature].
- datums: The list of datums that should be used in the transaction for the change outputs, the wallet will use the first datum in the list that is valid for the current transaction, if more than one datum is valid, the wallet will use separate datums for each change output.


```ts
type ScriptRequirement = {
  collateral?: cbor<transaction_unspent_output>,
  inputs?: List<cbor<transaction_unspent_output>>,
  reference_inputs?: List<cbor<transaction_unspent_output>>,
  outputs?: List<transaction_output>,
  mint?: Value,
  certificates?: List<Certificate>,
  withdrawals?: Dict<StakeCredential, Int>,
  validity_range?: ValidityRange,
  signatories?: List<KeyHash>,
  redeemers?: Dict<ScriptPurpose, Redeemer>,
  datums?: List<Hash<Blake2b_256, Data>, Data>
}
```

```ts
type Redeemer =
  | { type: RedeemerType.Data, redeemer: string }
  | { type: RedeemerType.Secret, redeemer: SecretId }
  | { type: RedeemerType.Signature, redeemer: [DataSpec, Primitive] };

enum RedeemerType {
  Data = 1,
  Secret = 2,
  Signature = 3,
}

type DataSpec = string;
type Primitive = string;
```

#### ValidityRange

```ts
type ValidityRange = {
  valid_before?: PosixTime,
  valid_after?: PosixTime,
}
```

#### Value
```ts
type Value = {
    amount: Coin,
    asset: String
}
```

### Aditional Error Types

#### CompletedTxError

```ts
CompletedTxErrorCode = {
	NotFound: 1,
	NotReady: 2
}
```

- NotFound - The transaction with the given id was not found.
- NotReady - The transaction with the given id is not ready yet.

### V2 Additional API Endpoints

#### `api.cip141.getScriptRequirements`: Promise<ScriptRequirement[]>

Errors: `APIError`

Returns a list of `ScriptRequirement` that will be used to validate any transaction sent to the wallet. Every field in the `ScriptRequirement` object is optional, and the wallet should only include the fields that are relevant to the current transaction.

For wallets with multiple spend conditions, separate entries in the list should be used to represent each spend condition. Wallet providers should implement UX to allow users to order the list of `ScriptRequirement` from most to least preferred. DApps should use the first entry in the list that is valid for the current transaction or select one based on the logic of their use-case.

#### `api.cip141getScript()`: Promise[<number<plutusVersion>>,<CBOR<plutusScript>>]

Errors: `APIError`

Returns the CBOR-encoded Plutus script that controls this wallet.

#### `api.cip141.submitUnsignedTx(tx: CBOR<unsignedTransaction>)`: Promise<hash32>

Errors: `APIError`, `TxError`

Submits a transaction to the wallet for signing. The wallet should check that the transaction is valid, gather the required signatures, compose the finalized transaction, and submit the transaction to the network. If the transaction is valid and the wallet is able to sign it, the wallet should return the transaction hash. If the transaction is invalid or the wallet is unable to sign it, the wallet should throw a `TxError` with the appropriate error code. The wallet should not submit the transaction to the network if it is invalid or the wallet is unable to sign it.

If the transaction contains hidden metadata, the wallet should not submit the transaction when it is ready, but return it to the DApp when the DApp calls the `getCompletedTx` function.

It is expected that this will be the endpoint used by all wallets that require multiple signatures to sign a transaction.

#### `api.cip141.getCompletedTx(txId: hash32)`: Promise[<CBOR<transaction>,CBOR<transaction_witness_set>>]

Errors: `APIError`, `CompletedTxError`

If the transaction is not ready, the wallet should throw a `CompletedTxError` with the appropriate error code. If the transaction is ready, the wallet should return the CBOR-encoded transaction and the signatures.

#### `api.cip141.getSecret(secretId: String)`: Promise<String>

Errors: `APIError`

Returns the secret associated with the given secretId. The secret should be returned as a hex-encoded string.

Wallets should request authorization from the user before returning the secret to the DApp.

#### `api.cip141.signRedeemer(data: Data, primitive : encriptionPrimitive  )`: Promise<CBOR<redeemer>>

Errors: `APIError`
Returns the CBOR-encoded redeemer.

### Altered API endpoints

#### `api.signTx(tx: CBOR<transaction>, partialSign: bool = false)`: Promise<CBOR<transaction_witness_set>>

This endpoint will now optionally return an error if the smart Wallet is a multiparty schema and signatures need to be gathered from multiple parties asynchronously.

#### `api.signData(addr: Address, payload: Bytes)`: Promise<DataSignature>

Plutus contracts cannot sign data, only validate transactions. This endpoint will be disabled when connecting to a wallet using this extension.

## Rationale: how does this CIP achieve its goals?

By altering the API endpoints and adding new ones, we can provide the necessary information for DApps to interact with Plutus wallets. This will allow any developer to integrate smart-wallets into their DApps, while providing a consistent interface for all wallets.

## Path to Active

### Acceptance Criteria

- [ ] The interface is implemented and supported by three different wallet providers.
- [ ] The interface is used by three different DApps to interact with wallet providers.

### Implementation Plan

- [x] Provide some reference implementation of wallet providers
- [x] [leo42/BroClanWallet](https://github.com/leo42/BroClanWallet)

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0142/README.md
---

- --
CIP: 142
Title: Web-Wallet Bridge - Network Determination
Status: Proposed
Category: Wallets
Authors:
- Steven Johnson <steven.johnson@iohk.io>
- Nathan Bogale <nathan.bogale@iohk.io>
Implementors:
- Lace <https://www.lace.io/>
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/209
- https://github.com/cardano-foundation/CIPs/pull/323
- https://github.com/cardano-foundation/CIPs/pull/960
- https://github.com/cardano-foundation/CIPs/pull/972
Created: 2024-01-17
License: CC-BY-4.0
- --

## Abstract

This CIP extends CIP-0030 to provide functionality for dApps to determine the specific network magic number of the connected Cardano network. While CIP-0030's `getNetworkId()` allows distinguishing between mainnet and testnet, this extension enables dApps to identify specific test networks through their magic numbers.

## Motivation: why is this CIP necessary?

Currently, CIP-0030 only provides a way to distinguish between mainnet (1) and testnet (0) through the `getNetworkId()` function. However, there are multiple test networks in the Cardano ecosystem (preview, preprod, etc.), each with its own magic number. dApps often need to know the specific test network they're connected to for proper configuration and interaction. This extension addresses this limitation by providing access to the network magic number.

## Specification

### Extension Identifier

This extension uses the following identifier:
```ts
{ "cip": 142 }
```

### API Extension

When this extension is enabled, the following function is added to the API under the `cip-142` namespace:

#### `api.cip142.getNetworkMagic(): Promise<number>`

Errors: `APIError`

Returns the magic number of the currently connected network. For example:
- Mainnet: 764824073
- Preview Testnet: 2
- Preprod Testnet: 1
- Custom Testnet: (other values)

This function will return the same value unless the connected account changes networks.

Example usage:
```typescript
const api = await window.cardano.lace.enable({
  extensions: [{ cip: 142 }]
});

const magic = await api.cip142.getNetworkMagic();
console.log(`Connected to network with magic number: ${magic}`);
```

### Error Handling

The function uses the existing `APIError` type from CIP-0030 with the following error codes:
- `InvalidRequest` (-1): If the request is malformed
- `InternalError` (-2): If an error occurs while retrieving the magic number
- `Refused` (-3): If access to the magic number is refused
- `AccountChange` (-4): If the account has changed

## Rationale: how does this CIP achieve its goals?

### Why a New Extension?

While CIP-0030's `getNetworkId()` provides basic network identification, the growing Cardano ecosystem requires more specific network identification, especially for development and testing purposes. This extension:

1. Maintains backward compatibility by not modifying existing CIP-0030 functionality
2. Uses explicit namespacing to avoid conflicts
3. Provides a simple, focused solution to a specific need

### Design Decisions

1. **Explicit Namespacing**: The extension uses the `cip-142` namespace to clearly separate its functionality from the base CIP-0030 API.
2. **Promise-based API**: Follows CIP-0030's pattern of returning Promises for consistency.
3. **Reuse of Error Types**: Leverages existing error types from CIP-0030 to maintain consistency.

## Path to Active

### Acceptance Criteria

- [ ] Implementation by at least three wallet providers
- [x] Eternl
- [ ] Lace
- [ ] Implementation by at least three web app providers
- [ ] Implementation by at one serialisation library or SDK
- [ ] No reported conflicts with other CIP-0030 extensions

### Implementation Plan

1. **Reach out to the Lace wallet team/architects**
- Validate technical feasibility of proposed changes
- Identify priorities for this implementation
- Identify potential integration challenges
2. **Involve ops and products team to drive the plan further**
- First implementation on the Catalyst playground
- Create a plan, validate and collect feedback and suggestions
3. **Implement the CIP-0142 extension in the Lace wallet**
- Implement and use the CIP-0142 extension in the Catalyst pla
- Validate against all specified acceptance criteria
4. **Reach out to other Cardano wallet providers**
- Introduce the CIP-0142 extension to other wallet providers (Nami)
- Collect feedback and suggestions
- Validate against acceptance criteria

- *Note:** This implementation plan is subject to iterative refinement based on ongoing technical assessments and stakeholder feedback.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0143/README.md
---

- --
CIP: 143
Title: Interoperable Programmable Tokens
Category: Tokens
Status: Inactive (incorporated into candidate CIP-0113)
Authors:
- Philip DiSarro <philipdisarro@gmail.com>
Implementors: []
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/444
- https://github.com/cardano-foundation/CIPs/pull/944
Solution-To: CPS-0003
Created: 2024-12-3
License: Apache-2.0
- --

## Abstract

This CIP proposes a robust framework for the issuance of interoperable programmable tokens on Cardano. Unlike all its predecessors, this framework allows these tokens to be used in existing dApps, and does not require dApps to be developed specifically for these tokens.

## Motivation: why is this CIP necessary?

This CIP proposes a solution to [CPS-0003 (Smart Tokens)](../CPS-0003).

With this framework we achieve programmability over the transfer of tokens (meta-tokens) and their lifecycle without sacrificing composability with existing dApps.

Answers to the open questions of CPS-0003:

1) How to support wallets to start supporting validators?

For every user address you can easily derive the equivalent smart token address. So to obtain a users total wallet balance including their programmable tokens, the wallet can query their programmable token address and their normal address and combine the two results.

2) How would wallets know how to interact with these tokens? - smart contract registry?

For any given programmable token transfer, wallets can easily determine which stake script needs to be invoked (that contains the transfer logic) directly from the chain (no offchain registry required) by querying the original minting tx.

3) Is there a possibility to have a transition period, so users won't have their funds blocked until the wallets start supporting smart tokens?

This framework will not cause any funds to be blocked. Transfers of non-programmable tokens will remain unaffected.

4) Can this be achieved without a hard fork?

Yes, this framework has been possible since the Chang hard fork.

5) How to make validator scripts generic enough without impacting costs significantly?

The impact that the required script executions have on the cost of transactions is negligible.


## Specification

### Programmable Logic Minting Policy

The `mkProgrammableLogicMinting` smart contract is responsible for the minting and issuance of new programmable tokens. Additionally, the contract ensures that an entry is added to the onchain directory linked list that permenantly associates the issued programmable token with its transfer script logic and issuer script logic.

```haskell
mkProgrammableLogicMinting ::
    Credential -- Minting logic credential
- > ScriptContext
- > ()
mkProgrammableLogicMinting mintingLogicCred = ...
```

The contract accepts a single parameter, *mintingLogicCred* that defines the specialized the minting logic for the programmable token. `mintingLogicCred` must be a `ScriptCredential` of a [withdraw-zero rewarding script](https://github.com/Anastasia-Labs/design-patterns/blob/main/stake-validator/STAKE-VALIDATOR-TRICK.md).

#### Supported Actions

The minting policy supports two actions:

##### Token Registration (`PRegisterPToken`)

- Enforces that an immutable entry for the programmable token is inserted into the programmable token directory.
- The directory entry associates the programmable token with a transfer logic script and a issuer logic script.
- The transfer logic script is a withdraw-zero script that must be invoked in every user transaction that spends the programmable token.
- The issuer logic script is a withdraw-zero script that must be invoked in every permissioned transaction that spends the programmable token.
- A permissioned action is an action that can only be performed by the token issuer, that bypasses the normal transfer logic of the system. An example is seizure / clawbacks which allow the issuer of a programmable token to reclaim the token from any UTxO at their discretion.
- Enforces that only a single new type of programmable token is issued in the transaction.
- Enforces that all minted programmable tokens must be sent to the `programmableLogicBase` contract.
- Enforces that the `mintingLogicCred` script is executed in the transaction see [the withdraw-zero trick](https://github.com/Anastasia-Labs/design-patterns/blob/main/stake-validator/STAKE-VALIDATOR-TRICK.md)

##### Token Minting/Burning (`PMintPToken`)

- Responsible for validating the minting / burning of programmable tokens.
- If this action is used to mint programmable tokens, then it enforces that all minted programmable tokens must be sent to the `programmableLogicBase` contract.
- Enforces that the `mintingLogicCred` script is executed in the transaction see [the withdraw-zero trick](https://github.com/Anastasia-Labs/design-patterns/blob/main/stake-validator/STAKE-VALIDATOR-TRICK.md)

### Programmable Logic Base Script

The `mkProgrammableLogicBase` is a spending script that manages the ownership and transfer of programmable tokens. The `mkProgrammableLogicBase` script forwards its logic to the `mkProgrammableLogicGlobal` via [the withdraw-zero trick](https://github.com/Anastasia-Labs/design-patterns/blob/main/stake-validator/STAKE-VALIDATOR-TRICK.md). The `mkProgrammableLogicGlobal` script is responsible for ensuring that programmable tokens must always remain within the `mkProgrammableLogicBase` script and that the associated `transferLogicScript` is invoked (or  in the case of a permissioned action, that the associated `issuerLogicScript` is invoked) for each unique programmable token spent.

#### Ownership Mechanism

The system enforces that programmable tokens must all reside at the `mkProgrammableLogicBase` script. As such the payment credential of any UTxO that contains programmable tokens will always be the `mkProgrammableLogicBase` script credential. This system leverages staking credentials to identify and manage ownership. This approach ensures that programmable tokens remain secure and can only be transferred by their rightful owners. The owner of a UTxO at the `mkProgrammableLogicBase` script is determined by its staking credential. If the UTxO's staking credential is a public key credential, then any transaction that spends that UTxO must be signed by the public key; the system refers to all such required signatures in a transaction as the transaction's required public key witnesses. If the UTxO's staking credential is a script credential then the associated script must be invoked in the transaction via [the withdraw-zero trick](https://github.com/Anastasia-Labs/design-patterns/blob/main/stake-validator/STAKE-VALIDATOR-TRICK.md); we refer to all such required scripts as the transaction's required script witnesses.

#### Supported Actions

The `mkProgrammableLogicGlobal` (and thus the `mkProgrammableLogicBase`) supports two actions:

```haskell
data ProgrammableLogicGlobalRedeemer
  = PTransferAct
      { proofs :: [PTokenProof]
      }
  | PSeizeAct
      { seizeInputIdx :: Integer
      , seizeOutputIdx :: Integer
      , directoryNodeIdx :: Integer
      }
```

Where `PTokenProof` is defined as,
```haskell
data PTokenProof
  = PTokenExists { nodeIdx :: Integer }
  | PTokenDoesNotExist { nodeIdx :: Integer }
```

##### Transfer Action (`PTransferAct`)
- Traverse the transaction inputs and compute the sum of all value spent from the `mkProgrammableLogicBase` script, which we refer to as the `totalValueSpent`.
- Enforce that the required witness for each input is present in the transaction
- If the staking credential of the input is a payment credential then the public key hash must be present in the transaction signatories.
- If the staking credential of the input is a script credential then the associated script must be invoked in the transaction.
- Simultaneously traverse the currency symbols in `totalValueSpent` and the `PTransferAct` proof list and compute the `totalProgrammableValueSpent` (value consisting of only programmable tokens).
- If a proof for a given currency symbol, `currentSymbol`, is `PTokenExists { nodeIdx ... }` then the reference input indexed by `nodeIdx` must satisfy the following:
- The reference input must be a valid programmable token directory node (i.e. it must contain a token with the `directoryNode` currency symbol).
- It must be the correct directory node for `currentSymbol` (i.e. the currency symbol must be equal to the node's key).
- The directory node's transfer logic script must be executed in the transaction.
- Together, these conditions enforce that the `currentSymbol` is indeed a programmable token and that the `transferLogicScript` associated with the programmable token is executed.
- If a proof for a given currency symbol, `currentSymbol`, is `PTokenDoesNotExist { nodeIdx ... }` then the reference input indexed by `nodeIdx` must satisfy the following:
- The reference input must be a valid programmable token directory node (i.e. it must contain a token with the `directoryNode` currency symbol).
- The directory node's `key` must be lexographically less than the `currentSymbol` and the directory node's `next` must be lexographically greater than the `currentSymbol`.
- Together, these conditions enforce that the `currentSymbol` is not a programmable token.
- Traverse the transaction outputs and compute `totalProgrammableValueProduced`, the sum of all value produced to the `mkProgrammableLogicBase` script.
- Enforce that the `totalProgrammableValueProduced` is greater than or equal to the `totalProgrammableValueSpent`.
- This insures that programmable tokens always remain at the `mkProgrammableLogicBase` script.

##### Seize Action (`PSeizeAct`)
- Enforce that the transaction input indexed by `seizeInputIdx`, which we refer to as the `programmableTokenInput`, is a UTxO from the `mkProgrammableLogicBase` script.
- Enforce that there is only a single input from the `mkProgrammableLogicBase` script in the transaction.
- Enforce that the reference input indexed by `directoryNodeIdx`, which we refer to as the `indexedDirectoryNode`, is a valid directory node (i.e. it must contain a token with the `directoryNode` currency symbol).
- Enforce that the `issuerLogicScript` in the directory node is invoked in the transaction.
- Enforce the the transaction output indexed by `seizeOutputIdx`, which we refer to as the `programmableTokenOutput`, satisfies the following criteria:
- The address of the `programmableTokenOutput` equal to the address of the `programmableTokenInput`.
- The value in the `programmableTokenOutput` is equal to the value in the `programmableTokenInput` after filtering the currency symbol of the programmable token associated with the `indexedDirectoryNode` (the node's `key`).
- The datum in the `programmableTokenOutput` is equal to the datum in the `programmableTokenInput`
- Together these conditions enforce that the permissioned actions of a programmable token are limited in scope such that they can only be used to transfer the associated programmable token from UTxOs, and cannot be used to modify those UTxOs in any other manner.

The system guarantees that each programmable token must have a transfer logic script (located in the associated directory node in the directory linked list). The transfer logic script for a programmable token is the smart contract that must be executed in every transaction that spends the programmable token. For example to have a stable coin that supports freezing / arrestability this script might require a non-membership merkle proof in a blacklist. This must be a staking script (or an observer script once CIP-112 is implemented), see
[the withdraw-zero trick](https://github.com/Anastasia-Labs/design-patterns/blob/main/stake-validator/STAKE-VALIDATOR-TRICK.md) for an explanation.

This framework doesn't require custom indexers to find user / script UTxOs, instead they can be easily queried by all existing indexers / wallets. For example, to obtain all the smart tokens in a user's wallet you can construct a franken-address where the payment credential is the `mkProgrammableLogicBase` credential and the staking credential is the user's public key credential and then query this address (in the same way you would query any normal address).

## Rationale: how does this CIP achieve its goals?

The existing proposed frameworks for programmable tokens are:
1. Ethereum Account Abstraction (emulate Ethereum accounts via Plutus contract data)
2. Smart Tokens - CIP 113
3. Arrestable assets - CIP 125

The issue with all of the above is that they are not interoperable with existing dApps. Thus entirely new dApps protocols would need to be developed specifically for transacting with the proposed smart tokens. Furthermore, in some of the above CIPs, each smart-token enabled dApp must be thoroughly audited to ensure that it is a closed system (ie. there is no way for tokens to be smuggled to non-compliant addresses) and thus there needs to be a permissioned whitelist of which addresses are compliant.

Additionally, these proposed solutions attempt to maintain interoperability:
1. CIP 68 Smart Tokens
2. Transfer Scripts (ledger changes required)

The CIP 68 approach allows the tokens to be used anywhere but if the contract does not obey the logic then the token can be invalidated (ie revoked). So if you put it into a liquidity pool and the batcher does not obey the token logic then your tokens can be revoked and it wouldn't be your fault.
The transfer scripts proposal is to introduce a new Plutus script type that would need to be invoked in any transaction that spends a smart token (identified by the policy id). The issue with this approach is that it introduces a huge potential for vulnerabilities and exploits to the existing ledger and these vulnerabilities are responsible for the vast majority of exploits and centralization risks in ecosystems with fully programmable tokens (ie Ethereum). Regardless, this means that these tokens could not be used on existing dApps, since they would require a new Plutus version with new features, and the ledger does not allow contracts that use new features to co-exist in transactions with contract from previous versions where those features did not exist.

Furthermore, all of the aforementioned proposals would require custom indexers and infrastructure to locate a user's (or smart contract's) programmable tokens. You could not simply query an address, instead you would need to query UTxOs from the contracts and check their datum / value (depending on the CIP) to determine the owner.

The above factors motivated the design of this framework. Some of the core unique properties of this framework include:
1. Each user gets their own programmable token address that can be easily derived their credentials.
2. Interoperability with existing dApps
3. Introduces no new risk / vulnerabilities into existing protocols
4. Doesn't require changes to the ledger
5. Smart tokens cannot be revoked by dApps that fail to follow the standard (unlike the CIP 68 case in which they can)
6. Very low effort (relative to the existing proposals) to implement and get it to production / mainnet adoption
7. Completely permissionless and natively interoperable with other smart tokens. IE anyone can mint their own smart tokens with their own custom logic and the correctness of their behavior will be enforced

## Path to Active

### Acceptance Criteria
- [x] Issuance of at-least one smart token via the proposed framework on the following networks:
- [x] 1. Preview testnet
- [x] 2. [Mainnet](https://cexplorer.io/asset/asset1dk6zekxuyuc6up56q7nkd7084k609k3gfl27n8/mint#data)
- [x] End-to-end tests of programmable token logic including arrestability, transfer fees, and blacklisting.
- [x] Finally, a widely adopted wallet that can read and display programmable token balances to users and allow the user to conduct transfers of such tokens.

### Implementation Plan
- [x] Implement the contracts detailed in the specification. Done [here](https://github.com/input-output-hk/wsc-poc/tree/main/src/lib/SmartTokens).
- [x] Implement the offchain code required to query programmable token balances and construct transactions to transfer such tokens. Done [here](https://github.com/input-output-hk/wsc-poc/tree/main/src/lib/Wst/Offchain).

## Copyright
This CIP is licensed under [Apache-2.0](http://www.apache.org/licenses/LICENSE-2.0).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0146/README.md
---

- --
CIP: 146
Title: Multi-signature wallet registration and discovery
Category: Wallets
Status: Proposed
Authors:
- Ola Ahlman <ola.ahlman@tastenkunst.io>
- Marcel Baumberg <marcel.baumberg@tastenkunst.io>
Implementors:
- Eternl <https://eternl.io/>
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/971
Created: 2025-01-22
License: CC-BY-4.0
- --

## Abstract

This document describes how to format and register a multi-signature wallet registration on the blockchain. The
individual parties that are part of the multi-signature wallet can through this registration transaction easily be
discovered and added to the Cardano wallet software that implement the standard.

This standard both extend on and add restrictions to
[CIP-1854 | Multi-signature HD Wallets](https://github.com/cardano-foundation/CIPs/tree/master/CIP-1854/README.md)
to provide the structure of transaction auxiliary data.

## Motivation: why is this CIP necessary?

Term     | Definition
- --      | ---
Multisig | Shorthand for Multi-party signature scheme.
CSL      | [Cardano serialization lib](https://github.com/Emurgo/cardano-serialization-lib) by Emurgo.

Multisig wallets have the ability to communicate scripts and metadata ahead of time through a registration
transaction utilizing the transaction auxiliary data. This is a convenient way for the multisig participants to
discover the multisig wallet before it's first usage.

A common structure of the auxiliary data is needed for interoperability between wallets and web applications. The
metadatum label `1854` is used to define the native script types added in the auxiliary data and optionally provide
information about the wallet and its participants.

## Specification

The following rules apply for a multisig registration to be valid:

- Auxiliary data with a non-empty native scripts array.
- Auxiliary data with label `1854` metadata that at a minimum includes a mapping for script types.
- `ScriptType` mapping array in metadata must match the length of, and directly corresponds to the elements in the
  `native_script` array of transaction auxiliary data.
- Native scripts array must at least include a payment script. Other script types are optional.
- Public key hashes (credentials) included in the native scripts must be derived in accordance with CIP-1854, ie using
  `purpose=1854'`.
- Key derivation limited to path `1854'/1815'/0'/x/y`, ie account `0` for best cross-project interoperability and performance.
- Key index (`y`) must be incremented by `1` for each publicly registered multisig wallet.
- Key for role (`x`) must use the same index (`y`) for all native scripts in the registration transaction.
- Additional optional metadata can be added to the `1854` metadatum label to describe the multisig.
- Optional icon fields is a URL to a non-animated image file, maximum 40kb in size.

### Data Types

#### MultiSigRegistration
Label `1854` metadata.
```ts
interface MultiSigRegistration {
  types: ScriptType[];
  name?: string;
  description?: string;
  icon?: string;
  participants?: MultiSigParticipants;
}
```

#### ScriptType
A number representing a specific type (role). This corresponds to the key derivation path role (`x`).
```ts
type ScriptType = number;
```

#### MultiSigParticipants
A mapping between key credential and participant details.
```ts
interface MultiSigParticipants {
  [key: string]: MultiSigParticipant;
}
```

#### MultiSigParticipant
Multisig participant details.
```ts
interface MultiSigParticipant {
  name: string;
  description?: string;
  icon?: string;
}
```

#### OffChainMultiSigRegistration
The hex-encoded bytes for a transaction auxiliary data `metadata` and `native_script` array.
```ts
interface OffChainMultiSigRegistration {
  metadata: string;
  native_scripts: string;
}
```

### Registration

After the multisig wallet has been defined according to the Cardano native script standard, and adhering to the
[rules](#specification), either a registration transaction can be put on the blockchain or a JSON download provided for
off-chain sharing.

> [!NOTE]
> The registration **must** use previously unused keys in the scripts **if** registered in a transaction.

The transaction auxiliary data metadata ([MultiSigRegistration](#multisigregistration)) should be formatted using
NoConversions JSON schema.

If using the CSL library, this can be accomplished with helper functions `encode_json_str_to_metadatum` and
`decode_metadatum_to_json_str` specifying `MetadataJsonSchema.NoConversions` as the JSON schema.

Input Output Global (IOG) documentation also describe the
[no schema](https://github.com/input-output-hk/cardano-node-wiki/blob/main/docs/reference/tx-metadata.md#no-schema)
JSON mapping in more detail.

#### Transaction

A registration transaction is the most user-friendly alternative as it allows for automatic multisig wallet discovery
by its participants.

#### Off-chain download

The off-chain JSON file should have the format of `OffChainMultiSigRegistration`.

### Discovery

Discovering registered multisig wallets on the blockchain that has public keys included for a participant wallet
can be done in the following way.

- Derive Ed25519 verification keys from path `1854'/1815'/0'/x/y`.
- Create key credentials, `blake2b-224` hash digests of derived keys in previous step.
- Search for multisig registration transactions on the blockchain that contain metadata with metadatum label `1854`
  and key credentials matching participant wallet. Only the first (oldest) encountered match should be returned.
- Use `types` field in metadata to map native scripts and figure out its purpose
- Repeat until no more matches are found, either sequentially or in bulk.

> [!NOTE]
> There might be updated metadata for the registration. In addition to locating the initial registration, a
> scan for updated metadata conforming to specification and at least one input UTxO matching the multisig payment script
> should be performed. If available, the last valid metadata update is to be used.

### Metadata update

Metadata included in the original registration transaction might need to be updated after the initial registration. To
support this, a metadata update transaction can be created that spends at least one input UTxO from the multisig wallet
to verify ownership. If this transaction includes label `1854` metadata according to specification mentioned in
[registration](#registration), this will replace and update previously registered metadata.

### Encrypted metadata (optional)

To increase anonymity, encrypting the metadata following the specification of [CIP-83 | Encrypted Transaction message/comment metadata](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0083/README.md)
is supported. For this to be valid, the `enc` key should be added to the root metadata object and each value
(**except** `types`) within the metadata should be base64 encrypted and split into 64 character chunks according to CIP-83 specification.

> [!NOTE]
> `types` mapping array within metadata should always be unencrypted!

> [!IMPORTANT]
> The fields for `name | desciprion | icon` for the wallet and each participant is in encrypted mode a string array.
> As encrypting the content might push the length outside the 64 character limitation, data needs to be split in 64
> character chunks and later merged when decrypting.

Example structure:
```json
{
  "1854": {
    "enc": "<encryption-method>",
    "types": [0,2],
    "name": ["base64-string"],
    "description": ["base64-string"],
    "icon": ["base64-string"],
    "participants": {
      "<pub-key-hash>": {
        "name": ["base64-string"],
        "description": ["base64-string"],
        "icon": ["base64-string"]
      },
      ...
    }
  }
}
```

## Rationale: how does this CIP achieve its goals?

The structure to handle registration of native scripts ahead of time has been available from the Allegra era and
beyond by including script pre-image in transaction auxiliary data. The principal aim for this specification is to
reduce friction and increase interoperability between wallet providers and web applications by adding some common
rules and restrictions for format of metadata and keys used in scripts.

### Q & A
A couple of questions was raised during discussions with team members and other projects when formalizing this
standard. The answers lay the ground in large for the specification and restrictions defined.

#### Why limit it to purpose 1854 and not allow both HD (1852) and Multi-Signature (1854) keys?
Mostly due to compatibility across projects and hardware devices as this is how the well established CIP-1854
specification describe that it should be done. Also for discovery, it reduces complexity and increases
performance by only having to scan for a restricted amount of keys.

#### Why not encourage and scan any account index, and not just index 0?
Account indexes is a very useful feature for normal wallets to separate funds, multi-delegation and much more.
However, for multisig wallets, they add little value. The idea behind a multisig is that each participant is its own
identity, either person or wallet. Having multiple keys in the multisig from the same root key adds false security.

#### But what if I want to utilise the benefits of account separation for fund splitting between the same set of participants?
This can easily be solved without additional accounts by adding an `after` timelock with the current slot height of
the blockchain. This creates a unique multisig wallet (address) even if the rest of the script is identical. This way,
an infinite number of multisig wallets can be created similar to account separation.

#### What roles should be supported on discovery?
This specification doesn't restrict discovery to any specific role, depending on the use case of the implementer and
additional roles defined in the future through new CIPs. CIP-1854 define role 0 (payment) and role 2 (stake) for
script wallets. CIP-105 was created to extend key definition with additional governance roles 3-5.

#### What about key role and index restrictions?
The main reason for the incrementing key index is to mitigate a possible attack vector where a malicious actor could
replay a publicly registered multisig wallet, modifying the script in a nefarious way. When the wallets connected to the
users multisig key are discovered, both the valid and the invalid multisig registrations would be discovered and the
user might interact with the malicious wallet. By only allowing a key to be registered once, this threat can be
eliminated.

Keeping key index in sync for all roles for the same registration makes key handling more manageable.

#### Is types mapping in metadata really necessary?
It's true that on the wallet side when deriving purpose `1854` keys and scanning for registered wallets, it's known
from what path the keys where derived and thus one could figure out the type of native script based on key credential.
However, enforcing the transaction to include metadata with metadatum label `1854` and the types mapping make it clear
that this is a multisig registration and easier to scan for.

## Path to Active

### Acceptance Criteria

- [ ] The specification is implemented by three wallet providers or dApps (web applications).
- [x] [Eternl Wallet](https://eternl.io)
- [ ] [MeshJS/multisig](https://github.com/MeshJS/multisig)

### Implementation Plan

- [x] Author to engage with wallet providers and web applications for feedback.
- [x] Discussion channel opened on [Cardano Improvement Proposals Discord server](https://discordapp.com/channels/971785110770831360/1336823914671767663)
- [x] [GitHub PR](https://github.com/cardano-foundation/CIPs/pull/971) for proposal.
- [x] Author to implement said standard in Eternl wallet.
- [ ] Collaborate with web applications and wallet providers to drive adoption of standard.

## Copyright

This CIP is licensed under
[CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0149/README.md
---

- --
CIP: 149
Title: Optional DRep Compensation
Category: Metadata
Status: Proposed
Authors:
- Philip DiSarro <info@anastasialabs.com>
Implementors: []
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/991
Created: 2025-02-19
License: CC-BY-4.0
- --

## Abstract
We propose an opt-in standard for DRep compensation that does not require any changes to the ledger. When delegating to a DRep a user can opt-in to donate a percentage of their staking rewards to their DRep.

## Motivation: why is this CIP necessary?
Introducing DRep compensation into the ledger will be a an absolutely massive undertaking, that will involve a large amount of engineering hours and a non-trivial price-tag. Also, the community has not yet reached consensus on whether DRep compensation is even desirable, or should be mandatory, or what the compensation scheme would even look like.

The best way to approach feature development is to release an MVP, evaluate its reception in the market, and use that data to determine the best way to proceed. This also allows us to introduce the feature without imposing it on anyone, it is opt-in, if you do not wish to compensate your DRep you can simply not opt-in. If, at any time, you wish to adjust the percentage of rewards that you are donating to your DRep you can just issue a new delegation certificate and adjust the amount or opt-out entirely.

## Specification

When tool or wallet builds a DRep vote delegation transaction (any transaction containing a `vote_deleg_cert`, `stake_vote_deleg_cert`, `vote_reg_deleg_cert` or `stake_vote_reg_deleg_cert ` certificate), the user should receive a prompt asking if they want to donate a percentage of their staking rewards to that DRep (i.e. if they want to opt-in to compensate their DRep).

If they choose to opt-in, they enter the percentage of their staking rewards that they would like their DRep to receive. The DRep delegation transaction will include that percentage in the transaction metadata, with metadatum label `3692`, in   the following format:

```json
{
    "3692": {
        "donationBasisPoints": PERCENTAGE_INTEGER_HERE
    }
}
```
Where the integer stored in the metadata is the thousandths place in the decimal so 1 would convey that the user is electing to donate 0.1% of their staking rewards to their DRep. This value should be obtained directly from the percentage the user described in the prompt (ie. the user put 0.5% in the prompt then the integer in the metadata would be 5).

When a user claims their staking rewards, their wallet will look up their delegation transaction and determine if they have opted-in to compensate their DRep (by checking the tx metadata). If metadata in the format described above is found, then the wallet will add an additional output to the reward withdrawal transaction that sends the intended percentage of their rewards to their DRep. If there is not enough ada in their reward balance to produce a output which satisfies minimum `lovelacePerUTxOWord`, the wallet will convey this to the user and let them decide how to proceed (or the user can establish how to handle this via wallet settings).

## Rationale: how does this CIP achieve its goals?
There is little community consensus regarding the topic of DRep delegation, it is a very controversial topic that people on both sides feel very passionate about.

Great features are the result of endless iteration, the first release is never perfect. We cannot have a meaningful discussion on the impact (both positive and negative) and desirability of DRep compensation without any concrete data from which we can draw conclusions. This implementation was designed to embody the philosophy of rapid feedback based iteration. It is extremely simple to implement and bring to production. This proposal was designed to be favorable to everyone regardless on whether or not they support DRep delegation as it is entirely optional and thus can be entirely ignored by those who do not want to engage with it; furthermore, unlike other solutions, it doesn't involve any changes to the ledger that would have an ecosystem wide impact.

## Path to Active

### Acceptance Criteria
- [ ] One major wallet supports this standard.
- [ ] Gov-tools supports this standard.

### Implementation Plan

- [ ] Gov-tools to implement the changes required to support this standard in their UI and backend.
- [ ] Lace to implement required DRep delegation metadata queries and construct payouts accordingly.

## Copyright
This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0150/README.md
---

- --
CIP: 150
Title: Block Data Compression
Status: Proposed
Category: Network
Authors:
- Alex Sierkov <alex.sierkov@gmail.com>
Implementors: []
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/993
Created: 2025-02-24
License: CC-BY-4.0
- --

## Abstract

This proposal presents a lightweight extension to the Cardano Network protocol that:
- Establishes a technical foundation for significantly increasing the ```max_block_size``` and ```active_slot_coefficient``` protocol parameters, improving the throughput and latency of the Cardano blockchain.
- Accelerates blockchain synchronization by optimizing the download process for new and lagging nodes.
- Lowers network traffic costs for Cardano stake pool operators through improved data transmission efficiency.

## Motivation: why is this CIP necessary?

### The Status Quo

The current block-generation time and maximum throughput of the Cardano blockchain are several orders of magnitude behind those of its more recent competitors. While ongoing research explores major architectural changes, such as sharding, this proposal offers an interim scalability improvement by enabling stake pool operators to better leverage modern hardware. By optimizing network performance with minimal protocol changes, this approach helps Cardano remain competitive until more comprehensive scaling solutions are ready for deployment.

### Capabilites That Drive Demand

Newer blockchains, such as Solana and Aptos, have shown that shorter block generation times and higher maximum throughput attract new use cases and drive greater demand for blockchain transactions. Thus, the claim that Cardano does not need to increase its throughput because blocks are not currently saturated overlooks the fact that alternative blockchains handle vastly more transactions—by orders of magnitude—than Cardano. This difference demonstrates strong demand for higher throughput and faster block generation.

### Broad Availability of Affordable High-Performance Hardware

Over the last decade, there has been a significant improvement in both CPU performance and network connectivity for rentable cloud servers. For example, servers with 24 CPU cores and 1 Gbps Internet connections are now available for lease for under $500 per month. By efficiently leveraging this hardware, it is possible to scale Cardano with minimal changes to the protocol.
Following sections present one approach to achieving this.

This document does not propose a specific minimum hardware configuration for block-producing nodes.
Instead, it demonstrates the scalability achievable with certain configurations and argues that Cardano's protocols and software should be capable of leveraging them.

Analyzing more powerful hardware configurations is also important because implementing this CIP may take a year or two. During that time, hardware performance is likely to improve while costs decline. Therefore, it is valuable to discuss hardware configurations that may become standard in the near future.

### WORM Access Pattern

A fundamental property of blockchain data is that historical blocks are immutable. Another key property is that each historical block is transferred to all other nodes, where it is read and validated.

Therefore, block data follows a **Write Once, Read Many (WORM)** access pattern. This pattern is common in online services that distribute digital data. To optimize for this, specialized compression techniques have been developed. These techniques prioritize compression ratio and decompression speed, as blockchain data is written once but read frequently. Since blocks are typically compressed before distribution, compression speed is of secondary importance.

One example of such a technique is the [ZStandard](https://github.com/facebook/zstd) algorithm, which strikes an excellent balance between compression ratio and decompression speed. The following section explores its effectiveness on Cardano Mainnet data.

### ZStandard Performance on Mainnet Data

To understand the behavior of ZStandard compression on the Cardano mainnet data, a quick simulation using two parameters has been conducted on epoch 525. The table below summarizes the simulation parameters, and the three charts display the results. The raw simulation results are provided in the [stats.json](stats.json) file.

| Simulated&nbsp;Parameter | Value&nbsp;Range | Description |
|---------------------    |-------------      |-------------|
| ```max_block_size```    | **64&nbsp;KB**&nbsp;to&nbsp;**64&nbsp;MB** | Block sizes  were simulated by dynamically regroupping real transactions from the Cardano mainnet to evaluate their impact on compression. |
| ```compression_level``` | **1**&nbsp;to&nbsp;**21**                  | A parameter that influences both the compression ratio and speed of the ```ZStandard``` algorithm, with higher levels generally providing greater compression at reduced speed. |

![Compression ratio](compression-ratio.png)
![Decompression speed](decode-speed.png)
![Compression speed](encode-speed.png)

The above charts show the following tendencies:
- The compression ratio increases with the block size.
- Block sizes below 4 MB are too small to achieve high compression ratios.
- The decompression speed remains above **800 MB per second** for all parameter values.
- Compression speed is primarily dependent on the compression level, with minor improvements from larger block sizes.
- **Compression level 9** offers a good balance of compression ratio and speed and will be used in further analysis of ZStandard performance.

### Retransmission Rounds and Block Propagation Time

For a newly generated block to have a high chance of being included in the blockchain, it must reach at least 90% of the active stake before the next block is produced. The table below illustrates the distribution of active stake in epoch 525. It shows that reaching the top 515 pools covers 90% of the active stake, while reaching the top 975 pools covers 99% of it.

| Number of Largest Pools (Epoch 525) | Percentage of Active Stake |
| --- | --- |
| 177 | 50.1% |
| 337 | 75.1% |
| 515 | 90.0% |
| 654 | 95.0% |
| 975 | 99.0% |
| 1373 | 99.9% |
| 2834 | 100.0% |

Each block-producing node has open connections to a small subset of block-producing stake pools. Therefore, block propagation requires multiple rounds of retransmission for a new block to reach most nodes.

The required number of retransmission rounds can be estimated from a typical stake pool’s network configuration. To highlight the benefits of leveraging modern hardware, we assume all pools operate on a symmetric 1 Gbps Internet connection. Such a connection allows a block-producing node to maintain 10 open connections with 100 Mbps of bandwidth reserved for each.

The number of retransmission rounds can be calculated as the logarithm (base number of connections) of the total number of stake pools. The table below illustrates that three retransmission rounds in this configuration are sufficient to deliver a new block to 1,000 nodes, while four rounds are needed for 10,000 nodes.

| Retransmission Round Number | Stake Pools Receiving New Block |
| --- | ---- |
| 1   | 10    |
| 2   | 100   |
| 3   | 1000  |
| 4   | 10000 |

Going forward, the required number of retransmission rounds is referred to as the **retransmission coefficient**. This coefficient is important because optimizations in data compression affect not just one retransmission round but all rounds. The implications of this will be explored further in the following sections.

### Impact of Data Compression on Network Propagation Time

The figure below illustrates the network propagation time of a block across three retransmission rounds. The evaluated compression settings are either no compression (level 0) or ZStandard level 9 and the assumed network bandwidth available for each connection is 100 Mbps. This analysis does not account for cross-continent round-trip delays or data processing overhead, which are discussed in the next section.

A key result is that 16 MB blocks—nearly 200 times larger than the current Cardano block size of 90 KB—can still be retransmitted in under one second with three rounds of retransmissions.

![3-round network propagation time](propagation-3-round.png)

Even with four retransmission rounds, the network propagation time for 16 MB blocks remains just above one second, as illustrated in the next figure. Therefore, accounting for additional delays and inefficiencies, it becomes possible to consider 2- or 3-second block-generation times.

![4-round network propagation time](propagation-4-round.png)

These results highlight how Cardano can leverage affordable high-performance servers and WORM-optimized compression algorithms to significantly improve its network efficiency

### Block Validation Time

Before retransmitting a block, a node must first validate it. This validation consumes CPU resources proportional to the number of transactions in the block, and thus, its size. This could make validation time a bottleneck, leading some to argue that optimizing network efficiency is a lower priority.

However, effective use of parallelization, together with the availability of affordable 24-core servers, can help manage the increased CPU demands without increasing block processing time.

Recent research ([Parallelized Ouroboros Praos](https://github.com/sierkov/daedalus-turbo/blob/main/doc/2024-sierkov-parallelized-ouroboros-praos.pdf), [Parallelization-Aware Plutus Benchmarking Dataset](https://github.com/sierkov/daedalus-turbo/tree/main/experiment/plutus-benchmark)) has shown that most resource-intensive steps of block validation can be parallelized with an efficiency coefficient above 0.9 for up to 24 worker threads:
- Validation of consensus rules.
- Verification of cryptographic and script transaction witnesses.
- Checking transaction inputs against the UTXO set.

### Cross-Continent Transmission Delays

A potential concern is that Cardano nodes are geographically distributed, leading to network packet round-trip times of up to 200 milliseconds in extreme cases. However, these cross-continent transmission delays can be mitigated through smarter algorithms for constructing node connectivity graphs. By optimizing these graphs, the impact of cross-continent delays can be minimized, ensuring that the delay penalty is incurred only once per full block retransmission cycle.

### Moving in Small Steps

Cardano's average block-generation period is 20 seconds, with a maximum block size of 90 KB. There is a substantial difference between these parameters and the discussed 16 MB block size with a 2- to 3-second block-generation period.

Reducing the block-generation time to 10 or 5 seconds and increasing the maximum block size to 1 MB would still provide a sufficient safety margin for block processing and cross-continent delays, while delivering a major immediate improvement to Cardano's scalability.

Furthermore, these incremental changes would provide Cardano with the necessary time and empirical data to refine its block processing and network propagation algorithms, ultimately enabling even lower propagation times, such as 3 or 2 seconds.

### Compresed Storage of Blockchhain Data

A final remark regarding compression is that the compression ratio increases with input size, even beyond the proposed block size of 16 MB. This fact can be leveraged to organize the compressed local storage of blocks:
- Grouping blocks into larger chunks to achieve better compression ratios.
- Utilizing the maximum compression level to further enhance the compression ratio.

### Secure Integration of ZStandard Compression

The ZStandard C library is a highly optimized and widely adopted compression algorithm. It is used by major organizations such as Google and Mozilla and is supported as a content encoding in browsers like Chrome and Firefox. Additionally, ZStandard participates in Google's [OSS-Fuzz](https://introspector.oss-fuzz.com/projects-overview) initiative, which highlights its code quality and robustness.

However, since ZStandard is implemented in C—a language without built-in memory safety—careful security measures must be in place to mitigate potential risks.

The primary security concern is a vulnerability in the decompression algorithm. The decompression code directly processes untrusted data received over the Internet. A malicious actor could craft a compressed block to exploit such vulnerabilities, potentially affecting all nodes, as every node must validate new blocks.

In contrast, the compression process presents a lower risk, as it operates only on known data controlled by the block producer.

Most Cardano nodes run on Linux, which provides several security mechanisms to mitigate these risks. By ensuring that the decompression process is secure on Linux, we can achieve significant risk reduction. The following strategies can be applied:
- **Isolation via IPC**: Running ZStandard in a separate process with minimal privileges and communicating through an IPC mechanism (e.g., a Unix socket). Even if an attacker exploits a vulnerability, they would only gain access to already published blocks or blocks pending validation, minimizing the impact.
- **Virtualization**: Running ZStandard within a lightweight virtualized environment for added isolation.
- **Dedicated Relay Node**: Executing ZStandard on a separate relay node, which does not store sensitive data such as private keys.

Among these, isolation via IPC is particularly attractive due to its minimal impact on performance and decompression latency. To demonstrate this approach, this proposal includes an example implementation that leverages Linux’s ```seccomp``` library to restrict available syscalls to only ```read```, ```write```, and ```exit```, significantly limiting the attack surface. Further details are provided in the section **Example Approach to Secure Integration of ZStandard Library on Linux**.

For enhanced security, these strategies can be combined to provide multiple layers of defense. Moreover, modern Linux distributions also include **Data Execution Prevention (DEP)** and **Address Space Layout Randomization (ASLR)** by default, which harden the system against memory-based exploits by preventing code execution in non-executable memory regions and randomizing memory addresses.

Another option for securely executing ZStandard compression code is **WebAssembly (WASM)**.
C code with minimal modifications can be compiled directly into WASM bytecode, enabling secure execution within a WebAssembly runtime. This approach has already been explored in projects such as [zstandard-wasm](https://github.com/fabiospampinato/zstandard-wasm).  Moreover, early benchmarks indicate that decompression performance is reduced by only a factor of two compared to native execution. Given this level of efficiency, WebAssembly presents a viable solution for reducing block propagation time while enhancing security.

Finally, ```ZStandard``` data format is well documented in [RFC8478](https://datatracker.ietf.org/doc/html/rfc8478), which allows for the development of decompression implementations in memory-safe languages. For example, a Rust-based implementation, [zstd-rs](https://github.com/KillingSpark/zstd-rs), already exists. However, its current decompression performance is reported to be approximately **3.5 times slower** than the C version.

## Specification

The technical foundation for the above improvements can be achieved with minimal changes to [the existing network protocol](https://ouroboros-network.cardano.intersectmbo.org/pdfs/network-spec/network-spec.pdf):
- Clients will use a new version number to signal support for compressed transfers during the handshake.
- The FetchBlock mini-protocol is extended to include a new Server Message, ```MsgCompressedBlocks``` in the StStreaming state.
- Stake pool operators can optionally indicate protocol version support in onchain data.

The following sections provide a detailed breakdown of these changes.

### New Protocol Version Number

The Ouroboros Network Specification includes an effective mechanism for feature extension via the Handshake mini-protocol. The next unallocated version number (e.g., 15) should be used by clients and servers to signal support for the extended FetchBlock mini-protocol.

### MsgCompressedBlocks Message for StStreaming State

```
MsgCompressedBlocks = [6, encoding, encoded_data]

; value 0 - no compression
; value 1 - ZStandard compression
; values 2+ - reserved for future use
encoding = uint

encoded_data = bytes
```

```MsgCompressedBlocks``` is a new server message used to transfer a sequence of blocks, extending the functionality of ```MsgBlock``` by allowing:
- The transmission of multiple blocks at once.
- The option to compress a sequence of blocks.

The server can choose to send either compressed or uncompressed blocks based on its configuration. This capability enables the server to send blocks as they are stored on disk, which improves performance and reduces CPU processing times during batch synchronization.

### Example Approach to Secure Integration of ZStandard Library on Linux

Since Cardano block-producers are run dominantly on Linux, the use of the **Isolation via IPC** tactic can be further strengthened using Linux's ```seccomp``` feature to minimize the potential attack surface. An example implementation of this approach is provided in the [secure-zstd](https://github.com/sierkov/secure-zstd) repository.

#### How It Works:
1. The managing (caller) process creates a Unix socket.
2. A new worker process is started, which communicates exclusively with the parent process through this socket.
3. The worker process pre-initializes the ZStandard library by running a compressor and decompressor on a statically defined dataset. This eliminates the need for further dynamic memory allocations.
4. The worker process closes all open file descriptors except for the Unix socket.
5. The worker process applies a ```seccomp``` profile to restrict system calls to only ```read```, ```write```, and ```exit```.
6. The worker process is then ready to handle compression and decompression requests from the caller process.

An attacker can still send malicious data to the Cardano Node. However, in such cases, the worker process will immediately crash, and the caller process will receive a corresponding notification.

Given the high quality of the ZStandard library, such a crash is likely indicative of an attack attempt, warranting an appropriate response:
- **Block the malicious peer** to prevent further communication and mitigate potential Denial-of-Service attacks.
- **Restart the worker process** to continue handling requests from a safe state.

The provided implementation is compact—approximately **200 lines of code** for [the worker process](https://github.com/sierkov/secure-zstd/blob/main/lib/seczstd/worker.c) and **100 lines of code** for [the caller](https://github.com/sierkov/secure-zstd/blob/main/lib/seczstd/caller.c). This allows for **easy security audits** compared to auditing the full ZStandard decompression library, which consists of about **15,000** lines of code.

Furthermore, the **Isolation via IPC** approach, extended with ```seccomp```, can be similarly applied to other untrusted data-processing tasks, such as newer but less-tested cryptographic libraries (e.g., potential Plutus builtins) and other use cases.

### Indicating Protocol Version Support in Stake Pool Operator On-Chain Data

Some clients may prefer receiving block data in a compressed form to save bandwidth. Providing a quick way to identify stake pool operators that support compressed transfers would be valuable.

Cardano stake pool operators can already publish a URL linking to a publicly available JSON file containing additional information about their pool. These URLs are recorded on-chain and can also be retrieved through a stake pool metadata aggregation service (SMASH).

This proposal extends the metadata JSON by adding an optional key, ```protocolVersions```, which contains a list of currently supported protocol versions as a JSON array.

```
protocolVersions = [12, 13, 15]
```

This allows clients to quickly determine whether a stake pool operator supports the latest protocol version, including features such as compressed block transfers.
This is particularly important during node bootstrapping, when a new node does not yet have an up-to-date list of block-producing nodes but needs to synchronize as quickly as possible.

### Possible Support Levels

The specification is intentionally designed to facilitate partial and incremental implementation. The following levels of support are possible:
- **Minimal level** – A minimal implementation only needs to recognize protocol version 15 during the handshake and be able to parse MsgCompressedBlocks messages.
  This implementation is straightforward and can be completed within days in most programming languages.
- **Intermediate level** – In addition to parsing compressed blocks, an intermediate implementation can compress block data on-the-fly for each transmitted block.
  Although on-the-fly compression is not CPU-efficient, ZStandard’s level 9 compression speed exceeds 100 MB/sec, allowing a single CPU core to fully saturate a 1 Gbps connection.
  Implementing this is also straightforward and should take about a week in most programming languages.
- **Advanced level** – To use CPU resources more efficiently, an advanced implementation may either cache previously compressed blocks in memory or store all block data in a compressed format.
This may require additional work, as certain software components may need to be modified to support compressed block data.
- **Full level** – A full implementation must leverage data compression for block storage, including storing compressed block data on disk
and enabling direct transmission of compressed block sequences from disk when a client requests a block range.

To be ready for testnet testing, all implementations should provide at least minimal support, and at least one implementation should support the intermediate level.

## Rationale: how does this CIP achieve its goals?

- Analyzes how support for data compression can serve as a technical foundation for drastically improving Cardano's scalability.
- Evaluates the performance of the ZStandard compression algorithm on Cardano mainnet data.
- Demonstrates how data compression, combined with more efficient use of affordable, high-performance server hardware, can scale Cardano’s transaction throughput by multiple orders of magnitude.
- Proposes specific changes to the networking protocol to enable data compression.

## Path to Active

### Acceptance criteria
- The proposed changes have been discussed and approved by subject matter experts.
- An implementation has been prepared and addresses all major concerns identified during discussions.
- The implementation has been tested on a testnet.
- A security model for using the ZStandard compression library on the mainnet has been presented and approved by subject matter experts.
- Any new concerns arising during testnet evaluation are addressed in an updated implementation, which is subsequently confirmed through a follow-up testnet evaluation.

### Implementation plan
- Develop the implementation. The individuals responsible for development will be designated after approval from subject matter experts has been received.
- Deploy and test on the testnet.
- Deploy on the mainnet following successful testnet evaluation.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0151/README.md
---

- --
CIP: 151
Title: On-Chain Registration - Stake Pools
Category: Metadata
Status: Proposed
Authors:
- Adam Dean <adam@crypto2099.io>
- Martin Lang <martin@martinlang.at>
Implementors:
- Cardano Signer: https://github.com/gitmachtl/cardano-signer/releases/tag/v1.23.0
- pg_cardano: https://github.com/cardano-community/pg_cardano/releases/tag/v1.0.5-p1
- Cardano Koios: https://github.com/cardano-community/koios-artifacts/tree/v1.3.2
- CNTools: https://github.com/cardano-community/guild-operators/tree/alpha
- SPO Scripts: https://github.com/gitmachtl/scripts
- Reference Implementation: https://github.com/crypto2099/calidus-demo
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/999
- https://forum.cardano.org/t/new-calidus-pool-key-for-spos-and-services-interacting-with-pools
Created: 2025-02-17
License: CC-BY-4.0
- --

## Abstract

[CIP-0088] defined a standard to submit on-chain registration certificates with
a specific initial focus on Cardano's _Native Scripts_ specifically related to
NFT and FT minting policies. This extension to the standard aims to provide
support for stake pools to register verifiable information on-chain.

## Motivation: why is this CIP necessary?

By extending the existing [CIP-0088] specification we can provide an extensible
framework for stake pool operators (SPOs) to provide verifiable, on-chain
information related to their pool operation. This method is preferred over a
change to the in-ledger stake pool registration certificates because additional
extensions and functionality may be added dynamically without changing the core
Cardano network ledger.

## Specification

[CIP-0088] provides a clear framework for extension and versioning of the
standard so this CIP introduces a _[Version 2]_ of the CIP-0088 Master CDDL
along with a number of other changes.

### Registration Metadata Format

`Version: 2`

| Index | Name                                                 | Type  | Required | Notes                                                        |
|-------|------------------------------------------------------|-------|----------|--------------------------------------------------------------|
| 0     | Version                                              | UInt  | Yes      | Should be set to 2                                           |
| 1     | [Registration Payload](#registration-payload-object) | Map   | Yes      | Object describing and providing details for the token policy |
| 2     | [Registration Witness](#registration-witness-array)  | Array | Yes      | Array of witness signatures used to validate the payload     |

### Registration Payload Object

The Token Registration Payload Object (TRPO) consists of 4 required fields and
optional additional fields to provide context and information. The top-level
metadata label of **867** has been chosen for the purposes of this standard.

#### Fields

| Index | Name              | Type   | Required | Notes/Examples                                                                                                                                                                                                                                    |
|-------|-------------------|--------|----------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 1     | Scope             | Array  | Yes      | An array defining the scope of this registration (for greater compatibility with CPS-0001). The first entry should be an unsigned integer value identifying the type of scope while the second entry addresses the specific scope of registration |
| 2     | Feature Set       | Array  | Yes      | An array of unsigned integers specifying none or more CIP standards utilized by the tokens of this project. Should reference the assigned CIP number.                                                                                             |
| 3     | Validation Method | Array  | Yes      | How should this payload be validated.                                                                                                                                                                                                             |
| 4     | Nonce             | UInt   | Yes      | A simple cache-busting nonce. Recommend to use the blockchain slot height at the time of submission. Only the highest observed nonce value should be honored by explorers.                                                                        |
| 6     | CIP Details       | Object | No       | If one or more of the CIPs addressed in the Feature Set have additionally defined metadata, it may be added here                                                                                                                                  |
| 7     | Calidus Key       | String | No       | An Ed25519 public key that is authorized by the owner to authenticate and sign messages on behalf of the _Scope_                                                                                                                                  |

> The following fields (1-4) are required in all token registration submissions.

##### 1. Scope

The scope entry indicates what type of entity is being registered with this
metadata.

- *Scopes**

| ID | Scope         | Format                             |
|----|---------------|------------------------------------|
| 0  | Native Script | `[0, h'policyID', [h'policyHex']]` |
| 1  | Stake Pool    | `[1, h'poolID']`                   |

0. **Native Scripts**: Native scripts should be specified as an array with the
   first entry indicating the type (Native Script), the second entry indicating
   the script hash (Policy ID) and the third entry consisting of an array with
   one or more 64-byte strings constituting the hex-encoded CBOR representation
   of the Native Script itself. In this way, CIP-88 registration may be
   submitted on-chain prior to any tokens being minted and can be used by
   validators to confirm the legitimacy of the certificate without any secondary
   information source.
1. **Stake Pool**: Stake pools are specified as an array with the first entry
   indicating the type (Stake Pool), the second entry is the Pool ID. The Pool
   ID is the blake2b-224 hash of the pool **cold** public key.

- *Example:**

Native Script:
<br />
`[0, h'3668b628d7bd0cbdc4b7a60fe9bd327b56a1902e89fd01251a34c8be', h'8200581c4bdb4c5017cdcb50c001af21d2488ed2e741df55b252dd3ab2482050']` <br />
Stake Pool:
<br />
`[1, h'71e3c37983775238704fad6d337ca632c713829292bf10a8e9fc50ad']`

#### 2. Feature Set

The _Feature Set_ is a simple array of unsigned integer values representing the
CIP standards that should be applied to the subject scope. For now this should
be an empty array when registering a stake pool.

- *Example:**

`[25, 27]`

#### 3. Validation Method

In order to minimize issues relating to capitalization and misspellings, we
should use a well-defined map of integer values for validation methods that will
be utilized by third party observers and processors to authenticate the payload.
The validation method entry should always be provided as an array with the first
element being an unsigned integer representing the method and additional entries
providing additional context to the validation as required.

- **Proposed Validation Methods***

| ID | Type                   | Format                              | Notes                                                                                                                                                                                                                                                                                                                                                                                                                                      |
|----|------------------------|-------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 0  | Ed25519 Key Signature  | `[0]`                               | The most basic and simplistic approach to signing and validation. In this case the Registration Witness object could contain one or more pubkey + signed witness objects. The payload to be signed should be the hex-encoded CBOR representation of the Registration Payload object.                                                                                                                                                       |
| 1  | Beacon/Reference Token | `[1, [h'<policyId>',h'<assetId>']]` | Similar to the approach utilized by [CIP-27](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0027). We could attach this metadata during a mint transaction for a specially formatted token under the policy ID in question. CIP-27 uses a "nameless" token that has an empty "Asset ID" for example. This may be a validation method that lends itself better to supporting token projects that are minted via Smart Contract. |
| 2  | CIP-0008 Signature     | `[2]`                               | Follow the specifications in [CIP-0008] to verify signatures                                                                                                                                                                                                                                                                                                                                                                               |

- *Examples:**

`[0]`<br />
`[1, [h'<policyId>',h'<assetId>']]`<br />
`[2]`

#### 4. Nonce

The nonce value is utilized to prevent a replay attack vector. The nonce value
should be an unsigned integer value that is always at least one greater than the
previously registered value. It is recommended that you use the current absolute
slot height of the chain when constructing a new registration to ensure that
this value always increases.

- *Example:**

`12345`

> Note: The following map keys and values are optional

#### 5. Data Oracle URI

To be utilized and expanded upon in a separate CIP, this should be a valid URI
pointing to a source of additional, potentially dynamic information relating to
the project and/or the tokens minted under it.

> This is not currently used for stake pool operator registrations.

#### 6. CIP-Specific Information

This entry, if present, should be a CIP ID indexed object containing additional
information pertaining to that CIP. When and where possible the CIP-Specific
registration should follow the CBOR-like declaration syntax to ensure that the
content is well-formed and easily parseable.

> There are currently no Stake Pool-specific CIPs to be used here but could be
> added in the future.

#### 7. Calidus Key

Calidus is a Latin word meaning: _fiery_, _spirited_, or _rash_. It is
synonymous with being a "quick" or "hot" key and was chosen by the authors
because other labels were causing confusion (update key, daily key, etc.)

The Calidus Key is an Ed25519 Public Key that is authorized to be used for
signing authentication or update transactions in the future on behalf of the
stake pool or native script being registered with this certificate.

The Calidus Key should be provided as the byte-array (hex-encoded) public key
that will be used for signing in the future. The active `Calidus Key` for the
registered entity should be the highest nonce value valid registration. If the
key is thought to be compromised previous keys may be invalidated by submitting
a new on-chain registration with a higher nonce value.

- *Example:**
`h'57758911253f6b31df2a87c10eb08a2c9b8450768cb8dd0d378d93f7c2e220f0'`

> **IMPORTANT NOTE:** This standard does not provide a mechanism to revoke the
> authorization of a Calidus key except for replacing it with a new key. Those
> wishing to revoke their key without replacement should submit an update
> registration with a blank key (all zeroes) in place of the Calidus key.
>
> Tooling providers should recognize this as explicitly revoking all previous
> registrations without adding a new key.
>
> Example: `h'0000000000000000000000000000000000000000000000000000000000000000'`

##### Bech32 Encoding

We define a prefix when Bech32 encoding the Calidus key to leave room for future
expansion and use similar to [CIP-129] and other Bech32 encodings used within
the Cardano ecosystem.

###### Binary format

In the header-byte, bits [7;4] indicate the type of key being used. The
remaining four bits [3;0] are used to define the credential type. There are
currently 1 types of credentials defined in the Cardano Conway era, this
specification will allow us to define a maximum of 16 different types of keys in
the future.

```
  1 byte     variable length
 <------> <------------------->
┌────────┬─────────────────────┐
│ header │        key      │
└────────┴─────────────────────┘
    🔎
    ╎          7 6 5 4 3 2 1 0
    ╎         ┌─┬─┬─┬─┬─┬─┬─┬─┐
    ╰╌╌╌╌╌╌╌╌ |t│t│t│t│c│c│c│c│
              └─┴─┴─┴─┴─┴─┴─┴─┘
```

###### Key Type Prefix

| Key Type (`t t t t . . . .`) | Key        |
|------------------------------|------------|
| `1010....`                   | Stake Pool |

###### Credential Type Prefix

| Credential Type (`. . . . c c c c`) | Semantic    |
|-------------------------------------|-------------|
| `....0001`                          | Key Hash    |
| `....0010`                          | Script Hash |

- *Stake Pool Calidus Key Example**

| Label                          | Value                                                              |
|--------------------------------|--------------------------------------------------------------------|
| Stake Pool Key Hash Prefix     | `a1`                                                               |
| Calidus Public Key             | `57758911253f6b31df2a87c10eb08a2c9b8450768cb8dd0d378d93f7c2e220f0` |
| Blake2b-224 Hash of Public Key | `171983a1178a55b02afacfd6ad6b516da375469fd7dbcf54a2f95823`         |
| Prefixed PubKey Hash           | `a1171983a1178a55b02afacfd6ad6b516da375469fd7dbcf54a2f95823`       |
| Bech32-encoded Calidus Key ID  | `calidus15yt3nqapz799tvp2lt8adttt29k6xa2xnltahn655tu4sgcph42p7`    |

![Calidus Key Flow Chart](CalidusKeyv2.jpg)

### Registration Witness Array

- *IMPORTANT**: This v2 CIP introduces two major changes to the signing method(s)
available and utilized.

1. Change #1: Signature Payload
1. Version 1 of the standard used the hex-encoded CBOR of the Token
       Registration Payload Object as the signing payload.
2. Version 2 of this standard uses the `blake2b-256` hash of the hex-encoded
       CBOR of the Token Registration Payload Object as the signing payload.
       This change was made to better support signing using hardware wallets (
       Ledger, Trezor, Keystone)
3. Objects in the signing payload **MUST BE** in numerical order by index in
       order to ensure that hashes can be correctly parsed by downstream
       tooling.
2. Change #2: Witness Structure Changes to Maps
1. Version 1 of the standard assumed basic Ed25519 CLI keys and signatures
       so the structure of witnesses in the Registration Witness Array were
       passed as simple arrays with the first element being a public key and the
       second element being the signature.
2. Version 2 of this standard introduces the ability to use [CIP-0008]
       signing (validation method `2`) which provides the ability for SPOs to
       sign with hardware wallets and [CIP-0030] web wallets as well. Please
       refer to [CIP-0008] for signature validation and payload structure
       details.

#### (Stake Pools)

The Witness Array **must** include a signature from the Pool Cold Key. The
format of witnesses will depend on the validation method used although all v2+
registrations should use the updated `blake2b-256` hash of the hex-encoded CBOR
of the Token Registration Payload Object as the signature payload.

##### Version 1 Witness

A CIP-88 v1 Witness is a simple array consisting of the hex-encoded byte array
of the public key as the first entry and the hex-encoded byte array of the
witness signature as the second entry.

- *Example**

```cbor
[
  [
    h'02b76ae694ce6549d4a20dce308bc7af7fa5a00c7d82b70001e044e596a35deb',
    h'23d0614301b0d554def300388c2e36b702a66e85432940f703a5ba93bfb1659a0717962b40d87523c507ebe24efbb12a2024bb8b14441785a93af00276a32e08'
  ]
]
```

##### Version 2 Witness

CIP-88 v2 (this CIP) introduces the version 2 witness structure which is a map
that allows us to provide additional scoped information for different witnesses
to suite a variety of purposes.

- *v2 Witness Schema**

```cbor
; In v2 witnesses are changed to a map with an optional type identifier.
; A default of 0 for "Witness Type Identifier" should be considered a COSE Signature from CIP-0008

COSE_Key = {
     1 : uint,              ; COSE Key Type
     3 : int,               ; COSE Key Algorithm
- 1 : uint,              ; EC Identifier
- 2 : bytes .size (32),  ; Public Key
}

COSE_Sign1_Payload = [
    bytes .size (41),
    uint,
    bytes .size (32),
    bytes .size (64),
]

COSE_Witness = {
  ? 0 : uint,                ; Witness Type Identifier (optional or 0)
    1 : COSE_Key,            ; COSE Key Header Object
    2 : COSE_Sign1_Payload,  ; COSE Signature Payload
}

v2_witness = {
    0 : uint,   ; Witness Type Identifier (Must be set)
    1 : bytes,  ; Witness Public Key
    2 : bytes,  ; Witness Signature
}
```

As documented in the CDDL above we define a COSE Witness ([CIP-0008]/CIP-0030
Compatible) as well as a generic v2 Witness structure while leaving the format
open to future expansion.

- *Fields**

- *0: Witness Type Identifier**

The Witness Type identifier is optional for a COSE Witness and is assumed to be
a value of `0`.

- *1: Witness Header or Public Key Identifier**

The `1` key is used to help identify the public key in the case of a simple
signature witness or provides the COSE Signature Headers in the case of a COSE
Witness (CIP-0008/CIP-0030).

- *2: Witness Signature Payload**

The `2` key is used to contain the actual signature payload for the witness. In
the case of the COSE signing this will be an array of elements used in the
validation process. In the case of a simple Ed25519 key signature, this will be
a simple hex-encoded CBOR witness signature.

> **Note:** COSE Signing as described in CIP-0008 uses a boolean value as the
> second entry of the COSE Payload to indicate whether the payload was hashed
> by the signing function before signing. Because Cardano on-chain metadata
> does not support boolean values, this must be converted to/from a binary (0 or
> 1\) value when encoding or decoding the payload.
>
> By default, the COSE Sign1 Payload contains an entry which is an object (map)
> containing a `hashed` key and a boolean `true/false` value. This entire object
> should be replaced with a simple `1` or `0` value representing the
`true/false`
> value of the `hashed` key.
>
> Example:
> `{"hashed": true}` becomes `1` and `{"hashed": false}` becomes `0`. In most
> cases it will also be required to convert from the `1` or `0` value stored in
> entry 2 of the COSE Sign1 Payload array back to object notation before
> validating.

## Rationale: how does this CIP achieve its goals?

This CIP was born out of a desire to allow SPOs to routinely identify themselves
to third-party services such as: voting platforms, social media, governance
without risking or compromising the keys used in the actual operation and setup
of the stake pool.

Thus, the concept of an authorized "hot key" (the Calidus Key) was born as a
solution to this authentication issue. Given that [CIP-0088] had already been
written and was extensible to support this registration with minimal additional
support effort, extending it was chosen in favor of other solutions.

## Path to Active

### Acceptance Criteria

This CIP should be considered for `Active` status once a substantial amount of
ecosystem tooling supports it and SPOs as well as third-party applications have
shown an interest in using it as a method of authentication and validation.

- Ecosystem Tooling
- [x] Cardano Signer
- [x] pg_cardano
- [x] cardano-hw-cli support
- [x] Cardano Koios
- [ ] Blockfrost
- [x] CN Tools
- [x] SPO Scripts
- Wallets
- [x] Eternl
- [x] Typhon
- Applications
- [ ] CExplorer
- [ ] CardanoScan
- [ ] AdaStat
- [ ] PoolTool.io
- [ ] DripDropz

### Implementation Plan

The authors of this CIP will continue to work on their own tooling and reference
implementations as well as reaching out to community and ecosystem partners to
drive adoption and usage of this standard.

## Copyright

This CIP is licensed under [CC-BY-4.0].

[CIP-0088]: https://github.com/cardano-foundation/CIPs/tree/master/CIP-0088

[Version 2]: CIP88_Master_v2.cddl

[CIP-0008]: https://github.com/cardano-foundation/CIPs/tree/master/CIP-0008

[CC-BY-4.0]: https://creativecommons.org/licenses/by/4.0/legalcode

[CIP-129]: https://github.com/cardano-foundation/CIPs/tree/master/CIP-0129

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0152/README.md
---

- --
CIP: 152
Title: Modules in UPLC
Status: Proposed
Category: Plutus
Authors:
- John Hughes <john.hughes@quviq.com>
Implementors: []
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/946
Created: 2024-11-12
License: CC-BY-4.0
- --
## Abstract

Cardano scripts are limited in complexity by the fact that each script
must be supplied in one transaction, whether the script is supplied in
the same transaction in which it is used, or pre-loaded onto the chain
for use as a reference script. This limits script code size, which in
turn limits the use of libraries in scripts, and ultimately limits the
sophistication of Cardano apps, compared to competing blockchains. The
script size limit is an aspect of Cardano that script developers
commonly complain about.

This CIP addresses this problem directly, by allowing reference inputs
to supply 'modules', which can be used from other scripts (including
other modules), thus allowing the code of a script to be spread across
many reference inputs. The 'main specification' requires *no* changes to
UPLC, PTLC, PIR or Plinth; only a 'dependency resolution' step before
scripts are run. Many variations are described for better performance,
including some requiring changes to the CEK machine itself.

Higher performance variations will be more expensive to implement; the
final choice of variations should take implementation cost into
account, and (in some cases) may require extensive benchmarking.

## Motivation: why is this CIP necessary?

Cardano scripts are currently subject to a fairly tight size limit,
deriving from the overall limit on transaction size; even when a
script is supplied in a reference UTxO, that UTxO must itself be
created by a single transaction, which is subject to the overall
transaction size limit. Competing blockchains suffer from no such
limit: on the Ethereum chain, for example, contracts can call one
another, and so the code executed in one transaction may come from
many different contracts, created independently on the
blockchain--each subject to a contract size limit, but together
potentially many times that size. This enables more sophisticated
contracts to be implemented; conversely, on Cardano, it is rather
impractical to implement higher-level abstractions as libraries,
because doing so will likely exceed the script size limit. This is not
just a theoretical problem: complaints about the script size limit are
commonly made by Cardano contract developers.

Thus the primary goal of this CIP is to lift the limit on the total
amount of code run during a script execution, by allowing part of the
code to be provided in external modules. By storing these modules on
the blockchain and providing them as reference UTxOs, it will be
possible to keep transactions small even though they may invoke a large
volume of code.

Once scripts can be split into separate modules, then the question
immediately arises of whether or not the script and the modules it
imports need to be in the same language. Today there are many
languages that compile to UPLC, and run on the Cardano
blockchain. Ideally it should be possible to define a useful library
in any of these languages, and then use it from all of them. A
secondary goal is thus to define a module system which permits this,
by supporting cross-language calls.

Note that many languages targetting UPLC already support modules. In
particular, Plinth already enjoys a module system, namely the Haskell
module system. This already enables code to be distributed across
several modules, or put into libraries and shared. Indeed this is
already heavily used: the DJED code base distributes Plinth code
across 24 files, of which only 4 contain top-level contracts, and the
others provide supporting code of one sort or another. Thus the
software engineering benefits of a module system are already
available. The *disadvantage* of this approach is that all the code is
combined into one script, which can easily exceed the size limit as a
result. Indeed, the DJED code base also contains an implementation of
insertion sort in Plinth, with a comment that a quadratic algorithm is
used because its code is smaller than, for example, QuickSort. There
is no clearer way to indicate why the overall limit on code size must be
lifted.

### The Situation on Ethereum

Ethereum contracts are not directly comparable to Cardano scripts;
they correspond to both the *on-chain* and the *off-chain* parts of
Cardano contracts, so one should expect Ethereum contracts to require
more code for the same task, since in Cardano only the verifiers need
to run on the chain itself. Nevertheless, it is interesting to ask
whether, and how, the need for modules has been met in the Ethereum
context.

Solidity does provide a notion of 'library', which collects a number
of reusable functions together. Libraries can be 'internal' or
'external'--the former are just compiled into the code of client
contracts (and so count towards its size limit), while the latter are
stored separately on the blockchain.

There is a problem of trust in using code supplied by someone else:
the documentation for the `ethpm` package manager for Ethereum warns
sternly

- *you should NEVER import a package from a registry with an unknown or
  untrusted owner**

It seems there *is* only one trusted registry, and it is the one
supplied as an example by the developers of `ethpm`. In other words,
while there is a package manager for Ethereum, it does not appear to
be in use.

This is not to say that code is never shared. On the contrary, there
is an open source repo on `github` called `OpenZeppelin` which appears
to be heavily used. It provides 264 Solidity files, in which 43
libraries are declared (almost all internal). It seems, thus, that
libraries are not the main way of reusing code in Solidity; rather it
is by calling, or inheriting from, another contract, that code reuse
primarily occurs.

A small study of 20 'verified' contracts running on the Ethereum chain
(verified in the sense that their source code was provided) showed that

- 55% of contracts consisted of more than one module
- 40% of contracts contained more than one 'application' module
- 55% of contracts imported `OpenZeppelin` modules
- 10-15% of contracts imported modules from other sources
- 5% of contracts were simply copies of `OpenZeppelin` contracts

Some of the 'other' modules were provided to support specific
protocols; for example Layr Labs provide modules to support their
Eigenlayer protocol for re-staking.

A sample of 20 is too small to draw very strong statistical conclusions,
but we can say that the 95% confidence interval for contracts to
consist of multiple modules is 34-74%.
Thus code sharing is clearly going on, and a significant number of
transactions exploit multiple modules. We may conclude that there is a
significant demand for modules in the context of smart contracts, even
if the total contract code still remains relatively small.


## Specification

### Adding modules to UPLC

This CIP provides the simplest possible way to split scripts across
multiple UTxOs; essentially, it allows any closed subterm to be
replaced by its hash, whereupon the term can be supplied either as a
witness in the invoking transaction, or via a [reference script](https://cips.cardano.org/cip/CIP-0033) in that
transaction. To avoid any change to the syntax of UPLC, hashes are
allowed only at the top-level (so to replace a deeply nested subterm
by its hash, we need to first lambda-abstract it). This also places
all references to external terms in one place, where they can easily
be found and resolved. Thus we need only change the definition of a
`Script`; instead of simply some code, it becomes the application of
code to zero or more arguments, given by hashes.

Currently, the definition of “script” used by the ledger is (approximately):
```
newtype Script = Script ShortByteString
```
We change this to:
```
newtype CompleteScript = CompleteScript ShortByteString

newtype Arg = ScriptArg ScriptHash

data Script =
  ScriptWithArgs { head :: CompleteScript, args :: [Arg] }

- - hash of a Script, not a CompleteScript
type ScriptHash = ByteString
```

Scripts in transactions, and on the chain, are represented in this
way, with dependencies that must be supplied in a transaction using
the script. During phase 2 verification we need to resolve the
arguments of each script before running it:
```
resolveScriptDependencies
  :: Map ScriptHash Script
- > Script
- > Maybe CompleteScript
resolveScriptDependencies preimages = go
  where
    go (ScriptWithArgs head args) = do
      argScripts <- traverse lookupArg args
      pure $ applyScript head argScripts
      where
        lookupArg :: Arg -> Maybe CompleteScript
        lookupArg (ScriptArg hash) = do
          script <- lookup hash preimages
          go script
```
The `preimages` map is the usual witness map constructed by the ledger,
so in order for a script hash argument to be resolved, the transaction
must provide the pre-image in the usual way. Note that arguments are
mapped to a `Script`, not a `CompleteScript`, so the result of looking
up a hash may contain further dependencies, which need to be resolved
recursively. A transaction must provide witnesses for *all* the
recursive dependencies of the scripts it invokes.

The only scripts that can be run are complete scripts, so the type of
`runScript` changes to take a `CompleteScript` instead of a `Script`.

#### Variation: Lazy Loading

With the design above, if any script hash is missing from the `preimages`,
then the entire resolution fails. As an alternative, we might replace
missing subterms by a dummy value, such as `builtin unit`, thus:
```
resolveScriptDependencies
  :: Map ScriptHash Script
- > Script
- > CompleteScript
resolveScriptDependencies preimages = go
  where
    go (ScriptWithArgs head args) =
      applyScript head (map lookupArg args)
      where
        lookupArg :: Arg -> CompleteScript
        lookupArg (ScriptArg hash) = do
          case lookup hash preimages of
	    Nothing     -> builtin unit
	    Just script -> go script
```
This would allow transactions to provide witnesses only for script
arguments which are actually *used* in the calls that the transaction
makes. This may sometimes lead to a significant reduction in the
amount of code that must be loaded; for example, imagine a spending
verifier which offers a choice of two encryption methods, provided as
separate script arguments. In any call of the verifier, only one
encryption method will be required, allowing the other (and all its
dependencies) to be omitted from the spending transaction.

#### Variation: Value Scripts

The goal of this variation is to eliminate the cost of evaluating
scripts, by converting them directly to values (in linear time). Since
UPLC runs on the CEK machine, this means converting them directly into
the `CekValue` type, *without* any CEK machine execution. To make this
possible, the syntax of scripts is restricted so that those parts that
would be evaluated during an application to the script arguments are
already (UPLC) values. That is, script code is syntactically
restricted to explicit λ-expressions with one λ per `ScriptArg`,
followed by a syntactic value. (Values are constants, variables,
built-ins, λ-abstractions, delayed terms, and SoP constructors whose
fields are also values).

This means that every script must take the form
`λA1.λA2....λAn.<value>`, where `n` is the number of `ScriptArg`s
supplied. Now, since variables in `CompiledCode` are de Bruijn indices
then the `n` λs can be omitted from the representation--we know how
many there must be from the number of `ScriptArg`s, and the names
themselves can be reconstructed.

There must be a dynamic check that the code of each script really is
of this form, but this check can be built into deserialization, and
thus need cost very little.

`Script`s in this restricted form can be mapped directly into CEK
values, without any CEK-machine evaluation steps. In pseudocode:
```
scriptCekValue
  :: Map ScriptHash CekValue
- > Script
- > CekValue
scriptCekValue scriptValues (ScriptWithArgs head args) =
  cekValue (Env.fromList [case lookup h scriptValues of
	   		    Just v -> v
			    Nothing -> vbuiltin unit
	   		 | ScriptArg h <- args])
	   (deserialize (getPlc head))

```
That is, a script is turned into a value by creating a CEK machine
environment from the values of the `ScriptArg`s, and converting the
body of the script (a syntactic value) in a CekValue in that
environment.

This pseudocode follows the 'lazy loading' variation; an easy
variation treats not finding a script hash as an error.

Syntactic values are turned into `CekValue`s by the following
function, which is derived by simplifying `computeCek` in
UntypedPlutusCore.Evaluation.Machine.Cek.Internal, and restricting it
to syntactic values.
```
cekValue
  :: Env
- > NTerm
- > CEKValue
cekValue env t = case t of
  Var _ varname      -> lookupVarName varName env
  Constant _ val     -> VCon val
  LamAbs _ name body -> VLamAbs name body env
  Delay _ body       -> VDelay body env
  Builtin _ bn       ->
    let meaning = lookupBuiltin bn ?cekRuntime in
    VBuiltin bn (Builtin () bn) meaning
  Constr _ i es      ->
    VConstr i (foldr ConsStack EmptyStack (map (cekValue env) es)
  _                  -> error
```
Converting a syntactic value to a CekValue does require traversing it,
but the traversal stops at λs and delays, so will normally traverse
only the top levels of a term.

Finally, if `preimages` is the `Map ScriptHash Script` constructed from
a transaction, then we may define
```
scriptValues = Map.map (scriptCekValue scriptValues) preimages
```
to compute the CekValue of each script.

Scripts are then applied to their arguments by building an initial CEK
machine configuration applying the script value to its argument value.



Note that this recursive definition of `scriptValues` could potentially allow an
attacker to cause a black-hole exception in the transaction validator,
by submitting a transaction containing scripts with a dependency
cycle. However, since scripts are referred to by hashes, then
constructing such a transaction would require an attack on the hash
function itself... for example a script hash `h` and values for `head`
and `args` such that
```
h = hash (Script head (h:args))
```
We assume that finding such an `h` is impossible in practice; should
this not be the case, or if we should wish to defend against an
attacker with the resources to find such an attack on the hash
function, then we must build a script dependency graph for each
transaction and check that it is acyclic before evaluating the scripts
in this way.

##### Cost

Converting `Script`s to `CekValue`s does require a traversal of all
`Script`s, and the top level of each `Script` value. This is linear
time in the total size of the scripts, though, and should be
considerably faster than doing the same evaluation using CEK machine
transitions. The conversion can be done *once* for a whole
transaction, sharing the cost between several scripts if they share
modules (such as frequently used libraries). So costs should be
charged *for the whole transaction*, not per script. The most accurate
cost would be proportional to the total size of values at the
top-level of scripts. A simpler approach would be to charge a cost
proportional to the aggregated size of all scripts, including
reference scripts--although this risks penalizing complex scripts with
a simple API.

##### Implementation concerns
The CEK implementation does not, today, expose an API for starting
evaluation from a given configuration, or constructing `CekValue`s
directly, so this variation does involve significant changes to the
CEK machine itself.

##### Subvariation: Module-level recursion

Many modules define recursive functions at the top-level. In this
variation, the innermost body of a script is further restricted to the
form `λSelf.<value>`, and `resolveScriptDependencies` applies an
implicit `fix` to the script body, after supplying the script
arguments.  Like the other λs binding script arguments, the `λSelf.`
need not appear in the actual representation; we know it has to be
there so we can just store the body of the `λ`. When a script is
evaluated, the value of the script is just added to the environment in
the same way as the script arguments. The script can then refer to
its own value using `Self`.

#### Variation: Explicit lambdas

This variation is a less-restrictive version of 'value scripts'. As in
the former case, we restrict scripts syntactically to explicit
λ-expressions binding the script arguments, but we do not restrict the
script body proper to be a syntactic value. As in the former case, the
λs need not be present in the `Script` representation, because their
number is known from the number of script arguments, and the bound
variables are deBruijn indices.

In this variation, script bodies cannot be converted to `CekValue`s
using `cekValue`; we actually have to run the CEK machine to evaluate
them. This requires extending the API of the CEK machine, to support
evaluating a UPLC term *in a given environment*, and returning a
`CekValue`, rather than a discharged `NTerm`, because discharging a
`CekValue` loses sharing. Losing sharing is unacceptable because it
introduces a potentially exponential space cost for acyclic
structures, and leads to non-termination in the case of cyclic
structures (created by 'Module-level recursion').

The implementation of the CEK machine currently always discharges
values before returning them; the core loop of the machine will need
to be modified to change this.

Since script bodies must be evaluated by running the CEK machine, then
it is possible to exceed the execution unit budget at any point during
the script evaluation. The budget must be checked during these
evaluations, and the budget for evaluating each script will depend on
the actual costs of evaluating all the previous ones.

To avoid circular dependencies, the scripts must be topologically
sorted before evaluation, so that no earlier script depends on a later
one. Topological sorting is linear time in the total number of scripts
and script arguments.

It is still possible to write a recursive definition of the
`scriptValues`, so that each script can depend on the *same* map, but
care is needed to avoid circular dependencies for the reasons
explained above.

#### A Note on Tuples

The following variations make heavy use of tuples which in practice
could grow quite large--tuples of modules, and modules as tuples of
exports. These variations only make sense if projection of a component
from a tuple is *efficient*, and in particular, constant time,
independent of the tuple size. At present, tuples are represented
using the SoP extension (CIP-85) as `constr 0 x1...xn`, but the only
way to select the `i`th component is using
```
  case t of (constr 0 x1...xi...xn) -> xi
```
which takes time linear in the size of the tuple to execute, because
all `n` components need to be extracted from the tuple and passed to
the case branch (represented by a function).

We assume below that there is an expression `proj i t` in UPLC, where
`i` is a constant, which efficiently extracts the `i`th component from
tuple `t`. There are several ways this could be implemented:

- `proj i t` could be added as a new construct to UPLC, together with
  extensions to the CEK machine to evaluate it.
- `proj` could be added as a new built-in to UPLC--probably a smaller
  change to the implementation, but less efficient (because `i` would
  be an argument needing evaluation, rather than a constant integer in
  the AST), and problematic to add to TPLC (because typing it requires
  dependent types).
- represent 'tuples' in this context as functions from indices to
  components, so `(x,y,z)` would be represented as
  ```
  λi. case i of 0 -> x
      	     	1 -> y
		2 -> z
  ```
  This requires support in UPLC for pattern-matching on integers in
  constant time, which is not implemented right now, but is on the
  horizon. It would also need dependent types to be typed, and so
  cannot be added to Plinth, PIR or PTLC.

In the sections below we just use tuples and the notation `proj i t`,
on the assumption that an implementation is chosen and deployed.

#### Variation: Tuples of modules

In the main specification in this CIP, script code is a curried
function of the script arguments; that is, imported modules are
supplied to scripts as individual arguments. In this variation, the
script code is instead an *uncurried* function of the script
arguments, which are tupled together to be passed to the script code.

This variation only makes sense if the 'value scripts' variation is
also adopted, and places an additional syntactic restriction on script
code: it must be of the form `λMods.e`, and all occurrences of `Mods`
in `e` must be of the form `proj i Mods` for some `i`. That is, it is
impossible to refer to the whole tuple of modules; scripts can refer
to only one module at a time.

To avoid additional overheads for scripts without arguments, we
redefine the `Script` type as follows:
```
data Script =
    CompleteScript CompleteScript
  | ScriptWithArgs { head :: CompleteScript, args :: [Arg] }
```
Here the `CompleteScript` alternative is used for scripts without
script arguments; such scripts are not applied to a tuple of modules
before use, and so need not be of the form `λMods.e`. (This is only
needed because a 'function of zero arguments' has no overhead, while a
function of a zero-tuple does).

##### Subvariation: Global module environment

In the 'tuples of modules' variation, each script is paremeterised on
a tuple of modules, and fetches the modules when needed by projecting
out a component of the tuple. In the 'global module environment'
subvariation, *all* the modules are placed in *one* tuple, from which
scripts fetch the modules they need.

The global module environment is constructed for the transaction as a
whole, containing all the scripts provided by the transaction. It
follows that the *same* module may end up in *different* components in
different transactions. Scripts refer to other modules via references
of the form `proj i Mods`, where `Mods` is the variable bound to the
tuple of modules. Before scripts are run, these references must be
replaced by `proj j Mods`, where `j` is the index of the corresponding
module in the global module environment. Thus it is necessary to
traverse the code of all the scripts, relocating module references to
refer to the global module environment instead. One this is done, then all
the script values can refer to the *same* tuple of modules.

###### Subsubvariation: Module environment built into the CEK machine

In this subsubvariation, the (single) tuple of modules is passed (as a
new implicit parameter) directly to the CEK machine, instead of being
passed as a parameter in UPLC. Consequently it cannot be accessed as a
UPLC variable; new UPLC constructs are needed instead. Since
references to the global tuple of modules always refer to a
- particular* module, then it suffices to add a construct of the form
```
data Term name uni fun ann = ..  | ModuleRef Int
```
such that `ModuleRef i` evaluates to the `i`th component of the global
module tuple.

Once again, the scripts provided in a transaction must refer to script
arguments using an index into *the script's own* script arguments;
before execution these indices must be replaced by the corresponding
indices in the global module environment, necessitating a traversal of
the script code to prepare it for execution.

##### Subvariation: Unboxed modules

In this subvariation, we distinguish between validation scripts and
scripts representing modules; the latter are subject to an additional
syntactic restriction that the script body must be a tuple. We change
the `Script` type accordingly
```
data Script = ValidatorScript         CompiledCode [ScriptArg]
            | ModuleScript            CompiledCode [ScriptArg]
```
so that the deserializer can easily check the new syntactic
restriction. `Script`s used as `ScriptArg`s may only be of the
`ModuleScript` form (this requires a dynamic check). The idea is that
a module provides a number of exports, which are the components of the
tuple. (Again, special cases for an empty list of script arguments
can be included in this type if desired).

In addition, expressions `M` referring to modules (of the form `proj j
Mods`) may only appear in contexts of the form `proj i M`, projecting
out one of the module exports. We call these terms 'export
references'.

With this restriction, a tuple of modules is now a tuple of tuples,
and the effect of the subvariation is to flatten that into a tuple of
exports instead. Every module export is assigned an index in the
resulting tuple, and the scripts must be preprocessed before execution
to replace the indexes in every export reference by the corresponding
index in the tuple--so `proj i (proj j Mods)` becomes `proj k Mods`
for `k` the index of the `i`th export of the `j`th module.

In the case of modules which are omitted from the transaction (see
'lazy loading'), the export references `proj i (proj j Mods)` should
be replaced by `builtin unit`. This is either the correct value, or
will cause a run-time type error (and thus verification failure) if
the value is used.

In the case of a global module environment, then since the placement
of modules in a global tuple depends on *all* the modules used in a
transaction, and since some of the scripts used by a transaction are
taken from pre-existing reference UTxOs, then this preprocessing
cannot be done in advance; it must be done during script verification
of the transaction.  This subvariation can be combined with 'module
environment built into the CEK machine', in which case the export
references are replaced by suitable `ModuleRef k` expressions as
before.  It does not change the `CompiledCode` stored in scripts; it
only affects the way that code is prepared for execution.

If the module environment is *local* for each script, then the
preprocessing can be done at *compile-time*--and scripts on the chain
can be stored as functions of one big tuple, containing all the
exports from modules imported into the script. There is a subtlety in
the case of lazy loading: `resolveScriptDependencies` becomes
responsible for creating a tuple with one entry per export, *even for
modules which are not provided in the transaction*. For this to be
possible, `resolveScriptDependencies` needs to know *how many* exports
the imported module has, even without access to its body. To make this
possible we can extend the `ScriptArg` type:
```
data ScriptArg = ScriptArg ScriptHash Int
```
In the case of a module which *is* supplied, there should be a dynamic
check that the number of components agrees with the `ScriptArg`; in
the case of a module which is not supplied in the transaction,
`resolveScriptDependencies` should just add this number of `builtin
unit`s to the tuple passed to the script. Note that an attacker could
force the transaction verifier to allocate a very large amount of
memory by supplying a very large integer here, in a transaction that
does not include the module itself--and so might be cheap. To prevent
this, transaction fees must take the *values* of these integers into
account; it may also be sensible to place an absolute upper limit on
the number of exports from a module.

##### Script traversal costs

The last two subvariations above both require a traversal of all the
script code in a transaction (including the code fetched from
reference scripts) to adjust module or export references. If they are
adopted, transaction fees should be increased by an amount linear in
the total script size to pay for this traversal.

### Modules in TPLC

No change is needed in TPLC.

### Modules in PIR

No change is needed in PIR.

### Modules in Plinth

The Plinth modules introduced in this CIP bear no relation to Haskell
modules; their purpose is simply to support the module mechanism added
to UPLC. They are first-class values in Haskell.

Just as we introduced a distinction in UPLC between `CompleteScript`
and `Script`, so we introduce a distinction in Plinth between
`CompiledCode a` (returned by the Plinth compiler when compiling a
term of type `a`), and `Module a` representing a top-level `Script`
with a value of type `a`.
```
newtype Module a = Module {unModule :: Mod}

newtype ModArg = ModArg ScriptHash

data Mod = forall b. Mod{ modCode :: Maybe (CompiledCode b),
     	   	     	  modArgs :: Maybe ([ModArg]),
			  modHash :: ScriptHash }
```

Here the `modArgs` correspond to the `ScriptArg`s in the UPLC case,
and the `modHash` is the hash of the underlying `Script`.  The type
parameter of `Module a` is a phantom parameter, just like the type
parameter of `CompiledCode a`, which tells us the type of value which
the application of the `modCode` to the `modArgs` represents.

We can convert any `ScriptHash` into a module:
```
fromScriptHash :: ScriptHash -> Module a
fromScriptHash hash = Module Nothing Nothing hash
```
and we can convert any `CompiledCode` into a module:
```
makeModule :: CompiledCode a -> Module a
makeModule code = Module (Just (Mod code)) (Just []) ...compute the script hash...
```

We also need a way to supply an imported module to a `Module`:
```
applyModule :: Module (a->b) -> Module a -> Module b
applyModule (Module (Mod (Just code) (Just args) _)) m =
  Module (Mod (Just code) (Just (args++[modHash m])) ...compute the script hash...)
```
As in UPLC, the intention is that scripts that import modules be
written as lambda-expressions, and the imported module is then
supplied using `applyModule`. No change is needed in the Plinth
compiler to support this mechanism.

Note that only a `Module` containing code and an argument list can
have the argument list extended by `applyModule`; this is because the
`ScriptHash` of the result depends on the code and the entire list of
arguments, so it cannot be computed for a module that lacks either of
these.

It is `Module` values that would then be serialised to produce scripts
for inclusion in transactions.

In the 'unboxed modules' variation we need to distinguish two kinds of
scripts, scripts which define modules, and scripts which define
validators. In Plinth, this distinction can be made in the types, by
turning the `Module` type into a GADT with an extra parameter, of type
```
data ScriptType = ModuleScript | ValidatorScript
```
`applyModule` would be given a more restrictive type:
```
applyModule :: Module s (a->b) -> Module ModuleScript a -> Module s b
```
thus ensuring that only scripts representing modules are passed as
script arguments.

### Plutus Ledger Language Versions

Plutus ledger language version is what "Plutus V1", "Plutus V2", "Plutus V3" refer to.
These are not distinct programming languages; the primary difference lies in the arguments the script receives from the ledger, and the value it returns[^1].
Plutus V1, V2 and V3 can therefore be understood as type signatures, in the sense that they each represent a subset of UPLC programs with specific types. Any UPLC program that matches the expected argument and return types can be considered and used as a Plutus V1, V2 or V3 script.
A new ledger era is the primary reason for introducing a new ledger language version, though technically there can be cases where a new ledger language version is necessary without a new ledger era.

Currently each script on-chain is tagged with a specific ledger language version - V1, V2, V3 or native script - and this version tag is a component of the script hash.
A logical approach, therefore, is to continue doing so for module scripts, and require that a validator script and all modules it references must use the same ledger language version; failure to do so leads to a phase-1 error.

A different approach is to distinguish between validator scripts and module scripts by applying version tags only to validator scripts.
Module scripts are untagged and can be linked to any validator script.
This makes module scripts more reusable, which is advantageous because in most cases, a UPLC program has the same semantics regardless of the ledger language version.

This is, however, not always the case because a few builtin functions have multiple semantic variants, and the variant used may differ depending on the ledger language version.
Nonetheless, if a module script depends on a particular ledger language version to work correctly, this requirement can be communicated through alternative means, e.g., as a piece of metadata in a module script registry.

Another drawback of untagged modules is that untagged modules will be a new concept that doesn't currently exist, and as a result, modules will not be usable in Plutus V1 through V3, and can only be used from Plutus V4 onwards.

### Plutus Core Versions

Plutus Core version is the usual sense of version pertaining to programming languages - in this instance the Plutus Core language.
So far there have been two Plutus Core versions: 1.0.0 and 1.1.0. 1.1.0 adds sums-of-products to the language by introducing two new AST node types: Case and Constr.
See [CIP-85](https://cips.cardano.org/cip/CIP-0085) for more details.
Each UPLC program is tagged with a Plutus Core version (where as for ledger language versions, only _scripts_ that exist on-chain, i.e., stored in UTXOs, are tagged with ledger language versions).

UPLC programs with different Plutus Core versions are incompatible and cannot be combined, and therefore, a validator script and all modules it references must share the same Plutus Core version; otherwise it is a phase-1 error.

### Implementation and Integration Considerations

Here we use the term "script" to refer to either a validator script (which needs to be run to validate a transaction) and a module script (which serves as a dependency for other scripts).
Both validators and modules can reference other modules.

The feature proposed in this CIP can only be released in a new ledger era.
As such, it is anticipated that it will be released alongside the next ledger era - the Dijkstra era.

Whether this feature can be used in existing Plutus ledger language versions (V1 through V3) depends on which of the options described in subsection _Plutus Ledger Language Versions_ (i.e., tagged or untagged modules) is chosen.
If tagged modules are adopted, the feature will be available across all Plutus language versions (V1 through V4) starting at the hard fork that introduces the Dijkstra era.
If untagged modules are adopted, it will only be usable in Plutus V4, as explained in the subsection.

The bulk of the implementation effort lies on the Plutus side, including updates to `plutus-ledger-api`, updates to the CEK machine, costing and benchmarking, among others.
The specifics will depend on which of the various alternatives outlined in this CIP is selected.
The Plutus team aims to complete the implementation of the selected approach according to its specification, in time for the Dijkstra era.

On the ledger and cardano-api side, the effort required to support this feature is not as substantial as it may appear to be.
This is because the ledger already supports reference inputs and reference scripts since the Babbage era, and this existing mechanism can largely be reused to accommodate module scripts.
The processes of storing a module script in a UTXO and using it in a transaction are similar to storing and using a reference script.

The main difference between reference scripts and module scripts is that a module script is, like an object file, not directly runnable but must be linked with a validator to form a runnable script.
To support this, the ledger and cardano-api will need to implement some changes.
The specifics will slightly vary depending on which of the alternative approaches is chosen, but it will generally involve the following.

Currently, deserialising a script returns a `ScriptForEvaluation`, which contains a deserialised script, along with the original serialised script. The ledger has a `PlutusRunnable` newtype that wraps `ScriptForEvaluation`.
With the introduction of modules, deserialising a script no longer produces a runnable script unless it is a self-contained validator that doesn't use modules.
Otherwise, the module hashes it references must be resolved and the modules linked before the validator can be executed.

To do so, the `plutus-ledger-api` package can implement one of two options, depending on which is more suitable for the ledger:
- Script deserialisation will be modified to return a new data type, `ScriptForLinking`.
  It is similar to `ScriptForEvaluation` except that the deserialized script is not necessarily a self-contained script and may be accompanied by a list of module hashes it needs.

  Then, a function `linkValidator :: Map ScriptHash ScriptForLinking -> ScriptHash -> LinkedScript` is provided that performs linking for a particular validator identified by `ScriptHash`, where `LinkedScript ~ UPLC.Program DeBruijn DefaultUni DefaultFun ()` is a fully linked script.
- Alternatively, the following function can be provided: `linkScripts :: Map ScriptHash SerialisedScript -> Map ScriptHash LinkedScript`, which performs deserialisation and linking for all (validator and module) scripts in one go.

In either case, the ledger should ensure that each script (including validator script and module script) is deserialised and processed no more than once.

Moreover, for the transaction builder to decide which modules a validator refers to are used at runtime, `plutus-ledger-api` will also expose the following function:

```haskell
getUsedModules ::
  MajorProtocolVersion ->
  EvaluationContext ->
- - | All scripts provided in a transaction
  Map ScriptHash SerialisedScript ->
- - | Hash of the validator
  ScriptHash ->
- - | Script arguments
  [Data] ->
- - | Hashes of used module scripts
  Set ScriptHash
```

The value type of the `Map` could instead be `ScriptForLinking` (i.e., deserialised script) rather than `SerialisedScript`.

This function is to be called by the code building transactions (e.g., `Cardano.Api.Fees.makeTransactionBodyAutoBalance`) to determine which modules are necessary to include in a transaction.

## Rationale: how does this CIP achieve its goals?

This CIP provides a minimal mechanism to split scripts across several
transactions. 'Imported' modules are provided in the calling
transaction and passed as arguments to the top-level script, and their
identity is checked using their hash. In the main specification, the
representation of modules is left entirely up to compiler-writers to
choose--a module may be any value at all (although some of the
variations do restrict the form). For example, one compiler might
choose to represent modules as a tuple of functions, while another
might map function names to tags, as Solidity does, and represent a
module as a function from tags to functions. Each language will need
to define its own conventions for module representations, and
implement them on top of this low-level mechanism. For example, a
typed language might represent a module as a tuple of exported values,
and store the names and types of the values in an (off-chain)
interface file. Clients could use the interface file to refer to
exported values by name, and to perform type-checking across module
boundaries.

### Recursive modules

This design does not support mutually recursive modules. Module
recursion is sometimes used in languages such as Haskell, but it is a
rarely-used feature that will not be much missed.

### Cross-language calls

There is no a priori reason why script arguments need be written in
the same high-level language as the script itself; thus this CIP
supports cross-language calls. However, since different languages may
adopt different conventions for how modules are represented, then some
'glue code' is likely to be needed for modules in different languages
to work together. In the longer term, it might be worthwhile defining
an IDL (Interface Definition Language) for UPLC, to generate this glue
code, and enable scripts to call code in other languages more
seamlessly. This is beyond the scope of this CIP; however this basic
mechanism will not constrain the design of such an IDL in the future.

In Plinth, because the `Module` type is a phantom type, it is easy to
take code from elsewhere and turn it into a `Module t` for arbitrary
choice of `t`; this can be used to import modules compiled from other
languages into Plinth (provided a sensible Plinth type can be given to
them).


### Static vs Dynamic Linking

With the introduction of modules, scripts are no longer
self-contained--they may depend on imported modules. This applies both
to scripts for direct use, such as spending verifiers, and to scripts
representing modules stored on the chain.  A module may depend on
imported modules, and so on transitively. An important question is
when the identity of those modules is decided. In particular, if a
module is replaced by a new version, perhaps fixing a bug, can
- existing* code on the chain use the new version instead of the old?

The design in this CIP supports both alternatives. Suppose a module
`A` imports modules `B` and `C`. Then module `A` will be represented
as the lambda-expression `λB.λC.A`. This can be compiled into a
`CompleteScript` and placed on the chain, with an empty list of
`ScriptArg`s, as a reference script in a UTxO, allowing it to be used
with any implementations of `B` and `C`--the calling script must pass
implementations of `B` and `C` to the lambda expression, and can
choose them freely. We call this 'dynamic linking', because the
implementation of dependencies may vary from use to use. On the other
hand, if we want to *fix* the versions of `B` and `C` then we can
create a `Script` that applies the same `CompleteScript` to two
`ScriptArg`s, containing the hashes of the intended versions of `B`
and `C`, which will then be supplied by
`resolveScriptDependencies`. We call this 'static linking', because
the version choice for the dependency is fixed by the script. It is up
to script developers (or compiler writers) to decide between static
and dynamic linking in this sense.

On the other hand, when a script is used directly as a validator then
there is no opportunity to supply additional arguments; all modules
used must be supplied as `ScriptArg`s, which means they are
fixed. This makes sense: it would be perverse if a transaction trying
to spend a UTxO protected by a spending validator were allowed to
replace some of the validation code--that would open a real can of
worms, permitting many attacks whenever a script was split over
several modules. With the design in the CIP, it is the script in the
UTxO being spent that determines the module versions to be used, not
the spending transaction. That transaction does need to *supply* all
the modules actually used--including all of their dependencies--but it
cannot choose to supply alternative implementations of them.

### In-service upgrade

Long-lived contracts may need upgrades and bug fixes during their
lifetimes. This is especially true of contracts made up of many
modules--every time a dependency is upgraded or receives a bug fix,
the question of whether or not to upgrade the client contract
arises. However, the problem of upgrading contracts *securely* is a
tricky one, and exists whether or not modules are used. Therefore this
CIP does not address this problem specifically: developers should use
the same mechanisms to handle upgrades of scripts with dependencies,
as they use to upgrade scripts without dependencies. The only thing we
note here is that the need for upgrades is more likely to arise when a
script depends on many others, so it is more important to be prepared
for it. Note that, because a script contains the hashes of its
dependencies, no 'silent' upgrade can occur: the hash of a script
depends on *all* of its code, including the code of its dependencies,
so any change in a dependency will lead to a change in the script
hash.

### Lazy loading

The 'lazy loading' variation in the specification section above
permits fewer modules to be supplied in a transaction.  Dependency
trees have a tendency to grow very large; when one function in a
module uses another module, it becomes a dependency of the entire
module and not just of that function. It is easy to imagine situations
in which a script depends on many modules, but a particular call
requires only a few of them. For example, if a script offers a choice
of protocols for redemption, only one of which is used in a particular
call, then many modules may not actually be needed. The variation
allows a transaction to omit the unused modules in such cases. This
reduces the size of the transaction, which need provide fewer
witnesses, but more importantly it reduces the amount of code which
must be loaded from reference UTxOs.

If a script execution *does* try to use a module which was not
provided, it will encounter a run-time type error and fail (unless the
module value was `builtin unit`, in which case the script will behave
as though the module had been provided).

To take advantage of this variation, it is necessary, when a
transaction is constructed, to *observe* which script arguments are
actually used by the script invocations needed to validate the
transaction. The transaction balancer runs the scripts anyway, and so
can in principle observe the uses of script arguments, and include
witnesses in the transaction for just those arguments that are used.

#### Balancer modifications

To take advantage of 'lazy loading', it's necessary to identify
reference scripts that are *dynamically* unused, when the scripts in a
transaction run. The best place to do that is in a transaction
balancer, which needs to run the scripts anyway, both to check that
script validation succeeds, and to determine the number of execution
units needed to run the scripts. We adopt the view that

- *A transaction balancer may drop reference inputs from a
   transaction, if the resulting transaction still validates**

We call reference scripts which are not actually invoked during script
verification 'redundant'; these are the reference scripts that can be
removed by the balancer.

##### First approach: search

The simplest way for a balancer to identify redundant reference inputs
is to try rerunning the scripts with an input removed. If script
validation still succeeds, then that input may safely be removed. The
advantages of this approach are its simplicity, and lack of a need for
changes anywhere else in the code. The disadvantage is that
transaction balancing may become much more expensive--quadratic in the
number of scripts, in the worst case.

The reason for this is that removing one script may make others
redundant too; for example if script A depends on script B, then
script B may become redundant only after script A has been
removed--simply evaluating script A may use the value of B, and
scripts are evaluated when they are passed to other scripts, whether
they are redundant or not. So if the balancer tries to remove B first,
then script verification will fail--and so the balancer must try again
to remove B after A has been shown to be redundant. Unless we exploit
information on script dependencies, after one successful script
removal then all the others must be revisited. Hence a quadratic
complexity.

In the case of 'value scripts' this argument does not apply:
evaluating a script will never fail just because a different script is not
present. In this case it would be sufficient to traverse all the
scripts once, resulting in a linear number of transaction
verifications.

##### Second approach: garbage collection

First the balancer analyses all the scripts and reference scripts in a
transaction, and builds a script dependency dag (where a script
depends on its `ScriptArg`s). Call the scripts which are invoked
directly in the transaction (as validators of one sort or another) the
- root* scripts.

Topologically sort the scripts according to the dependency relation;
scripts may depend on scripts later in the order, but not
earlier. Now, traverse the topologically sorted scripts in order. This
guarantees that removing a *later* script in the order does not cause
an *earlier* one to become redundant.

For each script, construct a modified dependency graph by removing the
script concerned, and then 'garbage collecting'... removing all the
scripts that are no longer reachable from a root. Construct a transaction
including only the reference scripts remaining in the graph, and run
script validation. If validation fails, restore the dependency graph
before the modification. If validation succeeds, the script considered
and all the 'garbage' scripts are redundant; continue using the now
smaller dependency graph.

When all scripts have been considered in this way, then the remaining
dependency graph contains all the scripts which are dynamically needed
in this transaction. These are the ones that should be included in the
transaction, either directly or as reference scripts.

The advantage of this approach is that only the code in the balancer
needs to be changed. The disadvantage is that transaction balancing
becomes more expensive: script verification may need to be rerun up to
once per script or reference script. In comparison to the first
approach above, this one is more complex to implement, but replaces a
quadratic algorithm by a linear one.

##### Third approach: modified CEK machine

The most direct way to determine that a script is not redundant is to
observe it being executed during script verification. Unfortunately,
the CEK machine, in its present form, does not make that
possible. Thus an alternative is to *modify the CEK machine* so that a
balancer can observe scripts being executed, and declare all the other
scripts redundant. In comparison to the first two approaches, this is
likely to be much more efficient, because it only requires running
script verification once.

The modifications needed to the CEK machine are as follows:

`CekValue`s are extended with *tagged values*, whose use can be
observed in the result of a run of the machine.
```
data CekValue uni fun ann =
  ...
  | VTag ScriptHash (CekValue uni fun ann)
```
In the 'value script' variation, no expression resulting in a `VTag`
value is needed, because `VTag`s will be inserted only by
`resolveScriptDependencies`. In other variations, a `Tag` constructor
must also be added to the `NTerm` type, to be added by
`resolveScriptDependencies`. In either case the version of
`runScriptDependencies` *used in the balancer* tags each value or
subterm derived from a `ScriptHash` `h` as `VTag h ...` (or `Tag h
...` in variations other than 'value scripts').

The CEK machine is parameterized over an emitter function, used for
logging. We can make use of this to emit `ScriptHash`es as they are
used. This allows the balancer to observe which `ScriptHash`es *were*
used.

Simply evaluating a tagged value, or building it into a
data-structure, does not *use* it in the sense we mean here: replacing
such a value with `builtin unit` will not cause a validation
failure. Only when such a value is actually *used* should we strip the
tag, emit the `ScriptHash` in the `CekM` monad, and continue with the
untagged value. This should be done in `returnCek`, on encountering a
`FrameCases` context for a tagged value, and in `applyEvaluate` when
the function to be applied turns out to be tagged, or when the
argument to a `builtin` turns out to be tagged.

Adding and removing tags must be assigned a zero cost *in the
balancer*, since the intention is that they should not appear in
transactions when they are verified on the chain. Thus a zero cost is
required for the balancer to return accurate costs for script
verification on the chain. On the other hand, if these operations *do*
reach the chain, then they should have a *high* cost, to deter attacks
in which a large number of tagging operations are used to keep a
transaction verifier busy. This can be achieved by adding a `BTag`
step kind to the CEK machine, a `cekTagCost` to the
`CekMachineCostsBase` type, and modifying the balancer to set this
cost to zero during script verification.

The advantage of this approach is that it only requires running each
script once in the balancer, thus reducing the cost of balancing a
transaction, perhaps considerably. The disadvantage is that it
requires extensive modifications to the CEK machine itself, a very
critical part of the Plutus infrastructure.

##### Fourth approach: lazy scripts

Another way to observe script uses *without* modifying the CEK machine
is to wrap them in `Delay` and force them at the point of use. The
balancer can then insert trace output of the script hash just inside
the `Delay`, and so observe which scripts are actually forced during
script execution.

The difficulties with this approach arise from the fact that delayed
closures must be *explicitly* forced in UPLC; this does not 'just
happen' when a delayed value is used. This means that corresponding
`Force` operations must also be added to scripts, and the question is:
who does this, and if it is to be done automatically, then how?

One possibility is that it is the developer's responsibility to force
script arguments at the point of use--that is, that the `Force`
operations needed would be written by the human programmer. It follows
that they would *always* be part of the script, even when running on
the chain, and so even on the chain script arguments would need to be
delayed (even if no trace output would be needed). This would increase
code size a little, and impose a force-delay overhead on every
cross-module reference, which is probably not acceptable.

The alternative is to have the balancer insert corresponding `Force`
operations, as well as the `Delay`s. A simple way to do so would be
to add a `Force` around every use of a variable corresponding to a
script argument--under the 'value scripts' syntactic restriction these
variables are easy to identify. These modifications would not be made
during normal script verification, which might therefore cost less--or
more--than the modified balancer run. The balancer would thus need to
perform script verification twice: once with `Delay` and
`Force`inserted to determine redundant scripts, and then a second time
(with redundant scripts removed) to determine the actual cost on the
chain.

The bigger problem with this approach, though, is that it will
- overestimate* the set of used scripts, leading to more scripts being
used in a transaction, and thus potentially exponentially more
expensive transactions. The reason for the overestimation is that
- all* occurrences of variables bound to script arguments are wrapped
in `Force`, even those that would not lead to untagging the
corresponding tagged value in the third approach above. For example,
suppose a variable bound to a script argument is passed as a parameter
to another function. With the simple `Force`-placement strategy
described above, the script argument would be forced *at that call*,
making the corresponding script appear to be used, even though the
function it is passed to might not actually use it in all cases. Hence
the set of scripts used would be overestimated.

One might use a more sophisticated strategy to insert `Force`
operations. For example, in the case described above one might pass
the script argument *unforced* to the function, and modify the
function to force it when it is used. This would require the balancer
to perform a flow analysis, to identify the functions that might be
passed a delayed script argument. Moreover, such functions might be
called *sometimes* with a delayed script argument, and sometimes
not. The code could be replicated to create two versions of such
functions. But with *n* script arguments, this might require up to
- 2^n* versions of each function, leading to an exponential increase in
code size. An attacker could exploit this to craft a transaction that
would cause the balancer to run out of memory. This is really not
attractive.

Finally, one might finesse these problems by modifying the CEK machine
to force delayed closures automatically where the value is required,
thus enabling explicit `Force` operations to be omitted. This would
effectively turn UPLC into a call-by-name programming language. That would

enable this problem to be solved more easily, but  at the cost of
reversing a rather fundamental design decision in UPLC--and probably
making the CEK machine a little bit slower, for all programs.

Thus it appears that there is no good way of using UPLC's existing
lazy evaluation to observe use of script arguments.

### Value Scripts

This section discusses the 'value scripts' variation.

The main specification in this CIP represents a `Script` that imports
modules as compiled code applied to a list of `ScriptHash`es. To
prepare such a script for running, `resolveScriptDependencies`
replaces each hash by the term it refers to, and builds nested
applications of the compiled code to the arguments. These applications
must be evaluated by the CEK machine *before* the script proper begins
to run. Moreover, each imported module is itself translated into such
a nested application, which must be evaluated before the module is
passed to the client script. In a large module hierarchy this might
cause a considerable overhead before the script proper began to
run. Worst of all, if a module is used *several times* in a module
dependency tree, then it must be evaluated *each time* it is
used. Resolving module dependencies traverses the entire dependency
- tree*, which may be exponentially larger than the dependency *dag*.

The value script variation addresses this problem head on. Scripts are
converted directly into CEK-machine values that can be invoked at low
cost. Each script is converted into a value only once, no matter how
many times it is referred to, saving time and memory when modules
appear several times in a module hierarchy.

On the other hand it does restrict the syntactic form of
scripts. Scripts are restricted to be syntactic lambda expressions,
binding their script arguments at the top-level. This is not so
onerous. But inside those λs, there must also be a syntactic
value. For example, consider a module represented by a tuple, whose
components represent the exports of the module. Then all of those exports
need to be syntactic values--an exported value could not be computed
at run-time, for example using an API exported by another
module. While many module exports are functions, and so naturally
written as λ-expressions (which are values), this restriction will be
onerous at times.

This method does require opening up the API of the CEK machine, so
that CEK values can be constructed in other modules, and introducing a
way to run the machine starting from a given configuration. So it
requires more invasive changes to the code than the main
specification.

#### `ScriptHash` allowed in terms?

An alternative design would allow UPLC terms to contain `ScriptHash`es
directly, rather than as λ-abstracted variables, to be looked up in a
global environment at run-time. This would address this same problem:
the cost of performing many applications before script evaluation
proper begins. It would also require changes to the CEK machine, and
is not really likely to perform better than the 'value scripts'
variation (in practice, the main difference is the use of a global
environment to look up script hashes, as opposed to many per-module
ones). However, this approach is less flexible because it does not
support dynamic linking (see Static vs Dynamic Linking above). Once a
`ScriptHash` is embedded in a term, then a different version of the
script cannot readily be used instead. Moreover, script hashes are
quite large (32 bytes), and including more than a few in a script
would quickly run into size limits.

#### Module-level recursion

This section discusses the `module-level recursion` subvariation of
the `value scripts` variation.

UPLC provides a fixpoint combinator, and this is how recursion is
compiled. For the sake of argument, consider the well-known fixpoint
combinator `Y` (in reality, `Y` is not suitable for use in a strict
programming language, so the UPLC version is slightly different). We
can imagine that a recursive function `f` is compiled as `Y h`, for
some suitable `h`.

The difficulty that arises is that `Y h` *is not a value*, and thus
cannot appear at the top-level of a module, under the 'value script'
restriction. It can be *normalised* into a value, of course, using
```
Y h ---> h (Y h)
```
and then reducing the application of `h`; this would need to be done
by a compiler generating UPLC with the `value script`
restriction. But reducing `h (Y h)` may well duplicate `Y h`. When
this happens at CEK runtime it is not a problem, because all the
occurrences of `Y h` are represented by the same pointer. But when the
reductions are applied by a compiler, and the resulting term is
serialized to UPLC code for inclusion in a script, then each
occurrence of `Y h` will be serialized separately, losing sharing and
causing code duplication in the resulting script. The result could be
- larger* code, the opposite of what we are trying to achieve. Thus
this method of compiling recursion fits badly with the 'value scripts'
variation.

Hence module-level recursion, which allows recursive occurrences of
script values to be referred to via the `Self` variable instead of
using a fixpoint combinator implemented in UPLC. To take advantage of
this feature, the compiler will need to float occurrences of `fix`
upwards, to the top-level of a module. This can be done using suitable
analogues of the rules
```
(..,fix (λx.e),...) ---> fix (λx.(..,e[proj i x/x],..))
```
where `i` is the index in the tuple at which `fix (λx.e)` appears,
`proj i x` selects the `i`th component from `x`, and `x` does not
occur free elsewhere in the tuple; a corresponding rule for
constructor applications; and
```
fix (λx. fix (λy.e)) ---> fix (λx. e[x/y])
```
Both these rules require adjusting deBruijn numbers in the UPLC
implementation.

The intention here is to implement module-level recursion using a
cyclic data-structure--the value restriction guarantees that the
module value `Self` is not needed to compute the top-level value of
the module, and thus there is no risk of falling into an infinite loop
at this point. (Of course, a recursive function can loop *when it is
called*, but constructing the function itself cannot loop because it
must be a syntactic λ-expression). This is a *more efficient* way to
implement recursion than the fixpoint combinators currently used in
UPLC, and so will probably become the preferred way to implement
recursion.

Note that if we adopt value scripts, but *not* module-level recursion,
then modules will be unable to export recursive functions without
'hiding' them in a value, such as a `Delay`.

#### Variation: Explicit lambdas

This variation lifts some of the restrictions of the 'value scripts'
approach, at the cost of running the CEK machine to evaluate each
module, and taking care to compute and check costs correctly for the
new CEK machine runs. This requires topological sorting of the scripts
in a transaction before evaluation, to guarantee that we do not
encounter a situation where script A depends on script B, but the
budget for computing script B depends on the cost of script A--such a
situation would lead to a blackhole error during script verification.

Because script bodies may now be arbitrary terms, 'module-level
recursion' is no longer essential--it is possible to use fixpoint
combinators in script bodies as at present. It would still improve
efficiency, of course.

Note that if modules *do* meet the syntactic restrictions of 'value
scripts', then this variation will be less efficient than 'value
scripts'--sometimes considerably so. This is because even evaluating,
say, a large tuple whose components are λ-expressions, leads the CEK
machine to descend into, evaluate, and return out of, each component,
thus performing several CEK transitions per element. The `cekValue`
function must also visit each component, of course, doing the same
work, but because this is done directly in Haskell then it will be
considerably more efficient.

This variation is compatible with the various tuple-based variations,
but when the script body is constrained to return a tuple then this
must be checked dynamically when CEK-evaluation is complete; the check
cannot be built into deserialization any more because it is no longer
syntactic.

#### Variation: Tuples of modules

This variation changes the way modules are referenced in scripts: in
the main specification, each imported module is bound to a name in the
environment, and referenced using the associated variable; in this
variation *all* imported modules are bound to a single name, and
modules are referenced by projecting the corresponding component from
the tuple bound to this name.

Thus: in the main specification, a module reference costs one name
lookup; in this variation, a module reference costs a name lookup plus
projection of a component from a tuple. However, because projecting a
component from a tuple is constant time, while the cost of a
name lookup is logarithmic in the number of names in the environment,
then this variation may reduce the cost of module references--since
scripts which import many modules will run with significantly fewer
names in the environment.

Note that the uncurried script form can be generated from the curried
one, by
- introducing a `λMods.` outermost,
- removing the `λ`s binding names to script arguments,
- substituting `proj i Mods` for the `i`th script argument name in the
script body

Thus there is no need for any change to earlier parts of the compiler,
or to the languages Plutus, PIR, or TPLC. Tuples of modules can be
introduced as a last step in the generation of UPLC.

##### Subvariation: Global module environment

The advantage of using a global module environment instead of one
tuple of modules per script is that only one, big, tuple of modules
per transaction need be constructed, instead of one per script. The
cost is an additional traversal of the script code, needed to adjust
module indices to refer to the correct index in the global tuple of
modules. By itself, this is unlikely to improve performance.

However, using a global module environment is a prerequisite to
building the module environment into the CEK machine. Doing the latter
transforms a module reference from a projection from the
tuple-of-modules variable, to a custom construction `ModuleRef i` that
directly accesses the module in the `i`th component of the global
module environment. This reduces the cost from a variable lookup plus
a projection, to just a projection; this can be expected to speed up every
reference to an external module.

##### Subvariation: Unboxed modules

This subvariation makes every reference to a module export cheaper, by
replacing two projections from a tuple by one. It does require
preprocessing script code before it is run, updating export references
to refer to the correct element of the large tuple combining several
modules. This requires a traversal of all the script code in a
transaction, which must be performed every time script verification is
run, including on the chain. Because of this, it makes most sense to
use this subvariation in combination with 'global module environment',
which also requires such a traversal. In both cases, the purpose is to
adjust references to refer to the correct index in the new, merged
data structure; a single traversal suffices to achieve both ends.

The syntactic restriction, requiring a module body to be a tuple of
exports, is not onerous. While some compilers might wish to represent
a module as built-in data, or as a function from a tag (as Solidity
does), this can be achieved by placing the intended module value as
the first component of a one-tuple. The implementation described here
optimises away the selection of an element from such a tuple, so the
restriction introduces no extra overhead in this case.

### Transaction fees

Imported modules are provided using reference scripts, an existing
mechanism (see CIP-33), or in the transaction itself. Provided the
cost of loading reference scripts is correctly accounted for, this CIP
introduces no new problems.

Note that there is now (since August 2024) a hard limit on the total
size of reference scripts used in a transaction, and the transaction
fee is exponential in the total script size (see
[here](https://github.com/IntersectMBO/cardano-ledger/blob/master/docs/adr/2024-08-14_009-refscripts-fee-change.md)).The
exponential fees provide a strong motivation to prefer the 'lazy
loading' variation in this CIP: even a small reduction in the number
of reference scripts that need to be provided may lead to a large
reduction in transaction fees.

The motivation for these fees is to deter DDoS attacks based on
supplying very large Plutus scripts that are costly to deserialize,
but run fast and so incur low execution unit fees. While these fees
are likely to be reasonable for moderate use of the module system, in
the longer term they could become prohibitive for more complex
applications. It may be necessary to revisit this design decision in
the future. To be successful, the DDoS defence just needs fees to
become *sufficiently* expensive per byte as the total size of
reference scripts grows; they do not need to grow without bound. So
there is scope for rethinking here.

Some of the variations in this CIP require a traversal of all the
script code in a transaction to adjust module references before
execution. This should be reflected by a component in the transaction
fee linear in the total size of scripts.

### Verification

Since scripts invoked by a transaction specify all their dependencies
as hashes, then the running code is completely known, and testing or
formal verification is no harder than usual. Standalone verification
of modules using 'dynamic linking' poses a problem, however, in that
the code of the dependencies is unknown. This makes testing
difficult--one would have to test with mock implementations of the
dependencies--and formal verification would require formulating
assumptions about the dependencies that the module can rely on, and
later checking that the actual implementations used fulfill those
assumptions.

### Impact on optimisation and script performance

Splitting a script into separately compiled parts risks losing
optimisation opportunities that whole-program compilation gives. Note
that script arguments are known in advance, and so potentially some
cross-module optimisation may be possible, but imported modules are
shared subterms between many scripts, and they cannot be modified when
the client script is compiled. Moreover, unrestrained inlining across
module boundaries could result in larger script sizes, and defeat the
purpose of breaking the code into modules in the first place.

On the other hand, since the size limit on scripts will be less of a
problem, then compilers may be able to optimize *more*
aggressively. For example, today the Plinth inliner is very careful
not to increase script size, but once modules are available it may be
able to inline more often, which can enable further optimizations.

Moreover, today we see examples of deliberate choice of worse
algorithms, because their code is smaller, and easier to fit within
the script size limit. Removing the need to make such choices can
potentially improve performance considerably.

### Example: Defining and using a Set module

As an example of how the module system might be used in a high-level
language, consider the following code, which defines and uses a module
implementing set insertion and membership testing, using an ordered
binary tree.
```
data Tree a = Leaf | Branch (Tree a) a (Tree a)

empTree = Leaf

insTree a Leaf = Branch Leaf a Leaf
insTree a (Branch l b r)
  | a < b = Branch (insTree a l) b r
  | a > b = Branch l b (insTree a r)
  | a== b = Branch l b r

memTree a Leaf = False
memTree a (Branch l b r)
  | a < b = memTree a l
  | a > b = memTree a r
  | a== b = True

data Set = Set {emptySet :: forall a. Tree a,
     	        insertSet :: forall a. Ord a => a -> Tree a -> Tree a,
		memberSet :: forall a. Ord a => a -> Tree a -> Bool}

setMod = Set empTree insTree memTree

setModule :: Module Set
setModule = makeModule ($$(PlutusTx.compile [| setMod |]))

client set redeemer _ = memberSet set redeemer s
  where s = insertSet set 1 (insertSet set 2 (emptySet set))

clientModule = makeModule ($$(PlutusTx.compile [| client |]))
 `applyModule` setModule
```
Here the module signature is represented by a Haskell record type;
Haskell records are compiled into tuples in UPLC, and the record
fields are all values (once fixpoints are floated upwards to the
module level), so the `setModule` in this example fits the 'unboxed
modules' syntactic restrictions. The client script takes the record as
an argument, and uses the module exports via record field selectors,
which compile to projections from the tuple. Thus the client also
meets the syntactic restrictions for 'unboxed modules'. To make use
of these modules, the off-chain code must construct a UTxO
containing `setModule` as a reference script, and include it as a
reference UTxO in transactions that use the client.

### Related work

#### Merkelized Validators

Philip DiSarro describes ["Merkelized
validators"](https://github.com/Anastasia-Labs/design-patterns/blob/main/merkelized-validators/merkelized-validators.md),
which offer a way of offloading individual function calls to stake
validators: the client script just checks that the appropriate stake
validator is invoked with a particular function-argument and result
pair, checks that the argument equals the argument it wants to call
the function with, and then uses the result as the result of the
function. The stake validator inspects the argument-result pair,
computes the function for the given argument, and checks that the
result equals the result in the pair. This design pattern enables the
logic of a script to be split between the client script and the stake
validator, thus circumventing the limits on script size. But the main
point is that the function call, whose result may be needed by several
validators, can be computed just *once* per transaction. More details
can be found
[here](https://github.com/Anastasia-Labs/design-patterns/blob/main/stake-validator/STAKE-VALIDATOR-TRICK.md).

Factoring out a shared part of the validation in this way is a
generally useful technique which is largely independent of the
existence of modules--this CIP does not remove the need for sharing
work between validators, and indeed this trick will work equally well
once modules are added. But as a way of *implementing* modules, it is
rather intricate and unsatisfactory.

#### The WebAssembly Component Model

The [Web Assembly Component
Model](https://component-model.bytecodealliance.org/) defines a
high-level IDL to enable components written in different programming
languages (such as C/C++, C#, Go, Python and Rust), to work together
in one WebAssembly system. WASM already has a module system, and a
WASM component may consist of a number of modules (written in the same
language). The focus here is on interaction between *different*
programming languages in a well-typed way. Defining such an IDL for
Cardano might be useful in the future, but it is too early to do so
now.

### Preferred Options

Allowing script code to be spread across many transactions lifts a
commonly complained-about restriction faced by Cardano script
developers. It permits more complex applications, and a much heavier
use of libraries to raise the level of abstraction for script
developers. Modules are already available on the Ethereum blockchain,
and quite heavily used. Adopting this CIP, in one of its variations,
will make Cardano considerably more competitive against other smart
contract platforms.

The *main alternative* in this CIP is the simplest design, is easiest
to implement, but suffers from several inefficiencies.

The *lazy loading* variation allows redundant scripts to be omitted
from transactions, potentially making transactions exponentially
cheaper. To take full advantage of it requires a balancer that can
drop redundant scripts from transactions. Three alternative methods
are described: *search*, the simplest, which must run script
verification a quadratic number of times in the number of scripts in
the worst case; *garbage collection*, a self-contained change to the
balancer which analyses script dependencies and thus needs to run script
verification only a linear number of times; a *modified CEK machine*
which adds tagged values to the machine, which the balancer can use to
identify redundant scripts in *one* run of script verification,
possibly requiring one more run to make accurate exunit cost estimates.

The *value scripts* variation restricts scripts to be explicit
λ-expressions binding the script arguments, with an innermost script
body which is a syntactic value. Such scripts can be converted to CEK
values in a single traversal; each script can be converted to a value
- once per transaction*, rather than at every use. *Module-level
recursion* enables recursive definitions to recurse via the module
itself, rather than locally, and makes the syntactic-value restriction
easier to satisfy. This variation is expected to reduce the start-up
costs of running each script considerably; on the down-side the
syntactic restriction would be a little annoying, and it requires CEK
operations which are not currently part of the API, so it requires
modifications to a critical component of the Plutus implementation.

The *explicit λs* variation is a half-way house between the main
variation and the 'value scripts' variation. It places less onerous
syntactic restrictions on script bodies, and as such can be used with
the existing implementation of recursion (although efficiency would
still benefit from module-level recursion). Cost accounting during
script evaluation is a little intricate. It requires modifications to
the loop at the core of the CEK machine.

The *tuples of modules* variation replaces parameters referring to
individual modules with a single parameter bound to a tuple of
modules, effectively uncurrying scripts wrt their module
parameters. At the cost of a traversal of all the script code in a
transaction to 'relocate' module references, it is possible to replace
many tuples-of-modules, one per script, by a global tuple of modules
for the entire transaction; a further improvement would then be to
unbox modules, replacing the global tuple of modules with a global
tuple of module exports. These variations reduce the cost of referring
to a module export, at the cost of an additional traversal of the
script code before execution. Extensive benchmarking would be needed
to decide whether or not they improve performance overall.

Performance can probably be improved further by building the module
environment in to the CEK machine.

The simplest alternative to implement would be the main alternative
without variations. A more efficient implementation would combine
value scripts with lazy loading, using tagged values in the CEK
machine to analyse dynamic script dependencies in the balancer, and so
drop redundant scripts from each transaction. Further improvements to
performance may be achievable using a global module environment, and
unboxed modules; because there are performance costs as well as
benefits to these approaches, extensive benchmarking would be required
to make an informed choice.

These latter variations all require modifications to the CEK machine
and to the balancer, as well as resolving dependencies in scripts;
that is, they are considerably more expensive to implement.

There are many variations proposed in this CIP; I do have an opinion
as to which choices might be best. These are opinions, so might be
proven wrong later by benchmarks.

Firstly, I believe lazy loading will be very valuable, especially for
small, cheap transactions.

I don't think lazy loading will make *any* transaction worse, it's
simply a win. In general, though, many choices have effects that will
differ for different transactions, speeding some up while slowing
others down.  Many variations do some work before running scripts, in
order to speed them up when they are actually run. These variations
will tend to make small, cheap transactions *more* expensive, because
there will be too little execution time to recoup the initial
investment, while they make long-running transactions cheaper. It's
necessary to make a choice here, and decide what kind of transactions
to prioritize. Personally, I favour keeping small, cheap transactions
cheap, even at the cost of making longer running transactions a bit
slower. So I favour choosing a variation that does work in advance
only if the break-even point is reached very quickly. This is a
personal opinion, and could be questioned: the important thing is
think about this issue and make the choices deliberately.

With this in mind, I am against variations that require a traversal of
all the script code *during phase 2 verification*. This would be, in
particular, the 'global module environment' and the 'module
environment built into the CEK machine' variations. Note that
syntactic restrictions which can be implemented during deserialization
do not require an extra traversal of the code in this sense. Also, the
'unboxed modules' variation--*in its local form*--requires a traversal
of the script code *which treats each script independently*, and so
can be done at compile-time. This is not either a run-time cost that
concerns me.

I like the 'value scripts' variation. I believe it may reduce costs
significantly for small, cheap transactions. It does require 'module
level recursion' as well, if recursion is to be compiled simply and
efficiently. It does constrain compilers a little bit; any compiler
that takes advantage of modules must generate code meeting the
restriction. But since modules are a new feature, no existing code
will be broken by this. It's also straightforward to meet the
restriction trivially, for example by wrapping the entire module body
in `Delay`. So I do not expect any major problems here.

The 'tuples of modules' variation, given efficient projections, should
simply be a performance improvement, so I am in favour of it. It does
require `resolveScriptDependencies` to *construct* the tuple before
script execution, but this replaces adding the modules one-by-one to
the environment, and should be cheaper. Moreover, accessing modules
should in most cases be slightly cheaper. Checking the restriction
that the variable bound to the tuple only appears as an argument to a
projection might be costly, requiring an additional traversal of the
code, but on the other hand that restriction exists to make adjustment
of the index possible, and this is only required by the 'global module
environment' variation. So, provided this is not adopted, one might
relax that restriction and *allow* scripts to refer to the tuple of
modules as a whole. That is an odd thing to do, but not actively
harmful.

The 'unboxed modules' variation is quite attractive, in its local
variant (where each script is passed a tuple of the exports from
modules that *that script* imports). In this variant a traversal of
the code to adjust references to module exports *is* needed, but can
be done at compile-time, so does not impose a cost during phase 2
verification. However, (in combination with 'lazy loading') this does
require `ScriptArg`s to be larger, making all scripts slightly larger
on the chain. Also, there is a start-up cost for the unboxing itself:
`resolveScriptDependencies` must copy *all* the exports from each
imported module into the same tuple (whether or not they will be
used). Modules may have quite a lot of exports--`Data.Map` for example
has 97--and many may not be used in any particular script. The benefit
of unboxing modules is slightly faster access to module exports when
the script runs, but for small, cheap runs we may never recover the
cost of building the unboxed tuple in the first place. On balance, I
would probably prefer *not* to do this, but this is not a strong
preference.

Finally, all the variations using tuples rely on efficient, constant
time projections of tuple components. These are not presently
available in UPLC--but they would benefit *all* UPLC users, not least
by providing an efficient implementation for Haskell record field
selectors in Haskell. Adding efficient projections for SoP data deserves
a CIP of its own; it is a prerequisite for many variations here, but
logically is not a part of implementing modules. A separate CIP should
be written for this in the near future--it should be straightforward
and uncontroversial compared to adding modules.


## Path to Active

### Acceptance criteria

- [ ] determine which approach outlined in this CIP will be selected
- [ ] `plutus` changes
- [ ] `cardano-ledger` changes
- [ ] `cardano-api` changes
- [ ] benchmarking and testing
- [ ] integrate the feature into `cardano-node`
- [ ] end-to-end testing
- [ ] release at the hard fork introducing the Dijkstra era

### Implementation Plan

This CIP will be implemented by the Plutus Core team with assistance from the Ledger team and the Cardano API team.
Should we decide to implement tagged modules - the safer and more recommended option - then modules will be usable in existing Plutus ledger language versions (V1, V2 and V3).
If we instead opt for untagged modules, modules will only be usable from Plutus V4 onwards.

Enabling modules on-chain requires a new ledger era.
Therefore modules will be enabled in the Dijkstra era at the earliest.

Developers of compilers for other languages targeting Untyped Plutus Core will need to update their languages and compilers accordingly if they wish to support modules.

Alternative Cardano node implementors must update their Plutus evaluator (unless a variant is chosen that doesn't require modifying the Plutus evaluator, which is unlikely), ledger, and transaction balancer to support this feature and align with the Haskell node.

## Acknowledgements

This CIP draws heavily on a design by Michael Peyton Jones, and has
benefitted greatly from discussion with Ziyang Liu, Roman Kireev, and
Phil Wadler.

## Copyright
This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).
## 


[^1]: At present, a newer ledger language version may have access to more builtin functions and more Plutus Core versions than an older ledger language version, but this difference is going away.

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0153/README.md
---

- --
CIP: 153
Title: Plutus Core Builtin Type - MaryEraValue
Category: Plutus
Status: Proposed
Authors:
- Philip DiSarro <philipdisarro@gmail.com>
Implementors: []
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/921
Created: 2024-10-03
License: CC-BY-4.0
- --

## Abstract

We propose to extend the `Data` type in Plutus Core with a new constructor `Value BuiltinValue`, introducing native, first-class support for multi-asset values that adhere to the invariants of multi-asset values in the Cardano ledger.

This addition enables on-chain scripts to operate on multi-asset values directly and efficiently, without resorting to nested map encodings. It eliminates the need for costly emulation of value operations, reduces validator script size, enables faster execution, and significantly lowers the ex-unit budgets required to process multi-asset logic. By elevating multi-asset values to a primitive type, this change aligns Plutus Core more closely with its primary domain—expressing constraints over the transfer of value—and unlocks substantial improvements in performance, safety, and consistency across the smart contract ecosystem.

## Motivation: why is this CIP necessary?

UPLC is not a general-purpose programming language. It is a language specifically designed to write validation logic to set constraints for the creation and transfer of Value and Data across the Cardano blockchain. **Given that half of the entire purpose of this language is to express constraints over the creation and transfer of** `Value`, why is Value treated as a standard library concept rather than a first-class language primitive?

The value proposition of domain-specific languages like UPLC is that they are purpose-built for a specific problem space, and as such, are able to outperform general-purpose languages within that domain. This foundational consideration seems to have been largely absent from the historical design of Plutus. It is difficult to reconcile the absence of first-class support for `Value` in a language explicitly designed to manage the transfer of value.

Currently, Plutus Core has no concept of multi-asset values. Instead, `Value` is a standard library construct, implemented as a nested `Map`:

```haskell
newtype Value = Value (Map CurrencySymbol (Map TokenName Integer))
```
While this representation superficially resembles the structure of ledger-level values, it lacks the necessary semantic guarantees. The critical invariants upheld by the ledger—such as omitting zero-valued assets, eliminating empty maps, and maintaining canonical order—must be manually replicated in every implementation targeting Plutus Core. This makes secure and correct usage of `Value` significantly harder than it should be.

Smart contract languages like Plinth, Aiken, Plutarch, and Helios must each implement their own standard library for value manipulation, resulting in duplicated logic, inconsistent behavior, and non-trivial security risks. Even operations as fundamental as value equality and union require verbose logic to normalize entries—adding cost and complexity to every validator.

It should not be the responsibility of language authors or smart contract developers to implement secure and efficient operations over a type that is fundamental to the domain itself.

## Specification

### MaryEraValue

A _map_ is a collection of _values_ indexed by _keys_. We consider a map to be _well-defined_ when each key uniquely identifies each value, and when additional structural and semantic constraints are satisfied. The canonical representation of Mary-era multi-asset Values in Plutus Core is a nested map where the keys of the outer map are `AsData CurrencySymbol`s and the keys of the inner map are `AsData TokenName`s and the _values_ of the inner map are `AsData Integer` (representing the quantity of the asset).

Here are the important invariants of the PlutusCore representation of the Mary-era Value types from the Cardano ledger:
1. All _keys_ in the outer map are unique (each `CurrencySymbol` appears only once)
2. All _keys_ in the inner map are unique (each `TokenName` appears only once)
3. The _keys_ in the outer map are ordered lexographically (`CurrencySymbol`s appear in ascending order)
4. The _keys_ in the inner map are ordered lexographically (`TokenName`s appear in ascending order)
5. There are no empty inner maps (The _value_ associated with any given `CurrencySymbol` _key_ cannot be an empty map)
6. There are no zero-quantity _values_ in the inner maps (The _value_ associated with any given `TokenName` _key_ in an inner-map cannot be zero).

### Mary-era Value as a builtin type

We propose introducing a new builtin type, `BuiltinValue`, along with a set of builtins for efficient manipulation of multi-asset values. These builtins will enforce all invariants defined for multi-asset values in the Cardano ledger. Additionally propose extending the Data type with a new constructor to support BuiltinValue:

```haskell
data Data =
      Constr Integer [Data]
    | Map [(Data, Data)]
    | List [Data]
    | I Integer
    | B BS.ByteString
    | Value BuiltinValue
- - ^ New constructor for `BuiltinValue`
```
This enables direct representation of BuiltinValue within `Data`, eliminating the expensive computation that would otherwise be required to convert back and forth between the `Data` representation of `Value` as nested Map structures and the `BuiltinValue` type.

### On the Necessity of a `Data` Constructor for `BuiltinValue`

Introducing a `BuiltinValue` type to Plutus Core without also adding a corresponding constructor to the `Data` type (i.e. `Value BuiltinValue`) would result in a fragmented and inefficient design. In such a scenario, smart contracts would require dedicated conversion builtins to translate between `BuiltinData`—the standard interface type in validators—and `BuiltinValue`. Although these conversions are linear in the size of the structure and less expensive than conversions like `Data → ScriptContext`, they are still nontrivial and, in practice, can become prohibitively expensive.

Consider, for instance, a `Value` containing hundreds of assets where a script only needs to access ADA. Converting the entire structure to a `BuiltinValue` simply to look up a single entry would be significantly more expensive than directly inspecting the original `Data` representation. Even more critically, this approach creates ambiguity for developers: they must decide when to convert, weigh the performance trade-offs, and handle two separate representations of the same underlying concept.

This defeats the entire purpose of introducing `BuiltinValue`, which is to simplify, secure, and optimize multi-asset handling in Plutus. If this proposal were to result in developers continuing to use nested `Map` structures within `Data` for performance reasons, it would not only fail to unify the representation of `Value` in Plutus Core—it would add yet another layer of complexity.

It is therefore a strong requirement of this proposal that `BuiltinValue` be made a first-class constructor within the `Data` type (i.e. `Value BuiltinValue`). Only with this addition can we ensure that all value-related logic is handled canonically, efficiently, and securely via the new builtins—eliminating the need for conversions or manual manipulation of `BuiltinData`.

### BuiltinValue Operations
We propose the introduction of the following builtin functions to support efficient and invariant-preserving manipulation of multi-asset values through a new `BuiltinValue` type.

These functions operate on the following types:
```haskell
type BuiltinValue -- A new primitive type
type BuiltinCurrencySymbol = BuiltinByteString
type BuiltinTokenName = BuiltinByteString
type BuiltinQuantity = BuiltinInteger
```

We propose the following set of builtin functions to accompany the new builtin type:
1. `insertCoin :: BuiltinCurrencySymbol -> BuiltinTokenName -> BuiltinInteger -> BuiltinValue -> BuiltinValue`
- it returns a Mary-era Value with the `Coin` inserted, silently discarding any previous value.
      If the `BuiltinInteger` argument (the quantity) is zero, the `Coin` is removed.
- Both `BuiltinCurrencySymbol` and `BuiltinTokenName` must be no longer than 32 bytes (unless the amount is zero).
- The amount (`BuiltinInteger`) must satisfy -2<sup>127</sup> ≤ amount ≤ 2<sup>127</sup>-1.
2. `lookupCoin :: BuiltinCurrencySymbol -> BuiltinTokenName -> BuiltinValue -> BuiltinQuantity`
- it returns the quantity of a given `Coin` in a Mary-era Value.
3. `unionValue :: BuiltinValue -> BuiltinValue -> BuiltinValue`
- it merges two provided values
- when there are collisions it adds the quantities, if the resulting sum is zero it removes the entry to maintain the Mary-era Value invariants (no zero-quantity entries, no empty inner maps) such that the result is a normalized value.
- This operation is commutative and associative, thus makes `BuiltinValue` a commutative semigroup.
4. `valueContains :: BuiltinValue -> BuiltinValue -> Bool`
- it compares the two Mary-era Values and determines if the first value is a superset of the second.
- All amounts in both values must be positive.
- `valueContains a b == True` if and only if: for each `(currency, token, quantity)` in `b`, `lookupCoin currency token a >= quantity`.
5. `valueData :: BuiltinValue -> BuiltinData`
- encodes a `BuiltinValue` as `BuiltinData`.
6. `unValueData :: BuiltinData -> BuiltinValue`
- decodes a `BuiltinData` into a `BuiltinValue`, or fails if it is not one.
- All currency symbols and token names must be no longer than 32 bytes.
- All amounts must lie between -2<sup>127</sup> and 2<sup>127</sup>-1 (inclusive).

A note on `valueData` and `unValueData`: in Plutus V1, V2 and V3, the encoding of `BuiltinValue` in `BuiltinData` is identical to that of the [existing `Value` type](https://plutus.cardano.intersectmbo.org/haddock/latest/plutus-ledger-api/PlutusLedgerApi-V1-Value.html#t:Value) in plutus-ledger-api.
This ensures backwards compatibility.
Beginning in the Dijkstra era, a new `Value` constructor will be added to `BuiltinData`, making `valueData` and `unValueData` constant time operations for Plutus V4 onwards.

## Rationale: how does this CIP achieve its goals?

### Efficiency

Builtins are strictly more efficient than user-defined operations evaluated by the CEK machine. Today, operations on `Value` must be implemented at the Plutus level using nested `Map` structures and generic functional constructs. These are significantly slower and more memory-intensive than equivalent builtins due to their interpretive overhead and lack of optimization opportunities.

By introducing a dedicated `BuiltinValue` type and associated builtins, we enable:

- Elimination of the need to deconstruct and traverse deeply nested data-encoded maps to perform operations on `Value`s, which significantly improves execution efficiency.
- Drastically reduced script sizes, since `Value` operations no longer need to be defined in the bytecode of every validator; they are now provided natively by the language as builtins.

This directly addresses a major pain point for developers working with large or complex multi-asset values.

### Security & Abstraction

Introducing a single, standardized `BuiltinValue` type — including a constructor on `Data` and associated conversion builtins — eliminates the need for dual representations of multi-asset values. Without this, developers would be forced to evaluate trade-offs in every instance where value manipulation is required: either operate directly on `BuiltinData`, or incur the overhead of converting to and from SOP-style encodings in order to gain performance benefits from subsequent operations on the structured representation. This added friction increases both complexity and fragmentation across the ecosystem.

Instead, developers will have:

- A consistent and canonical representation of multi-asset values.
- Simple, expressive, and performant builtins for common operations.
- Fewer footguns and edge cases to reason about in critical onchain logic.

By elevating `Value` to a first-class builtin type, this CIP ensures that the semantics of `Value` and its operations are uniform across all languages targeting Plutus Core. Currently, each language (e.g., Aiken, Plutarch, Helios, Plu-Ts, etc.) must independently define a `Value` type and implement its operations in their standard libraries. This fragmentation introduces the risk of divergent behaviors and subtle inconsistencies across Cardano’s smart contract ecosystem.

With `BuiltinValue`, all implementations inherit a single, authoritative definition of `Value` and its operations, removing this burden from language authors and guaranteeing consistency across the entire platform.

This CIP ultimately shifts the responsibility for correctness and optimization away from the application layer and into the language itself — the appropriate place for enforcing invariants and performance guarantees for such a core domain primitive.

## Path to Active

### Acceptance Criteria

- [ ] The feature is implemented according to the implementation plan and merged into
the master branch of the [plutus][6] repository.
- [ ] [cardano-ledger][1] is updated to include new protocol parameters to control costing of
the new builtins.
- [ ] The feature is integrated into [cardano-node][2] and released as part of a hard fork.

### Implementation Plan

The implementation of this CIP should not proceed without an empirical assessment of the effectiveness of the new primitives, as per the following plan:

1. Implement the new primitives according to the specification.
2. Assign a preliminary cost to the new builtin functions. Consider similar operations and their current costs.
3. Create variants of the [existing benchmarks][3] and potentially add some more.
4. Check that the builtin operations over `BuiltinData = Value` are indeed significantly faster.

If the preliminary performance investigation was not successful, this CIP should be revised
according to the findings of the experiment. Otherwise, the implementation can proceed:

5. Determine the most appropriate costing functions for modelling the builtin's performance
and assign costs accordingly.
6. Add the new builtin type and functions to the appropriate sections in the [Plutus Core
Specification][4].
7. Formalize the new builtin type and functions in the [plutus-metatheory][5].
8. The final version of the feature is ready to be merged into [plutus][6] and accepted by
the Plutus Core team.

## Copyright
This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

[1]: https://github.com/IntersectMBO/cardano-ledger "cardano-ledger"
[2]: https://github.com/IntersectMBO/cardano-node "cardano-node"
[3]: https://github.com/IntersectMBO/plutus/tree/master/plutus-benchmark/script-contexts "script-context-benchmarks"
[4]: https://plutus.cardano.intersectmbo.org/resources/plutus-core-spec.pdf "Formal Specification of the Plutus Core Language"
[5]: https://github.com/IntersectMBO/plutus/tree/master/plutus-metatheory "plutus-metatheory"
[6]: https://github.com/IntersectMBO/plutus/ "plutus"

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0155/README.md
---

- --
CIP: 155
Title: SRV registry
Status: Proposed
Category: Network
Authors:
- Marcin Szamotulski <marcin.szamotulski@iohk.io>
Implementors: N/A
Discussions:
- Submission: https://github.com/cardano-foundation/CIPs/pull/1033
Created: 2025-04-22
License: CC-BY-4.0
- --

## Abstract

This CIP defines the procedure for assigning Service Record (SRV) prefixes for Cardano applications (like `cardano-node`, `mithril`, etc.).

This creates a means of sharing authoritative information between SPOs who deploy services and _decentralised protocol_ developers who write them.

## Motivation: why is this CIP necessary?

The **Cardano Ledger** allows the use of SRV records in the **SPO** registration certificate (see [register-stake-pool]).

Having access to the ledger (either directly or through tools like `cardano-cli`) will allow decentralised protocols developers to construct networks based on registered relays.

In this CIP, we propose to make them usable by decentralised applications running on `Cardano`.


All involved parties (SPOs, decentralised protocol developers, etc) need to agree on SRV prefixes used by various decentralised protocols and thus a public registry governed by a public CIP process is required to foster decentralised protocols that co-exist with the **Cardano** network, like **Mithril** (see [CIP#137]).

We consider decentralised protocols as applications, which construct their own network and relay on ledger peers published on the block-chain. This include node implementations, overlay networks like Mithril, or Layer 2 protocols like Hydra.

### Problem

Bootstraping networks requires sharing information about available services.

Historically, this was done by sharing IP or DNS names and PORT numbers (note that `A` or `AAAA` records do not contain PORT numbers), but this approach has its limitations.
SRV records were designed to make deployment independent of hard-coded service end-points (see [RFC#2782] or [cloudflare documentation][srv]).  They include PORT numbers together with a DNS name (point to an `A` or `AAAA` record) together with `TTL`, priority, weight, and some other data.
Thus they can be instrumental in `Cardano`, since relays are registered on the blockchain through the registration certificate.

By using SRV records in the registration certificate (which is supported by the `cardano-ledger`, but not by `cardano-node`), we wish to solve this problem not just for `Cardano` node implementation, but also for any decentralised protocol that requires constructing its own network.

SRV provides a mechanism for exposing decentralised protocols co-deployed with a node, like **mithril** or **hydra**.

Making such services discoverable is one of the key features addressed by this CIP.

In this CIP, we propose prefixes for both **Cardano** and **Mithril**; in the future, other services can be registered through a CIP process, thus starting a registry of prefixes used by the **Cardano** ecosystem.

SPOs who deploy services need to configure their system according to the registry, e.g. SPO's cardano relays node MUST be available at `_cardano._tcp.<SPO_DOMAIN_NAME>`, as other nodes on the system will be looking at this address.

Decentralised protocol developers SHOULD submit proposals to the SRV Prefix Registry, so SPOs, who deploy them, can have an authoritative information how to do it.

## Specification

### SRV Prefix Registry

The registry is available in `JSON` format [registry.json].

### SRV Prefix Semantics

When a **cardano** node implementation reads an SRV record from a ledger, it must add the _cardano_ prefix from the table above before making a DNS lookup, e.g. it should do a DNS query for `_cardano._tcp.<srv-record-from-ledger>`.

This design allows decentralised protocols to use SRV records registered in the ledger for different purposes, e.g. a **mithril** node can use them to learn about end-points of its network.

Each prefix SHOULD start with `_cardano._tcp` or `_cardano._udp`, to avoid clashes with services not related to `Cardano`.

#### SRV Registry Rules

- Each decentralised protocol can have at most one entry in the registry.
- A CIP process assigns new entries to [registry.json], after a careful consideration and consultation with all the involved parties (see #acceptance-criteria below).
- Entries cannot be removed, but can be revoked by assigning a `Revoked` status.
  This can only happen if a decentralised protocol is no longer supported.

### Example

When registering a cardano pool on `example.com` domain using an `SRV` record, one should use:
```shell
cardano-cli latest stake-pool ... --multi-host-pool-relay example.com
```
(see [register-stake-pool]); and configure SRV record at `_cardano._tcp.example.com` to point to **Cardano** relays, `_mithril._tcp.example.com` to point to **Mithril** relays (see [srv], currently under development).

A **Cardano** node implementation, when retrieving information from a registration certificate from the ledger, will receive `example.com`, and it must prepend the `_cardano._tcp` prefix to make an SRV query.  Such a query might return:

```
_cardano._tcp.example.com 86400 IN SRV 10 5 3001 cardano.example.com
```
From this, we learn that a Cardano node is available on port `3001` on IPs resolved by a regular DNS query to `cardano.example.com`.
Refer to the [Cloudflare documentation][srv] for a deeper understanding of other fields.


## Rationale: how does this CIP achieve its goals?

This CIP constructs a process to maintain SRV registry, and thus provides authritative information for SPOs and decentralised protocol developers.


## Path to Active

### Acceptance Criteria

The CIP should be accepted by:

- [ ] [**Technical Steering Committee**][tsc]

And when there's no major objection from one of the currently involved parties:

- [x] [**Cardano-Node Team**][cardano-node] accepts the proposal
- [ ] [**Amaru Team**][amaru] accepts the proposal
- [ ] [**Gouroboros Team**][gouroboros] accepts the proposal
- [ ] [**Hydra Team**][hydra] accepts the proposal
- [x] [**Mithril Team**][mithril] accepts the proposal

### Implementation Plan

Each **Cardano** node implementation or other tools which rely on SRV records stored in the ledger should comply with this proposal,
e.g. whenever obtaining _multi-pool relay information_ one needs to prepend a registered prefix before making an SRV query.


## Copyright

This CIP is licensed under [CC-BY-4.0]: https://creativecommons.org/licenses/by/4.0/legalcode

[CIP#137]: ../CIP-0137
[register-stake-pool]: https://developers.cardano.org/docs/operate-a-stake-pool/register-stake-pool
[RFC#2782]: https://datatracker.ietf.org/doc/html/rfc2782
[srv]: https://www.cloudflare.com/en-gb/learning/dns/dns-records/dns-srv-record/

[amaru]: https://github.com/pragma-org/amaru
[cardano-node]: https://github.com/IntersectMBO/cardano-node
[mithril]: https://github.com/input-output-hk/mithril
[gouroboros]: https://github.com/blinklabs-io/gouroboros
[tsc]: https://docs.intersectmbo.org/intersect-overview/intersect-committees/technical-steering-committee-tsc
[hydra]: https://github.com/cardano-scaling/hydra
[register-stake-pool]: https://developers.cardano.org/docs/operate-a-stake-pool/register-stake-pool/#generate-the-stake-pool-registration-certificate

[registry.json]: ./registry.json

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0156/README.md
---

- --
CIP: 156
Title: Plutus Core Builtin Function - multiIndexArray
Category: Plutus
Status: Proposed
Authors:
- Yura Lazaryev <yuriy.lazaryev@iohk.io>
- Philip DiSarro <info@anastasialabs.com>
Implementors: []
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/1050
Created: 2025-07-07
License: CC-BY-4.0
- --

## Abstract

We propose adding a new builtin function, `multiIndexArray`, to Plutus Core. This function takes a list of integer indices and an array, returning a list of the array elements at those positions in the specified order.

## Motivation: why is this CIP necessary?

Plutus Core arrays (CIP-0138) support O(1) individual lookup via `indexArray`. However, extracting multiple elements requires repeated calls to `indexArray`, which:

1. Increases script size and execution cost.
2. Complicates on-chain logic for batching lookups or reordering.
3. Prevents efficient bulk access and traversal in a user-defined order.

A single `multiIndexArray` call reduces code and cost overhead by batching lookups and delivering elements in the desired sequence.

## Specification

Add the following builtin function:

```haskell
multiIndexArray :: forall a. List Integer -> Array a -> List a
```

- **Inputs**:
1. A `List Integer` of zero-based indices, in the order elements should be retrieved.
2. An `Array a` to index.
- **Output**: A `List a` containing the elements at the specified indices, in the same order. In case of repeated indices, the same element is returned multiple times.
- **Error handling**: If any index is out of bounds (< 0 or ≥ lengthOfArray), the entire call fails with the same error semantics as `indexArray`.
- **Cost**: Time and memory usage are linear in the length of the index list.

## Rationale: how does this CIP achieve its goals?

By batching multiple lookups into one builtin, `multiIndexArray`:

- Eliminates repetitive script code for loops or folds over `indexArray`.
- Reduces execution budget and size overhead of repeated builtins.
- Guarantees elements are returned in caller-specified order, enabling efficient streaming or traversal.

### Alternatives considered

- **List of Maybe a**: Returning `Nothing` for out-of-bounds indices would require a `Maybe` builtin type, increasing complexity.
- **Default value argument**: Allowing a default on lookup failure complicates strict evaluation and error detection.
- **Slice and manual mapping**: Users could write a `slice` or fold, but this remains code-heavy and costly.
- **Returning Array plus helper**: Have `multiIndexArray :: List Integer -> Array a -> Array a` return an `Array a` of selected elements and provide a new helper `arrayToList :: Array a -> List a`. This avoids constructing a list directly but requires adding `arrayToList` as a builtin and introduces extra conversion and costing complexity.

Failing on first error mirrors `indexArray` and keeps the API simple.

## Path to Active

### Acceptance Criteria

- [ ] Merge implementation into the Plutus Core repo.
- [ ] Update `cardano-ledger` costing parameters for `multiIndexArray`.
- [ ] Integrate into `cardano-node` and schedule for a protocol upgrade.

### Implementation Plan

1. Add `multiIndexArray` to Plutus Core spec and runtime.
2. Define preliminary cost model (linear in index list length for both CPU usage and memory usage).
3. Write conformance tests covering valid and out-of-bounds cases.
4. Extend an E2E test suite to include `multiIndexArray` scenarios.
5. Benchmark against manual `indexArray` loops to refine costing.
6. Update formal documentation (`plutus-metatheory`, spec PDF).
7. Complete integration and include in the next hard fork.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0161/README.md
---

- --
CIP: 161
Title: Ouroboros Phalanx - Breaking Grinding Incentives
Category: Consensus
Status: Proposed
Authors:
- Nicolas Henin <nicolas.henin@iohk.io>
- Raphael Toledo <raphael.toledo@iohk.io>
Solution-To:
- CPS-0017
- CPS-0021
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/1065
Created: 2025-07-25
License: Apache-2.0
- --

## Abstract

We propose an extension to Ouroboros Praos, called **Ouroboros Phalanx**. The name derives from the [**Phalanx**](https://en.wikipedia.org/wiki/Phalanx), an **Ancient Greek military formation** where soldiers stood in tightly packed units, shielding one another to form a nearly impenetrable defense. Just as the phalanx multiplied the strength of individual soldiers through coordination, this protocol enhances Cardano’s consensus by reinforcing its resistance to adversarial attacks.

At its core, **Phalanx Protocol** strengthens the **VRF-based randomness generation sub-protocol** that underpins leader election. It introduces an additional cryptographic primitive that is **lightweight for honest participants** yet **computationally expensive for adversaries** seeking to bias slot leader distributions. This design does not eliminate grinding attacks outright but makes them **economically infeasible at scale**.

By addressing both [CPS-0021: Randomness Manipulation](https://github.com/cardano-foundation/CIPs/tree/master/CPS-0021) and [CPS-0017: Settlement Speed](https://github.com/cardano-foundation/CIPs/tree/master/CPS-0017), Phalanx achieves two goals simultaneously:
- It raises the cost of grinding attacks by a factor of roughly <strong>10<sup>10</sup></strong>.
- It reduces settlement time by approximately **20–30%** compared to unmodified Praos, without compromising security.

Ouroboros Phalanx therefore represents a **complementary advancement**: reinforcing Cardano’s consensus security while improving performance, and ensuring the network remains robust against future adversarial strategies.

<details>
<summary><h2>Table of Contents</h2></summary>

- [Abstract](#abstract)
- [Motivation: why is this CIP necessary?](#motivation-why-is-this-cip-necessary)
- [Specification](#specification)
- [1. High-Level Overview ](#1-high-level-overview)
- [1.1 Changes Relative to Praos](#11-changes-relative-to-praos)
- [1.2 Inputs & Outputs ](#12-inputs--outputs)
- [1.2.1 The η Stream](#121-the-η-stream)
- [1.2.2 The pre-ηₑ Synchronizations](#122-the-pre-ηₑ-synchronizations)
- [1.2.3 The Φ Stream ](#123-the-φ-stream)
- [1.2.3.1 The Setup](#1231-the-setup)
- [1.2.3.2 The Lifecycle](#1232-the-lifecycle)
- [1.2.4 The η Generations](#124-the-η-generations)
- [2. The Φ Cryptographic Primitive](#2-the-φ-cryptographic-primitive)
- [2.1. Expected Properties](#21-expected-properties)
- [2.2. Verifiable Delayed Functions (VDF)](#22-verifiable-delayed-functions-vdf)
- [2.3 Wesolowski's VDF](#23-wesolowskis-vdf)
- [2.3.1 VDF Primitives](#231-vdf-primitives)
- [2.3.2 VDF Aggregation Primitives](#232-vdf-aggregation-primitives)
- [3. Φ Stream Specification](#3-φ-stream-specification)
- [3.1 Distribution of Φ Iterations](#31-distribution-of-φ-iterations)
- [3.2 The State Machine](#32-the-state-machine)
- [3.2.1 Diagram Overview](#321-diagram-overview)
- [3.2.2 Parametrization Phase](#322-parametrization-phase)
- [3.2.3 Initialization Grace Phase](#323-initialization-grace-phase)
- [3.2.3.1 Initialize Command](#3231-initialize-command)
- [3.2.3.2 Tick Commands & Grace Period](#3232-tick-commands--grace-period)
- [3.2.4 Computation Phase](#324-computation-phase)
- [3.2.4.1 VDF integration](#3241-vdf-integration)
- [3.2.4.2 The States](#3242-the-states)
- [3.2.4.3 ProvideAttestedOutput & Tick Commands](#3243-provideattestedoutput--tick-commands)
- [3.2.5 Catch-up Phase](#325-catch-up-phase)
- [3.2.5.1 The States](#3251-the-states)
- [3.2.5.2 ProvideMissingAttestedOutput & Tick Commands](#3252-providemissingattestedoutput--tick-commands)
- [3.2.6 Closure Phase](#326-closure-phase)
- [3.2.6.1. The States](#3261-the-states)
- [3.2.6.2. The Successful Scenario: The `Close` Command](#3262-the-successful-scenario-the-close-command)
- [3.2.6.3. `tick` Command](#3263-tick-command)
- [3.2.6.4. The Failure Scenario: Ungraceful Closure](#3264-the-failure-scenario-ungraceful-closure)
- [4. Recommended Parameter Values](#4-recommended-parameter-values)
- [4.1. VDF Security Parameters λ and ρ](#41-vdf-security-parameters-λ-and-ρ)
- [4.2. Time Budget Tᵩ and Derived T](#42-time-budget-tᵩ-and-derived-t)
- [4.2.1. Specialized ASIC vs CPU-Based Chips](#421-specialized-asic-vs-cpu-based-chips)
- [4.2.2. Deriving from Tᵩ to T](#421-deriving-from-tᵩ-to-t)
- [5. Efficiency Analysis](#5-efficiency-analysis)
- [5.1. Block Publication](#51-block-publication)
- [5.2. Block Verification](#52-block-verification)
- [5.2.1. When Not Syncing](#521-when-not-syncing)
- [5.2.2. When Syncing](#522-when-syncing)
- [6. CDDL Schema for the Ledger](#6-cddl-schema-for-the-ledger)

- [Rationale: How does this CIP achieve its goals?](#rationale-how-does-this-cip-achieve-its-goals)
- [1. How Phalanx Addresses CPS-21 - Ouroboros Randomness Manipulation ?](#1-how-phalanx-addresses-cps-21---ouroboros-randomness-manipulation)
- [1.1 Problem Overview](#11-problem-overview)
- [1.2 Phalanx Cost Amplification per Grinding Attempt](#12-phalanx-cost-amplification-per-grinding-attempt)
- [1.3 Phalanx Cost Amplification per Grinding Attack](#13-phalanx-cost-amplification-per-grinding-attack)
- [1.3.1 Formula](#131-formula)
- [1.3.2 Estimated Formula Using Mainnet Cardano Parameters](#132-estimated-formula-using-mainnet-cardano-parameters)
- [1.3.3 Impact of Tᵩ on Canonical Scenarios](#133-impact-of-tᵩ-on-canonical-scenarios)
- [1.3.4 Impact of Tᵩ on Feasibility Categories](#134-impact-of-tᵩ-on-feasibility-categories)
- [1.4. Conclusion: How Much Risk is Mitigated?](#14-conclusion-how-much-risk-is-mitigated)
- [2. How Phalanx Improves CPS-17 - Settlement Speed ?](#2-how-phalanx-improves-cps-17---settlement-speed)
- [2.1 Settlement times without grinding attacks](#21-settlement-times-without-grinding-attacks)
- [2.2 How Grinding Power affects Settlement times](#22-how-grinding-power-affects-settlement-times)
- [2.3 How Phalanx improves compared to Praos?](#23-how-phalanx-improves-compared-to-praos-)
- [2.4 Advocating for Peras: Phalanx as a Complementary Layer](#24-advocating-for-peras-phalanx-as-a-complementary-layer)
- [3. Why VDFs Were Chosen over other Cryptographic Primitives ?](#3-why-vdfs-were-chosen-over-other-cryptographic-primitives-)
- [3.1 Requirements](#31-requirements)
- [3.2 Primitive selection](#32-primitive-selection)
- [3.2.1 RSA solutions](#321-rsa-solutions)
- [3.2.1.1 Designs](#3211-designs)
- [3.2.1.2 Properties](#3212-properties)
- [3.2.2 ECC solutions](#322-ecc-solutions)
- [3.2.2.1 Designs](#3221-designs)
- [3.2.2.2 Properties](#3222-properties)
- [3.2.3 Class group solutions](#323-class-group-solutions)
- [3.2.3.1 Design](#3231-design)
- [3.2.3.2 Properties](#3232-properties)
- [3.2.4 OWF solutions](#324-owf-solutions)
- [3.2.4.1 Proofs of knowledge](#3241-proofs-of-knowledge)
- [3.2.4.2 OWFs](#3242-owfs)
- [3.2.4.3 Design](#3243-design)
- [3.2.4.4 Properties](#3244-properties)
- [3.3 Primitive recommendation](#33-primitive-recommendation)
- [Path to Active](#path-to-active)
- [Acceptance Criteria](#acceptance-criteria)
- [Implementation Plan](#implementation-plan)
- [References](#references)
- [Copyright](#copyright)

</details>

## Motivation: why is this CIP necessary?

This proposal strengthens Cardano’s consensus protocol (Ouroboros Praos) against a class of attacks known as *grinding attacks*. These attacks allow adversaries to bias the randomness used in block leader elections in their favor, statistically slowing down settlement times and thys weakening the effectivness of Praos.

The improvement introduces an additional computation step that is lightweight for honest participants but significantly more expensive for attackers, making grinding attacks economically infeasible.

### Recommended Configuration

As an initial configuration, we recommend **12 hours of cumulative and distributed execution** of this cryptographic primitive per epoch on standard CPU architectures.
- The epoch is divided into **1-hour intervals**.
- The **first leader of each interval** must produce the corresponding proof.
- For any individual node, this requirement represents roughly **527 seconds (≈10 minutes)** of computation.

The algorithm is designed so that with **128-bit confidence**, all required proofs will be produced on time by the end of each epoch.

### Security Improvements

The proposal increases substantially the computational cost of a grinding attack by a factor of approximately <strong>10<sup>10</sup></strong> compared to the current situation.

To maintain this level of security over time:
- Governance may choose to **increase the 12-hour budget** as the cost of computation decreases.
- Execution could migrate to **ASIC-based architectures**, preserving the same budget while maintaining security guarantees, and later increasing the budget further.

Beyond parameter updates, adoption of this proposal would necessarily require a **hard fork**, since it modifies the consensus protocol in two fundamental ways:
1. The randomness for slot distribution is extended from **1 epoch to 2 epochs**. At the start of epoch *e*, the snapshot of the stake distribution will be taken at the end of *epoch e−2*, rather than at the end of *epoch e−1* as in Praos today.
2. The **general method of generating slot leader distributions** is changed, making leader election more resilient to adversarial bias.

### Consensus Performance

This proposal is not only about security, but also about **consensus performance**.

In Praos, because grinding allows adversaries to choose among multiple possible slot leader distributions, the probability of “bad events” (such as rollbacks or settlement failures) is statistically amplified compared to the honest model.

- If a bad event occurs with probability $`\varepsilon`$ under unbiased randomness,
- An adversary able to try $`R`$ independent randomness candidates can increase the likelihood of that event up to $`R \cdot \varepsilon`$ (by the union bound).

This translates into slower settlement and weaker guarantees for the network as a whole. By substantially reducing $`R`$ compared to Praos, we limit the impact of grinding attacks and therefore improve settlement. In fact, the recommended configuration reduces settlement time by approximately **20–30%** while maintaining equivalent security.

### Relationship to Peras

[Ouroboros Peras](https://peras.cardano-scaling.org/) is a recent extension of Praos designed to **accelerate settlement**.
- Instead of waiting for the traditional 2160-block window (around 5 days) to guarantee finality, Peras introduces **stake-weighted voting and certified blocks**.
- Randomly chosen committees of stake pool operators can “vote” on blocks, and when enough votes are collected, the block receives a certificate.
- Certified blocks are treated as more important in the chain, which enables **settlement in just 1–2 minutes**.

Peras is fully compatible with Praos:
- When enough committee members participate, it achieves **rapid settlement**.
- When they do not (e.g., if too many operators are offline), Peras **gracefully falls back to Praos**. Peras would only fall back to Praos if there were massive network disruption or an attack by a 25% adversary.

In these fallback situations, the network still relies on Praos’ guarantees—precisely where Phalanx becomes relevant as a **complementary defense layer**. Phalanx ensures that even when Peras cannot certify blocks, the protocol still benefits from:
- **Stronger protection against grinding attacks**, and
- **Faster settlement** compared to unmodified Praos.

Together, they form a **complementary pair**:
- **Peras** provides speed when conditions are favorable.
- **Phalanx** ensures resilience and strong security guarantees in all cases.

### Technical Depth

The remainder of this document provides the full technical specification for node implementors and researchers. Because Cardano’s security is grounded in **cryptography, probability, and statistical guarantees**, understanding the full details of this proposal requires technical knowledge in these areas. The complete specification is therefore dense: it describes mathematical models, cryptographic primitives, and rigorous proofs to ensure the system can be trusted at scale. Readers interested only in the high-level motivation and community impact may stop here.

Please refer to the CPD "[Ouroboros Randomness Generation Sub-Protocol – The Coin-Flipping Problem](https://github.com/cardano-foundation/CIPs/tree/master/CPS-0021/CPD/README.md)" for a detailed understanding of **randomness generation, leader election in Praos, and the coin-flipping dilemma in consensus protocols**. Moving forward, we will **dive into the core details**, assuming you have the **relevant background** to understand the proposal.


## Specification

The core principle of the proposed protocol change is to **substantially escalate the computational cost of each grinding attempt for an adversary**. To achieve this, every honest participant is required to perform a designated computation for each block they produce over an epoch (**432,000 slots - 5 days**) - note that this computation can be preprocessed locally at the beginning of the epoch. Consequently, an adversary attempting a grinding attack must **recompute these operations for every single attempt**, while being **constrained by the grinding window**, which dramatically increases the resource expenditure. By enforcing this computational burden, we **drastically reduce the feasible number of grinding attempts** an adversary with a fixed resource budget can execute, making randomness manipulation **more expensive and significantly less practical**.

### 1. High-Level Overview

#### 1.1. Changes Relative to Praos

In **Phalanx** , the randomness generation and leader election flows are modified as follows:

![alt text](./image/Praos-vs-Phalanx-Highl-Level.png)

1. The **stake distribution stabilization phase** is shifted **back by one epoch :** The **active** **stake distribution** $`\mathbf{SD}_e`$ used for leader election is now derived from the **end of $epoch_\text{e-3}$** instead of **$epoch_\text{e-2}$**  as in the original Praos protocol.
2. The **honest contribution inclusion phase**, which originally resulted in a **ηₑ candidate**, is also **shifted back by one epoch**, aligning with the adjusted **stake distribution stabilization**. This value is now referred to as the **pre-ηₑ candidate**, signifying its role as an **intermediate randomness nonce** in the sub-protocol.
3. The **pre-ηₑ candidate**, once stabilized (after $`3 \cdot \frac{k}{f}`$), undergoes a **sequence of incremental operations** using a **new deterministic cryptographic primitive Φ (Phi)**. This sequence spans a full epoch size, specifically during the interval:$`\left[\frac{9k}{f} \cdot \text{epoch}_{e-2},  \frac{9k}{f} \cdot \text{epoch}_{e-1}\right)`$.
4. The final **ηₑ (eta nonce)**, resulting from the Φ computation, completely determined by the prior stabilized pre-seed pre-ηₑ, does not need stabilization and is availablea a whole $`\frac{k}{f}`$ slots before the start of $`\text{epoch}_e`$ .

#### 1.2. Inputs & Outputs

The Randomness Generation sub-protocol pipelines two parallel streams η stream and Φ Stream, which synchronize at $`9 \cdot \frac{k}{f}`$ at each epoch :

![alt text](./image/Phalanx-Streams.png)

##### 1.2.1. The η stream

- Already present in Praos and retained in Phalanx
- Updated with every block produced in the blockchain tree, a η stream captures intermediate values $`\eta^\text{evolving}_t`$ in the block headers, defined as follows:

```math
   \eta^{\text{evolving}}_{t+1} =
   \begin{cases}
   \eta^{\text{evolving}}_{t}\ \star\ \mathsf{VRF}^\text{Output}_\text{t+1} & \text{when BlockProduced}(t) \\
   \eta^{\text{evolving}}_{t}  & \text{otherwise.}
   \end{cases}

```
```math
\text{BlockProduced}(t) =
\begin{cases}
\text{true} & \text{if a block is produced at time } t, \\
\text{false} & \text{otherwise.}
\end{cases}
```

| **where** ||
|---------------|-----------------|
| $`\text{ProtocolParameter}_\text{extraEntropy} `$ | The evolving nonce is initialized using the extraEntropy field defined in the protocol parameters.|
| $`\mathsf{VRF}^\text{Output}_\text{i} `$ | The **VRF output** generated by the $` \text{slot}_\text{i} `$ Leader and included in the block header |
| $a\ \star\ b$    | The concatenation of $a$ and $b$ , followed by a BLAKE2b-256 hash computation.


##### 1.2.2. The pre-ηₑ Synchronizations

- To generate $`\eta_\text{e}`$ for epoch $`e`$, the stream Φ Stream is reset with the value of η stream at $`t=9.\frac{k}{f}`$ at $epoch_{e-2}$
- This specific value of η stream is referred to as **$`\text{pre-}\eta_e`$** and defined as :
```math
\text{pre-}\eta_e= \eta^{evolving}_{9.\frac{k}{f}(epoch_{e-2})}
```

##### 1.2.3. The Φ Stream

###### 1.2.3.1. The Setup

The stream is bootstrapped by calling the parametrize function of the cryptographic primitive $`\Phi`$ with:
```math
Φ.\text{Stream.State} \leftarrow \Phi.\text{parametrize}(\lambda, T_\Phi)
```
where :
- $`\lambda`$ is a security parameter for the cryptographic primitive $`\Phi`$.
- $`T_\Phi`$, a time-bound parameter representing the required computation  $`\Phi`$ duration, independent of available computing power.
- Any change to these 2 parameters would require a decision through Cardano governance.
- $\Phi.\text{Stream.State}$ will contain derived configuration specific to the algorithm and the cryptographic primitive used.

###### 1.2.3.2. The Lifecycle

It is reset at every pre-ηₑ synchronization point every $`10.\frac{k}{f}`$ slots :
```math
Φ.\text{Stream.State} \leftarrow \Phi.\text{initialize}(Φ.\text{Configuration}, \text{pre-}\eta)
```
At each slot $t$, update the stream state by :
```math
Φ.\text{Stream.State} \leftarrow \Phi.\text{tick}(Φ.\text{Stream.State, t})
```
A node must be able to determine, based on the current state, whether it should begin computing $\Phi$ iterations in order to provide a proof at its next scheduled leader slot (see [Section "3.2.4.1. VDF integration"](#3241-vdf-integration) for details):
```math
\{0,1\} \leftarrow \Phi.\text{shouldCompute}(Φ.\text{Stream.State, nextElectedSlot})
```
A node must be able to compute a specific chunk of the $`\Phi`$ iterations independently of any global state.
The result is an *attested output*—a pair $`\phi_x =(\pi_x,\ o_x)`$ where :

- $`o_x`$ is the computed output for iteration $`x`$,
- $`\pi_x`$ is a cryptographic proof attesting that $`o_x`$ was correctly derived from the input according to the rules of $`\Phi`$.
- Since this operation may be long-lived, intermediate attested outputs should be persistable to disk, allowing the node to stop, resume, or cancel computation from the latest completed sub-computation.

A subset of block-producing slots must include in their block bodies a unique attested output $`\phi_x`$ with $`x \in \{1,\ \dots,\ i \}`$ denoting the iteration index within the $`\Phi`$ computation :
- Each attested output updates the stream state as follows:
```math
 \Phi.\text{StreamState} \leftarrow \Phi.\text{provideAttestedOutput}(\Phi.\text{StreamState},\ t,\ \phi_x)
```
- Each attested output must be verifiable both:
- **logically**, to ensure it corresponds to the correct slot and index, and
- **cryptographically**, to confirm that the computation was effectively executed

```math
\{0,1\} \leftarrow \Phi.\text{verify}(\Phi.\text{StreamState},\ t,\ \phi_x)
```

At the synchronization point $`\text{pre-}\eta_{e+1}`$, the stream is closed providing $`\phi^{final}_e = (\phi_e,\ \phi^{aggregated}_e)`$ with the last attested output $`\phi_e\text{,}`$ along with an **aggregated proof** $`\phi^{final}_e`$. This aggregated proof fastens node synchronisation by allowing nodes to efficiently verify the final result without checking each individual $`\phi_x`$ produced during the computation phase :

```math
\phi^{final}_e \leftarrow \Phi.\text{close}( \Phi.\text{StreamState})
```

##### 1.2.4. The η Generations
- This is the final nonce $`\eta_\text{e}`$ used to determine participant eligibility during epoch $`e`$.
- It originates from the operation $`\star`$ with $`\phi^{\text{stream}}_{t}`$ at $`\text{pre-}\eta_\text{e+1}`$ synchronization and η stream  $`\text{when } t = \text{end of epoch}_\text{e-3}`$ and the combination of the outputs $`\{o_i\}_{[1,i]}`$ using an aggregation function $`f_\text{agg}`$.

```math
\eta_\text{e} = \eta^\text{evolving}_{epoch_\text{e-3}}\ \star\ f_\text{agg}(o_1, \dots, o_e) , \quad \text{when } t = \text{pre-}\eta_\text{e+1}\text{ synchronization }
```

### 2. The Φ Cryptographic Primitive

#### 2.1. Expected Properties

The Φ cryptographic primitive is a critical component of the Phalanx protocol, designed to increase the computational cost of grinding attacks while remaining efficient for honest participants. To achieve this, Φ must adhere to a set of well-defined properties that ensure its security, efficiency, and practical usability within the Cardano ecosystem. These properties are outlined in the table below :

| **Property**              | **Description**                                                                                                   |
|---------------------------|-------------------------------------------------------------------------------------------------------------------|
| **Functionality**         | Must be a well-defined mathematical function, ensuring a unique output for each given input (unlike proof-of-work, which allows multiple valid outputs). |
| **Determinism**           | Must be fully deterministic, with the output entirely determined by the input, eliminating non-deterministic variations. |
| **Efficient Verification**| Must allow for fast and lightweight verification, enabling rapid validation of outputs with minimal computational overhead. |
| **Compact Representation**| Input and output sizes should be small enough to fit within a block, optimizing on-chain storage efficiency. Further reductions are desirable where feasible. |
| **Lower Bound on Computation** | Computational cost of evaluation should be well-characterized and predictable, with a lower bound that is difficult to surpass, ensuring adversaries cannot gain an unfair efficiency advantage. |
| **Ease of Implementation & Maintenance** | Should be simple to implement and maintain, ensuring long-term usability and minimizing technical debt. |
| **Adaptive Security**     | Function and its parameters should be easily reconfigurable to accommodate evolving threats, such as advances in computational power or new cryptographic attacks. |

#### 2.2. Verifiable Delayed Functions (VDF)

Verifiable Delayed Functions (VDFs) are cryptographic primitives designed to take a certain amount of time to compute, regardless of how much computing resources are available. This delay is enforced by requiring a specific number of sequential steps that cannot be easily sped up through parallel processing. Once the computation is done, the result, $`y = g^{2^T}`$, comes with a proof, $`\pi`$, that can be checked quickly and efficiently by anyone. Importantly, for a given input, the output is always the same (deterministic function), ensuring consistency. They usually rely on repeatedly squaring numbers in a mathematical setting that prevents shortcuts and enables quick verification.

As one can see, VDFs present _functionality_, _determinism_, _efficient verification_ and _lower bound on computation_. The _compact representation_ depends on the chosen group as well as the instantiation, which we will tackle later on. The _implementation and maintenance_ is straightforward as the output of a VDF is a simple exponentiation of a group element, only the square operation is needed to be implemented to compute it. As for the proof, this depends on the precise VDF instantiation. Finally, the system is "adaptively secure" as we can set up a group with high security to be reused for a whole epoch or more, and set the number of squaring, also called difficulty, depending on how much computation we want the nodes to perform.

Verifiable Delayed Functions were introduced by Boneh et al. [[6]](https://eprint.iacr.org/2018/601.pdf) where the authors suggest several sequential functions combined with the use of proof systems in the incrementally verifiable computation framework (IVC) for viable proof generation and fast verification.
VDF variants revolve around two primary SNARK-free designs: one from Pietrzak [[36]](https://drops.dagstuhl.de/storage/00lipics/lipics-vol124-itcs2019/LIPIcs.ITCS.2019.60/LIPIcs.ITCS.2019.60.pdf) and the second from Wesolowski [[35]](https://eprint.iacr.org/2018/623.pdf). They differ in the proof design.

In Wesolowski’s paper, the proof is defined as $g^{{2^T} /\ p}$ where $g$ is the challenge, $T$ the difficulty and $p$ is a prime number found by hashing the VDF input and output together.
The proof is thus a single group element that can be computed in at most $2\cdot T$ group operations and constant space, or $(1+1/s) \cdot T$ time where the number $s$ is both the number of processors and space while the verification takes $\text{log}_2 T$ scalar multiplications in $\mathbb{Z}/p$ and two small exponentiations in the group $\mathbb{G}$. The proving time can further be optimized to $O(T /\ \text{log}(T))$ group multiplications by reusing the evaluation intermediary results.
Wesolowski also presents aggregation and watermarking methods. The aggregation method does not consist in aggregating multiple proofs but computing a proof of several VDF challenges. This is done by batching all inputs and outputs together and creating a proof for this batched input. The watermarking is done by computing the VDF twice, once normally and another time on a combination of the challenger’s id and VDF input.

In Pietrzak’s paper, the proof is a tuple of group elements $\pi = \{x^{2^{T / 2^i}}\}$, of size logarithmic in $T$, that can be computed in $(1+2 /\sqrt{T})\cdot T$ time and can be optimized to $O(\sqrt{T} \cdot \text{log}_2 T)$ multiplications. The verification takes $2 \cdot \text{log}_2T$ small exponentiations. Subsequent work on Pietrzak’s paper shows how VDFs challenges can be structured in a Merkle tree to get a proof of the whole tree.

We will choose Wesolowski design over Pietrzark because of its space efficiency and possibility to aggregate proofs.

Specialized hardware such as ASICs can be used to evaluate VDF output much faster, up to a factor 5 in Chia's VDF project while Ethereum considers a factor 10. This, while unfortunate, is not prohibitive in our context as we only consider the use of VDFs for their computational cost. An attacker would still require a substantial budget to perform an anti-grinding attack in addition to purchasing at scale the specialized hardware that is not inexpensive nor readily available (Chia' ASICs can be purchased on a case per case basis for $1,000). We can also note that any solution would still be affected by hardware, like in the case of proof of works and hash farms.

#### 2.3. Wesolowski's VDF

##### 2.3.1. VDF Primitives

To define Wesolowski VDF construction, we first introduce a series of hash functions: $`\text{Hash}^{(n)}_\mathbb{N}`$, which samples random integers of $`n`$ bits, $`\text{Hash}^{(n)}_\text{prime}`$, which samples a random integer from the set of the first $`2^{2n}`$ prime numbers, and $`\text{Hash}_\mathbb{G}`$, which samples a random group element of the class group $`\mathbb{G}`$.

We define the interface of a Verifiable Delay Function as $`\texttt{VDF} = (\texttt{Setup},\ \texttt{Evalute},\ \texttt{Prove},\ \texttt{Verify})`$, and define its underlying functions based on class groups as follows:

- $`(\mathbb{G},\ \Delta,\ \cdot) \leftarrow \texttt{VDF.Setup}(\lambda,\ \Delta_{\text{challenge}})`$
  Takes as input a **security parameter** $`\lambda \in \mathbb{N}`$ and a **challenge discriminant** $`\Delta_{\text{challenge}} \in \{0,1\}^*`$. This challenge discriminant acts as a source of public entropy used to deterministically derive the group discriminant $\Delta$, which defines a group of unknown order $\mathbb{G}$ along with its group operation $`\cdot`$. The use of a challenge ensures that the resulting group is unbiasable and unpredictable, preventing adversarial precomputation. We shall drop the group settings $`(\mathbb{G},\ \Delta,\ \cdot)`$ from further functions for readability. Internally, we expect the setup procedure to invoke the following sub-operations:
```math
    \Delta \leftarrow \texttt{VDF.CreateDiscriminant}(\lambda,\ \Delta_{\text{challenge}})
```
```math
  (\mathbb{G},\ \cdot) \leftarrow \texttt{VDF.DeriveClassGroup}(\lambda,\ \Delta)
```

- $`y \leftarrow \texttt{VDF.Evaluate}(\ x,\ I)`$
  Given a challenge $`x \in \mathbb{G}`$ and a number of iterations $`I \in \mathbb{N}`$, computes the  output $`y = x^{2^I}`$.

- $`\pi \leftarrow \texttt{VDF.Prove}(\ x,\ y,\ I)`$
  Given a challenge and output $`(x,y) \in \mathbb{G}^2`$, computes the VDF  **proof** as $`\pi = x^{2^I / p}`$ where $`p \leftarrow \text{Hash}^{(2 \lambda)}_\text{prime}(x \| y)`$ is sampled from the first $`2^{2 \lambda}`$ prime numbers.

- $`\{0,1\} \leftarrow \texttt{VDF.Verify}( \ x,\ y,\ I,\ \pi)`$
  Returns 1 if $`\pi`$ successfully attests that $`y = x^{2^I}`$ with overwhelming probability, that is if $\pi^r \cdot x^p == y$ where $`p \leftarrow \text{Hash}^{(2 \lambda)}_\text{prime}(x \| y)`$ and $`r \leftarrow 2^I \text{mod}\ p`$. Returns 0 otherwise.

##### 2.3.2. VDF Aggregation Primitives

In this section, we present a mechanism for producing a Wesolowski VDF **aggregation proof**. This construction enables efficient synchronization for network participants and may play a central role in deriving the final epoch nonce $`\eta_e`$.
The aggregation mechanism has the following interface $`\texttt{VDF.Aggregation} = (\text{Init},\ \text{Update},\ \text{Prove},\ \text{Verify})`$ whose functions will be detailled afterwards. We assume that a class group $`\mathbb{G}`$ has already been set up, by $`(\mathbb{G},\ \Delta,\ \cdot) \leftarrow \texttt{VDF.Setup}(\lambda,\ \Delta_{\text{challenge}})`$.

- *N.B.** We are showing here the core algorithms for simplicity and readability. In practice, we may use further techniques, for instance using an arbitrary byte and the epoch's number as personalization tags to ensure domain separation.

At the beginning of each epoch, we initialize the VDF accumulators' state that will be used to generate the VDF aggregation proof using $`\texttt{VDF.Aggregation.Init}`$.
<center>

| `Initialize accumulators` | $`(\text{Acc}_x, \text{Acc}_y, \alpha) \leftarrow \texttt{VDF.Aggregation.Init}(\lambda, \text{pre-}\eta_e)`$     |
| ------------------------- | ------------------------- |
| **Input Parameters**      | <ul><li>$`\lambda \in \mathbb{N}`$ — Security parameter.</li><li>$\text{pre-}\eta_e \in \{0,1\}^{256}$ — 256-bit pre-nonce entropy for epoch $e$.</li></ul> |
| **Steps**                 | <ol><li>Compute all VDF challenges:<br>$`\forall i\ x_i \leftarrow  \text{Hash}_\mathbb{G}(\text{pre-}\eta_e \|\|\ i) `$</li><li>Compute the initial aggregation nonce:<br>$`\alpha \leftarrow  \text{Hash}^{(\lambda)}_\mathbb{N}(x_1 \|\| \dots \|\| x_n) `$</li><li>Initializes the accumulators to the identity:<br>$`(\text{Acc}_x,\ \text{Acc}_y) \leftarrow (1_\mathbb{G},\ 1_\mathbb{G})`$</li></ol>                                                                                                          |
| **Returned Output**       | $`(\text{Acc}_x,\ \text{Acc}_y,\ \alpha)`$ — Input and output accumulators and initial aggregation nonce.   |

</center>

Every time a VDF output is published on-chain, if no previous VDF outputs are missing, we shall update the accumulators' state using $`\texttt{VDF.Aggregation.Update}`$.
<center>

| `Update accumulators` | $`(\text{Acc}_x, \text{Acc}_y, \alpha) \leftarrow \texttt{VDF.Aggregation.Update}(\lambda, (x_i, y_i),\ (\text{Acc}_x,\ \text{Acc}_y,\ \alpha))`$     |
| ------------------------- | ------------------------- |
| **Input Parameters**      | <ul><li>$`\lambda \in \mathbb{N}`$ — Security parameter.</li><li>$`(x_i,\ y_i)`$ — Pair of VDF input and output for current interval.</li><li>$`(\text{Acc}_x,\ \text{Acc}_y,\ \alpha)`$ — Accumulators' state.</li></ul> |
| **Steps**                 | <ol><li>Compute the aggregation nonce:<br>$`\alpha \leftarrow  \text{Hash}^{(\lambda)}_\mathbb{N}(\alpha\ \|\|\ y_i) `$</li><li>Update the accumulators:<br>$`(\text{Acc}_x, \text{Acc}_y) \leftarrow (\text{Acc}_x \cdot x_i^\alpha,\ \text{Acc}_y \cdot y_i^\alpha)`$</li></ol>                                                                                                    |
| **Returned Output**       | $`(\text{Acc}_x,\ \text{Acc}_y,\ \alpha)`$ — Updated accumulator's state.   |

</center>

Once all VDF outputs have been generated and the accumulators updated, we can generate the VDF aggregation proof $`\pi`$ using $`\texttt{VDF.Aggregation.Prove}`$.
<center>

| `Prove accumulators` | $`\pi \leftarrow \texttt{VDF.Aggregation.Prove}(\ (\text{Acc}_x,\ \text{Acc}_y,\ \_\alpha),\ I)`$     |
| ------------------------- | ------------------------- |
| **Input Parameters**      | <ul><li>$`(\text{Acc}_x,\ \text{Acc}_y,\ \alpha)`$ — Accumulators' state.</li><li>$`I \in \mathbb{N}`$ — Per-interval iteration count for the VDF.</li></ul> |
| **Steps**                 | <ol><li>Compute the accumulator proof as a VDF proof:<br>$`\pi \leftarrow \texttt{VDF.Prove}(\text{Acc}_x,\ \text{Acc}_y,\ I)`$</li></ol>                                                                                                    |
| **Returned Output**       | $`\pi`$ — Aggregated proof.   |

</center>

The VDF aggregation proof $`\pi`$ can then be efficiently be verified using $`\texttt{VDF.Aggregation.Verify}`$.
<center>

| `Verify accumulators` | $`\{0,1\} \leftarrow \texttt{VDF.Aggregation.Verify}(\ (\text{Acc}_x,\ \text{Acc}_y,\ \_\alpha),\ I,\ \pi)`$     |
| ------------------------- | ------------------------- |
| **Input Parameters**      | <ul><li>$`(\text{Acc}_x,\ \text{Acc}_y,\ \alpha)`$ — Accumulators' state.</li><li>$`I \in \mathbb{N}`$ — Per-interval iteration count for the VDF.</li><li>$`\pi \in \mathbb{G}`$ — Aggregated VDF proof.</li></ul> |
| **Steps**                 | <ol><li>Verfy the accumulators' proof:<br>$`b \leftarrow \texttt{VDF.Verify}(\text{Acc}_x,\ \text{Acc}_y,\ I,\ \pi)`$</li></ol>                                                                                                    |
| **Returned Output**       | $`b`$ — Verification bit.   |

</center>

### 3. Φ Stream Specification

We previously outlined the purpose of the Phalanx sub-protocol and introduced the cryptographic primitive underpinning its security guarantees. In this section, we provide a precise technical specification of the protocol, focusing on how the $`\Phi`$ iterations are distributed and how Wesolowski’s Verifiable Delay Function (VDF) is integrated into the process.

#### 3.1. Distribution of Φ Iterations

As previously mentioned, Φ Stream is divided into epoch-sized *lifecycle segments*. Each segment begins with an **initialize** function, ends with a **close** function, and is immediately followed by the start of a new segment.

We further partition this segment into **intervals**, each large enough to guarantee (with 128-bit confidence) that at least one block will be produced within it. This corresponds to **3435 slots** per interval. For simplicity, we round this to **3600 slots** (~1 hour), resulting in exactly 120 intervals per segment, which conveniently aligns with the 120 hours in five days.

<details>
<summary>🔍 How 128-bit Confidence gives 3435 Slots ?</summary>
<p>

<span style="display:block; font-size:1.25em; font-weight:bold"> 📦 Guaranteeing Honest Block Inclusion with 128-bit Confidence in our context </span>

We want to make sure that, in any given interval of $N$ slots, there's **at least one honest block** produced — with a failure probability of at most $2^{-128}$ (which is a standard for cryptographic security).

It is also important to note that we are operating in a context where fork-related concerns can be safely abstracted away. In particular, if an adversary were to attempt a private chain attack and succeed, it would imply that their chain is denser and that the proof of $\Phi$ computation is valid. In this setting, forks do not undermine security—they actually improve the probability of having at least one valid computation published within the interval.

This means: $`\Pr(\text{at least one honest block in } N \text{ slots}) \geq 1 - 2^{-128}`$

<span style="display:block; font-size:1.1em; font-weight:bold"> 🎲 Step 1 — What’s the chance of *not* getting an honest block? </span>

Each slot gives honest participants a chance to be selected as leader.

Let:

- $f = 0.05$ → probability a slot is active
- $\sigma = 0.51$ → at least 51% of stake is honest

Then the chance that **no honest party** is selected in a slot is: $`(1 - f) + f \cdot(1-\sigma) = 0.95 + 0.05 \cdot (1-0.51) \approx 0.97449`$


So, the chance that **at least one honest party** is selected in a slot is: $`p_h = 1 - 0.97449 = 0.0255`$


This means that **each slot has a 2.584% chance** of having an honest leader.

<span style="display:block; font-size:1.1em; font-weight:bold"> 📐 Step 2 — What about across $N$ slots? </span>

The chance that **no honest block** is produced in $N$ consecutive slots is: $`(1 - p_h)^N`$

We want this to be **less than or equal to** $2^{-128}$, so: $(1 - p_h)^N \leq 2^{-128}$

<span style="display:block; font-size:1.1em; font-weight:bold"> ✏️ Step 3 — Solve for $N$ </span>

Take log base 2 of both sides:

$\log_2((1 - p_h)^N) \leq \log_2(2^{-128})$

$N \cdot \log_2(1 - p_h) \leq -128$

$N \geq \frac{-128}{\log_2(1 - p_h)}$

Now plug in:


$`\log_2(1 - 0.0255) = \log_2(0.97449) \approx -0.03726`$

$`N \geq \lceil \frac{128}{0.03726} \rceil = 3435`$

To guarantee with 128-bit confidence that an interval contains **at least one honest block**, the interval must be at least **3435 slots** long.

This ensures security even if up to **49% of stake is adversarial**.
## 

</p>
</details>
<br>

This structure can be illustrated as follows:

![alt text](./image/intervals.png)

As previously described, we configure the stream using two key parameters, most notably the total computational budget $`T_\Phi`$. This value defines the total amount of work we require SPOs to collectively perform.

We split $`T_\Phi`$ into discrete **iterations**, each with the following properties:

- Iterations are fully independent and can be computed in parallel.
- Slot leaders are responsible for submitting a proof of computation for the specific iteration assigned to them.
- These computations are fully decoupled, there is no requirement to wait for previous iterations, enabling input precomputation and reducing latency.
- All iterations must eventually be completed, and an additional and final iteration is used to aggregating all outputs along with a corresponding proof.
- The iterations are then used to compute the epoch randomness $\eta_e$.

Each iteration is mapped to a specific interval, with the following constraints:

- The first interval is intentionally left without an assigned iteration, giving slot leaders time to compute the first output for interval \#2 and allowing precomputation of the challenges.
- Each interval must be longer than the time required to compute a single iteration (i.e., the iteration duration must be less than one hour).
- The first slot leader to produce a block within an interval is responsible for submitting the corresponding attested output.

At first glance, we could divide $`T_\Phi`$ evenly across the 120 intervals. However, to ensure resilience, the protocol must remain robust even in extreme scenarios—such as a global outage causing **36 hours** of consecutive downtime (**30% of an epoch**). This scenario is detailed in the [Cardano Disaster Recovery Plan](https://iohk.io/en/research/library/papers/cardano-disaster-recovery-plan).

A global outage implies a sequence of blockless intervals. To tolerate such conditions, the protocol must be able to handle up to **36 intervals without block production**.  To address this, we introduce a catch-up mechanism:
- We reserve the final 36 intervals of the segment specifically for recovering any missing attested outputs.
- These missing outputs must be submitted in order, according to their original indices, ensuring deterministic reconstruction of the full computation stream.


We define **4 sequential phases** in the stream lifecycle:

- 🟧 **Parametrization Phase** :
  The stream is configured but not yet active. Parameters such as $`\lambda`$ (computation hardness) and $`\#\text{iterations}_\phi`$ (number of iterations) are established during this phase.

- 🟩 **Initialization Grace Phase**:
  The stream is activated, and Stake Pool Operators (SPOs) are given a grace period to begin the first iteration of the computation.

- 🟥 **Computation Phase**:
  During this phase, the protocol expects attested outputs to be published on-chain. It consists of **82 computation iterations**, each producing an intermediate output that contributes to the final result.

- 🟦 **Catch-up & Closure Phase**:
- A bounded recovery window that allows SPOs to submit any **missing attested outputs**, ensuring the completeness of the computation prior to finalization.
- A final dedicated interval to compute the **aggregation** of all previous outputs and derive the epoch’s final randomness $`\eta_e`$. This phase **seals the stream** and concludes a lifecycle.

The diagram below illustrates how the lifecycle segment is structured:

![alt text](./image/structured-intervals.png)

### 3.2. The State Machine

#### 3.2.1. Diagram Overview

The figure below presents the **state transition diagram** for the Phalanx computation stream. Each node represents a distinct state in the lifecycle of the stream, and the arrows indicate transitions triggered by `Tick` events. These transitions are guarded by boolean predicates evaluated at each slot (e.g., `isWithinComputationPhase`, `isWithinCurrentInterval`).

![Phalanx State Transition Diagram](./image/state-transition-diagram.png)

In the following sections, we provide a detailed breakdown of each phase of the state machine, specifying its purpose, entry conditions, timing constraints, and transitions.

#### 3.2.2. 🟧 Parametrization Phase

At the setup of $`\phi^{stream}`$, the total number of VDF iterations is derived from the time-bound parameter $`T_\Phi`$, using a reference hardware profile that reflects the minimal computational capacity expected of SPOs. While this derivation may not be fully automatable in practice, we include it here to clarify how time constraints are mapped to iteration counts during configuration.

Importantly, this **parametrization phase** occurs only once, either during the initial bootstrap of the stream or following a transition from the `Closed` to `Initialized` state.

<center>

| `Parametrized`  |  $`\Phi.\text{Stream.State} \in \texttt{Parametrized} : \left\{ {securityParameter} \in \mathbb{N},\quad I \in \mathbb{N} \right\}`$ |
| ---------------------------- | ---------------------------------------------------------------------------------------------------------------------------- |
| **Fields**       | <ul><li>$`\text{securityParameter} \in \mathbb{N}`$ — The **security parameter**, defining the size (in bits) of the VDF discriminant.</li><li>$`I \in \mathbb{N}`$ — The **per-interval VDF iteration count**, computed from $`T_\Phi`$ and evenly distributed across 82 computation intervals.</li></ul> |


|  `parametrize` | $`\Phi.\text{Stream.State} \leftarrow \Phi.\text{parametrize}(\lambda,\ T_\Phi)`$|
| ------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |
| **Input Parameters**          | <ul><li>$`\lambda \in \mathbb{N}`$ — **Security parameter**, defines the size (in bits) of the VDF discriminant.</li><li>$`T_\Phi \in \mathbb{N}`$ — **Time budget** in seconds, representing the total computation time under reference hardware.</li></ul> |
| **Derivation Logic**  | <ul><li>$`\#\text{Iterations}_\Phi \leftarrow \texttt{VDF}.\texttt{IterationsFromDuration}(T_\Phi)`$</li><li>$`\#\text{Iterations}_\phi \leftarrow \left\lfloor \frac{\#\text{Iterations}_\Phi}{82} \right\rfloor`$</li></ul>                                  |
| **Returned State** | $`\texttt{Parametrized} \left\{ \text{securityParameter} \leftarrowtail \lambda,\quad I \leftarrowtail \#\text{Iterations}_\phi \right\}`$|

</center>

#### 3.2.3.  🟩 Initialization Grace Phase

Initialization occurs at every pre-ηₑ synchronization point, followed by an *Initialization Grace* period during which the protocol waits long enough for the first iteration to be computed and its proof to be included within the first computation interval. This process recurs every $`10 \cdot \frac{k}{f}`$ slots.

##### 3.2.3.1. Initialize Command
We show here how to initialize the class-group based VDF algorithm when generating a group for each different epoch. Were we to use the same group for many, if not all, epochs, we would run these steps in the *Parametrization phase* and change the discriminant seed $`\Delta_{\text{challenge}}`$ accordingly, e.g. if we use the same group forever we could use $`\Delta_{\text{challenge}} \leftarrow \text{Hash}(\text{bin}(\text{``IOHKPhalanx2025"}))`$.

<center>

| `Initialized` | $`\Phi.\text{Stream.State} \in \texttt{Initialized} : \left\{ \text{parametrized} \in \texttt{Parametrized},\ \text{group} \in \mathbb{G},\  \text{discriminant} \in \mathbb{Z},\ \text{operation} : \mathbb{G} \times \mathbb{G} \to \mathbb{G} \right\}`$|
| ----------- | -------------- |
| **Fields**  | <ul><li>$\text{parametrized} \in \texttt{Parametrized}$ — Reference to the prior configuration (security parameter and iteration count).</li><li>$\text{group} \in \mathbb{G}$ — VDF group used for exponentiation.</li><li>$\text{discriminant} \in \mathbb{Z}$ — Epoch-specific VDF discriminant.</li><li>$\text{operation} : \mathbb{G} \times \mathbb{G} \to \mathbb{G}$ — Group operation used for VDF evaluation (e.g., modular exponentiation).</li><li>$\text{epochId}_e \in \mathbb{N}$ — Numerical identifier for epoch $e$.</li><li>$\text{pre-}\eta_e \in \{0,1\}^{256}$ — 256-bit pre-nonce entropy for epoch $e$.</li></ul> |

</center>

<center>

| `initialize`           | $\Phi.\text{Stream.State} \leftarrow \Phi.\text{Initialize}(\text{parametrizedState},\ \text{epochId}_e,\ \text{pre-}\eta_e)$ |
| -------------------- | ----------------------------------------- |
| **Input Parameters** | <ul><li>$\text{parametrizedState} = (\lambda,\ I) \in \texttt{Parametrized}$ — Configuration from the prior Parametrized state.</li><li>$\text{epochId}_e \in \mathbb{N}$ — Numerical identifier for epoch $e$.</li><li>$\text{pre-}\eta_e \in \{0,1\}^{256}$ — 256-bit pre-nonce entropy for epoch $e$.</li></ul>              |
| **Derivation Logic** | <ul><li>$`\Delta_{\text{challenge}} \leftarrow \text{Hash}(\text{bin}(\text{epochId}_e) \ \|\ \text{pre-}\eta_e)`$</li><li>$`(\mathbb{G},\ \Delta,\ \cdot) \leftarrow \texttt{VDF.Setup}(\lambda,\ \Delta_{\text{challenge}})`$</li></ul> |
| **Returned State**   | $`\texttt{Initialized} \left\{ \text{parametrized} \leftarrowtail (\lambda,\ I),\ \text{group} \leftarrowtail \mathbb{G},\ \text{discriminant} \leftarrowtail \Delta,\ \text{operation} \leftarrowtail \cdot , \ \text{epochId}_e \leftarrowtail \text{epochId}_e ,\ \text{pre-}\eta_e  \leftarrowtail \text{pre-}\eta_e  \right\}`$                                        |

</center>

##### 3.2.3.2. Tick Commands & Grace Period

<center>

| `AwaitingComputationPhase` | $`\Phi.\text{Stream.State} \in \texttt{AwaitingComputationPhase} : \left\{ \text{initialized} \in \texttt{Initialized},\ \text{currentSlot} \in\ \mathbb{N} \bmod \left(10 \cdot \frac{k}{f}\right) \right\}`$ |
|--------------------|-------------|
| **Fields** | <ul><li>$`\text{initialized} \in \texttt{Initialized}`$ — Reference to the prior initialization.</li><li>$`\text{currentSlot} \in\ \mathbb{N} \bmod \left(10 \cdot \frac{k}{f}\right)`$ — The current slot in the chain timeline.</li></ul> |

</center>

- *Initial tick transition to `AwaitingComputationPhase`:**

<center>

| `tick`           | $\Phi.\text{Stream.State} \leftarrow \Phi.\text{tick}(\text{initializedState})$ |
| -------------------- | ----------------------------------------- |
| **Input Parameters** | <ul><li>$\text{initializedState} \in \texttt{Initialized}$ — Configuration from the prior Initialized state.</li></ul>              |
| **Returned State**   | $`\texttt{AwaitingComputationPhase} \left\{ \text{initialized} \leftarrowtail initialiinitializedzedState,\ \text{currentSlot} \leftarrowtail 0 \right\}`$

</center>

- *Subsequent ticks on `AwaitingComputationPhase`:**

<center>

| `tick`               | $`\Phi.\text{Stream.State} \leftarrow \Phi.\text{tick}(\text{awaitingComputationPhaseState})`$ |
|--------------------|---------------------------------------------------------------------------------------------------|
| **Input Parameters** | <ul><li>$`\text{awaitingComputationPhaseState} \in \texttt{AwaitingComputationPhase}`$ — Configuration from the prior Initialized state.</li></ul> |
| **Returned State**   | $` \begin{cases} \text{When } \texttt{isWithinInitializationGracePhase} :\ \texttt{AwaitingComputationPhase}\ \{ \text{initialized} ,\ \text{currentSlot++} \} \\\\ \text{When } \texttt{isWithinComputationPhase} :\ \texttt{AwaitingAttestedOutput}\ \{ \text{initialized} ,\ \text{currentSlot++} \} \end{cases}`$ |

</center>

#### 3.2.4.  🟥  Computation Phase

##### 3.2.4.1. VDF integration

We are now entering the **Computation Phase**. We have waited long enough for the first slot leader within the initial interval to have the opportunity to produce a block and submit the corresponding attested output. However, because the slot distribution is privately visible, leaders within the interval cannot determine whether they are the first to produce a block.

Each leader is free to adopt their own strategy for deciding whether to initiate the proof of computation. A simple and conservative approach is to wait until $`\text{currentSlot} \geq \text{nextElectedSlot} - \left(\frac{T_\Phi}{82} + C\right)`$, where $`C`$ is a small constant. At that point, the leader may begin computing. If a block has already been produced by then, the leader can either skip the computation or abort it if already in progress. This delay increases the chances that any earlier eligible leaders have already submitted their outputs, thereby minimizing the risk of redundant proofs.

To publish the first block of interval $`i \in [1..82]`$ of epoch $`e`$, the node invokes:

```math
(y_i, \pi_i) \leftarrow \Phi.\text{compute}(\text{initialized} \in \texttt{Initialized},\ i \in \texttt{Interval})
```

This function internally calls the VDF primitives: $`y_i \leftarrow \texttt{VDF.Evaluate}((\mathbb{G},\ \Delta,\ \cdot), \ x_i,\ I)`$ and $`\pi \leftarrow \texttt{VDF.Prove}((\mathbb{G},\ \Delta, \cdot),\ x_i,\ y_i,\ I)`$ with inputs constructed as:

- $`x_i \leftarrow \text{Hash}(\text{b``challenge"} ||\ \text{bin}(e) ||\ \text{pre-}\eta_e || \text{bin}(i))`$
- The parameters $`(\mathbb{G}, \Delta, \cdot)`$ and $`I`$ are retrieved from the `Initialized` state.

Finally, the node includes the attested outputs in the block header:

- $`y_i`$: the VDF output for interval $`i`$
- $`\pi_i`$: the corresponding VDF proof for interval $`i`$

In rare cases, an interval may produce no block, and consequently, no expected proof for the corresponding iteration. The computation phase simply acknowledges these gaps; they are handled during the subsequent **Catch-up Phase**, which is specifically designed to resolve such missing outputs.

##### 3.2.4.2. The States

During the computation phase, the stream alternates between two closely related states: `AwaitingAttestedOutput` and `AttestedOutputProvided`. These two states are **structurally identical**, meaning they share the same underlying fields. What distinguishes them is their **semantic role** in the protocol’s lifecycle:

- `AwaitingAttestedOutput` represents the period **before** an attestation has been submitted for the current interval.
- `AttestedOutputProvided` signals that the attestation for the current interval has been **successfully received and verified**.

The field structure for both is as follows:

```math
\Phi.\text{Stream.State} \in \texttt{AwaitingAttestedOutput} : \left\{
  \begin{aligned}
    &\text{initialized}     &&\in\ \texttt{Initialized}, \\
    &\text{currentSlot}     &&\in\ \mathbb{N} \bmod \left(10 \cdot \frac{k}{f}\right), \\
    &\text{attestedOutputs} &&\in\ \left[\texttt{Maybe}\ (y, \pi)\right]^{82}
  \end{aligned}
\right\}
```

<center>

| **Field**   | **Description** |
| ---- | -------- |
| $`\text{initialized} \in \texttt{Initialized}`$ | Reference to the prior initialization state.|
| $`\text{currentSlot} \in\ \mathbb{N} \bmod \left(10 \cdot \frac{k}{f}\right)`$| The current slot in the timeline.  |
| $`\text{attestedOutputs} \in \left[\texttt{Maybe}\ (y, \pi)\right]^{82}`$ | <ul><li>An array of optional attested outputs, one per computation interval.</li><li>Each index corresponds to a specific interval and may contain a proof pair $`(y, \pi)`$.</li><li>If the output is not yet submitted, the entry is `None`.</li></ul> |

</center>

The `AttestedOutputProvided` state reuses the exact same structure:

```math
\Phi.\text{Stream.State} \in \texttt{AttestedOutputProvided} : \left\{
  \begin{aligned}
    &\text{initialized}     &&\in\ \texttt{Initialized}, \\
    &\text{currentSlot}     &&\in\ \mathbb{N} \bmod \left(10 \cdot \frac{k}{f}\right), \\
    &\text{attestedOutputs} &&\in\ \left[\texttt{Maybe}\ (y, \pi)\right]^{82}
  \end{aligned}
\right\}
```

This version aligns both the field names and their types in two neat columns. Let me know if you'd prefer the braces to be placed differently (e.g. outside the alignment block) for aesthetic reasons.

<center>

| **Field**   | **Description**    |
| ------------- | --------------- |
| $`\text{initialized} \in \texttt{Initialized}`$ | Reference to the prior initialization state.     |
| $`\text{currentSlot} \in\ \mathbb{N} \bmod \left(10 \cdot \frac{k}{f}\right)`$ | The current slot in the timeline.   |
| $`\text{attestedOutputs} \in \left[\texttt{Maybe}\ (y, \pi)\right]^{82}`$ | <ul><li>An array of optional attested outputs, one per computation interval.</li><li>Each index corresponds to a specific interval and may contain a proof pair $`(y, \pi)`$.</li><li>If the output hasn't been submitted yet, the entry is `None`.</li><li>For the current interval, the output **has already been provided** in this state.</li></ul> |

</center>

##### 3.2.4.3. ProvideAttestedOutput & Tick Commands

The `provideAttestedOutput` command is used to submit a new attested output $`\phi_i = (y_i, \pi_i)`$ for a specific interval $`i`$, when the protocol is in the `AwaitingAttestedOutput` state. This function verifies the validity of the provided proof and updates the stream state accordingly :

<center>

| `provideAttestedOutput` | $`\Phi.\text{Stream.State} \leftarrow \Phi.\text{provideAttestedOutput}(\text{awaitingAttestedOutputState},\ \phi_i)`$ |
|-------------------------|--------------------------------------------------------------------------------------------------------------------------|
| **Input Parameters**    | <ul><li>$`\text{awaitingAttestedOutputState} \in \texttt{AwaitingAttestedOutput}`$ — Current state awaiting an attested output $`\phi_i`$ for interval $`i`$.</li><li>$`\phi_i = (y_i, \pi_i)`$ — Attested output and corresponding proof.</li></ul> |
| **Property Check**      | <ul><li>Ensure $`\phi_i`$ is valid by verifying: $`\texttt{VDF.Verify}((\mathbb{G},\ \Delta,\ \cdot),\ x_i,\ y_i,\ I,\ \pi_i)`$</li> <li>Where:<br> $`x_i = \text{Hash}(\text{b``challenge"}\ \|\|\ \text{bin}(e)\ \|\|\ \text{pre-}\eta_e\ \|\|\ \text{bin}(i))`$<br> $`I \in \mathbb{N}`$ is the per-interval iteration count.</li></ul> |
| **Returned State**      | $`\texttt{AttestedOutputProvided}\ \{ \text{initialized},\ \text{currentSlot} + 1,\ \text{attestedOutputs}[i] \leftarrowtail \phi_i \}`$ — Updated state reflecting the verified attestation. |

</center>

Once an attested output has been provided, the next slot may trigger a `tick` event. If no further action is taken, the system must determine whether it remains within the current interval, moves to the next, or enters the catch-up phase. The following command captures this logic when starting from the `AttestedOutputProvided` state :

<center>

| `tick`               | $`\Phi.\text{Stream.State} \leftarrow \Phi.\text{tick}(\text{attestedOutputProvidedState})`$ |
|----------------------|---------------------------------------------------------------------------------------------------|
| **Input Parameters** | <ul><li>$`\text{attestedOutputProvidedState} \in \texttt{AttestedOutputProvided}`$ — The current state after an attestation has been successfully provided.</li></ul> |
| **Returned State**   | $`\begin{cases} \text{When } \texttt{isWithinCurrentInterval} &: \texttt{AttestedOutputProvided} \{ \dots,\ \text{currentSlot++} \} \\\\ \text{When } \texttt{isWithinNextInterval} &: \texttt{AwaitingAttestedOutput} \{ \dots,\ \text{currentSlot++} \} \\\\ \text{When } \texttt{isWithinCatchUpPhase} &: \texttt{AwaitingMissingAttestedOutput} \{ \dots,\ \text{currentSlot++} \} \end{cases}`$ |

</center>

Alternatively, when still waiting for an attestation and no block was produced, a `tick` may trigger a transition based on the current time. This command applies to the `AwaitingAttestedOutput` state before any attestation has been submitted :

<center>

| `tick`               | $`\Phi.\text{Stream.State} \leftarrow \Phi.\text{tick}(\text{awaitingAttestedOutputState})`$ |
|--------------------|---------------------------------------------------------------------------------------------------|
| **Input Parameters** | <ul><li>$`\text{awaitingAttestedOutputState} \in \texttt{AwaitingAttestedOutput}`$ — Current state awaiting an attested output $`\phi_i`$ for interval $`i`$.</li><li>$`\phi_i = (y_i, \pi_i)`$ — Attested output and corresponding proof.</li></ul> |
| **Returned State**   | $` \begin{cases} \text{When } \texttt{isWithinCurrentInterval} :\ \texttt{AwaitingComputationPhase}\ \{ \text{..} ,\ \text{currentSlot++} \} \\\\ \text{When } \texttt{isWithinCatchUpPhase} :\ \texttt{AwaitingMissingAttestedOutput}\ \{ \text{..} ,\ \text{currentSlot++} \}  \\\\ \text{When } \texttt{isClosable} :\ \texttt{AwaitingGracefulClosure}\ \{ \text{..} ,\ \text{currentSlot++} \}\end{cases}`$ |

</center>

`isClosable` indicates that all attested outputs have been successfully provided, and only the final interval remains, during which the outputs are aggregated and the seed $`\eta_e`$ is derived and recorded on-chain.

#### 3.2.5.  🟦 Catch-up Phase

This Catch-up Phase closely resembles the preceding Computation Phase, but its purpose is to recover from any blockless intervals that may have occurred — albeit such cases are extremely rare.

The phase spans a total of 37 intervals: 36 are reserved to account for up to 36 consecutive intervals without block production (e.g., a global outage affecting 30% of an epoch), and 1 final interval is allocated for the Closure Phase. As in the Computation Phase, missing attested outputs must be submitted in order, one per interval.

The faster these missing outputs are provided, the sooner the state machine can transition to the final phase. Although the protocol allocates 37 intervals to handle the worst-case scenario, recovery may complete much earlier in practice.

This section focuses solely on the Catch-up Phase; the next section will describe the process of stream closure.

##### 3.2.5.1. The States

Structurally, we define two states that are similar in form and semantics to `AwaitingMissingAttestedOutput` and `AttestedMissingOutputProvided`:

```math
\Phi.\text{Stream.State} \in \texttt{AwaitingMissingAttestedOutput} : \left\{
  \begin{aligned}
    &\text{initialized}     &&\in\ \texttt{Initialized}, \\
    &\text{currentSlot}     &&\in\ \mathbb{N} \bmod \left(10 \cdot \frac{k}{f}\right), \\
    &\text{attestedOutputs} &&\in\ \left[\texttt{Maybe}\ (y, \pi)\right]^{82}
  \end{aligned}
\right\}
```

```math
\Phi.\text{Stream.State} \in \texttt{AttestedMissingOutputProvided} : \left\{
  \begin{aligned}
    &\text{initialized}     &&\in\ \texttt{Initialized}, \\
    &\text{currentSlot}     &&\in\ \mathbb{N} \bmod \left(10 \cdot \frac{k}{f}\right), \\
    &\text{attestedOutputs} &&\in\ \left[\texttt{Maybe}\ (y, \pi)\right]^{82}
  \end{aligned}
\right\}
```

This phase focuses on recovering the missing attested outputs—specifically, the `None` entries in the `attestedOutputs` array. The goal during this phase is to have those missing values provided.This phase operates under strict sequential expectations where the missing attested outputs must be provided in order, one per interval, as in the Computation Phase. To make this explicit, we define the sequence of expected indices as follows:

```math
\text{ExpectedMissingIndices} := \left\{ i \in \{1, \dots, 82\} \mid \text{attestedOutputs}[i] = \texttt{Nothing} \right\}
```
This ordered set defines the exact sequence in which the missing attestations must be submitted during the Catch-up Phase.


##### 3.2.5.2. ProvideMissingAttestedOutput & Tick Commands

The `provideMissingAttestedOutput` command is used to submit a missing attested output $`\phi_i = (y_i, \pi_i)`$ for a specific interval $`i`$, when the protocol is in the `AwaitingMissingAttestedOutput` state. This function checks the validity of the proof and updates the stream state accordingly:

<center>

| `provideMissingAttestedOutput` | $`\Phi.\text{Stream.State} \leftarrow \Phi.\text{provideMissingAttestedOutput}(\text{awaitingMissingAttestedOutputState},\ \phi_i)`$  |
| ----- | --- |
| **Input Parameters**           | <ul><li>$`\text{awaitingMissingAttestedOutputState} \in \texttt{AwaitingMissingAttestedOutput}`$ — State awaiting a missing attestation $`\phi_i`$ for interval $`i`$.</li><li>$`\phi_i = (y_i, \pi_i)`$ — Attested output and its proof.</li></ul>                                            |
| **Property Check**             | <ul><li>Verify $`\phi_i`$ with: $`\texttt{VDF.Verify}((\mathbb{G},\ \Delta,\ \cdot),\ x_i,\ y_i,\ I,\ \pi_i)`$</li><li>Where:<br> $`x_i = \text{Hash}(\text{b``challenge"}\ \|\|\ \text{bin}(e)\ \|\|\ \text{pre-}\eta_e\ \|\|\ \text{bin}(i))`$</li><li>$`I \in \mathbb{N}`$ is the per-interval iteration count.</li></ul> |
| **Returned State**             | $`\texttt{MissingAttestedOutputProvided} \{ \text{initialized},\ \text{currentSlot} + 1,\ \text{attestedOutputs}[i] \leftarrowtail \phi_i \}`$ — Updated state reflecting the accepted missing output.                                                                                                      |

</center>

Once a missing attested output has been provided, the next slot may trigger a `tick` event. The system must determine whether it remains within the current interval, moves to the next, or enters the closure phase. The following command captures this logic when starting from the `MissingAttestedOutputProvided` state :

<center>

| `tick`               | $`\Phi.\text{Stream.State} \leftarrow \Phi.\text{tick}(\text{missingAttestedOutputProvidedState})`$ |
|----------------------|---------------------------------------------------------------------------------------------------|
| **Input Parameters** | <ul><li>$`\text{missingAttestedOutputProvidedState} \in \texttt{MissingAttestedOutputProvided}`$ — The current state after an attestation has been successfully provided.</li></ul> |
| **Returned State**   | $`\begin{cases} \text{When } \texttt{isWithinCurrentInterval} &: \texttt{MissingAttestedOutputProvided} \{ \dots,\ \text{currentSlot++} \} \\\\ \text{When } \texttt{isWithinNextInterval} &: \texttt{AwaitingMissingAttestedOutput} \{ \dots,\ \text{currentSlot++} \} \\\\ \text{When } \texttt{isClosable} &: \texttt{AwaitingGracefulClosure} \{ \dots,\ \text{currentSlot++} \} \\\\ \text{When } \texttt{isUngracefullyClosable} &: \texttt{UngracefullyClosed} \{.., {pre-}\eta_e = {initialized.}{pre-}\eta_e \} \} \end{cases}`$ |

</center>

Alternatively, when still waiting for an attestation and no block was produced, a `tick` may trigger a transition based on the current time. This command applies to the `AwaitingMissingAttestedOutput` state before any attestation has been submitted :

<center>

| `tick`               | $`\Phi.\text{Stream.State} \leftarrow \Phi.\text{tick}(\text{awaitingMissingAttestedOutputState})`$ |
|--------------------|---------------------------------------------------------------------------------------------------|
| **Input Parameters** | <ul><li>$`\text{awaitingMissingAttestedOutputState} \in \texttt{AwaitingMissingAttestedOutput}`$ — Current state awaiting an attested output $`\phi_i`$ for interval $`i`$.</li><li>$`\phi_i = (y_i, \pi_i)`$ — Attested output and corresponding proof.</li></ul> |
| **Returned State**   | $` \begin{cases} \text{When } \texttt{isWithinCurrentInterval} :\ \texttt{AwaitingMissingAttestedOutput}\ \{ \text{..} ,\ \text{currentSlot++} \} \\\\ \text{When } \texttt{isClosable} :\ \texttt{AwaitingGracefulClosure}\ \{ \text{..} ,\ \text{currentSlot++} \} \\\\ \text{When } \texttt{isUngracefullyClosable} :\ \texttt{UngracefullyClosed} \{.., {pre-}\eta_e = {initialized.}{pre-}\eta_e \} \}\end{cases}`$ |

</center>

`isUngracefullyClosable` indicates that the end of the lifecycle segment has been reached (i.e., `currentSlot++ == 0`), while some attested outputs are still missing. When this condition holds, the lifecycle is forcefully closed in an ungraceful manner.


#### 3.2.6 ⬛ Closure Phase

We now enter the final phase of the lifecycle, during which all collected outputs are expected to be aggregated and recorded on-chain, and the seed $\eta_e$ derived and committed.

- *Successful Scenarios:**
In these cases, all attested outputs have been provided by the end of the catch-up phase.

- **Best-case scenario:** The closure phase begins at interval 84, giving the system 37 intervals to perform output aggregation and seed commitment under normal operating conditions.
- **Worst-case scenario:** The catch-up mechanism is fully utilized, and the system enters the closure phase at interval 120, the very last interval of the lifecycle segment. Even so, all necessary outputs have been successfully provided.

- *Failure Scenario:**

This occurs when the lifecycle segment reaches its end (i.e., the full $10 \cdot \frac{k}{f}$ slots), and despite the entire duration of the catch-up mechanism (up to interval 120), either some required attested outputs remain missing, or all outputs have been delivered but the final aggregation has not occurred.
This scenario represents an extremely rare event—statistically far beyond 128-bit confidence—and reflects a severe disruption in which no blocks have been produced for over 36 hours. These edge cases are represented in the diagram by the transition `Tick / isUngracefullyClosable`.

##### 3.2.6.1. The States

In this phase, we define two states:

- `AwaitingGracefulClosure`: This state signifies that all 82 attested outputs have been successfully collected. At this point, the outputs are no longer optional—each index is populated with a verified pair $`(y, \pi)`$.

```math
\Phi.\text{Stream.State} \in \texttt{AwaitingGracefulClosure} : \left\{
  \begin{aligned}
    &\text{initialized}     &&\in\ \texttt{Initialized}, \\
    &\text{currentSlot}     &&\in\ \mathbb{N} \bmod \left(10 \cdot \frac{k}{f}\right), \\
    &\text{attestedOutputs} &&\in\ \left[(y, \pi)\right]^{82}
  \end{aligned}
\right\}
```

- `Closed`: This is a final state in the stream lifecycle. It signifies that the aggregated output has been computed and verified, and the final epoch randomness \$`\eta_e`\$ has been successfully derived—achieving the core objective of the protocol. This state is reached in response to either a `Close` command :

```math
\Phi.\text{Stream.State} \in \texttt{Closed} : \left\{
  \begin{aligned}
    &\text{initialized}      &&\in\ \texttt{Initialized}, \\
    &\text{attestedOutputs}  &&\in\ \left[(y, \pi)\right]^{82}, \\
    &\text{aggregatedOutput} &&\in\ (x, y, \pi), \\
    &\eta_e                  &&\in\ \{0,1\}^{256}
  \end{aligned}
\right\}
```

- `UngracefullyClosed`: This is a terminal state in the stream lifecycle. It indicates that either not all expected attested outputs were provided, or the aggregated output could not be computed. As a result, $`{pre-}\eta_e`$ is returned as the final value of $`\eta_e`$. Statistically, this state is highly unlikely to occur, but it is explicitly handled for completeness and structural consistency of the state machine. The transition to this state is triggered by `Tick` in combination with the `isUngracefullyClosable` condition.

```math
\Phi.\text{Stream.State} \in \texttt{UngracefullyClosed} : \left\{
  \begin{aligned}
    &\text{initialized}      &&\in\ \texttt{Initialized}, \\
    &\text{attestedOutputs} &&\in\ \left[\texttt{Maybe}\ (y, \pi)\right]^{82} \\
    &{pre-}\eta_e                  &&\in\ \{0,1\}^{256}
  \end{aligned}
\right\}
```

##### 3.2.6.2. The Successful Scenario: The `Close` Command

At this stage, the system is in the `AwaitingGracefulClosure` state. All necessary data has been collected, and a block can now be produced within the remaining time before the end of the stream lifecycle (as previously discussed, this could occur at the 84th or 120th interval, depending on how smoothly the lifecycle progressed).

In this scenario, the first block producer within the remaining intervals must include the following values in the block body:

- $`(y, \pi)`$: The aggregated output of the $`\Phi`$ computation, representing the final result and its corresponding proof.
- $`\eta_e`$: The final objective of the protocol—a 256-bit epoch randomness beacon, which will be used to seed leader election in the next epoch.

These values complete the stream and trigger the transition to the `Closed` state.

<center>

| `Close`    | $`\Phi.\text{Stream.State} \leftarrow \Phi.\text{Close}((x, y, \pi),\ \text{awaitingGracefulClosureState})`$  |
| -------------------- | ---- |
| **Input Parameters** | <ul><li>$`\text{awaitingGracefulClosureState} \in \texttt{AwaitingGracefulClosure}`$ — State indicating readiness for closure.</li><li>$`(y,\ \pi)`$ — Aggregated output and its proof for the entire stream.</li></ul>                                                                                                    |
| **Property Check**   | <ul><li>Verify the aggregated output with:<br> $`\texttt{VDF.Aggregation.Verify}((\mathbb{G},\ \Delta,\ \cdot),\ \lambda,\ x,\ y,\ \text{attestedOutputs},\ \pi)`$</li><li>Where:<br> $`\lambda`$ is the security parameter, <br> $`x`$ is the aggregated input of the $`\Phi`$ computation<br>$`\text{attestedOutputs} = \text{awaitingGracefulClosureState.attestedOutputs}`$</li></ul> |
| **Epoch Randomness** | $`\eta_e = \text{Hash}^{(256)}(y)`$ — Apply the SHA-256 hash function to $`y`$.  |
| **Returned State**   | $`\texttt{Closed} \{ \text{initialized},\ \text{attestedOutputs},\ (x, y, \pi),\ \eta_e \}`$ — Final state embedding the verified computation and the derived epoch randomness.  |

</center>

##### 3.2.6.3. `tick` Command

The `tick` command handles time progression within the `AwaitingGracefulClosure` state:

- **`isUngracefullyClosable`** is true when the stream reaches the end of its lifecycle segment (i.e., $`\texttt{currentSlot} + 1 \equiv 0 \pmod{T}`$) and some attested outputs are still missing. When this condition holds, the system transitions to the `UngracefullyClosed` state.

- **Otherwise**, the command simply increments the `currentSlot` field to reflect time advancement.

<center>

| `tick`               | $`\Phi.\text{Stream.State} \leftarrow \Phi.\text{tick}(\text{awaitingGracefulClosureState})`$ |
|--------------------|---------------------------------------------------------------------------------------------------|
| **Input Parameters** | <ul><li>$`\text{awaitingGracefulClosureState} \in \texttt{AwaitingMissingAttestedOutput}`$ — State indicating readiness for closure.</li></ul> |
| **Returned State**   | $`\begin{cases} \text{When } \texttt{isUngracefullyClosable} :\ \texttt{UngracefullyClosed} \{.., {pre-}\eta_e = {initialized.}{pre-}\eta_e \} \\\\ \text{Otherwise} :\ \texttt{AwaitingGracefulClosure} \{\, \ldots,\, \texttt{currentSlot} + 1 \,\} \end{cases}`$ |

</center>

`isUngracefullyClosable` indicates that the end of the lifecycle segment has been reached (i.e., `currentSlot++ == 0`), while some attested outputs are still missing. When this condition holds, the lifecycle is forcefully closed in an ungraceful manner. Otherwise we are just update the current slot field

##### 3.2.6.4. The Failure Scenario: Ungraceful Closure

If the protocol reaches the end of its lifecycle, or it becomes evident that too many consecutive blockless intervals have occurred—such that a valid $`\eta_e`$ can no longer be derived—the `Tick` signal triggers the `isUngracefullyClosable` condition. This causes a transition to the terminal state `UngracefullyClosed`, in which the precomputed value $`{pre-}\eta_e`$ is adopted as the official $`\eta_e`$.

While this state is statistically unexpected, it is explicitly defined to ensure completeness and structural consistency within the state machine.

### 4. Recommended Parameter values

There are **two categories of parameters** in the Phalanx protocol that Cardano governance will need to oversee. The first category concerns the **VDF parameters**, which are essential to maintaining the protocol’s cryptographic security. The second concerns the **total time budget** $T_\Phi$ that will be required from SPOs during the **Computation Phase**.

The goal of this section is to provide **recommended values** for these parameters along with the **rationale** behind their selection, so that governance is well-equipped to **adjust them over time** in response to advances in adversarial computational power.

#### 4.1 VDF Security Parameters λ and ρ

The VDF component in Phalanx relies on class group security, which is parameterized by two values:

- $`\lambda`$, the **class group security parameter**, and
- $`\rho`$, the **grinding resistance parameter**, newly introduced in the Phalanx design.

Several combinations of $`\lambda`$ and $`\rho`$ are considered in the literature, depending on the desired level of paranoia or efficiency. Based on the recommendations from the paper [Trustless Unknown-Order Groups]((https://arxiv.org/pdf/2211.16128)), we highlight the following trade-offs:

- A **paranoid** setting, $(\lambda, \rho) = (128, 128)$, requires a **group size of \~3400 bits**.
- A more **realistic yet cautious** setting, $(\lambda, \rho) = (128, 55)$, brings the group size down to **\~1900 bits**, but requires discriminants of at least **3800 bits**.
- Based on those benchmarks and our needs, we opt for $(\lambda, \rho) = (128, 64)$, which corresponds to:

- a **discriminant size of 4096 bits**,
- and a **group size of 2048 bits**.

This strikes a balance between long-term security and practical efficiency:

- On one hand, **breaking the class group** is considered harder than **finding a collision in a 256-bit hash** (our minimum security baseline).
- On the other hand, by following the paper’s recommendation and selecting a slightly lower $`\rho = 64`$, we can **reduce the size of on-chain group elements** while maintaining sufficient resistance against grinding.

Since Phalanx is designed to operate with a **single class group instance “for the lifetime of the protocol”** (reparametrization would require explicit governance intervention), this configuration $(\lambda, \rho) = (128, 64)$ ensures protocol simplicity, consistency, and operational predictability.

#### 4.2 Time Budget Tᵩ and Derived T

In terms of efficiency, the section [**1. How Phalanx Addresses CPS-21 – Ouroboros Randomness Manipulation**](#1-how-phalanx-addresses-cps-21---ouroboros-randomness-manipulation) in the *Rationale* part of this document illustrates, through three scenarios $`\text{Phalanx}_{1/10}`$, $`\text{Phalanx}_{1/100}`$, and $`\text{Phalanx}_{\text{max}}`$, how different time budgets (2 hours, 12 hours, and 5 days, respectively) improve the protocol’s security guarantees against grinding attacks.

In terms of computational load, we recommend setting the time budget at a **midpoint between minimal and maximal protocol capacity**, corresponding to approximately **12 hours** of execution on typical, CPU-based hardware as recommended for SPOs. However, this choice should ultimately be guided by **settlement time performance requirements** across the ecosystem, including the needs of **partner chains and other dependent components**.

##### 4.2.1 Specialized ASIC vs CPU-Based Chips

We need to account for the possibility that adversaries may equip themselves with specialized ASIC chips optimized for computing Wesolowski’s Verifiable Delay Function (VDF). The Chia team has already developed and deployed such **ASIC Timelords**, which demonstrate between **3× and 5× performance improvements** compared to standard CPU-based implementations. These ASICs reach up to **1,000,000 iterations per second**, while commodity CPU Timelords typically max out around **260,000 IPS** ([Chia Network, Oct 2023](https://www.chia.net/2023/10/26/were-going-to-ludicrous-speed)).

To mitigate this performance asymmetry, our initial strategy is to require a **12-hour equivalent workload on standard CPU hardware** (as in $`\text{Phalanx}_{1/10}`$), which is calibrated to provide the **same security guarantees** as a less aggressive configuration like $`\text{Phalanx}_{1/100}`$. This gives us a conservative baseline for security, assuming an adversary might leverage ASIC acceleration.

Critically, scaling this kind of grinding capability is expensive. For an adversary to mount an effective grinding attack, they would need to build and operate a farm of VDF-optimized ASICs — a non-trivial financial and operational challenge. Chia’s rollout of these units has been tightly controlled and aimed at ensuring global distribution, not centralization ([Chia Global Expansion Update, June 2024](https://www.chia.net/2024/06/10/global-expansion-of-chia-network-security-with-successful-asic-timelord-rollout)).

- *Mid-term**, we propose encouraging — or potentially requiring — stake pool operators (SPOs) to adopt VDF ASIC hardware. This would ensure that honest participants remain competitive and are not systematically outperformed by well-resourced adversaries.

- *Long-term**, our strategy involves standardizing the use of verifiable delay ASICs across the network, or alternatively, establishing a dynamic calibration process. This would allow iteration requirements to be periodically adjusted based on the evolving performance gap between commodity and specialized hardware, maintaining consistent and predictable security guarantees.

In summary, while ASIC-equipped adversaries could, in theory, gain a computational advantage during the grinding window, the cost and scale required to pose a real threat remains high. Our mitigation strategy is to raise the honest baseline to neutralize this advantage and prepare for possible hardware evolution over time.

##### 4.2.1 Deriving from Tᵩ to T

We recommend a **12-hour computation budget** on standard **CPU-based machines**, which we estimate to be **10× slower** than specialized ASICs available to adversaries. This configuration corresponds to **Phalanx<sub>1/10</sub>** in terms of **time budget**, while achieving **Phalanx<sub>1/100</sub>** in terms of **security guarantees** against grinding attacks.

However, this **time budget** ($T\_\Phi$) is a high-level abstraction. To implement it concretely, we must derive the **number of VDF iterations** ($T$) required for each **first block of an interval**. Assuming 82 intervals per epoch, this translates to:

```math
T = \frac{T_\Phi}{82} = \frac{12 \text{ hours} \times 3600 \text{ seconds/hour}}{82} \approx 527 \text{ seconds}
```

So, we ask for approximately **10 minutes of VDF computation** per published interval block on standard CPU-based hardware.

To translate this into a concrete number of **VDF iterations** ($T$), we rely on performance benchmarks from the implementation available in the repository [rrtoledo/chiavdf](https://github.com/rrtoledo/chiavdf). This library is a **highly optimized and production-hardened** implementation of Wesolowski's VDF, currently in use by the **Chia blockchain**. We have made minor, superficial modifications to this codebase solely to facilitate benchmarking and increase the discriminant size.

Thanks to its well-established performance profile, this implementation provides a dependable baseline for estimating how many iterations can be executed within a fixed time frame on standard CPU hardware. Under our test environment—an Ubuntu machine equipped with an **Intel® Core™ i9-14900HX (32 cores, 64 GiB RAM)**—we observed approximately **27 million iterations** in a 10-minute window.

- *Note** : We recommend that this benchmark be re-run on a **dedicated, representative SPO machine** to calibrate a more accurate production value for $T$.

### 5. Efficiency analysis

#### 5.1 Block Publication

To publish a block, a node must:

- Perform $T$ squarings to compute the output,
- Execute $O(T / \log(T))$ operations for the proof generation.

We now show benchmarks for evaluating and proving together VDFs, as well as individually, for different discriminant sizes done on a Ubuntu computer with Intel® Core™ i9-14900HX with 32 cores and 64.0 GiB RAM.  For a 4,096 bit long discriminant, we perform around 45,000 iterations per second, and so evaluate and prove a VDF in 22.6s.

<center>

|   Size Discriminant |   #iterations |      IPS |   Evaluate and Prove  (s) |   σ proving | Eval (s)       |   σ proving | Prove (s)      |   σ proving |
|-------------------- | ------------- | -------- | ------------------------- | ----------- | -------------- | ----------- | -------------- | ----------- |
|                 256 |        10,000 | 637252   |                    0.0187 |    0.000676 | 1.57E-02 (84%) |    0.000563 | 8.00E-03 (43%) |    0.000461 |
|                 256 |       100,000 | 641349   |                    0.172  |    0.00115  | 1.56E-01 (91%) |    0.00188  | 5.68E-02 (33%) |    0.00165  |
|                 256 |     1,000,000 | 627336   |                    1.72   |    0.0215   | 1.59E+00 (93%) |    0.0197   | 4.88E-01 (29%) |    0.00633  |
|                 512 |        10,000 | 367449   |                    0.0322 |    0.000635 | 2.72E-02 (85%) |    0.000648 | 1.31E-02 (41%) |    0.000204 |
|                 512 |       100,000 | 378942   |                    0.289  |    0.0029   | 2.64E-01 (92%) |    0.00283  | 8.76E-02 (31%) |    0.000893 |
|                 512 |     1,000,000 | 378204   |                    2.83   |    0.0287   | 2.64E+00 (94%) |    0.0279   | 7.29E-01 (26%) |    0.00873  |
|                1024 |        10,000 | 206186   |                    0.0537 |    0.000902 | 4.85E-02 (91%) |    0.00076  | 2.00E-02 (38%) |    0.000364 |
|                1024 |       100,000 | 211921   |                    0.503  |    0.00722  | 4.72E-01 (94%) |    0.00721  | 1.45E-01 (29%) |    0.00198  |
|                1024 |     1,000,000 | 213319   |                    4.92   |    0.0506   | 4.69E+00 (96%) |    0.0475   | 1.20E+00 (25%) |    0.0136   |
|                2048 |        10,000 | 103135   |                    0.105  |    0.000285 | 9.70E-02 (92%) |    0.000303 | 3.77E-02 (36%) |    0.000122 |
|                2048 |       100,000 | 105315   |                    1.01   |    0.0165   | 9.50E-01 (94%) |    0.0123   | 2.78E-01 (28%) |    0.00516  |
|                2048 |     1,000,000 | 107038   |                    9.75   |    0.0746   | 9.34E+00 (96%) |    0.0828   | 2.20E+00 (23%) |    0.0209   |
|                4096 |        10,000 |  44567.8 |                    0.244  |    0.00463  | 2.24E-01 (92%) |    0.00454  | 8.30E-02 (35%) |    0.00168  |
|                4096 |       100,000 |  45848.6 |                    2.31   |    0.0253   | 2.18E+00 (95%) |    0.0229   | 6.00E-01 (26%) |    0.0089   |
|                4096 |     1,000,000 |  46293.2 |                   22.6    |    0.16     | 2.16E+01 (96%) |    0.148    | 4.79E+00 (22%) |    0.0422   |

</center>

#### 5.2 Block Verification

##### 5.2.1 When Not Syncing

To verify a VDF proof, a node performs:

- 2 hashes,
- 4 small exponentiations,
- 3 group multiplications.

Over an epoch with $N$ intervals, this results in:
- $2 \cdot N$ hashes,
- $4 \cdot N$ small exponentiations,
- $3 \cdot N$ group multiplications.


We now show verification benchmarks for discriminants of different sizes done on the same machine as before. For a 4,096 bit long discriminant, the verification takes around 15ms.

<center>

|   Size Discriminant |   #iterations | Verification (ms)   |   σ verification |
|-------------------- | ------------- | ------------------- | ---------------- |
|                 256 |        10,000 | 1.74E+00 (10%)      |           0.0786 |
|                 256 |       100,000 | 1.12E+00 (1%)       |           0.0471 |
|                 256 |     1,000,000 | 1.75E+00 (1%)       |           0.151  |
|                 512 |        10,000 | 3.25E+00 (11%)      |           0.0562 |
|                 512 |       100,000 | 1.89E+00 (1%)       |           0.0268 |
|                 512 |     1,000,000 | 2.11E+00 (1%)       |           0.134  |
|                1024 |        10,000 | 3.66E+00 (7%)       |           0.117  |
|                1024 |       100,000 | 3.21E+00 (1%)       |           0.0467 |
|                1024 |     1,000,000 | 3.20E+00 (1%)       |           0.135  |
|                2048 |        10,000 | 7.00E+00 (7%)       |           0.0409 |
|                2048 |       100,000 | 9.33E+00 (1%)       |           0.147  |
|                2048 |     1,000,000 | 6.40E+00 (1%)       |           0.218  |
|                4096 |        10,000 | 1.58E+01 (7%)       |           0.316  |
|                4096 |       100,000 | 1.47E+01 (1%)       |           0.248  |
|                4096 |     1,000,000 | 1.37E+01 (1%)       |           0.303  |

</center>

##### 5.2.2 When Syncing

When synching, the nodes only need to update the accumulators and verify the final aggregation proof. As such, the node perform in total arounf half as less operations than verifying all proofs individually. More particularly, we have:
- $2 \cdot N$ hashes,
- $2 \cdot (N + 1)$ small exponentiations.
- $2 \cdot N + 1$ group multiplications,

Note: The exponentiations involving the $\alpha_i$ values are **half as expensive** as those in the full proof verification.

For a discriminant of 4096 bits, we benchmarks the aggregation functions on the same machine as before. We can see that updating the accumulators in the aggregation indeed takes half time as much as verifying a single VDF proof, and verifying the aggregation is as cheap as a normal VDF proof and that proving the aggregation is more expensive than a VDF output, this is due to the absence of intermediary value found when evaluating the VDF input, but less expensive than evaluating a VDF.

<center>

| Size Discriminant | #iterations | $`\texttt{VDF.Aggregation.Update}`$ (ms)| $`\texttt{VDF.Aggregation.Prove}`$  (s)| $`\texttt{VDF.Aggregation.Verify}`$ (ms)|
| ----------------- | ----------- | --------------------------------------- | -------------------------------------- | --------------------------------------- |
|            $4096$ |       1,000 |                                 8.0E+00 |                                2.3E-03 |                                 1.7E+01 |
|            $4096$ |      10,000 |                                 8.0E+00 |                                3.0E-02 |                                 1.7E+01 |
|            $4096$ |     100,000 |                                 8.0E+00 |                                3.0E+00 |                                 1.7E+01 |
|            $4096$ |   1,000,000 |                                 8.0E+00 |                                3.1E+01 |                                 1.7E+01 |

</center>

### 6. CDDL Schema for the Ledger

To support Phalanx, **one block per interval** (every 3600 slots), across **83 intervals per epoch**, must include **2 group elements**. Each of these elements can be compressed to approximately $3/4 \cdot \log_2(|\Delta|)$ bits. Based on our recommended discriminant size of **4096 bits**:

- **3,104 bits (388 bytes)** per element (the benchmarked library adds 4 bytes to each element),
- **6,208 bits (776 bytes)** per block (2 elements),
- **515,264 bits (64,408 bytes ≈ 64.4 KB)** per epoch (83 blocks).

Phalanx requires a single addition to the block body structure in the ledger: the field `phalanx_challenge`.

```diff
 block =
   [ header
   , transaction_bodies         : [* transaction_body]
   , transaction_witness_sets   : [* transaction_witness_set]
   , auxiliary_data_set         : {* transaction_index => auxiliary_data }
   , invalid_transactions       : [* transaction_index ]
- , ? phalanx_challenge        : vdf_attested_output
   ]
```

The structure `phalanx_challenge` is defined as follows:

```cddl
vdf_attested_output =
  [ output : vdf_size
  , proof  : vdf_size
  ]

vdf_size = [ bytes, bytes .size 388 ]
```

We initially evaluated including the `phalanx_challenge` (i.e., the VDF attested output) in the **block header** (instead as proposed in the **block body**) colocated with the VRF outputs. However, this approach raised concerns due to **header size constraints**.

The current **maximum block header size** in Cardano is **1100 bytes**, although actual usage today averages around **860 bytes**. Since TCP packet limits suggest keeping headers under **1500 bytes** (1,460 without headers), the available headroom is approximately **600 bytes**. The full `vdf_attested_output` in its default form requires:

- **388 bytes** per group element (assuming the lowest acceptable security parameters)
- 2 group elements (output + proof)
- Total: **776 bytes**

This would **exceed the 1500-bytes limit**, risking fragmentation and violating guidance from the Cardano networking team. We could safely decrease the group element size by decreasing the security parameters if we were to generate new class groups at each epoch. Doing so would however render the protocol more complex and potentially weaken the security of the protocol as we may have more chances to generate a weak class group.

## Rationale: How does this CIP achieve its goals?

### 1. How Phalanx Addresses CPS-21 - Ouroboros Randomness Manipulation?

#### 1.1 Problem Overview

[CPS-0021 / Ouroboros Randomness Manipulation](https://github.com/cardano-foundation/CIPs/tree/master/CPS-0021) examines the *Randomness Generation Sub-Protocol* within *Ouroboros Praos* ⚙️, highlighting its vulnerabilities and their implications for *Cardano’s* **security** 🔒. Key insights include:

- **Randomness Vulnerability**: *Ouroboros Praos* employs **VRFs** for randomness generation, but this approach is susceptible to *grinding attacks*, where adversaries manipulate outcomes to influence **leader election**, threatening Cardano’s **fairness** ⚖️ and **integrity**.
- **Attack Likelihood**: Attacks become significantly more feasible when an adversary controls **over 20% of the total stake** (approximately **4.36 billion ADA**, as of March 2025), while smaller stakes (e.g., **5%**) make such attempts highly unlikely over extended periods.
- **Economic Barrier**: Gaining enough stake to execute an attack requires a **substantial investment** 💰—billions of USD for a **20% share**—posing a financial risk, as a successful attack could devalue the asset and undermine network trust.
- **Computational Feasibility**: The feasibility of attacks varies widely based on the computational resources an adversary can deploy, becoming progressively more accessible as stake accumulates:
- Small-scale attacks, costing as little as ~**$56**, are easily achievable with minimal resources, such as a standard computer, making them a low-barrier threat that even individual actors could attempt.
- Large-scale attacks, costing up to ~**$3.1 billion**, require extensive computational infrastructure, such as large data centers with millions of CPUs or ASICs, placing them in a range from feasible for well-funded entities (e.g., corporations or nation-states) to nearly impractical for most adversaries due to the immense resource demands.
- The intensity of these attacks scales with stake: the more stake an adversary holds, the greater their influence over **leader election**, amplifying their ability to manipulate randomness. In a simplistic view, this can be likened to manipulating a $256$-bits nonce—a value $\rho$ ranging from $0$ to $256$— where higher stake progressively grants more control, potentially allowing full manipulation of the nonce at the upper limit.
- The wide cost disparity reflects how the complexity of the attack—such as the scope of the targeted time window and the depth of evaluation—drastically increases resource needs, acting as a natural deterrent for more ambitious manipulations.

To illustrate the **Computational Feasibility**, the graph below (sourced from the **CPD**, Section [**3. The Cost of Grinding: Adversarial Effort and Feasibility**](./CPD/README.md#3-the-cost-of-grinding-adversarial-effort-and-feasibility)) maps attack feasibility across four scenarios—**Ant Glance**, **Ant Patrol**, **Owl Stare**, and **Owl Survey**—based on the nonce value $\rho$ (0 to 256 bits). Each scenario reflects different attack complexities, with feasibility shifting as computational and economic demands grow:

<div align="center">
<img src="./image/grinding_depth_scenarios_cost_with_feasibility_layers_gradient.png" alt="Grinding Depth Scenarios with Feasibility Thresholds"/>
</div>

The table below delineates the **$\rho$ values** at which each scenario transitions across feasibility categories, illustrating the computational and economic thresholds:

<center>

| **Feasibility Category**                  | **🔵 Ant Glance**   | **🟠 Ant Patrol**   | **🟢 Owl Stare**   | **🔴 Owl Survey**   |
|--------------------------------------------|---------------------|---------------------|--------------------|--------------------|
| **🟢 🌱 Trivial for Any Adversary**        | $0 \to 53.6$        | $0 \to 32.9$        | $0 \to 31.6$       | $0 \to 31.1$       |
| **🟡 💰 Feasible with Standard Resources** | $53.6 \to 60$     | $32.9 \to 39.5$     | $31.6 \to 38.3$    | $31.1 \to 37.8$    |
| **🟠 🏭 Large-Scale Infrastructure Required** | $60 \to 69.7$  | $39.5 \to 49.5$     | $38.2 \to 48.2$    | $37.8 \to 47.7$    |
| **🔴 🚫 Borderline Infeasible**            | $69.7 \to 79.4$     | $49.5 \to 59.5$     | $48.2 \to 58.2$    | $47.7 \to 57.7$    |
| **🔴 🚫 Infeasible**                      | $79.4 \to 256$      | $59.5 \to 256$      | $58.2 \to 256$     | $57.7 \to 256$     |

</center>

- *Context**: The scenarios represent increasing attack sophistication (e.g., *Ant Glance* is a quick, low-effort attack; *Owl Survey* is a comprehensive, resource-intensive one). As $\rho$ increases, so does the difficulty, shifting feasibility from trivial (e.g., a lone actor with a laptop) to infeasible (e.g., requiring nation-state-level resources).

These thresholds reveal critical vulnerabilities in Cardano’s current consensus design. **Phalanx** aims to mitigate these risks.  In the following section, we revisit the core computational model, introduce the proposed enhancements, and quantify how they shift the feasibility landscape in favor of security.

#### 1.2 Phalanx Cost Amplification per Grinding Attempt

In **Phalanx**, we introduce an additional parameter and **computational cost**, denoted $T_\Phi$, for each **grinding attempt**. This cost represents the total cumulative effort required to compute $i$ iterations of the $\Phi$ primitive. This additional cost directly impacts the total estimated **time per grinding attempt**, as originally defined in [CPD Section 3.3.4 - Total Estimated Time per Grinding Attempt](https://github.com/cardano-foundation/CIPs/tree/master/CPS-0021/CPD/README.md#334-total-estimated-time-per-grinding-attempt). The baseline grinding time in **Praos** is:

```math
T_{\text{grinding}}^{\text{Praos}} = \frac{\rho}{2} T_{\text{BLAKE2b}} + w_T \cdot ( T_{\mathsf{VRF}} + T_{\text{eligibility}} ) + T_{\text{eval}}
```

With **Phalanx**, the total grinding time per attempt is updated to include $T_\Phi$:

```math
T_{\text{grinding}}^{\text{Phalanx}} = \frac{\rho}{2} T_{\text{BLAKE2b}} + w_T \cdot ( T_{\mathsf{VRF}} + T_{\text{eligibility}} ) + T_{\text{eval}} + T_\Phi
```
Where:
- $T_{\mathsf{VRF}}$ is the **VRF evaluation time**,
- $T_{\text{eligibility}}$ is the **eligibility check time**,
- $T_{\text{BLAKE2b}}$ is the time for the **hashing operation**,
- $w_T$ is the **target window size** (seconds),
- $\rho$ is the **grinding depth**,
- $T_{\text{eval}}$ is the **nonce selection and evaluation time** (**attack-specific**).
- $T_\Phi$ is the additional computational cost of **Phalanx**


The introduction of $T_\Phi$ substantially increases the **computational burden** for adversaries, as they must **recompute** the $\Phi^i$ function for each of the $2^\rho$ possible **nonces** evaluated during a grinding attack. In contrast, for **honest participants**, this computation is **distributed** across the epoch, ensuring it remains **manageable and efficient**.


#### 1.3 Phalanx Cost Amplification per Grinding Attack

Building on the updated **grinding time formula** introduced in the previous section, which incorporates the additional **computational cost** $T_\Phi$, we can now revise the formula for a grinding attack from [CPD Section 3.4.1 - Formula](https://github.com/cardano-foundation/CIPs/tree/master/CPS-0021/CPS/CPD/README.md#341-formula), where we defined a total attack time that must fit within the **grinding opportunity window** $w_O$:

```math
\frac{2^{\rho} \cdot T_{\text{grinding}}^{\text{Phalanx}}}{N_{\text{CPU}}} \leq w_O
```
which leads to the lower bound on computational power ($N_\text{CPU}$) :

```math
N_{\text{CPU}} \geq \left \lceil \frac{2^{\rho} \cdot T_{\text{grinding}}^{\text{Phalanx}}}{w_O} \right \rceil
```

##### 1.3.1 Formula

<div style="font-size:0.8em; font-weight:bold; margin-top:0.5em">

Expanding $T_{\text{grinding}}^{\text{Phalanx}}$

</div>

From **Section 1.1**, the per-attempt grinding time under **Phalanx** is:

```math
T_{\text{grinding}}^{\text{Phalanx}} = \frac{\rho}{2} T_{\text{BLAKE2b}} + w_T \cdot ( T_{\mathsf{VRF}} + T_{\text{eligibility}} ) + T_{\text{eval}} + T_{\Phi}
```

Substituting this into the inequality:

```math
N_{\text{CPU}} \geq \left \lceil \frac{2^{\rho} \cdot \left( \frac{\rho}{2} T_{\text{BLAKE2b}} + w_T \cdot ( T_{\mathsf{VRF}} + T_{\text{eligibility}} ) + T_{\text{eval}} + T_{\Phi} \right)}{w_O} \right \rceil
```

<div style="font-size:0.8em; font-weight:bold; margin-top:0.5em">

Expanding $w_O$ in Terms of $\rho$ and $f$

</div>


The grinding opportunity window is:

```math
\frac{X_A(w)}{f} \leq w_O \leq \frac{w}{f}
```

Assuming worst-case upper bound $w_O = \frac{w}{f}$ and noting $w < 2 \cdot \rho - 1$, we substitute:

```math
N_{\text{CPU}} \geq \left \lceil f \cdot \frac{2^{\rho} \cdot \left( \frac{\rho}{2} T_{\text{BLAKE2b}} + w_T \cdot ( T_{\mathsf{VRF}} + T_{\text{eligibility}} ) + T_{\text{eval}} + T_{\Phi} \right)}{w} \right \rceil
```

Bounding $w < 2 \cdot \rho - 1$:

```math
N_{\text{CPU}} \geq \left \lceil f \cdot \frac{2^{\rho} \cdot \left( \frac{\rho}{2} T_{\text{BLAKE2b}} + w_T \cdot ( T_{\mathsf{VRF}} + T_{\text{eligibility}} ) + T_{\text{eval}} + T_{\Phi} \right)}{2 \cdot \rho - 1} \right \rceil
```

Rewriting:

```math
N_{\text{CPU}} \geq \left \lceil f \cdot 2^{\rho} \cdot \left( \frac{\frac{\rho}{2} T_{\text{BLAKE2b}}}{2 \cdot \rho - 1} + \frac{w_T \cdot ( T_{\mathsf{VRF}} + T_{\text{eligibility}} )}{2 \cdot \rho - 1} + \frac{T_{\text{eval}}}{2 \cdot \rho - 1} + \frac{T_{\Phi}}{2 \cdot \rho - 1} \right) \right \rceil
```

Approximating $2 \cdot \rho - 1 \approx 2 \rho$:

```math
N_{\text{CPU}} > \left \lceil \frac{f}{2 \rho} \cdot 2^{\rho} \cdot \left( \rho T_{\text{BLAKE2b}} + 2 w_T \cdot ( T_{\mathsf{VRF}} + T_{\text{eligibility}} ) + 2 T_{\text{eval}} + 2 T_{\Phi} \right) \right \rceil
```

Simplified:

```math
N_{\text{CPU}} > \left \lceil f \cdot 2^{\rho - 2} \cdot T_{\text{BLAKE2b}} + \frac{f \cdot 2^{\rho}}{2 \rho} \cdot \left( w_T \cdot ( T_{\mathsf{VRF}} + T_{\text{eligibility}} ) + T_{\text{eval}} + T_{\Phi} \right) \right \rceil
```

Or grouped as:

```math
N_{\text{CPU}} > \left \lceil f \cdot 2^{\rho - 2} \cdot T_{\text{BLAKE2b}} + \frac{f}{\rho} \cdot 2^{\rho - 1} \cdot \left( w_T \cdot ( T_{\mathsf{VRF}} + T_{\text{eligibility}} ) + T_{\text{eval}} + T_{\Phi} \right) \right \rceil
```

##### 1.3.2 Estimated Formula Using Mainnet Cardano Parameters

Starting from the final expression at the end of the last section:

```math
N_{\text{CPU}} > \left \lceil f \cdot 2^{\rho-2} \cdot T_{\text{BLAKE2b}} + \frac{f}{\rho} \cdot 2^{\rho-1} \cdot \left( w_T \cdot ( T_{\mathsf{VRF}} + T_{\text{eligibility}} ) + T_{\text{eval}} + T_{\Phi} \right) \right \rceil
```
<div style="font-size:0.8em; font-weight:bold; margin-top:0.5em">
Applying Cardano Mainnet Parameters
</div>

Using Cardano’s mainnet values:

- $T_{\mathsf{VRF}} = 10^{-6}$ seconds (1 microsecond) – Time to evaluate a Verifiable Random Function.
- $T_{\text{BLAKE2b}} = 10^{-8}$ seconds (0.01 microseconds) – Time for a BLAKE2b-256 hash operation.
- $f = \frac{1}{20} = 0.05$ – Active slot coefficient.
- $k = 2160$
- Slot duration = 1 second.

Since the eligibility check is negligible, set \$T\_{\text{eligibility}} \approx 0\$:

Substitute into the expression:

- First term:

  ```math
  f \cdot 2^{\rho-2} \cdot T_{\text{BLAKE2b}} = 0.05 \cdot 2^{\rho-2} \cdot 10^{-8} = 5 \cdot 10^{-10} \cdot 2^{\rho-2}
  ```

- Second term:

  ```math
  \frac{f}{\rho} \cdot 2^{\rho-1} \cdot \left( w_T \cdot 10^{-6} + T_{\text{eval}} + T_{\Phi} \right)
  = \frac{0.05 \cdot 2^{\rho-1}}{\rho} \cdot \left( 10^{-6} w_T + T_{\text{eval}} + T_{\Phi} \right)
  ```

The estimated number of CPUs required is:

```math
N_{\text{CPU}} > \left \lceil
5 \cdot 10^{-10} \cdot 2^{\rho - 2} +
\frac{5 \cdot 10^{-8} \cdot 2^{\rho - 1}}{\rho} \cdot w_T +
\frac{5 \cdot 10^{-2} \cdot 2^{\rho - 1}}{\rho} \cdot T_{\text{eval}} +
\frac{5 \cdot 10^{-2} \cdot 2^{\rho - 1}}{\rho} \cdot T_{\Phi}
\right \rceil
```

##### 1.3.3 Impact of Tᵩ on Canonical Scenarios

Now that we have an updated formula, we can evaluate how **Phalanx** directly affects the cost of grinding attempts when compared to the original CPD scenarios. As previously discussed, the goal is to strike a balance between the effort expected from honest **SPOs** during an epoch and the computational burden imposed on an adversary attempting to evaluate multiple $`\eta_e`$ candidates in preparation for an attack.

To anchor this analysis, we introduce a baseline configuration denoted as $`\text{Phalanx}_\text{1/100}`$: an overhead equal to **1/100 of an epoch**, corresponding to $432{,}000 \div 100 = 4{,}320$ slots. This represents a **modest but meaningful choice** — substantial enough to raise the adversary’s cost significantly, yet conservative enough to avoid overloading honest participants. In contrast, imposing a full-epoch overhead would likely be excessive in practice, potentially destabilizing the protocol or placing undue demands on block producers. We may refer to that upper bound as $`\text{Phalanx}_{\text{max}}`$, and the present section aims to explore and recommend a viable configuration somewhere between this maximum and our conservative baseline.

Since each slot lasts 1 second, the $`\text{Phalanx}_\text{1/100}`$ overhead equates to **4,320 seconds**, or exactly **1 hour and 12 minutes**.

We now revisit the canonical scenarios from [CPD Section 3.5 – Scenarios](https://github.com/input-output-hk/ouroboros-anti-grinding-design/blob/main/CPS/Readme.md#35-scenarios), and extend each one with a **Phalanx-enhanced variant** that incorporates this fixed computational cost: $`T_{\Phi} = 4320 \, \text{seconds}`$. The resulting **$N_{\text{CPU}}$ formulas** are derived by substituting each scenario’s respective values for $`w_T`$ and $`T_{\text{eval}}`$ into the base expression from **Section 1.2.2**, now augmented with the constant Phalanx term $`T_{\Phi}`$.

```math
N_{\text{CPU}} > \left \lceil
5 \cdot 10^{-10} \cdot 2^{\rho - 2} +
\frac{5 \cdot 10^{-8} \cdot 2^{\rho - 1}}{\rho} \cdot w_T +
\frac{5 \cdot 10^{-2} \cdot 2^{\rho - 1}}{\rho} \cdot T_{\text{eval}} +
\frac{216 \cdot 2^{\rho - 1}}{\rho}
\right \rceil \quad \text{(Phalanx}_\text{1/100}\text{)}
```

```math
N_{\text{CPU}} > \left \lceil
5 \cdot 10^{-10} \cdot 2^{\rho - 2} +
\frac{5 \cdot 10^{-8} \cdot 2^{\rho - 1}}{\rho} \cdot w_T +
\frac{5 \cdot 10^{-2} \cdot 2^{\rho - 1}}{\rho} \cdot T_{\text{eval}}
\right \rceil \quad \text{(Praos)}
```

The table below summarizes the expressions for each scenario:

<center>

| **Scenario**            | $\text{Praos}$  | $\text{Phalanx}_\text{1/100}$       |
|------------------------|---------------|--------------------|
| **Ant Glance**         | $5 \cdot 10^{-10} \cdot 2^{\rho-2}$           |  $5 \cdot 10^{-10} \cdot 2^{\rho-2} + 216 \cdot \frac{2^{\rho-1}}{\rho}$|
| **Ant Patrol**         | $5 \cdot 10^{-10} \cdot 2^{\rho-2} + 2.16 \cdot 10^{-2} \cdot \frac{2^{\rho-1}}{\rho}$  |$5 \cdot 10^{-10} \cdot 2^{\rho-2} + 2.16 \cdot 10^{-2} \cdot \frac{2^{\rho-1}}{\rho} + 216 \cdot \frac{2^{\rho-1}}{\rho}$|
| **Owl Stare**          | $5 \cdot 10^{-10} \cdot 2^{\rho-2} + 5 \cdot 10^{-2} \cdot \frac{2^{\rho-1}}{\rho}$  | $5 \cdot 10^{-10} \cdot 2^{\rho-2} + 5 \cdot 10^{-2} \cdot \frac{2^{\rho-1}}{\rho} + 216 \cdot \frac{2^{\rho-1}}{\rho}$ |
| **Owl Survey**         | $5 \cdot 10^{-10} \cdot 2^{\rho-2} + 7.16 \cdot 10^{-2} \cdot \frac{2^{\rho-1}}{\rho}$ | $5 \cdot 10^{-10} \cdot 2^{\rho-2} + 7.16 \cdot 10^{-2} \cdot \frac{2^{\rho-1}}{\rho} + 216 \cdot \frac{2^{\rho-1}}{\rho}$ |

</center>

The **graph below** presents the **logarithmic cost** (in **USD**) of executing **grinding attacks** as a function of the **grinding depth** ($`\rho`$), across both **Praos** and **$\text{Phalanx}_{1/100}$** scenarios.

- **Solid lines** correspond to the original **Praos configurations** (*Ant Glance*, *Ant Patrol*, *Owl Stare*, and *Owl Survey*).
- **Dashed lines** represent the respective **$\text{Phalanx}_\text{1/100}$ variants**, each incorporating a fixed additional computational overhead of $`T_{\Phi} = 4320 \, \text{seconds}`$.
- The **shaded feasibility regions** reflect increasing **economic difficulty levels**, based on thresholds defined in [**CPD Section 3.6 – Grinding Power Computational Feasibility**](https://github.com/input-output-hk/ouroboros-anti-grinding-design/blob/main/CPS/Readme.md#36-grinding-power-computational-feasibility).


<div align="center">
<img src="./image/grinding_depth_scenarios_cost_praos_vs_phalanx.png" alt="Cost of Grinding Attacks: Praos vs Phalanx Scenarios"/>
</div>

✏️ **Note**: The Python script used to generate this graph is available here ➡️ [**scenario\_cost\_praos\_vs\_phalanx.py**](./graph/scenario_cost_praos_vs_phalanx.py)

<div style="font-size:0.8em; font-weight:bold; margin-top:0.5em">
 Interpretation of the Graph
</div>

The graph highlights how the **$\text{Phalanx}_\text{1/100}$ protocol** dramatically increases the **cost of grinding attacks** compared to **Praos**, using a logarithmic scale to represent costs in **USD** as a function of the grinding depth $`\rho`$:

1. **Consistent Cost Increase Across All $\rho$ Values**
  The differences (deltas) between **$\text{Phalanx}_\text{1/100}$** and **Praos** scenarios remain stable across all grinding depths due to the logarithmic scale. This allows us to make generalizable observations regardless of $\rho$.

2. **Moderate Gap Between Scenarios Within $\text{Phalanx}_\text{1/100}$**
  Variations between different **$\text{Phalanx}_\text{1/100}$** scenarios (e.g., Ant Glance vs Owl Survey) are relatively modest. For example:
- At $`\rho = 100`$, the cost difference between **Owl Survey ($\text{Phalanx}_\text{1/100}$)** and **Owl Survey (Praos)** is about **3.5** orders of magnitude in $`\log_{10}(\text{Cost})`$.

3. **Significant Overhead Introduced by $\text{Phalanx}_\text{1/100}$**
  The **computational burden** imposed by Phalanx is substantial.
- At $`\rho = 150`$, the cost delta between **Owl Survey ($\text{Phalanx}_\text{1/100}$)** and **Ant Glance (Praos)** reaches nearly **9.8**, representing a **10⁹.⁸×** increase in expected cost for the attacker.
- This effectively pushes grinding attacks into the **"infeasible" zone** for a wide range of strategies.

4. **Strategic Uniformity Under $\text{Phalanx}_\text{1/100}$**
  All **$\text{Phalanx}_\text{1/100}$** scenario curves tightly cluster together, showing minimal divergence across evaluation complexity ($T_{\text{eval}}$) and observation scope ($w_T$).
- This implies that **Phalanx equalizes grinding costs** across adversarial strategies.
- Practically, this means defenders (e.g., protocol designers) can reason about attack feasibility without considering specific adversarial tactics. One cost curve is sufficient.

We can now **simplify and generalize** the grinding cost formulas for different **Phalanx configurations**, along with their **estimated order-of-magnitude improvements** over Praos:

<center>

| **Configuration**                | **Time Budget** | **Grinding Cost Formula**                               | **Cost Amplification** |
| ------------------------------- | --------------- | ------------------------------------------------------- | -------------------------- |
| $`\text{Phalanx}_{1/100}`$      | 2 hours         | $`\frac{2.16 \cdot 10^{2} \cdot 2^{\rho - 1}}{\rho}`$ | $\boldsymbol{10^{10.2}}$×     |
| $`\text{Phalanx}_{1/10}`$       | 12 hours        | $`\frac{2.16 \cdot 10^{3} \cdot 2^{\rho - 1}}{\rho}`$ | $\boldsymbol{10^{11.2}}$×     |
| $`\text{Phalanx}_{\text{max}}`$ | 5 days          | $`\frac{2.16 \cdot 10^{4} \cdot 2^{\rho - 1}}{\rho}`$ | $\boldsymbol{10^{12.2}}$×     |

</center>


- *N.B.** We can note that even with the use of ASICs, with a speed up of 3x to 10x, Phalanx would still add a significant term and reduce the cost amplification to still acceptable levels.

<div align="center">
<img src="./image/grinding_depth_scenarios_cost_praos_vs_full_phalanx_scenarios.png" alt="Cost of Grinding Attacks: Praos vs Phalanx Scenarios"/>
</div>

✏️ **Note**: The Python script used to generate this graph is available here ➡️ [**scenario\_cost\_praos\_vs\_phalanx-full-scenarios.py**](./graph/scenario_cost_praos_vs_phalanx-full-scenarios.py).

These results confirm that even the **minimal configuration** ($`\text{Phalanx}_{1/100}`$) yields a **$10^{10.6}$-fold increase** in the computational cost of a grinding attack — a formidable barrier for adversaries. More aggressive deployments such as $`\text{Phalanx}_{1/10}`$ and $`\text{Phalanx}_{\text{max}}`$ push this cost further, to $10^{11.6}$ and $10^{12.6}$ times that of Praos, respectively — while still remaining practical for honest participants.


##### 1.3.4 Impact of Tᵩ on Feasibility Categories

This **simplification** allows us to **revisit and improve** the **feasibility category table** presented in the **Problem Overview section** :

<div align="center">
<img src="./image/grinding_depth_scenarios_cost_with_feasibility_layers_gradient-phalanx.png" alt="Cost of Grinding Attacks: Praos vs Phalanx Scenarios"/>
</div>

✏️ **Note**: The **code** to generate this **graph** is available at ➡️ [**this link**](./graph/scenario-cost-cross-thresholds.py).

The **tables below** present first the **original Praos feasibility intervals**, followed by the **adjusted categories under Phalanx** :

<center>

| **Feasibility Category**                      | **🔵 Ant Glance**   | **🟠 Ant Patrol**   | **🟢 Owl Stare**    | **🔴 Owl Survey**   | **$`\text{Phalanx}_{1/100}`$** | **$`\text{Phalanx}_{1/10}`$** | **$`\text{Phalanx}_{max}`$** |
| --------------------------------------------- | ------------------- | ------------------- | ------------------- | ------------------- | ----------------------- | ---------------------- | --------------------- |
| **🟢 🌱 Trivial for Any Adversary**           | $`0 \to 53.6`$    | $`0 \to 32.9`$    | $`0 \to 31.6`$    | $`0 \to 31.1`$    | $`0 \to 19.6`$        | $`0 \to 16.3`$       | $`0 \to 13.0`$      |
| **🟡 💰 Feasible with Standard Resources**    | $`53.6 \to 60.0`$  | $`32.9 \to 39.5`$ | $`31.6 \to 38.3`$ | $`31.1 \to 37.8`$ | $`19.6 \to 26.3`$     | $`16.3 \to 23.0`$    | $`13.0 \to 19.6`$   |
| **🟠 🏭 Large-Scale Infrastructure Required** | $`60.0 \to 69.7`$   | $`39.5 \to 49.5`$ | $`38.3 \to 48.2`$ | $`37.8 \to 47.7`$ | $`26.3 \to 36.2`$     | $`23.0 \to 32.9`$    | $`19.6 \to 29.6`$   |
| **🔴 🚫 Borderline Infeasible**               | $`69.7 \to 79.4`$ | $`49.5 \to 59.5`$ | $`48.2 \to 58.2`$ | $`47.7 \to 57.7`$ | $`36.2 \to 46.2`$     | $`32.9 \to 42.9`$    | $`29.6 \to 39.5`$   |
| **🔴 🚫 Infeasible**                          | $`79.4 \to 256`$  | $`59.5 \to 256`$  | $`58.2 \to 256`$  | $`57.7 \to 256`$  | $`46.2 \to 256`$      | $`42.9 \to 256`$     | $`39.5 \to 256`$    |

</center>

The **Phalanx tables** include **delta improvements** for each **Praos scenario**. A **positive** $\Delta$ implies that **Phalanx forces infeasibility earlier**, i.e., at a lower $`\rho`$ value, thereby **increasing adversarial cost** :

<center>

| **Scenario**      | $`\Delta \text{Phalanx}_{1/100}`$ | $`\Delta \text{Phalanx}_{1/10}`$ | $`\Delta \text{Phalanx}_{max}`$ |
| ----------------- | ------------------------- | ------------------------ | ------------------------------ |
| **🔵 Ant Glance** | $`+34.0`$               | $`+36.5`$              | $`+39.9`$                    |
| **🟠 Ant Patrol** | $`+13.3`$               | $`+16.6`$              | $`+20.0`$                    |
| **🟢 Owl Stare**  | $`+12.0`$               | $`+15.3`$              | $`+18.7`$                    |
| **🔴 Owl Survey** | $`+11.5`$               | $`+14.8`$              | $`+18.2`$                    |

</center>

#### 1.4 Conclusion: How Much Risk is Mitigated?

To quantify the **security improvement**, we compute the **percentage reduction in the “Trivial for Any Adversary” interval** compared to Praos. This represents the portion of grinding attacks that are now **pushed into more difficult feasibility regions**.

<center>

| **Scenario**      | **Praos Trivial** | $`\Delta \text{Phalanx}_{1/100}`$ | **% Reduction** |$`\Delta \text{Phalanx}_{1/10}`$ | **% Reduction** | $`\Delta \text{Phalanx}_{max}`$ | **% Reduction** |
| ----------------- | ----------------- | --------------------------- | --------------- | -------------------------- | --------------- | ------------------------- | --------------- |
| 🔵 **Ant Glance** | 53.6              | 19.6                        | **−63.4%**      | 16.3                       | **−69.6%**      | 13.0                      | **−75.7%**      |
| 🟠 **Ant Patrol** | 32.9              | 19.6                        | **−40.4%**      | 16.3                       | **−50.5%**      | 13.0                      | **−60.5%**      |
| 🟢 **Owl Stare**  | 31.6              | 19.6                        | **−38.0%**      | 16.3                       | **−48.4%**      | 13.0                      | **−58.9%**      |
| 🔴 **Owl Survey** | 31.1              | 19.6                        | **−37.0%**      | 16.3                       | **−47.6%**      | 13.0                      | **−58.2%**      |

</center>

These results show that **Phalanx makes low-effort grinding substantially harder**, reducing adversarial opportunity for trivial manipulation by up to **76%** in the most favorable configuration, and by **at least 37%** across all attack types and parameterizations.

This concludes our **high-level assessment of feasibility mitigation** in security terms. In the next section, **“2. How Phalanx Improves CPS-17 – Settlement Speed?”**, we will examine how this risk reduction translates into a much more **tangible and practical benefit**: **faster and more reliable settlement times in Ouroboros**.

### 2. How Phalanx Improves CPS-17 - Settlement Speed?

Let us recall that, like **Bitcoin**, **Cardano** relies on **probabilistic** and **unbiased randomness** for **leader election**. As a result, both systems inherently provide **statistical consensus guarantees**. For **Stake Pool Operators (SPOs)**, being elected as a **slot leader** grants some **control** over the protocol. This control increases with **stake**—more skin in the game means more chances to be selected. However, due to the **randomized** nature of the leader election, SPOs cannot predict or influence exactly *when* they will be selected.

This makes **undesirable events**—such as **regional concentrations** of slot leadership deviating from the expected distribution, or **control over multiple consecutive blocks**—**statistically quantifiable** and, in the absence of **grinding attacks**, **extremely unlikely**. These include risks like **rollbacks**, **$k$-common prefix violations**, or **private chain attacks**. This is precisely the **security model** Ouroboros **Praos** was designed around—and so far, it has held up well.

However, if **adversaries** manage to control more than **20% of the stake**, they gain **significant** and *exponentially growing* **grinding power**. This power allows them to **bend** the **statistical distribution** of events in their favor. For example, achieving a **grinding depth** of **79.4** means they can select from among **$2^{79.4}$ (~ $10^{24}$)** possible distributions to **optimize** the **timing** and **nature** of their attacks. At that scale, they can deliberately **amplify** the probability of "**bad events**" and execute a variety of **targeted attacks** against the protocol.

In this section, we narrow our focus to a specific class of such bad events: those that **bias or delay the confirmation time of transactions on Cardano**. We’ll show how this issue is **directly tied to adversarial grinding power**, and how reducing that power leads to **faster and more reliable settlement guarantees**, thereby directly addressing  [CPS-0017 / Settlement Speed](https://github.com/cardano-foundation/CIPs/tree/master/CPS-0017).

#### 2.1 Settlement times without grinding attacks

In longest-chain consensus protocols like Ouroboros Praos, settlement time refers to the duration required for a transaction to be considered irreversibly included in the blockchain with high probability. Without grinding attacks, this is primarily determined by the risk of chain reorganizations (e.g., forks or common prefix violations), where an adversary might create a competing chain that overtakes the honest one. The goal is to compute the minimum number of confirmations (blocks appended after the transaction's block) needed to achieve a target security level, such as a failure probability below $2^{-30}$ or $2^{-60}$.

The methodology for computing settlement bounds, as detailed in the paper ["Practical Settlement Bounds for Longest-Chain Consensus" (Gaži et al., 2022)](https://eprint.iacr.org/2022/1571.pdf), uses a phase-based analytical model that divides time into intervals separated by the maximum network delay $\Delta$ (e.g., 2-5 seconds for Cardano). It tracks metrics like "margin" (for PoW) or "reach" and "margin" (for PoS) to bound the probability of an adversary overtaking the honest chain.

To obtain metrics, you can run the software from [https://github.com/renling/LCanalysis/](https://github.com/renling/LCanalysis/), which implements these evaluations in MATLAB. Clone the repo, open `PoSRandomWalk.m` (for PoS like Cardano), set parameters (e.g., honest ratio $\beta = 0.7$ (30% of stake adversary), network delay $\Delta = 5s$), and run to output failure probabilities vs. confirmations. Below is a representative graph:

![alt text](./image/settlement-times-30-2s.png)

✏️ **Note**: The Python script used to generate this graph is available here ➡️ [**settlement-time-without-grinding.py**](./graph/settlement-time-without-grinding.py).

A **60-bit confidence level** (failure probability ≤ **$2^{-60}$**) is a common threshold where events are considered negligible in practice. In the graph above, for example, this corresponds to a confirmation interval of **[438, 527]** blocks. Beyond this, the choice of confidence level and thus the number of confirmations required for transaction settlement, becomes **application-specific**, balancing security against efficiency.

#### 2.2 How Grinding Power Affects Settlement Times

What does it mean for settlement times when, in a scenario like the **Owl Survey Glance**, the adversary can observe a large portion of the candidate randomness distribution and perform an attack with a grinding power of $2^{57.7}$?

A grinding power of $2^{57.7}$ / a grinding depth of **57.7 bits**, implies that:

- The adversary can simulate approximately $2^{57.7}$ distinct randomness outcomes, derive the associated leader schedules for the next epoch, and select the most favorable one.
- This drastically increases the likelihood of **bad events** (e.g., settlement failures) compared to a protocol with unbiased randomness.
- More precisely, if a bad event occurs with probability $\varepsilon$ under honest randomness, then an adversary capable of evaluating $R$ different randomness candidates can amplify this probability up to $R \cdot \varepsilon$ (by the union bound).

In practical terms, such grinding power **extends the number of confirmations required** to reach a desired security level. The stronger the adversary’s grinding capability, the longer it takes for a transaction to be considered truly settled.

- *For example**, assume that on mainnet, a rollback probability of $2^{-60}$ is achieved for a block at index $x$ after $y$ subsequent blocks are appended. If an adversary possesses grinding power of $2^{57.7}$, the effective risk increases to:

```math
2^{-60} \cdot 2^{57.7} = 2^{-2.3}
```

To preserve the original $2^{-60}$ confidence level *under adversarial grinding*, the protocol must instead target a baseline security of:

```math
2^{-(60 + 57.7)} = 2^{-117.7}
```

This implies that **many more blocks must be appended** before a block is considered settled, thereby **significantly increasing settlement times**.

In the example above, we used the **Owl Survey Glance** scenario, which is the most computationally expensive in terms of grinding *cost*. However, when establishing a protocol-wide security threshold, it is more prudent to anchor on the **worst-case grinding *power*** — that is, the scenario with the **highest grinding depth**. In our analysis, this is the **Ant Glance** scenario, with a grinding depth of **79.4 bits**. To maintain the original $2^{-60}$ confidence level under such an adversary, the protocol must instead target:

```math
2^{-(60 + 79.4)} = 2^{-139.4}
```

This defines a **stricter baseline** and ensures security even against the most favorable conditions for the adversary. In our previous scenario (30% adversary and 5-second network delay), the effect can be visualized as follows:

![alt text](./image/settlement-times-30-2s-praos.png)

✏️ **Note**: The Python script used to generate this graph is available here ➡️ [**settlement-time-praos.py**](./graph/settlement-time-praos.py).

Where the confirmation interval was **\[438, 527]** blocks in the absence of a grinding attack, it increases to **\[864, 1037]** blocks under grinding power $2^{57.7}$ in the **Owl Survey** scenario, and further to **\[1024, 1229]** blocks under the same grinding power in the **Ant Glance** scenario.

Assuming a block is produced every 20 seconds, this extends the required confirmation window from approximately **\[2.43, 2.93] hours** to **\[4.80, 5.76] hours** in the Owl Survey case, and up to **\[5.69, 6.83] hours** in the Ant Glance case — more than doubling the settlement time.

As discussed in [**Section 1: How Phalanx Addresses CPS-21 – Ouroboros Randomness Manipulation**](#1-how-phalanx-addresses-cps-21--ouroboros-randomness-manipulation), this is a key challenge in Praos: the presence of multiple attack scenarios with varying grinding power makes it difficult to define a single, consistent security threshold for settlement — a complexity that **Phalanx simplifies** by unifying the treatment of adversarial power across scenarios.

#### 2.3 How Phalanx improves compared to Praos ?

In the conclusion of [**Section 1.4: How Much Risk Is Mitigated?**](#14-conclusion-how-much-risk-is-mitigated), we quantified Phalanx's improvement over Praos in terms of **grinding depth reduction** as follows:

<center>

| **Scenario**      | $`\Delta \text{Phalanx}_{1/100}`$ | $`\Delta \text{Phalanx}_{1/10}`$ | $`\Delta \text{Phalanx}_{\text{max}}`$ |
| ----------------- | ----------------------------------- | ---------------------------------- | ---------------------------------------- |
| **🔵 Ant Glance** | $`+34.0`$                         | $`+36.5`$                        | $`+39.9`$                              |

</center>

In our previous examples, we are given that under **Praos**, the Ant Glance scenario results in a required security level of $`2^{-139.4}`$, which translate into the following threshold for the Phalanx configurations :

<center>

| **Configuration**        | **Computation**       | **Resulting Security Level** |
| ------------------------ | --------------------- | ---------------------------- |
| $`\text{Phalanx}_{1/100}`$      | $2^{-139.4 + 34.0}$ | $2^{-105.4}$    |
| $`\text{Phalanx}_{1/10}`$       | $2^{-139.4 + 36.5}$ | $2^{-102.9}$    |
| $`\text{Phalanx}_{\text{max}}`$ | $2^{-139.4 + 39.9}$ | $2^{-99.5}$     |

</center>

This can be visualized as follows:

![alt text](./image/settlement-times-30-2s-phalanx.png)

✏️ **Note**: The Python script used to generate this graph is available here ➡️ [**settlement-time-phalanx.py**](./graph/settlement-time-phalanx.py).

In the absence of a grinding attack, the confirmation interval was **\[438, 527]** blocks. Under the actual version of **Praos**, accounting for a grinding depth of 79.4 bits in the **Ant Glance** scenario, this interval increases to **\[1024, 1229]** blocks.

However, with Phalanx applied, the required confirmation windows are **significantly reduced**:

<center>

| **Configuration**                 | **Confirmation Interval** | **Duration @ 20s/block** |
| --------------------------------- | ------------------------- | ------------------------ |
| $`\text{Phalanx}_{1/100}`$      | \[773, 928]               | \~4.29 h → \~5.16 h      |
| $`\text{Phalanx}_{1/10}`$       | \[754, 906]               | \~4.19 h → \~5.03 h      |
| $`\text{Phalanx}_{\text{max}}`$ | \[729, 876]               | \~4.05 h → \~4.87 h      |

</center>

Compared to Praos' ~5.69 h → ~6.83 h (from blocks 1024 to 1229), these configurations reduce settlement time by approximately 20–30% while maintaining equivalent security.

#### 2.4 Advocating for Peras: Phalanx as a Complementary Layer

- *[Ouroboros Peras](https://peras.cardano-scaling.org/)** is a recent protocol extension designed to accelerate settlement in Cardano by introducing **stake-weighted voting and certified blocks**. Built as a lightweight augmentation of Praos, it enables rapid finality—often within **1 to 2 minutes**—by allowing randomly selected committees to vote on blocks and issue certificates that elevate their importance in the chain selection rule ([Peras Intro](https://peras.cardano-scaling.org/docs/intro/)). Critically, Peras maintains full compatibility with Praos' security guarantees, reverting gracefully when quorum is not reached ([Peras FAQ](https://peras.cardano-scaling.org/docs/faq/)). Rather than replacing Praos, it overlays an additional mechanism for **fast, probabilistically final settlement**, offering a much-needed middle ground between immediate confirmation and the traditional **2160-block** security window.

While Peras dramatically reduces settlement times compared to both Praos and Phalanx, it does so through a **certification mechanism** that depends on the timely participation of randomly selected committees. In practice, this mechanism offers fast settlement when quorum is achieved—but when participation conditions are not met (e.g., insufficient online stake or network asynchrony), **Peras gracefully falls back to standard Praos behavior** ([Peras Technical Report](https://peras.cardano-scaling.org/docs/reports/tech-report-2/)). This fallback mode retains the full settlement guarantees we've detailed in this CIP and in the accompanying [CPS-18: Ouroboros Randomness Manipulation](https://github.com/cardano-foundation/CIPs/pull/0000) and [CIP-Phalanx](https://github.com/input-output-hk/ouroboros-anti-grinding-design). In such scenarios, settlement times revert to those defined under grinding-aware Praos parameters—precisely where Phalanx becomes relevant as a **complementary defense layer**, ensuring that even in fallback conditions, the chain benefits from **stronger security guarantees** and **significantly improved settlement times** compared to unmodified Praos.

Finally, a point of critical importance that we emphasized in the [CPS-21: Ouroboros Randomness Generation Sub-Protocol – The Coin-Flipping Problem](https://github.com/cardano-foundation/CIPs/tree/master/CPS-0021): **today, the protocol remains effectively blind to grinding attacks**. Within the *window of opportunity* defined in the CPD, detecting randomness manipulation is inherently difficult—both in terms of real-time monitoring and retrospective analysis. However, [Ouroboros Peras](https://peras.cardano-scaling.org/docs/intro/) introduces a meaningful defense: by reducing settlement times to **1–2 minutes** ([Peras FAQ](https://peras.cardano-scaling.org/docs/faq/)), it can **close the grinding window entirely**, provided it does **not fall back to Praos mode** during this period. If such a fallback *does* occur within the grinding window, it becomes a suspicion that a grinding attempt may be underway. In this case, examining which SPOs abstained from voting during the certification phase could provide actionable insights to **identify adversarial behavior**. In this light, Peras is not only a mechanism for faster settlement—it also contributes to **resilience against randomness manipulation**.

We therefore **strongly recommend deploying both mechanisms in tandem**:

- [**Peras**](https://peras.cardano-scaling.org/) for rapid probabilistic finality and real-time detection,
- [**Phalanx**](./Readme.md) as a fallback that offers **quantifiable grinding resistance** even in worst-case epochs.

- *Note:** If Peras committee selection ultimately relies on randomness from the standard beacon in its production version, it too inherits vulnerability to grinding — especially when a **Praos epoch precedes a Peras epoch**. In such cases, **Phalanx mitigates the grinding risk at the source**, reducing the manipulation surface of the beacon and thereby **indirectly strengthening Peras**. This highlights the **complementarity** of the two systems: **each reinforces the other**.

### 3. Why VDFs Were Chosen over other Cryptographic Primitives ?

As shown previously in the CPS and CPD, Cardano’s randomness generation currently is biasable and this CIP aims at presenting solutions on top of the current Praos’ randomness generation algorithm to disincentivize adversaries from performing grinding attacks by increasing their computational cost. We do not intend to change the protocol in depth, as this would need a much greater initiative that may not bear fruits, but add an additional layer of security on top of the current protocol only.

To argue about our decision, i.e. increasing the attack cost, we first list different ways to fix the last revealer attack as suggested in [1](https://eprint.iacr.org/2015/1249.pdf) that present a similar issue when combining different sources of randomness.
- _Simultaneous lottery draws, so that all random nonces are revealed at once._ Unfortunately this solution is not possible in our context as nonces are revealed iteratively in block headers so that they can be easily extractable and verifiable from the blockchain directly.
- _Using a slow function to generate the randomness on top of the revealed nonces, so that the adversary cannot decide in time whether to reveal their nonces or not._ In practice, time assumptions are delicate in cryptography for theoretical reasons (potential attacks, better algorithms) and practical ones (Moore’s law).
- _Using a commitment, so that the revealed nonces are combined to some previously committed value._ This solution is not feasible as we would either need to rely on trusted parties, which is contrary to blockchain’s operandi, or to reveal the committed values, which is equivalent to RANDAO.
- _Limiting the entropy of the last lottery draws, by combining it with sufficiently many low entropy - a single bit- randomness._ This solution is impractical as we would still have a revealer attack, but on the lone bits.

As such, we should focus from now on using a weakened slow function, that is instead of solely relying on time based guarantees, we will principally count on computational costs: we will append to our existing protocol a computationally costly chain of computation that the adversary will have to process for each grinding attempt.

#### 3.1 Requirements

When choosing a cryptographic primitive, we need to balance several criteria. In particular, checking its _security strength and maturity_, _performance_, _deployability_ and _compliance_:
- _Security strength & Maturity_:  the primitive is resistant to known attacks and comprise a sufficient security margin. Furthermore, it has been extensively reviewed by the cryptographic community, has been developed transparently and has been accepted and standardized.
- _Performance_: the primitive is efficient in terms of size (input, output and if applicable proof size), and computation (CPU cycles, memory footprint, and power consumption) with respect to the application and intended platform.
- _Deployability_: the primitive should be easy to set up, upgrade and, in case of attacks and if possible, switch
- _Compliance_: the primitive should be free of licensing restrictions and meet regulatory standards.

We furthermore require the following properties for the Phalanx project. The cryptographic primitive must be an **_NP deterministic function_**. More precisely, a primitive whose verification time is fast, that for each input corresponds to a unique output and whose latter is fixed.

We can either support a primitive which computation can be split in different iterations, each of which is verifiable, or which is finely tunable so that we can solve a challenge in less than a block time and can be used in cascade. Being able to generate and verify a single proof for the whole chain of computation would be another advantage in the context of syncing.

#### 3.2 Primitive selection

To ensure fast verification, we face a first choice: relying on a cryptographic primitive based on trapdoor assumptions, which present NP problems and by definition have fast verification, or combine a primitive without fast verification with an efficient proof system such as a Succinct Non-interactive ARgument of Knowledge (SNARK).

##### 3.2.1 RSA solutions

An RSA group is the multiplicative group of integers modulo $N$, where $N$ is the product of two large prime numbers $p$ and $q$, $N = p \cdot q$. This group is called RSA after the RSA cryptosystem by Rivest, Shamir and Adleman where the public encryption key is the group modulus $N$ and a small exponent $e$, while the corresponding  decryption key is the number $d$ such that $d \cdot e \equiv 1\ (\text{mod }\Phi(N))$ where $\Phi(N) = (p−1)(q−1)$, where $p$ and $q$ remain private. To break the RSA cryptosystem, the adversary has to factorize $N$ into its prime $p$ and $q$ which can be done most efficiently with the General Number Field Sieve algorithm, based on the NFS [[2]](https://dl.acm.org/doi/pdf/10.1145/100216.100295), in sub-exponential time. To reach 128 bit of security, the modulus must be at least 2,048 bit long, and preferably at least 3,072 bit long, according to NIST [[3]](https://csrc.nist.gov/pubs/sp/800/78/5/final).

###### 3.2.1.1 Designs

Three problems defined on RSA groups satisfy the requirements: solving the RSA problem or the integer factorization, or using verifiable delayed functions (VDFs, [[6]](https://eprint.iacr.org/2018/601.pdf)).
RSA problem. The setup consists in generating an RSA public key $(N,\ e)$ where $N$’s factorization is unknown and a ciphertext c. The challengers then have to find the plaintext corresponding to that ciphertext, that is finding the eth root the ciphertext modulo N, i.e. finding $m$ such that $c \equiv m \cdot e (\text{mod } N)$. The verification is straightforward, re-encrypting the plaintext and checking it equals the ciphertext.
The most efficient method to solve this problem is by first factoring the modulus $N$, which cannot be done in polynomial time without a quantum computer (in which case we would use Shor’s algorithm). The best published algorithm to solve this problem with classical computers is the general number field sieve (GNFS), that is sub-exponential in time.
Integer factorization. This is a simpler case to the RSA problem: only the group modulus is given and needs to be factorized, by the same algorithm.
VDF. Similarly to the other problems, we first start by generating an unknown order group of modulus $N$ but also sample a random group element $g$. The challenge then consists in raising this element to a big exponent of the form $2^T$ where $T$ is set depending on the difficulty, the computation or time we want the challenger to need to solve the problem. The challengers eventually compute and output $y = x^{2^T} (\text{mod }N)$ by squaring the integer $x$ exactly $T$ times as well as generate an additional proof of this result. The verification consists in verifying the proof passes successfully together with the input, output and modulus.

###### 3.2.1.2 Properties

- *Security Strength & Maturity.** RSA cryptography, since its introduction in 1977, has reached a high level of maturity and is widely considered one of the most reliable and well-understood public-key cryptographic systems. Its security is based on the computational difficulty of factoring large composite numbers, a problem that has remained challenging even with significant advances in both hardware and algorithmic techniques. Over the years, RSA has undergone extensive cryptanalysis, making it one of the most scrutinized cryptographic algorithms. Its applications have become deeply embedded in a wide range of security protocols, such as SSL/TLS for secure communications, digital signatures, and encryption. RSA is however vulnerable to quantum attacks; when large-scale quantum computers become practical, RSA’s security could be broken by quantum algorithms like Shor's algorithm, making it less future-proof compared to post-quantum cryptographic algorithms.

- *Performance.** One of the main drawbacks of the RSA cryptosystem relies on its inefficiency due to large modulus, making the group element large space-wise and operations computationally expensive.

- *Deployability.**  As solving the RSA problem or integer factorization consists in breaking the group security, groups latter cannot be continuously reused in this scenario. More particularly, after finding the factorization of the group modulus, decrypting further ciphertexts in the same group becomes trivial. As for solving a VDF puzzle, the group can be reused safely as long as the modulus is of sufficient size, at least 2,048 bit-long. We can in that scenario choose a known secure modulus, whose factorization is unknown, such as an RSA challenge to create a group. Such trusted unknown moduli are however limited in numbers and we would have to generate new ones, in a trustless manner, when updating security parameters or in case of an, potentially post-quantum, attack.
In our context, setting up RSA groups would be challenging to say the least, as we would need to generate groups of unknown order, that is the RSA modulus must be public while the underlying prime numbers must remain unknown. There is no known method to generate such groups, even inefficiently, which becomes especially critical if we have to do it repeatedly. Generating such a group might be achievable via multi-party computation (MPC) where the network would compute random numbers passing distributive primality tests. This would however be highly impractical.

- *Compliance.** RSA is compliant with a wide range of security standards and regulations. It is one of the most widely accepted public-key cryptosystems and has been incorporated into many cryptographic protocols, including SSL/TLS for secure web communication, digital signatures, and email encryption. RSA complies with industry standards such as FIPS 186-4, X.509, PKCS#1 and NIST guidelines.
None of the methods, GNFS or VDFs, are proprietary and there exists open source code implementing these.

##### 3.2.2 ECC solutions

Elliptic Curve Cryptography (ECC) is a form of public-key cryptography based on the mathematical structure of elliptic curves over finite fields. More particularly, ECC relies on a safe subgroup of elliptic curves, usually defined on a prime field for security and efficiency. It provides strong security with smaller key sizes compared to traditional methods like RSA, needing 256 to 388 bit long prime only [[3]](https://csrc.nist.gov/pubs/sp/800/78/5/final),  making it ideal for constrained environments. To break ECC, one has to compute the discrete logarithm of the group (ECDLP), which can be done most efficiently with Pollard's Rho algorithm that solves the discrete logarithm in $\mathcal{O}(n​^{1/2})$ time and $\mathcal{O}(1)$ space.

###### 3.2.2.1 Designs

The main problem satisfying our requirements is solving the discrete logarithmic on a secure subgroup of an elliptic curve. In that case, the setup consists in generating a curve and generator $G$, and sampling a random point $P$ from its secure subgroup. The challengers then have to find the scalar a such that $P = a \cdot G$. Verification is also straightforward, as it consists in raising $G$ to the power $a$ and verifying it equals $P$.
The most efficient methods to find this scalar include the Index Calculus and Pollard’s $\rho$.

###### 3.2.2.2 Properties

- *Security Strength & Maturity.** Elliptic Curve Cryptography has reached a high level of maturity over the past few decades and is widely regarded as a modern, efficient alternative to traditional public-key cryptosystems like RSA. Its security is based on the hardness of the Elliptic Curve Discrete Logarithm Problem (ECDLP), which has been extensively analyzed, making ECC a trusted and well-understood cryptographic method. ECC is now widely adopted in industry standards, including TLS, SSH, Cardano, Bitcoin, and other blockchain technologies, where its efficiency and robustness are critical.
ECC is also vulnerable to post-quantum attacks and can be broken in polynomial time with Pollard's Rho or the Index Calculus algorithm.

- *Performance.** ECC is known for its great performance, particularly in terms of computational efficiency and resource utilization. Compared to traditional public-key systems like RSA, ECC achieves the same level of security with much smaller key sizes, which translates into faster computation, reduced storage requirements, and lower power consumption.

- *Deployability.**  To make sure that our elliptic curves are not known too long in advance, or are precomputed in sufficient numbers, to mitigate preprocessing [[12]](https://eprint.iacr.org/2017/1113.pdf)  as much as possible, we would need to generate the curves on the fly. While RSA groups only rely on the generation of sufficiently large prime numbers, ECC has an array of attacks to look out for as described in safecurves website and paper [[7]](https://eprint.iacr.org/2024/1265.pdf). As such, generating a secure elliptic curve is a complex and challenging task. Nevertheless, there have been methods to generate efficiently safe elliptic curves ([[8]](https://core.ac.uk/download/pdf/11679572.pdf), [9](https://link.springer.com/content/pdf/10.1007/s00145-009-9037-2.pdf), [[10]](https://infoscience.epfl.ch/server/api/core/bitstreams/e2890c5e-2c1e-42e0-92d6-29c6d8d33acf/content)) on the fly but these methods still necessitate minutes worth of probabilistic computation that is not easily verifiable. As finding the discrete logarithm of a number on a curve that has already been broken is significantly easier, thanks to the costly precomputation in  Pollard’s Rho algorithm that can be reused (also succinctly mentioned in [10, attacking multiple keys]), we would have to regularly change the elliptic curve which would make ensuring their number is sufficiently large an important yet difficult challenge to solve.


- *Compliance.** ECC is widely compliant with numerous industry standards and regulations, making it a trusted choice for modern cryptographic applications, including NIST guidelines, FIPS 186-4 and IETF standards for secure communication protocols.
None of the methods, Index Calculus or Pollard’s $\rho$, are proprietary and there exists open source code implementing these.

##### 3.2.3 Class group solutions

The class group of a number field is the group of fractional ideals modulo principal ideals, whose security is partially determined by a parameter called a discriminant. Class group of binary quadratic forms [[14]](https://github.com/Chia-Network/vdf-competition/blob/master/classgroups.pdf) omits trusted setup as the group order, also called class number, is believed to be difficult to compute when the discriminant is sufficiently large - more particularly the class number grows linearly to the square root of the discriminant. For a class group to be secure, the group size and discriminant must be sufficiently long - respectively at least 1,900 and 3,800 bit-long for 128 bit of security [[4]](https://arxiv.org/pdf/2211.16128)- negative, square free and congruent to 0 or 1 modulo 4. Similarly to ECC, to break a class group security one has to find a class group discrete logarithm (CDLP) which can be done most efficiently with index calculus algorithms that reduce CDLP to integer factorization in sub-exponential time [[5]](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=b1e2870db7c2f1cdeb916afe072d84e581ce68b5).

###### 3.2.3.1 Design

Similarly to previous solutions, class groups present two types of problems satisfying the requirements: breaking the discrete logarithm by finding the class order, or using verifiable delayed functions.
CDLP. In that case, the setup consists in generating a discriminant and generator $G$, and sampling a random point P from its secure subgroup. The challengers then have to find the scalar a such that $P = a \cdot G$. Verification is also straightforward, as it consists in raising $G$ to the power $a$ and verifying it equals $P$.
The most efficient methods to find this scalar include the Index Calculus algorithm.
VDF. Similarly to the CLPD, we first start by generating a discriminant and sample a random group element $G$. The challenge then consists in raising this element to a big exponent of the form $2^T$ where $T$ is set depending on the difficulty, the computation or time we want the challenger to need to solve the problem. The challengers eventually compute and output $y = x^{2^T} (\text{mod} N)$ by squaring the integer $x$ exactly $T$ times as well as generate an additional proof of this result. The verification consists in verifying the proof passes successfully together with the input, output and modulus.

###### 3.2.3.2 Properties

- *Security Strength & Maturity.** Class group-based cryptography has reached a moderate level of maturity in cryptographic research. While not as widely deployed as more traditional cryptographic methods like RSA or ECC, class group cryptography has gained attention due to its potential resistance to quantum computing attacks. The mathematical foundations, particularly the hardness of the class group discrete logarithm problem, are well-understood, and class group cryptosystems have been rigorously analyzed. However, practical deployment is still in the early stages, with ongoing efforts focused on optimizing efficiency, key management, and standardization.

- *Performance.** Class group-based cryptography is generally less efficient than RSA or ECC due to the size of their elements and the computational complexity of the composition of elements.
More particularly, to achieve strong security, class groups’ discriminants must be several thousands bit long, and group elements half of this. Operations are thus costly, especially as composition in class groups rely on finding the greatest common denominator between such numbers that is particularly expensive.

- *Deployability.**  Setting up class groups, even though their order is hidden, is much easier than previously discussed solutions as it consists in practice to generate a sufficiently long negative square-free random integer d, and such that d ≡ 1 mod 4. as discriminant. Generating a random element in a class group by hashing also is however more of a delicate but still feasible task as mentioned in [[11]](https://eprint.iacr.org/2024/034.pdf). Mysten Labs recently iterated on this work and published a more efficient and secure hash function [[38]](https://eprint.iacr.org/2024/295.pdf) to class groups. Interestingly, there exist algorithms that have been designed to reuse the underlying group such as cascaded and continuous VDFs [[13]](https://par.nsf.gov/servlets/purl/10159432).

- *Compliance.** Since class group-based cryptography is still being researched, it is not as broadly standardized or regulated as more established cryptographic techniques like ECC. That said, once formal standards and guidelines are developed and adopted, class group-based cryptography could achieve compliance with relevant legal and regulatory frameworks. None of the VDF proof generation algorithms are proprietary and there exists open source code implementing these.
Other groups
We mostly focused on commonly used groups, such as RSA and ECC, and class groups whose usage have been increasing lately, notably because of the popularity of VDF primitives. There exist however other groups such as lattices which are one of the main candidates for post quantum cryptography, supersingular isogenies, whose security is dubious at the moment since the attack on SIDH in 2022, and hyperelliptic Jacobians groups, which are still novel and need further time to get confidence in their security and for more protocols to be built upon, to cite a few.

##### 3.2.4 OWF solutions

To widen our spectrum of solutions, we are now exploring solutions based on well-established non-trapdoored cryptographic functions and pair them with efficient proof systems to enable fast verification.
Hash-based approaches are generally more cost-effective than asymmetric cryptography, do not depend on potentially vulnerable trapdoors, and can be implemented using widely deployed primitives. They are well understood both cryptographically and economically, especially given the prevalence of hash farms.
The main drawback of hash functions lies in their verification: traditionally, verification requires recomputing the hashes, which can be too time-consuming for our use case, especially when considering synching. To address this, we propose leveraging proof systems, such as Succinct Non-interactive Arguments of Knowledge (SNARKs) and Scalable Transparent ARguments of Knowledge (STARKs) to reduce verification time. This introduces a modest overhead in the form of small proof sizes—on the order of hundreds of bytes—which remains acceptable.
Although SNARKs are relatively new and involve complex protocols, their adoption is growing, with some blockchains like Mina and Midnight fully built around them. While their use may raise concerns, it remains a practical choice. It is worth noting, however, that SNARKs are not quantum-resistant—unlike their hash-based counterpart, STARKs, which do offer quantum resistance.

###### 3.2.4.1 Proofs of knowledge

Proofs of knowledge have become an especially active and dynamic area of research in recent years. The foundations were laid in the 1990s with key contributions such as Bellare et al.'s work on Probabilistically Checkable Proofs (PCPs, [[18]](https://dl.acm.org/doi/pdf/10.1145/167088.167174)), Kilian’s results on interactive arguments of knowledge derived from PCPs [[17]], and Micali’s introduction of Computationally Sound Proofs (CS Proofs [[16]](https://people.csail.mit.edu/silvio/Selected%20Scientific%20Papers/Proof%20Systems/Computationally_Sound_Proofs.pdf)), which transformed interactive proofs into non-interactive ones using the Fiat-Shamir heuristic.
In 2016, Groth introduced one of the most efficient PCP-based proof systems to date [[15]](https://eprint.iacr.org/2016/260.pdf), offering significant improvements in both verification time and proof size. Its main drawback, however, is its reliance on a lengthy trusted setup that cannot be reused across different applications.
Subsequent advancements built on this foundation, with SNARKs compiling from interactive oracle proofs (IOPs) and polynomial commitment schemes (PCs) such as Plonk [[19]](https://eprint.iacr.org/2019/953.pdf) and Marlin [[20]](https://eprint.iacr.org/2019/1047.pdf). Researchers introduced novel techniques to optimize proving time—either by reducing asymptotic complexity, such as replacing FFTs with multivariate polynomials, or by enhancing circuit efficiency through the use of lookup tables [[23]](https://eprint.iacr.org/2020/315.pdf), custom gates [[24]](https://docs.zkproof.org/pages/standards/accepted-workshop3/proposal-turbo_plonk.pdf), and cryptographic primitives tailored for specific applications.
More recently, proof aggregation has emerged as a promising paradigm. Techniques like folding and recursive proofs—exemplified by concepts such as Proof-Carrying Data (PCD, [[21]](https://eprint.iacr.org/2012/095.pdf)) and Incrementally Verifiable Computation (IVC, [[22]](https://g-city.sass.org.cn/_upload/article/files/b4/b1/dcb2f5064216b5751c14bc8366f8/e092766a-ddaa-4fa1-b052-8662bad2d2b6.pdf#page=12))—enable efficient step-by-step computation and verification.
Despite ongoing debates about their security—particularly around the soundness of modeling a random oracle (RO) inside a SNARK—these systems are increasingly being integrated into blockchain technologies. Projects like ZCash, Mina, and Midnight blockchains leverage SNARKs for their powerful compression capabilities, and in some cases, for their privacy-preserving features as well.

###### 3.2.4.2 OWFs

- *Non-Algebraic standard hashes.** SHA-2, SHA-3, and BLAKE2 are prominent cryptographic hash functions widely used today. SHA-2, standardized by NIST in 2001, remains the industry standard due to its strong security and broad adoption in applications like TLS and cryptocurrencies.
Keccak [[25]](https://eprint.iacr.org/2015/389.pdf), selected through a NIST competition in 2015 as the new standard SHA-3, offers a fundamentally different sponge-based design, providing an alternative with enhanced flexibility and resilience at the cost of lower throughput.
BLAKE2 [[26]], developed as a high-performance finalist in the same SHA-3 competition, is favored for its speed and security, often outperforming both SHA-2 and SHA-3 in practical settings. While not standardized by NIST, BLAKE2 is widely trusted and increasingly adopted in modern cryptographic implementations.
Together, these functions represent a balance of security, performance, and diversity in cryptographic hashing today.

While these hash functions are very efficient on CPU, they are very expensive to verify with classic SNARKs, as the latter are working on prime fields and not bits. Proving hash evaluation is several orders of magnitude higher than evaluating on CPU making this solution very impractical. Simple benchmarks demonstrate such results, with the generation of a proof asserting the evaluation of a few hundreds of hashes taking tens of seconds, while the evaluation itself is of the order of the microsecond. For instance, according to Figure 1, the a hundred evaluations of SHA-256 would take 32μs on CPU and require 300,000 gates. To generate a proof of these evaluations, we would require a circuit of size 219 , i.e. the smallest power of 2 above 300,000, which takes 6s to 18s depending on the commitment scheme, making this solution, combining standard hash functions and SNARKs, highly impractical.

<center>

<img src="./image/hash_functions_comparison.png" width="500px" >

Figure 1, taken from Reinforced Concrete paper [[27]](https://dl.acm.org/doi/pdf/10.1145/3548606.3560686). Performance of various hash functions in the zero knowledge (preimage proof) and native (hashing 512 bits of data) settings on Intel i7-4790 CPU (3.6 GHz base frequency, 4 core, 8 threads).
</center>

<center>

| $\text{log}_2(\text{gates})$ |   #gates   | Proving time - KZG (ms) | Proving time - IPA (ms) |
| :--------------------------: | :-------------------: | :------------------------------: |:-------------------------------: |
| $8$                          |  256     	           |  43	                            |  77	                             |
| $9$                          |  512	                 |  58	                            |  105	                           |
| $10$                         |  1,024	               |  75	                            |  153	                           |
| $11$                         |  2,048	               |  100                             |  210	                           |
| $12$                         |  4,096   	           |  157                             |  330	                           |
| $13$                         |  8,192   	           |  218                             |  500	                           |
| $14$                         |  16,384  	           |  342                             |  856	                           |
| $15$                         |  32,768  	           |  540                             |  1,432	                         |
| $16$                         |  65,536  	           |  917	                            |  2,590	                         |
| $17$                         |  131,072 	           |  1,646	                          |  4,779	                         |
| $18$                         |  262,144 	           |  3,028	                          |  9,199                           |
| $19$                         |  524,288 	           |  6,231	                          |  18,496	                         |
| $20$                         |  1,048,576 	         |  12,743	                        |  37,287	                         |

Table 2. Halo2 benchmarks, using KZG [[28]](https://www.cypherpunks.ca/~iang/pubs/PolyCommit-AsiaCrypt.pdf) and IPA [[29]](https://eprint.iacr.org/2017/1066.pdf) commitment schemes on Intel(R) Core(TM) i9-14900HX (2.2 GHz base frequency, 24 cores, 32 threads).
</center>


- *Memory-hard functions (MHFs).** are primitives relying on hash functions designed to resist attacks by requiring significant memory and computational effort, making them particularly interesting in our use case, where memory would become another bottleneck to an adversary attempting a grinding attack.
Argon2, the winner of the Password Hashing Competition in 2015, is the current industry standard due to its strong security, configurability, and resistance to known attacks.
Balloon Hashing offers a simpler design focused on provable security guarantees and ease of analysis but is less widely adopted.
The MHF scrypt, introduced earlier and used notably in cryptocurrencies like Litecoin, was among the first practical memory-hard functions but has seen some theoretical attacks exploiting trade-offs between memory and computation.
Of the three, only Argon2 is formally standardized in RFC 9106 and recommended for new applications, while scrypt remains popular in legacy systems and Balloon Hashing is still primarily academic.
Unfortunately, these primitives are much more expensive than hashes on CPU as well as on SNARKs, where the memory requirements become even more prohibitive.

- *SNARK-friendly hashes.** A novel branch of research started with the adoption of SNARKs to design SNARK friendly hash functions. We can classify them in two categories: algebraic or not. Algebraic hashes include, but are not limited to, Poseidon [[30]](https://www.usenix.org/system/files/sec21-grassi.pdf), Anemoi [[31]](https://hal.science/hal-04276646v1/file/2022-840%281%29.pdf), Rescue [[32]]((https://eprint.iacr.org/2020/1143.pdf)) which are based on prime fields. Choosing carefully the fields can result in optimizations of 2 to 3 orders of magnitude in SNARKs, but with higher CPU time unfortunately. For instance, a hundred evaluations of Poseidon hash would take 1.9ms, compared to 32μs for SHA-256, on CPU, but the proof generation would take 1s to 3s, compared to 6s to 18s for SHA-256.
Other, non algebraic, hash functions have also been created such as Reinforced Concrete [[27]](https://dl.acm.org/doi/pdf/10.1145/3548606.3560686) and Monolith [[33]](https://ojs.ub.ruhr-uni-bochum.de/index.php/ToSC/article/download/11810/11315) to minimize the cost of binary operations by making the most of lookup tables, which store binary operations on vectors of bits.
The fact that these hash functions are less efficient on CPUs is not problematic as we are only interested in computational cost. Unfortunately, the ratio between CPU and prove generation time still remains too high for our usage. More novel techniques in SNARKs, such as IVC or folding, would be needed to make the “snarkification” of hash practical but these progresses have yet to reach maturity, be it in both theory and practice.
Another caveat to using SNARK-friendly hashes would be that adversaries could afford specialised hardware such as CPUs with special instructions such as AVX2, or GPUs, FPGAs or ASICs to accelerate prime field operations and widen the gap between honest users and adversaries.

###### 3.2.4.3 Design
Using OWFs and SNARKs in the context of Phalanx is straightforward. To each iteration is associated a input that we have to recursively hash a number of times set by the total duration and number of iterations with the desired primitive. Once the result is computed, a SNARK proof can be generated proving the correctness of the computation. We can remark that IVC based solutions are particularly adapted as a choice for SNARK primitves as we can prove a batch of iterations per step of IVC. Both the hash output and the SNARK are then published.

###### 3.2.4.4 Properties

- *Security Strength & Maturity.** While traditional hashes have strong security, more novel ones, especially the more usable with SNARKs, can be deemed too novel for adoption. SNARKs, and SNARKs friendly primitives, are very complex pieces of technology that have been broken before and are still evolving at a rapid pace. SNARKs are not postquantum resistant but STARKs are.

- *Performance.** While hash functions are extremely efficient on commodity hardware, the proof generation with current SNARKs is far too slow for this solution to be practical

- *Deployability.**  SNARKs are difficult to deploy, they rely on different libraries that are not easy to update. Changing of SNARKs is also tedious as circuits would very likely need to be rewritten, adding further risk and complexity.

- *Compliance.** Hash functions are standardized and libraries are easily available. SNARK solutions are not copyrighted, there is however a limited number of available libraries, which can either be open source or proprietary (SP1, RISC0, STARKNET…).

#### 3.3 Primitive recommendation

The combination of OWFs and SNARKs, however elegant it may be for its modularity, is not practical for the proof generation overhead being prohibitive.
Trapdoor based solutions seem to be the best candidates for anti-grinding solutions. Out of the ones considered, VDFs seem the most practical primitive thanks to the possibility of reusing the group, and class groups offer the simplest deployment. The main caveat of such a solution is in its relative novelty, regular assessment would need to be done to ensure correct and up to date parametrization.

## Path to Active

### Acceptance Criteria

The proposal will be considered **Active** once the following criteria are met:

- [ ] The revised `cardano-node` implementation passes all **node-level conformance test suites**.
- [ ] A formal **security audit** is completed and its findings reviewed.
- [ ] The solution demonstrates **stable and expected behavior in testnet environments**.
- [ ] The **hard fork is successfully executed** and the protocol transition is secure.
- [ ] The **community agrees on the initial Phalanx protocol parameters** and on a clear policy for their future updates.
- [ ] The upcoming CIP introducing a **Consensus** category may define further acceptance criteria, which will be incorporated accordingly.

### Implementation Plan

To fulfill the above criteria, the following steps are planned:

- [ ] Triage and scope confirmation by Intersect’s **Core Infrastructure** and **Consensus** teams.
- [ ] Coordination with ongoing workstreams on consensus protocol enhancements:

- [ ] Compatibility with **Peras**
- [ ] Compatibility with **Leios**
- [ ] Compatibility with **Ouroboros Genesis**
- [ ] Development and publication of a **community communication plan** covering:

- The initial values of Phalanx parameters
- The procedure for evaluating and updating these parameters
- [ ] Integration of a **Wesolowski-style VDF library** into [`cardano-crypto-class`](https://github.com/IntersectMBO/cardano-base/blob/master/cardano-crypto-class/cardano-crypto-class.cabal)
- [ ] Implementation of the **node-level logic**, including support for the hard fork mechanism
- [ ] Construction and execution of a comprehensive **node-level conformance test suite**

## References

1. [Ouroboros Randomness Generation Sub-Protocol – The Coin-Flipping Problem](https://github.com/cardano-foundation/CIPs/tree/master/CPS-0021/CPD/README.md)
2. [Cardano Disaster Recovery Plan](https://iohk.io/en/research/library/papers/cardano-disaster-recovery-plan)
3. [Baigneres, Thomas, et al. "Trap Me If You Can--Million Dollar Curve." Cryptology ePrint Archive (2015).](https://eprint.iacr.org/2015/1249.pdf)
4. [Lenstra, Arjen K., et al. "The number field sieve." Proceedings of the twenty-second annual ACM symposium on Theory of computing. 1990.](https://dl.acm.org/doi/pdf/10.1145/100216.100295)
5. [National Institute of Standards and Technology (NIST). (April 2010). Special Publication  800-78-5: Cryptographic Algorithms and Key Sizes for Personal Identity Verification.](https://csrc.nist.gov/pubs/sp/800/78/5/final)
6. [Dobson, Samuel, Steven Galbraith, and Benjamin Smith. "Trustless unknown-order groups." arXiv preprint arXiv:2211.16128 (2022).](https://arxiv.org/pdf/2211.16128)
7. [Hamdy, Safuat, and Bodo Möller. "Security of cryptosystems based on class groups of imaginary quadratic orders." Advances in Cryptology—ASIACRYPT 2000: 6th International Conference on the Theory and Application of Cryptology and Information Security Kyoto, Japan, December 3–7, 2000 Proceedings 6. Springer Berlin Heidelberg, 2000.](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=b1e2870db7c2f1cdeb916afe072d84e581ce68b5)
8. [Boneh, Dan, et al. "Verifiable delay functions." Annual international cryptology conference. Cham: Springer International Publishing, 2018.](https://eprint.iacr.org/2018/601.pdf)
9. [Bernstein, Daniel J., and Tanja Lange. "Safe curves for elliptic-curve cryptography." Cryptology ePrint Archive (2024).](https://eprint.iacr.org/2024/1265.pdf)
10. [Baier, Harald. Efficient algorithms for generating elliptic curves over finite fields suitable for use in cryptography. Diss. Technische Universität, 2002.](https://core.ac.uk/download/pdf/11679572.pdf)
11. [Konstantinou, Elisavet, et al. "On the efficient generation of prime-order elliptic curves." Journal of cryptology 23.3 (2010): 477-503.](https://link.springer.com/content/pdf/10.1007/s00145-009-9037-2.pdf)
12. [Miele, Andrea, and Arjen K. Lenstra. "Efficient ephemeral elliptic curve cryptographic keys." Information Security: 18th International Conference, ISC 2015, Trondheim, Norway, September 9-11, 2015, Proceedings 18. Springer International Publishing, 2015.](https://infoscience.epfl.ch/server/api/core/bitstreams/e2890c5e-2c1e-42e0-92d6-29c6d8d33acf/content)
13. [Seres, István András, Péter Burcsi, and Péter Kutas. "How (not) to hash into class groups of imaginary quadratic fields?." Cryptographers’ Track at the RSA Conference. Cham: Springer Nature Switzerland, 2025.](https://eprint.iacr.org/2024/034.pdf)
14. [Corrigan-Gibbs, Henry, and Dmitry Kogan. "The discrete-logarithm problem with preprocessing." Advances in Cryptology–EUROCRYPT 2018: 37th Annual International Conference on the Theory and Applications of Cryptographic Techniques, Tel Aviv, Israel, April 29-May 3, 2018 Proceedings, Part II 37. Springer International Publishing, 2018.](https://eprint.iacr.org/2017/1113.pdf)
15. [Ephraim, Naomi, et al. "Continuous verifiable delay functions." Annual International Conference on the Theory and Applications of Cryptographic Techniques. Cham: Springer International Publishing, 2020.](https://par.nsf.gov/servlets/purl/10159432)
16. [Long, Lipa. "Binary quadratic forms.", (2018)](https://github.com/Chia-Network/vdf-competition/blob/master/classgroups.pdf)
17. [Groth, Jens. "On the size of pairing-based non-interactive arguments." Advances in Cryptology–EUROCRYPT 2016: 35th Annual International Conference on the Theory and Applications of Cryptographic Techniques, Vienna, Austria, May 8-12, 2016, Proceedings, Part II 35. Springer Berlin Heidelberg, 2016.](https://eprint.iacr.org/2016/260.pdf)
18. [Micali, Silvio. "CS proofs." Proceedings 35th Annual Symposium on Foundations of Computer Science. IEEE, 1994](https://people.csail.mit.edu/silvio/Selected%20Scientific%20Papers/Proof%20Systems/Computationally_Sound_Proofs.pdf)
19. [Kilian, Joe. "A note on efficient zero-knowledge proofs and arguments." Proceedings of the twenty-fourth annual ACM symposium on Theory of computing. 1992.](https://dl.acm.org/doi/pdf/10.1145/129712.129782)
20. [Bellare, Mihir, et al. "Efficient probabilistically checkable proofs and applications to approximations." Proceedings of the twenty-fifth annual ACM symposium on Theory of computing. 1993.](https://dl.acm.org/doi/pdf/10.1145/167088.167174)
21. [Gabizon, Ariel, Zachary J. Williamson, and Oana Ciobotaru. "Plonk: Permutations over lagrange-bases for oecumenical noninteractive arguments of knowledge." Cryptology ePrint Archive (2019).](https://eprint.iacr.org/2019/953.pdf)
22. [Chiesa, Alessandro, et al. "Marlin: Preprocessing zkSNARKs with universal and updatable SRS." Advances in Cryptology–EUROCRYPT 2020: 39th Annual International Conference on the Theory and Applications of Cryptographic Techniques, Zagreb, Croatia, May 10–14, 2020, Proceedings, Part I 39. Springer International Publishing, 2020.](https://eprint.iacr.org/2019/1047.pdf)
23. [Bitansky, Nir, et al. "Recursive composition and bootstrapping for SNARKS and proof-carrying data." Proceedings of the forty-fifth annual ACM symposium on Theory of computing. 2013.](https://eprint.iacr.org/2012/095.pdf)
24. [Valiant, Paul. "Incrementally verifiable computation or proofs of knowledge imply time/space efficiency." Theory of Cryptography: Fifth Theory of Cryptography Conference, TCC 2008, New York, USA, March 19-21, 2008. Proceedings 5. Springer Berlin Heidelberg, 2008.](https://g-city.sass.org.cn/_upload/article/files/b4/b1/dcb2f5064216b5751c14bc8366f8/e092766a-ddaa-4fa1-b052-8662bad2d2b6.pdf#page=12)
25. [Gabizon, Ariel, and Zachary J. Williamson. "plookup: A simplified polynomial protocol for lookup tables." Cryptology ePrint Archive (2020).](https://eprint.iacr.org/2020/315.pdf)
26. [Gabizon, Ariel, and Zachary J. Williamson. "Proposal: The turbo-plonk program syntax for specifying snark programs.", 2020](https://docs.zkproof.org/pages/standards/accepted-workshop3/proposal-turbo_plonk.pdf)
27. [Bertoni, Guido, et al. "Keccak." Annual international conference on the theory and applications of cryptographic techniques. Berlin, Heidelberg: Springer Berlin Heidelberg, 2013.](https://eprint.iacr.org/2015/389.pdf)
28. [Aumasson, Jean-Philippe, et al. "BLAKE2: simpler, smaller, fast as MD5." International Conference on Applied Cryptography and Network Security. Berlin, Heidelberg: Springer Berlin Heidelberg, 2013.](https://eprint.iacr.org/2013/322.pdf)
29. [Grassi, Lorenzo, et al. "Reinforced concrete: A fast hash function for verifiable computation." Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security. 2022.](https://dl.acm.org/doi/pdf/10.1145/3548606.3560686)
30. [Kate, Aniket, Gregory M. Zaverucha, and Ian Goldberg. "Constant-size commitments to polynomials and their applications." International conference on the theory and application of cryptology and information security. Berlin, Heidelberg: Springer Berlin Heidelberg, 2010.](https://www.cypherpunks.ca/~iang/pubs/PolyCommit-AsiaCrypt.pdf)
31. [Bünz, Benedikt, et al. "Bulletproofs: Short proofs for confidential transactions and more." 2018 IEEE symposium on security and privacy (SP). IEEE, 2018.](https://eprint.iacr.org/2017/1066.pdf)
32. [Grassi, Lorenzo, et al. "Poseidon: A new hash function for {Zero-Knowledge} proof systems." 30th USENIX Security Symposium (USENIX Security 21). 2021.](https://www.usenix.org/system/files/sec21-grassi.pdf)
33. [Bouvier, Clémence, et al. "New design techniques for efficient arithmetization-oriented hash functions: Anemoi permutations and Jive compression mode." Annual International Cryptology Conference. Cham: Springer Nature Switzerland, 2023.](https://hal.science/hal-04276646v1/file/2022-840%281%29.pdf)
34. [Szepieniec, Alan, Tomer Ashur, and Siemen Dhooghe. "Rescue-prime: a standard specification (SoK)." Cryptology ePrint Archive (2020).](https://eprint.iacr.org/2020/1143.pdf)
35. [Grassi, Lorenzo, et al. "Monolith: Circuit-friendly hash functions with new nonlinear layers for fast and constant-time implementations." IACR Transactions on Symmetric Cryptology 2024.3 (2024): 44-83.](https://ojs.ub.ruhr-uni-bochum.de/index.php/ToSC/article/download/11810/11315)
36. [Wesolowski, Benjamin. "Efficient verifiable delay functions." Advances in Cryptology–EUROCRYPT 2019: 38th Annual International Conference on the Theory and Applications of Cryptographic Techniques, Darmstadt, Germany, May 19–23, 2019, Proceedings, Part III 38. Springer International Publishing, 2019.](https://eprint.iacr.org/2018/623.pdf)
37. [Pietrzak, Krzysztof. "Simple verifiable delay functions." 10th innovations in theoretical computer science conference (itcs 2019). Schloss Dagstuhl–Leibniz-Zentrum für Informatik, 2019.](https://drops.dagstuhl.de/storage/00lipics/lipics-vol124-itcs2019/LIPIcs.ITCS.2019.60/LIPIcs.ITCS.2019.60.pdf)
38. [Chalkias, Kostas Kryptos, Jonas Lindstrøm, and Arnab Roy. "An efficient hash function for imaginary class groups." Cryptology ePrint Archive (2024).](https://eprint.iacr.org/2024/295.pdf)


## Copyright
This CIP is licensed under [Apache-2.0](http://www.apache.org/licenses/LICENSE-2.0).

Portions of this document were prepared with the assistance of AI-based tools.The use of AI was limited to drafting, editing, and improving clarity of expression. All **technical ideas, specifications, and cryptographic designs** originate from the human authors, who take full responsibility for their novelty, correctness, and originality.

The AI contribution is comparable to that of a copy-editor: it helped improve formatting, emphasis, and readability, but did not generate or propose the underlying concepts.

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0163/README.md
---

- --
CIP: 163
Title: Time-Bound Delegation with Dynamic Rewards
Category: Ledger
Status: Proposed
Authors:
- Ryan Wiley rian222@gmail.com
Implementors: []
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/1077
Created: 2025-08-15
License: CC-BY-4.0
- --

## Abstract

Cardano’s staking and on‑chain voting have drifted toward passivity. As described in CPS-0022, long‑lived “set‑and‑forget” delegations let rewards and governance weight ossify, particularly in wallets that have been permanently lost, diluting staking rewards and voting power engaged participants and harming network security. This CIP reframes delegation as an **active, periodically affirmed contribution to network security** and realigns incentives so rewards flow to those presently and capable of participating. At a high level, it introduces (i) **reward-account liveness with inactivity expiry** (proof-of-life via any witness-bearing action on the staking credential), and (ii) **full-pot reward distribution** to currently eligible delegators and pools each epoch.

If the network’s total staking ratio remains roughly unchanged, these changes are expected to increase rewards for all delegators and stake pool operators by a small annual percent. Together, these changes encourage ongoing engagement, reduce sticky/lost‑stake externalities, and keep realized emissions consistent with monetary policy while respecting user property rights.

## Motivation: why is this CIP necessary?

Many users interpret staking rewards as akin to bank interest. If that were the intent, the ledger would not need operator selection, delegation, or performance‑based rewards. Yield could simply accrue automatically. In Cardano, **staking is voluntary and active**. It is how the protocol decides who gets to make blocks, and rewards compensate the combination of (a) **committing stake** to secure the network and (b) **selecting and monitoring** a competent, honest operator. "On a high level, the goal of the incentives mechanism is to incentivize stakeholders to follow the protocol and thereby to guarantee the secure and efficient operation of Cardano" (SL‑D5 §5.1)

- Delegation is a liquid vote for a pool operator. Delegators are expected to reevaluate and move if performance, fees, or behavior change.
- Lost or inactive stake no longer contributes to present‑day security yet can lock in block‑production influence and siphon rewards from active participants.

This CIP addresses the broader drift toward **passive and indifferent delegations.** Securing the network is an **active act**, periodically affirmed by the holder, and rewarded accordingly. Reward-account inactivity introduces a light-touch **proof-of-life** via any witness-bearing action on the staking credential, covering both stake pool and dRep participation. Furthermore, full‑pot distribution provides a stabilizing incentive: if total stake falls low enough to threaten security, yields rise automatically for active delegators and SPOs, encouraging new delegation rather than letting rewards drip back to reserves.

## Specification

#### Current behavior (for context)

- **Indefinite delegations.** Delegation certificates today have no expiry. Once a stake credential delegates to a stake pool or a dRep, that delegation remains valid indefinitely until the holder submits a new delegation (or deregisters the stake key).
- **Rewards return-to-reserves.** After monetary expansion and fees are combined and the treasury cut is taken, the distributable rewards pot is scaled by the network’s active‑stake ratio; only that fraction is paid out, and the remainder is returned to reserves. This occurs because rewards are computed using only the active stake snapshot and any residual R not assigned to reward accounts (∆r₂ = R − Σrs) is returned to reserves, per the epoch‑boundary and reward‑update rules (SL‑D5 §11.1, §11.5, §11.8–§11.10).

This CIP intentionally limits itself to two changes to the ledger rules. All other reward mechanics (e.g., pledge influence, saturation, treasury cut) are unchanged:

### 1) Reward-account inactivity expiry (proof-of-life)

- Add a **single protocol parameter** (measured in epochs), to be set via governance:
- `delegatorInactivity`
- **Scope & terminology.** A *reward account* (i.e., staking credential) is the object that delegates, accrues, and withdraws rewards. The ledger maintains an `expirationEpoch` for each registered reward account.
- **Inactivity rule.** During epoch transitions, when stake and voting power are computed, **expired reward accounts** (those with `expirationEpoch < currentEpoch`) are **ignored** for pool leader election, rewards distribution, and governance tallying.
- **Proof-of-life.** Any action that requires a **witness by the reward account** resets its expiration to `currentEpoch + delegatorInactivity`. Examples include: reward withdrawal, (re)delegation to an SPO, (re)delegation to a dRep, and stake-key registration or deregistration. (Wallets commonly withdraw rewards when submitting any transaction, so ordinary usage renews automatically.)
- **Ineligible for rewards.** For epochs in which a reward account is inactive, its epoch reward share is **not credited** to that account. Those uncredited rewards are instead distributed with the rest of the rewards pot (see next section).
- **Migration.** On activation, determine the epoch that the last witness occurred for each rewards account.  Then initialize each account's `expirationEpoch` to that epoch plus `delegatorInactivity`.

### 2) Full‑pot reward distribution

- In each epoch, distribute the **full rewards pot** (monetary expansion + fees, less the treasury cut, blocks not produced [Eta], and unmet pledge) to eligible delegators and pools without returning any remainder to reserves **(i.e., eliminate the residual ∆r₂ described in SL‑D5 §11.10 and distribute that proportion as well)**.
- “Eligibility” remains exactly as today, except that stake controlled by an **expired reward account** does **not** earn rewards (and expired reward accounts contribute **no** governance voting power).

- *Note:** The initial numeric value for `delegatorInactivity` is intentionally **left undefined** here and will be chosen through community governance and deliberation. Potential jittering or lottery mechanisms to stagger per-epoch expiries are not mentioned in this proposal, but may be considered separately if operational data warrants it.

## Rationale: how does this CIP achieve its goals?

- **Active security & correct economics.** Periodic renewal makes delegation an affirmative act of securing the network. Engaged holders are compensated for informed operator selection, not mere passage of time.
- **Mitigate sticky stake.** Long‑dormant delegations can prop up legacy control of block production. Expirations reduce that inertia by requiring a simple, periodic affirmation from real, reachable owners.
- **Curbing ossification & compounding.** Expirations prevent unreachable wallets from indefinitely accumulating rewards or voting power without threatening their principal investment.
- **Dynamic & responsive yields.** Full‑pot distribution increases pool/delegator yield when participation drops and compresses it when participation rises, creating a simple and transparent balancing mechanism.
- **Alignment with policy.** Realized emissions track the monetary expansion set by Rho more closely (rather than being artificially reduced by the active‑stake ratio).
- **Respect for property.** No seizure or clawback occurs. Inactive accounts simply stop earning until the account becomes active againn.
- **Governance clarity.** Because expired reward accounts contribute zero voting power, governance power reflects currently affirmed participation (CIP-1694’s “inactive dRep” guardrails remain complementary but insufficient on their own).
- **Sharper SPO accountability.** Periodic renewal nudges delegators to review operator fees, performance, and conduct. If a pool underperforms or raises costs, stake can organically reallocate on renewal.
- **Fairer path for new pools.** Expirations free stake that would otherwise remain inert with incumbents, reducing entrenched advantages from lost delegations and lowering barriers for competent new operators to attract delegation.

### Option to adjust Rho

Distributing the full-rewards pot will stop sending residual rewards back to reserves. If Rho is left unchanged, reserves will deplete faster by roughly that amount and true emissions will track much closer to the monetary expansion set by Rho. As a result, this will cause active participants to see a small boost to their annual yields. If the community prefers to preserve the current rate of emissions, the community can instead reduce Rho to offset this through a parameter change governance action. However, this will trade away the increase in rewards. This CIP intentionally leaves this choice out of scope.

A key practical advantage of distributing the full rewards pot while tightening eligibility for rewards (e.g., by requiring a witness within `delegatorInactivity` epochs) and optionally lowering `Rho` to compensate is that it slows the depletion of reserves, yet still marginally increases rewards for active delegators and SPOs. Empirical projections (e.g., Leios/CPS modeling and CPS‑22 forecasts) suggest sustaining current reward trajectories could require materially higher transaction throughput or faster reserve drawdown. By excluding long‑dormant credentials from the payout base we simply avoid paying rewards to wallets that are unlikely to ever reuse them. That reduction in the set of eligible reward accounts lengthens reserve lifetime in a predictable way while still delivering higher per‑unit yields to genuinely active delegators and SPOs.

This is a policy lever with clear tradeoffs: the community can preserve the nominal emission schedule by lowering `Rho`, which maintains the same long‑term reserve trajectory but gives up the near‑term yield uplift for active participants; or the community can keep `Rho` and accept faster nominal depletion while shifting more reward flow to active participants today. Because the change is expressed as an eligibility rule rather than a confiscation, it preserves property rights and is operationally simple to reason about.  It therefore offers a modest, low‑risk way to buy more runway for broader, longer‑term value‑creation efforts (e.g., higher fees/tx volume, DeFi growth) that are the real sustainable solution.

### Prior rationale and revised censorship assessment

The Shelley-era design explicitly worried about an incentive for large pools to **censor delegation certificates** if rewards were distributed in ways that made existing actors worse off when new delegations appeared. To mitigate this, the specification chose (a) to normalize reward shares by **total stake** rather than active stake so that new delegations do not dilute incumbents’ proportional rewards. Also, (b) to send **undistributed** rewards back to reserves. See *Shelley Delegation & Incentives Design Spec.*, SL‑D1 v1.21 (2020‑07‑23), §5.5.1 “Relative Stake: Active vs Total” (p. 35), and change log rev. 17 (“undistributed rewards go to the reserves”). The spec also anticipated concerns about pools rejecting or disadvantaging delegation transactions (Appendix D.2, p. 52).

- *Why this is acceptable to revisit now.** Today’s network conditions and incentives make sustained censorship of delegation renewals economically implausible and strategically dominated by other attack vectors:

- **Healthy stake distribution & coordination threshold.** With broad dispersion of block‑producing power, a would‑be censor would need to withhold a large number of small delegation transactions across many blocks and coordinate with many other producers to move rewards meaningfully. Any unilateral attempt simply fails whenever other producers include those transactions.
- **Externality of “success.”** Under full‑pot distribution, even a successful bout of censorship raises everyone else’s yields, not just the censor’s. This dilutes the payoff while increasing incentives for others to defect and include the transactions.
- **Richer targets exist.** In a DeFi‑enabled ecosystem, selective censorship of high‑value transactions (e.g., liquidations/arbitrages) offers a concentrated, private payoff from censoring few targeted transactions, whereas delegation censorship requires suppressing many of them for marginal gains.
- **Operational risk and visibility.** Prolonged, coordinated censorship would be conspicuous, invite social and governance backlash, and likely degrade a censor’s reputation and delegation flow, offsetting any transient gain.

These factors, together with periodic account-liveness expiry/renewal that naturally spreads liveness-reset actions over time, support removing the “return to reserves” scaling while keeping incentives aligned with **active participation**.

### Security consideration: participation shocks after large expirations

Introducing inactivity expiry could, in the short term, allow portions of stake to lapse if large wallets forget or delay renewal. This temporarily reduces the **active‑stake ratio**, widening the gap between total and active stake. In such windows, the cost for an adversary to assemble a large share of active stake is lower than usual, and block production can concentrate among a smaller set of pools until new delegations occur.

- *Balancing mechanism in this CIP.** The proposal deliberately pairs expirations with full‑pot reward distribution. Because the entire pot is distributed over the then‑active stake, the per‑unit yield rises automatically when participation drops and compresses when it rises. This creates a fast, transparent negative‑feedback loop:

- If active stake decreases, epoch yield increases for active delegators/SPOs. Capital is attracted back until the security margin normalizes.
- Likewise if active stake increases, epoch yield decreases. Though without changes to Rho the net result of this CIP will still increase rewards all the way up to 100% of active-stake ratio.

This dynamic couples network security to real participation and reduces the risk of active-stake dropping too low.

### Evidence and modeling

- **Reserves trajectory** will be closer to true Rho after this CIP is implemented
- Red = ADA in reserves if we followed Rho exactly
- Yellow = ADA in reserves account for actual blocks produced (Eta) and missing pledge. This is the track the reserves is expected to follow after the inclusion of this CIP.
- Green = Actual amount of ADA in reserves

  ![Reserve Depletion)](Fig1.png)

- Source: [https://x.com/C1cADA_Markus/status/1636023370532749314](https://x.com/C1cADA_Markus/status/1636023370532749314)*

- Estimated **~1% APY uplift** from full‑pot distribution:

Given:

We can calculate the expected ROI for an average pool with 35M stake and 1M pledge before this CIP as follows:

- *Constants:**

- $S = 35,698,219,658$ (circulating supply)
- $\text{Reserves} = 6,955,875,027$
- $k = 500 \;\Rightarrow\; z_0 = 0.002$
- $a_0 = 0.3$
- $\rho = 0.003$
- $\tau = 0.2$

- *Epoch rewards pot after treasury (not accounting for fees or missed blocks):**

$$
R = (\rho \cdot \mathrm{Reserves}) \cdot (1 - \tau) = 16{,}694{,}100.0648
$$

- *Pool (average pool with 35M stake and 1M pledge):**

- $x = 35,000,000$ (stake)
- $y = 1,000,000$ (pledge)

$$
z = \frac{35{,}000{,}000}{35{,}698{,}219{,}658} = 0.000980441051
$$

$$
s = \frac{1{,}000{,}000}{35{,}698{,}219{,}658} = 0.000028012601
$$

$$
z' = \min(z, z_{0}), \quad s' = \min(s, z_{0})
$$

- *Base reward (from Shelley formula):**

$$
B = \frac{R}{1+a_{0}} \left(
z' + s' \cdot a_{0} \cdot \frac{\,z' - s' \cdot \frac{(z_{0}-z')}{z_{0}}\,}{z_{0}}
\right)
$$

- *For this pool:**

$$
B = 12{,}642.580060
$$

- *Standard distribution (spec):** distributes $R$ to active pools and returns the rest to reserves

$$
f_{\mathrm{std}} = B = 12{,}642.580060
$$

```math
\begin{aligned}
\mathrm{ROI}_{\mathrm{pool,std}} &= \frac{f_{\mathrm{std}}}{x+y} \\
&= \frac{12{,}642.580060}{36{,}000{,}000} \\
&= 0.000351182779
\end{aligned}
```

```math
\begin{aligned}
\mathrm{Annual\ ROI} &= \left(1 + \mathrm{ROI}_{\mathrm{pool,std}}\right)^{73} - 1 \\
&= 0.025963163 \;\; (\approx 2.5963\%)
\end{aligned}
```

This gives us about **2.60% Annual ROI** for that pool.
## 


- *Full pot rewards distribution: distribute all of $R$ with no residual**

Let $W = \sum_i B_i$ over all eligible pools (whole circulation). Typically $W \approx 1$.

Then normalize:

$$
f_{\mathrm{full}} = R \cdot \frac{B}{W} \;\approx\; (1+a_{0}) \cdot B = 16{,}435.354078
$$

```math
\begin{aligned}
\mathrm{ROI}_{\mathrm{epoch,full}} &= \frac{f_{\mathrm{full}}}{x+y} \\
&= \frac{16{,}435.354078}{36{,}000{,}000} \\
&= 0.000456537613
\end{aligned}
```

```math
\begin{aligned}
\mathrm{ROI}_{\mathrm{annual,full}} &= \left(1 + \mathrm{ROI}_{\mathrm{epoch,full}}\right)^{73} - 1 \\
&= 0.033880957 \;\; (\approx 3.3881\%)
\end{aligned}
```

The result is about **3.39% Annual ROI**. Roughly a 0.79% increase.

- *Note:** These calculations do not take into account any collected transaction fees, pool performance, or pool fees.

These effects are also modeled in the open source tool available at [https://spo-incentives.vercel.app/](https://spo-incentives.vercel.app/). Setting the rewards formula radio button to “Current” represents the current rewards calculation behavior. Setting it to “Full” represents the rewards calculation after applying this CIP. It is also recommended to experiment with the “Staked Ratio” slider under both of those settings to see how it will affect rewards.

## Path to Active

### Acceptance Criteria

- **CIP Editor Approval** – Cardano CIP Editors must confirm that the specification is complete, unambiguous, and internally consistent with existing CIPs.
- **Consensus on initial parameter value(s)** – An initial value for the new protocol parameter `delegatorInactivity` in epochs must be agreed upon before hard-fork combinator (HFC) activation. The choice should consider operational viability, empirical analyses, and community feedback.
- **Endorsement by Technical Bodies** – The Cardano Parameter-Change Proposals (PCP) Committee and the Intersect Technical Steering Committee (TSC) should both recommend the proposal as technically sound and aligned with the protocol’s long-term roadmap.
- **Stakeholder Concurrence** – A majority of stake pool operators (SPOs), ecosystem tooling maintainers, dReps, and other infrastructure providers must signal readiness to upgrade.
- **Governance Ratification** – The on-chain Hard-Fork Governance Action must pass the requisite dRep and Constitutional Committee thresholds, establishing legal-constitutional legitimacy and stakeholder support for the change.

### Implementation Plan

- **Community Deliberation (Preparation Phase)**
- Publish the finalized CIP revision and present it to the PCP committee, TSC, CIP Editors, and wider community channels (Discord, X, Cardano Forum, etc.).
- Collect structured feedback, particularly on candidate values for the new parameter values and iterate until broad technical consensus emerges.
- **Specification & Code Integration (Development Phase)**
- Once initial parameter values are determined, integrate the new rewards calculation logic, delegation certificate expiry, and governance features for the new parameters into cardano-node and related libraries (ledger, CLI, wallet APIs).
- Submit pull requests to the canonical repositories; obtain code reviews from IOG, CF, and community contributors.
- Release a new protocol version that includes the changes made in this CIP.
- Use a dedicated pre-production testnet that mirrors main-net parameters but enforces the new changes, allowing SPOs and exchanges to test end-to-end flows.
- **Readiness Sign-off (Testing Phase)**
- Require at least two weeks of uninterrupted testnet stability plus green results from regression and property-based tests.
- Monitor ecosystem dApps and tooling to confirm that major node implementations, explorers, wallets, and exchange integrations support the new rule set.
- **On-chain Governance (Ratification Phase)**
- File the Hard-Fork Governance Action on-chain with the agreed initial parameter values tagged for the next hard fork event.
- Modify the existing Cardano Constitution to include definitions and guardrails for the new protocol parameters and have it ratified by the tripartite government of Cardano.
- Mobilize dRep outreach to ensure quorum and super-majority passage; concurrently, the Constitutional Committee validates procedural compliance.
- **Hard-Fork Activation (Deployment Phase)**
- Upon successful vote, the hard fork event is automatically triggered upon epoch turnover.
- Monitor main-net metrics during the changeover epoch; provide real-time support for any late-upgrading SPOs.

## References

- Kant, P.; Brünjes, L.; Coutts, D. *Design Specification for Delegation and Incentives in Cardano — Shelley* (SL‑D1 v1.21, 23 Jul 2020). Especially §5.5.1 “Relative Stake: Active vs Total” (p. 35); Appendix D.2 “Won’t stake pools reject delegation certificates that delegate away from them?” (p. 52); and change log rev. 17 (“Undistributed rewards go to the reserves, not to the treasury.”).
- Corduan, J.; Vinogradova, P.; Güdemann, M. *A Formal Specification of the Cardano Ledger* (Shelley Ledger: SL‑D5 v1.0, updated 23 Mar 2023). Section 11: Rewards and the Epoch Boundary — overview (§11.1), snapshots (§11.5), epoch transition (§11.8), rewards distribution (§11.9), and reward update/return‑to‑reserves residual (∆r₂) (§11.10).
- Wiley, R. *CPS-0022: Sticky Stake and Time-Bound Delegation* (2025). Available at: https://github.com/Cerkoryn/CIPs/blob/sticky-stake/CPS-0022/README.md.

## Acknowledgements

This CIP could not have been created without the support, assistance, and input of all participants in the community-led SPO Incentives Working Group.

- Stef M [RABIT]
- Rich Manderino [ECP]
- Wayne Cataldo [OTG]
- Homer [AAA]
- Chad [BBHMM]
- Mark H [UPSTR]
- Carlos Lopez de Lara [Input|Output]
- Pedro Lucas
- Seomon
- OYSTR Pool

## Copyright
This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-0381/README.md
---

- --
CIP: 0381
Title: Plutus support for Pairings over BLS12-381
Status: Active
Category: Plutus
Authors:
- Iñigo Querejeta-Azurmendi <inigo.querejeta@iohk.io>
Implementors:
- Kenneth MacKenzie <kenneth.mackenzie@iohk.io>
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/220
- https://github.com/cardano-foundation/CIPs/pull/506
Created: 2022-02-11
License: Apache-2.0
- --

## Abstract
This CIP proposes an extension of the current plutus functions to provide support for basic operations over BLS12-381
curve to the plutus language. We expose a candidate implementation, and describe clearly the benefits that this
would bring. In a nutshell, pairing friendly curves will enable a large number of cryptographic primitives that will
be essential for the scalability of Cardano.

## Motivation: why is this CIP necessary?
Pairing Friendly Curves are a type of curves that provide the functionality of computing pairings. A pairing is a
binary function that maps two points from two groups to a third element in a third target group. For a more in-depth
introduction to pairings, we recommend reading [Pairings for Beginners](https://www.craigcostello.com.au/tutorials) or
[Pairings for Cryptographers](https://eprint.iacr.org/2006/165). For level of detail required in this document, it is
sufficient to understand that a pairing is a map: e:G1 X G2 -> GT, which satisfies the following properties:

- Bilinearity: for all a,b in F^*_q, for all P in G1, Q in G2: e(aP,bQ)=e(P,Q)^(ab)
- Non-degeneracy: e != 1
- Computability: There exists an efficient algorithm to compute e

where G1, G2 and GT are three distinct groups of order a prime q.
Pairing computation is an expensive operation. However, they enable a range of interesting
cryptographic primitives which can be used for scaling Cardano and many other use cases. We now
provide a list of use cases of pairings as well as an estimated operation count to understand the number of
'expensive' operations that need to be computed for each of them (a preliminary benchmark can be found in
Section 'Costing of function').

- **Sidechains** are a crucial component for the scalability of Cardano, and its interoperability with other
chains/tokens/smart contracts. However, sidechains need to periodically commit their state to the Cardano mainnet to
provide the same security guarantees as the latter. This periodical commitment is performed through a threshold
signature by the dynamic committee of the Sidechain. The most interesting construction for medium sized committees
is ATMS, presented in the paper [Proof of Stake Sidechains](https://cointhinktank.com/upload/Proof-of-Stake%20Sidechains.pdf),
in Section 5.3, which requires pairings. We have yet not found an efficient solution that does not require pairings.
ATMS is a k-of-t threshold signature scheme (meaning that we need at least k signers to participate).
- **Zero Knowledge Proofs** are an incredibly powerful tool. There exist different types of zero knowledge proofs,
but the most succinct (and cheaper to verify) rely on pairings for verification. Zero knowledge proofs can be used
to make Mithril certificates, or sidechain checkpoints even more succinct, or to create layer 2 solutions to provide
scalability (by the means of [ZK-Rollups](https://ethereum.org/en/developers/docs/scaling/layer-2-rollups/), which
are used to scale Ethereum, e.g. [Loopring](https://loopring.org/#/), [zkSync](https://zksync.io/) or
[Aztec](https://aztec.network/) among others). Plonk verification is quite complex, and differs depending on the
number of custom gates. Implementations may differ, and adding custom gates or blinders will affect these estimates.
Pairing evaluations would not be affected, but scalar multiplications and additions over G1 will increase linearly with
respect to the additional gates and blinders. In general, it is not expected that the number of custom gates is larger
than a few dozens. In this section we expose the verifier complexity as described
[here](https://eprint.iacr.org/2019/953.pdf) (version last checked April 2022). We count every challenge computation
as a single hash evaluation. We omit the `scalar X scalar` multiplications and additions.We assume that point validation
is performed by the external C library, and therefore do not count that in this estimate. The computation is the following:
- 6 hash computations
- 1 vanishing polynomial (1 scalar exponentiation)
- Quotient polynomial evaluation (1 scalar inverse)
- First part of batch poly commitment (9 scalar mults and 9 additions in G1)
- Fully batched poly commitment (5 scalar mults and 5 additions over G1)
- Group encoded batch verification (1 scalar mult over G1)
- Batch-validate all evaluations (3 scalar mults and 4 additions over G1, plus two Miller loops + final verify)
- **Hydra** is another crucial component for scalability of Cardano (there is a series of blog posts available, with a
good summary of the solution [here](https://iohk.io/en/blog/posts/2021/09/17/hydra-cardano-s-solution-for-ultimate-scalability/)).
Hydra relies on a multisignature scheme, where all participants of the side channel need to agree on the new state.
This can be achieved with non-pairing friendly curves (as it is currently designed), but pairing based signature schemes
provide much more elegant constructions that reduce interaction among signers. Moreover, Hydra tails could benefit from
SNARKs for proving correct spending of a set of transactions. For costing multisignatures we use BLS, whose verification
is relatively simple. One only needs to compute 2 Miller loops and a final verify operation.Some applications might be
interested in a smart contract aggregating signatures or keys. For this, we require n
group additions over G1 and G2 respectively, for n the number of submitted signatures.
- **Mithril** currently does not require plutus support. However, Mithril, as a technology, allows for signature generation
representing all stakeholders of Cardano (or any proof of stake system). These types of certificates might eventually
be used for certifying sidechains, and plutus support will be crucial. Again, Mithril relies on pairing based signatures.
Let k be the number of submitted signatures (as above, k is a security parameter determining the number of
required signatures). However, in mithril k << N where N is the total number of eligible signers. Mithril
verification goes as follows:
- Check the k is large enough
- k G1 additions
- k G2 additions
- k (log_2(N) hash computations + equality checks)
- 2 Miller loops + final verify
- **ATALA** is a decentralized identification mechanism. One of the properties they want to provide is anonymity: users
can selectively disclose attributes of their certificate or prove statements regarding them without disclosing their identity.
Up to date, the most recent, efficient and interesting solutions to provide these are pairing based
([Hyperledger Fabric/Idemix standardisation effort](https://hyperledger-fabric.readthedocs.io/en/release-2.2/idemix.html#underlying-cryptographic-protocols),
[coconut credentials used by Nym](https://blog.nymtech.net/nyms-coconut-credentials-an-overview-4aa4e922cd51), among
others). We use Coconut certs as an example. There is no decision on what type of construction we will
eventually use. Coconut credentials (and other types of credentials we are looking at) are built in a way that
it is efficient to prove statements about attributes. To this end, it is required to build, and be able to verify,
relations over discrete logarithm values. However, for sake of simplicity in this computation, we consider the
simplest form of anonymous credentials, which contains no attributes. We note that proving statements regarding
attributes does not require further pairing evaluations.
- Proof verification reduces to a relation involving elements in G1 and G2. This verification does not require the
    pairing evaluation, but does require G1 and G2 additions and multiplications.
- 2 Miller loops + final verify

## Specification
We now provide the technical specification.

### Names and types/kinds for the new functions or types
The added types will be the following, all of which can be represented as a byte array. Even if these types are
equivalent to byte arrays of a given size, it makes sense to include these types, to enforce deserialization, and
therefore some checks on the data used by the smart contract. In particular, `bls12_381_G1_element` and
`bls12_381_G2_element` can only be generated
with byte arrays that represent a point which is part of the prime order subgroup. On the other hand,
`bls12_381_MlResult` can only be generated as a result of the `bls12_381_millerLoop` computatio.

- `bls12_381_G1_element`
- `bls12_381_G2_element`
- `bls12_381_MlResult`

We need to support the binary operation of G1 and G2 (which are additive groups), as well as the binary operation
over MlResult (which is represented as a multiplicative group). We also want to enable hashing to G1 and G2.
In particular, we expose the hash to curve (which we denote with `hashToGroup`) algorithm as described in
[hash to curve draft](https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-hash-to-curve#section-8),
using `BLS12381G1_XMD:SHA-256_SSWU_RO_` and `BLS12381G2_XMD:SHA-256_SSWU_RO_` for G1 and G2 respectively.
We do not include the type of the Scalar, nor operations related to it. These type of operations can already be
performed with the `Integer` type. In particular, we need the following functions:

- Group operations:
- `bls12_381_G1_add :: bls12_381_G1_element -> bls12_381_G1_element -> bls12_381_G1_element`
- `bls12_381_G1_scalarMul :: Integer -> bls12_381_G1_element -> bls12_381_G1_element`
- `bls12_381_G1_neg :: bls12_381_G1_element -> bls12_381_G1_element`
- `bls12_381_G2_add :: bls12_381_G2_element -> bls12_381_G2_element -> bls12_381_G2_element`
- `bls12_381_G2_scalarMul :: Integer -> bls12_381_G2_element -> bls12_381_G2_element`
- `bls12_381_G2_neg :: bls12_381_G2_element -> bls12_381_G2_element`
- `bls12_381_mulMlResult :: bls12_381_MlResult -> bls12_381_MlResult -> bls12_381_MlResult`
- Pairing operations:
- `bls12_381_millerLoop :: bls12_381_G1_element -> bls12_381_G2_element -> bls12_381_MlResult`
- `bls12_381_finalVerify :: bls12_381_MlResult -> bls12_381_MlResult -> Bool` This performs the final
  exponentiation (see section `An important note on MlResult elements` below).
- Hash to curve. We include hash-to-curve functions, as per [Hashing to Elliptic Curves](https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-hash-to-curve)
  internet draft. Refer to [this](#hash-to-curve) section for further details:
- `bls12_381_G1_hashToGroup :: ByteString -> ByteString -> bls12_381_G1_element`
- `bls12_381_G2_hashToGroup :: ByteString -> ByteString -> bls12_381_G2_element`

On top of the elliptic curve operations, we also need to include deserialization functions, and equality definitions
among the G1 and G2 types.

- Deserialisation (more information of the choice of compressed form over uncompressed form [here](#compressed-vs-decompressed)):
- `bls12_381_G1_compress :: bls12_381_G1_element -> ByteString`
- `bls12_381_G1_uncompress :: ByteString -> bls12_381_G1_element`
- `bls12_381_G2_compress :: bls12_381_G2_element -> ByteString`
- `bls12_381_G2_uncompress :: ByteString -> bls12_381_G2_element`
- Equality functions:
- `bls12_381_G1_equal :: bls12_381_G1_element -> bls12_381_G1_element -> Bool`
- `bls12_381_G2_equal :: bls12_381_G2_element -> bls12_381_G2_element -> Bool`

This makes a total of 17 new functions and three new types.

We follow the [ZCash Bls12-381 specification](https://github.com/supranational/blst#serialization-format) for the
serialization of elements:
- Fq elements are encoded in big-endian form. They occupy 48 bytes in this form.
- Fq2 elements are encoded in big-endian form, meaning that the Fq2 element c0 + c1 * u is represented by the Fq
  element c1 followed by the Fq element c0. This means Fq2 elements occupy 96 bytes in this form.
- The group G1 uses Fq elements for coordinates. The group G2 uses Fq2 elements for coordinates.
- G1 and G2 elements are encoded in compressed form (just the x-coordinate). G1 elements occupy 48 bytes and
  G2 elements occupy 96 bytes.

The most-significant three bits of a G1 or G2 encoding should be masked away before the coordinate(s) are
interpreted. These bits are used to unambiguously represent the underlying element:
- The most significant bit, when set, indicates that the point is in compressed form.
- The second-most significant bit indicates that the point is at infinity. If this bit is set, the remaining bits of
  the group element's encoding should be set to zero.
- The third-most significant bit is set if (and only if) this point is in compressed form, and it is not the point at
  infinity and its y-coordinate is the lexicographically largest of the two associated with the encoded x-coordinate.

We include the serialisation of the generator of G1 and the generator of G2:
- generator G1:
```
[151, 241, 211, 167, 49, 151, 215, 148, 38, 149, 99, 140, 79, 169, 172, 15, 195, 104, 140, 79, 151, 116, 185,
5, 161,78, 58, 63, 23, 27, 172, 88, 108, 85, 232, 63, 249, 122, 26, 239, 251, 58, 240, 10, 219, 34, 198, 187]
```
- generator G2:
```
[147, 224, 43, 96, 82, 113, 159, 96, 125, 172, 211, 160, 136, 39, 79, 101, 89, 107, 208, 208, 153, 32, 182,
26, 181, 218, 97, 187, 220, 127, 80, 73, 51, 76, 241, 18, 19, 148, 93, 87, 229, 172, 125, 5, 93, 4, 43, 126,
2, 74, 162, 178, 240, 143, 10, 145, 38, 8, 5, 39, 45, 197, 16, 81, 198, 228, 122, 212, 250, 64, 59, 2, 180,
81, 11, 100, 122, 227, 209, 119, 11, 172, 3, 38, 168, 5, 187, 239, 212, 128, 86, 200, 193, 33, 189, 184]
```

#### Hash to curve
We expose the hash-to-curve functions following the [Hashing to Elliptic Curves](https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-hash-to-curve)
internet draft. The function signature takes as input two `ByteString`s and returns a point. The first
`ByteString` is the message to be hashed, while the second `ByteString` is the Domain Separation Tag (DST).
For more information on the DST, see [section 3.1](https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-hash-to-curve#name-domain-separation-requireme)
of the internet draft. We limit the DST to be at most 255 bytes, following the standard specification. If
applications require a domain separation tag that is longer than 255 bytes, they should convert it to a smaller
DST following the instructions of the standard draft (see [section 5.3.3](https://datatracker.ietf.org/doc/html/draft-irtf-cfrg-hash-to-curve#name-using-dsts-longer-than-255-)).

Some libraries expose the possibility to use yet another `ByteString` when calling the hash-to-curve function.
See for example the [`blst` library](https://github.com/supranational/blst/blob/master/src/hash_to_field.c#L121).
We choose not to include this extra `ByteString` in the function signature, because it is not part of the standard
draft. In the case where we want to match a hash that did use this `aug` `ByteString`, one simply needs to prepend
that value to the message. One can verify that by running the test-vector generation script introduced in
[cardano-base](https://github.com/input-output-hk/cardano-base/blob/master/cardano-crypto-tests/bls12-381-test-vectors/src/main.rs#L222..L231).

#### Compressed vs Decompressed
To recap, we have types `bls12_381_G1_element` and `bls12_381_G2_element` each of which is essentially a pair
of values `(x,y)` that satisfy an equation of the form `y^2 = x^3+ax+b`.  The blst library provides two
serialisation formats for these:

- The serialised format, where you have a bytestring encoding both the `x` and `y` coordinates of a point.
  A serialised `bls12_381_G1_element` takes up 96 bytes and a serialised `bls12_381_G2_element` takes up 192
  bytes.
- The compressed format, where you have a bytestring that only contains the `x` coordinate.  When you
  uncompress a compressed point to get an in-memory point, the `y` coordinate has to be calculated from
  the equation of the curve.  A compressed `bls12_381_G1_element` takes up 48 bytes and a compressed
  `bls12_381_G2_element` takes 92 bytes.

The PLC implementation currently uses the compressed format for serialising `bls12_381_G1_element` and
`bls12_381_G1_element`s.  There are at least three places where this is (or could be) used:

- Storing a group element as a bytestring inside a Data object which will be passed as a parameter to a script.
- During flat serialisation of PLC scripts (constants from G1 and G2 are converted into bytestrings and then
  flat deals with these as usual).
- In the concrete PLC/UPLC syntax, where constants are written as hex strings representing compressed points.

The serialised format could also be used for all of these.  The advantage of doing that is that deserialisation
is much cheaper than uncompression (which involves calculating a square root in a finite field, which is
expensive in general), but the disadvantage is that the serialised format requires twice as much space as
the compressed form.  We ran some benchmarks to determine CPU costs (in ExUnits) for both
deserialisation, and uncompression and came up with the following:
```
bls12_381_G1_deserialize : 701442
bls12_381_G1_uncompress  : 16511372

bls12_381_G2_deserialize : 1095773
bls12_381_G2_uncompress  : 33114723
```

For G1 uncompression is about 23 times more expensive than deserialisation, and for G2 uncompression is about
30 times more expensive than deserialisation.  The maximum CPU budget per script is currently 1,000,000,000,
so a single G2 uncompression is about 0.3% of the total allowance, whereas a G2 deserialisation is about 0.01%.
This might seem like a compelling reason to prefer serialisation over compression, but our claim is that the time
saving is not worthwhile because you can't fit enough serialised points into a script to make the speed gain
significant. The bls12-381-costs program runs a number of benchmarks for execution costs of scripts that exercise
the BLS builtins. One of these creates UPLC scripts which include varying numbers of compressed points and
at run-time uncompresses them and adds them all together; bls-381-costs runs these scripts and prints out their
costs as fractions of the maximum CPU budget and maximum script size (currently 16384). Here are the results:

Uncompress n G1 points and add the results
```
## n     script size             CPU usage               Memory usage

- 68   (0.4%)             100   (0.0%)             100   (0.0%)
 10     618   (3.8%)       185801250   (1.9%)           45642   (0.3%)
 20    1168   (7.1%)       371912820   (3.7%)           88002   (0.6%)
 30    1718  (10.5%)       558024390   (5.6%)          130362   (0.9%)
 40    2268  (13.8%)       744135960   (7.4%)          172722   (1.2%)
 50    2818  (17.2%)       930247530   (9.3%)          215082   (1.5%)
 60    3368  (20.6%)      1116359100  (11.2%)          257442   (1.8%)
 70    3918  (23.9%)      1302470670  (13.0%)          299802   (2.1%)
 80    4468  (27.3%)      1488582240  (14.9%)          342162   (2.4%)
 90    5018  (30.6%)      1674693810  (16.7%)          384522   (2.7%)
100    5568  (34.0%)      1860805380  (18.6%)          426882   (3.0%)
110    6118  (37.3%)      2046916950  (20.5%)          469242   (3.4%)
120    6668  (40.7%)      2233028520  (22.3%)          511602   (3.7%)
130    7218  (44.1%)      2419140090  (24.2%)          553962   (4.0%)
140    7768  (47.4%)      2605251660  (26.1%)          596322   (4.3%)
150    8318  (50.8%)      2791363230  (27.9%)          638682   (4.6%)
```

Uncompress n G2 points and add the results
```
## n     script size             CPU usage               Memory usage

- 68   (0.4%)             100   (0.0%)             100   (0.0%)
 10    1098   (6.7%)       363545910   (3.6%)           45984   (0.3%)
 20    2128  (13.0%)       728715130   (7.3%)           88704   (0.6%)
 30    3158  (19.3%)      1093884350  (10.9%)          131424   (0.9%)
 40    4188  (25.6%)      1459053570  (14.6%)          174144   (1.2%)
 50    5218  (31.8%)      1824222790  (18.2%)          216864   (1.5%)
 60    6248  (38.1%)      2189392010  (21.9%)          259584   (1.9%)
 70    7278  (44.4%)      2554561230  (25.5%)          302304   (2.2%)
 80    8308  (50.7%)      2919730450  (29.2%)          345024   (2.5%)
 90    9338  (57.0%)      3284899670  (32.8%)          387744   (2.8%)
100   10368  (63.3%)      3650068890  (36.5%)          430464   (3.1%)
110   11398  (69.6%)      4015238110  (40.2%)          473184   (3.4%)
120   12428  (75.9%)      4380407330  (43.8%)          515904   (3.7%)
130   13458  (82.1%)      4745576550  (47.5%)          558624   (4.0%)
140   14488  (88.4%)      5110745770  (51.1%)          601344   (4.3%)
150   15518  (94.7%)      5475914990  (54.8%)          644064   (4.6%)
```

It's clear from these figures that the limiting factor is the script size: about 300 G1 points or 150 G2 points
can be processed in a single script before exceeding the script size limit, but the maximum CPU usage is only 55%
of the the maximum CPU budget. If the serialisation (involving both x- and y-coordinates) was used instead then
there would be some saving in execution time, but a single script would only be able to process about half as
many points and it's unlikely that the time savings would compensate for that. For example, uncompressing 50 G2
points would cost about 1,655,000,000 CPU ExUnits and deserialising them would cost about 54,788,000, which is
1,600,000,000 ExUnits cheaper. At the time of writing, this equates to 0.27 Ada. However, 50 serialised G2
points take up 4800 more bytes than 50 compressed ones, and the extra bytes would cost 0.36 Ada. Thuis using
serialisation would cost 0.09 Ada more than using compression for the same number of points.

In summary: even though uncompression is a lot more expensive per point than deserialisation, the size savings
due to compression actually outweigh the speed gains due to serialisation because bytes per script are a lot
more expensive than ExUnits per script in real terms. For this reason, we propose to support the compressed
format and only the compressed format.

#### An important note on MlResult elements
We intentionally limit what can be done with the MlResult element. In fact, the only way that one can generate a
MlResult element is using the `bls12_381_millerLoop` function. Then these elements can only be used for operations among
them (multiplication) and a final equality check (denoted `finalVerify`). In other words, MlResult elements are only there to eventually
perform some equality checks. We thought of including the inverse
function to allow division, but given that we simply allow for equality checks, if one needs to divide by `A`,
then `A` can simply move to the other side of the equality sign. These limitations allow us for a performance
trick, which is also used for the verification of BLS signatures. In a nutshell, a pairing is divided into two
operations: (i) Miller loop, and (ii) final exponentiation. Both are expensive, but what we can do is first
compute the Miller loop, which already maps two points from G1 and G2 to MlResult. Then we can use this result
to perform the operations over these points (multiplications). Finally, when we want to check for equality, we invert
one of the two points (or equivalently in this case we compute the conjugate), and multiply the result by the
other point. Only now we compute the final exponentiation
and verify that the result is the identity element. In other words:
- the 'partial pairing' function, `bls12_381_millerLoop` simply
computes the Miller loop
- the equality check function, `bls12_381_finalVerify`, first
computes an inverse, then a multiplication, a final exponentiation and checks that the element is the identity.

While the results of the Miller loop are already elements in Fp12, they are not necessarily part of the group. This
is why we call the type used in the built-ins `bls12_381_MlResult` rather than `bls12_381_GT`.

Using the estimates in the section `Costing of functions`, we can see how this representation of
GT elements is beneficial. Assume that we want to check the following relation:
`e(A,B) * e(C,D) =? e(E,F) * e(G,H)`. The Miller loop takes around 330us, and the final exponentiation
around 370us. A full pairing would be 700us, and therefore checking this relation would cost around
2,8ms. If, instead, we break down the pairing into a Miller loop and a final exponentiation, and only
compute the latter once, the cost of the relation above would be 330 * 4 + 370 = 1.7ms.

For this reason it is important to limit what can be done with `bls12_381_MlResult`, as the pairing really is not the full
pairing operation, but only the Miller loop.
### Source implementation
- [BLST](https://github.com/supranational/blst) library, providing the algebraic operations.
- [cardano-base](https://github.com/input-output-hk/cardano-base/tree/master/cardano-crypto-class/src/Cardano/Crypto/EllipticCurve)
with the haskell FFI to the BLST library.
- [plutus](https://github.com/input-output-hk/plutus/pull/5231)

Other libraries of interest
- [Ethereum support for BLS12-381](https://eips.ethereum.org/EIPS/eip-2537). Not directly relevant as this is an
Ethereum Improvement Proposal for a precompiled solidity contracts.

### Comparison with existing functions
We present what would be the alternatives of using pairings in the different use cases presented above.

- Sidechain bridges using the current technology would rely on either of the two possibilities:
- Require the bridge committee to interact during signature, or to rely on a precomputation phase. Current
  solutions only support non-robust signature schemes, meaning that if one signer misbehaves, the whole signature
  procedure needs to be restarted. This could seriously hinder sidechains.
- Non-aggregation of signatures. This would result in a linear "checkpoint certificate" with respect to the
    number of signers (both in communication and computation complexity). Basically, all committee members need to
    submit their signature, and the smart contract needs to verify all ed25519 signatures.
- Zero Knowledge Proofs cannot be verified with current functions available in Plutus. There exist proofs that can
be instantiated over non-pairing friendly curves, but these result in logarithmic sized proofs and linear verification
with respect to the computation to prove, while solutions that rely on pairings can be represented more concisely, and
are cheaper to verify.

### Reason for exposing curve operations API
One might be concerned of why we are exposing such low-level primitives, instead of exposing higher level protocol
functions, such as `VerifyBlsSignature` or `VerifyZKP`. The motivation behind that is because pairings can enable a
big number of use cases, and covering all of those can considerably extend the list of required functions.

### Curve specifications
The BLS12-381 curve is fully defined by the following set of parameters (coefficient A=0 for all BLS12 curves). Taken from
[EIP 2537](https://eips.ethereum.org/EIPS/eip-2537):
```
Base field modulus = 0x1a0111ea397fe69a4b1ba7b6434bacd764774b84f38512bf6730d2a0f6b0f6241eabfffeb153ffffb9feffffffffaaab
B coefficient = 0x000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000004
Main subgroup order = 0x73eda753299d7d483339d80809a1d80553bda402fffe5bfeffffffff00000001
Extension tower
Fp2 construction:
Fp quadratic non-residue = 0x1a0111ea397fe69a4b1ba7b6434bacd764774b84f38512bf6730d2a0f6b0f6241eabfffeb153ffffb9feffffffffaaaa
Fp6/Fp12 construction:
Fp2 cubic non-residue c0 = 0x000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001
Fp2 cubic non-residue c1 = 0x000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001
Twist parameters:
Twist type: M
B coefficient for twist c0 = 0x000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000004
B coefficient for twist c1 = 0x000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000004
Generators:
G1:
X = 0x17f1d3a73197d7942695638c4fa9ac0fc3688c4f9774b905a14e3a3f171bac586c55e83ff97a1aeffb3af00adb22c6bb
Y = 0x08b3f481e3aaa0f1a09e30ed741d8ae4fcf5e095d5d00af600db18cb2c04b3edd03cc744a2888ae40caa232946c5e7e1
G2:
X c0 = 0x024aa2b2f08f0a91260805272dc51051c6e47ad4fa403b02b4510b647ae3d1770bac0326a805bbefd48056c8c121bdb8
X c1 = 0x13e02b6052719f607dacd3a088274f65596bd0d09920b61ab5da61bbdc7f5049334cf11213945d57e5ac7d055d042b7e
Y c0 = 0x0ce5d527727d6e118cc9cdc6da2e351aadfd9baa8cbdd3a76d429a695160d12c923ac9cc3baca289e193548608b82801
Y c1 = 0x0606c4a02ea734cc32acd2b02bc28b99cb3e287e85a763af267492ab572e99ab3f370d275cec1da1aaa9075ff05f79be
Pairing parameters:
|x| (Miller loop scalar) = 0xd201000000010000
x is negative = true
```
One should note that base field modulus is equal to 3 mod 4 that allows an efficient square root extraction.

### Rationale: how does this CIP achieve its goals?
The reason for choosing the BLS12-381 over the BN256 curve is that the former is claimed to provide 128 bits of security,
while the latter was reduced to 100 bits of security after the extended number field sieve (a new algorithm to compute
the discrete logarithm) was [shown to reduce the security](https://eprint.iacr.org/2016/1102.pdf) of these curves.

An [EIP](https://eips.ethereum.org/EIPS/eip-2537) for precompiles of curve BLS12-381 already exists, but has been
stagnant for a while. Nonetheless, Zcash, MatterLabs and Consensys support BLS12-381 curve, so it is certainly widely
used in the space.

Further reading regarding curve BLS12-381 can be found [here](https://hackmd.io/@benjaminion/bls12-381) and the
references thereof cited.

### Trustworthiness of implementations
- [BLST](https://github.com/supranational/blst) library—
[audited by NCC Group](https://research.nccgroup.com/wp-content/uploads/2021/01/NCC_Group_EthereumFoundation_ETHF002_Report_2021-01-20_v1.0.pdf)
and [being formally verified](https://github.com/GaloisInc/BLST-Verification) by Galois.

We also have an [analysis by Duncan Coutts](https://github.com/cardano-foundation/CIPs/pull/220#issuecomment-1508306896https://github.com/cardano-foundation/CIPs/pull/220#issuecomment-1508306896)
for effects of including this library for continuous integration and long term maintainability:

In addition to security audits on the proposed implementation, it is important that we review the inclusion of
the library for practical issues of build systems, and long term maintainability.

In particular in this case the library will be used in the chain format and in chain verification. This means
we have special constraints that the format and verification logic must never change. Or more specifically:
it must be possible forever to be able to verify the existing uses on the chain. So even if there are upgrades
or format changes in future, it must still be possible to decode and verify the old uses. This is not a
constraint that most software has, and so many libraries are not designed to work within that constraint.

The proposed implementation is https://github.com/supranational/blst

- The implementation is in C and assembly for x86 and ARM v8. This is good for the existing Haskell
  implementation of the Cardano node because integrating with C libraries is relatively straightforward, it's
  well supported by the build and CI system and does not pull in too many extra dependencies. (Contrast for
  example if it were a Rust library where we would have serious practical problems on this front.)
- The implementation appears to have been designed with blockchain implementations in mind. This is a good
  sign for the long term maintainability because it probably means that the library authors will continue to
  support the existing format and semantics even if there are changes or improvements.
- The implementation is claimed to be compliant with draft IETF specifications. There is a risk that the
  specs may change before being declared final, and the library may be updated to follow. There is a risk
  that the Cardano community will have to support the old version forever. Though given the point above,
  it's probably the case that library updates would still provide compatibility with the current IETF drafts
  and serialisation formats.

So overall this seems like something the core development team and Cardano community could support long term
without too high a cost. Though we should be aware of the risk that we may have to support an old version of
the library, if the library gets changed in incompatible ways.

To ensure no compatibility surprises, it is worth considering forking the repository at a specific commit/version
and building the node using that version. This is to guarantee the version remains available. Then for any future
updates, the fork repo could be updated to a new version explicitly, with appropriate compatibility checks to
ensure the existing on-chain uses are still compatible.
### Costing of function

We did some [preliminary costing](https://github.com/input-output-hk/plutus/tree/kwxm/BLS12_381/prototype/plutus-benchmark/bls-benchmarks)
of the BLS functions and the following cost of the new built-in functions:
```
bls12_381_G1_compress    : 3341914
bls12_381_G1_uncompress  : 16511372
bls12_381_G1_add         : 1046420
bls12_381_G1_equal       : 545063
bls12_381_G1_hashToCurve : 66311195 + 23097*x
bls12_381_G1_scalarMul   : 94607019 + 87060*x (we use 94955259, with x = 4)
bls12_381_G1_neg         : 292890
bls12_381_G2_compress    : 3948421
bls12_381_G2_uncompress  : 33114723
bls12_381_G2_add         : 2359410
bls12_381_G2_equal       : 1102635
bls12_381_G2_hashToCurve : 204557793 + 23271*x
bls12_381_G2_scalarMul   : 190191402 + 85902*x
bls12_381_G2_neg         : 307813
bls12_381_GT_finalVerify : 388656972
bls12_381_GT_millerLoop  : 402099373
bls12_381_GT_mul         : 2533975
blake2b_256                     : 358499 + 10186*x (521475, with x = 16)
addInteger                      : 85664 + 712*max(x,y) (88512, with x = y = 4)
multiplyInteger                 : 1000 + 55553*(x+y) (641924, with x = y = 4, and we include the price of modular reduction, as we need one per mult)
divideInteger                   : if x>y
then  809015 + 577*x*y
else  196500
modInteger                      : 196500
expInteger                      : We estimate 32 mults and adds (23373952)
```

Using these preliminary benchmarks, we performed some estimates of how much it would cost to verify Groth16 or Plonk
proofs using the bindings. Details can be found [here](https://hackmd.io/X80zXoxWQrqSLaO0nizjaA). The estimates for
Groth16 (~23% of the execution budget required for a proof verification) were confirmed by the benchmarks shared above.

### Plutus implementor
IOG internal. PR open for Plutus bindings https://github.com/input-output-hk/plutus/pull/5231

## Path to Active

### Acceptance Criteria

- [x] Confirmation from IOG Plutus Team that this curve support is included in a scheduled Plutus release.
- Included within the Chang #1 hardfork

### Implementation Plan

- [x] Confirmation from IOG Plutus Team that [CIP-0035 Processes](https://github.com/cardano-foundation/CIPs/tree/master/CIP-0035#processes) for changes to Plutus have been satisfied.

## Copyright

This CIP is licensed under [Apache-2.0](https://www.apache.org/licenses/LICENSE-2.0).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-1694/README.md
---

- --
CIP: 1694
Title: A First Step Towards On-Chain Decentralized Governance
Status: Active
Category: Ledger
Authors:
- Jared Corduan <jared.corduan@iohk.io>
- Andre Knispel <andre.knispel@iohk.io>
- Matthias Benkort <matthias.benkort@cardanofoundation.org>
- Kevin Hammond <kevin.hammond@iohk.io>
- Charles Hoskinson <charles.hoskinson@iohk.io>
- Samuel Leathers <samuel.leathers@iohk.io>
Discussions:
- <https://github.com/cardano-foundation/CIPs/pull/380>
- <https://forum.cardano.org/t/swarm-session-cip-1694/114453>
- <https://twitter.com/IOHK_Charles/status/1632211417221701632>
- <https://twitter.com/RichardMcCrackn/status/1632514347195850752>
- <https://twitter.com/RichardMcCrackn/status/1633135124500865024>
- <https://twitter.com/_KtorZ_/status/1632356766368382976>
- <https://twitter.com/_KtorZ_/status/1630087586193584128>
- <https://twitter.com/_KtorZ_/status/1631430933219012608>
- <https://twitter.com/technologypoet/status/1632158736985866241>
- <https://twitter.com/danny_cryptofay/status/1631606919071776768>
- <https://www.youtube.com/watch?v=2hCnmMG1__8>
- <https://www.youtube.com/watch?v=KiLhhOVXQOg>
- <https://github.com/cardano-foundation/CIPs/pull/916>
Created: 2022-11-18
License: CC-BY-4.0
- --

## Abstract

We propose a revision of Cardano's on-chain governance system to support the new requirements for Voltaire.
The existing specialized governance support for protocol parameter updates and MIR certificates will be removed,
and two new fields will be added to normal transaction bodies for:

1. governance actions
2. votes

- *Any Cardano user** will be allowed to submit a **governance action**.
We also introduce three distinct governance bodies that have specific functions in this new governance framework:

1. a constitutional committee
2. a group of delegated representatives (henceforth called **DReps**)
3. the stake pool operators (henceforth called **SPOs**)

Every governance action must be ratified by at least two of these three governance bodies using their on-chain **votes**.
The type of action and the state of the governance system determines which bodies must ratify it.

Ratified actions are then **enacted** on-chain, following a set of well-defined rules.

As with stake pools, any Ada holder may register to be a DRep and so choose to
represent themselves and/or others.  Also, as with stake pools, Ada holders may, instead, delegate their voting
rights to any other DRep.
Voting rights will be based on the total Ada that is delegated, as a whole number of Lovelace.

The most crucial aspect of this proposal is therefore the notion of **"one Lovelace = one vote"**.

For the many contributors to this proposal, see [Acknowledgements](#acknowledgements).
## Motivation: why is this CIP necessary?

- [Goal](#goal)
- [Current design](#current-governance-mechanism-design)
- [Shortcomings of the Shelley governance design](#shortcomings-of-the-shelley-governance-design)
- [Out of scope](#out-of-scope)

### Goal

We're heading into the age of Voltaire, laying down the foundations for decentralized decision-making.
This CIP describes a mechanism for on-chain governance that will underpin the Voltaire phase of Cardano.
The CIP builds on and extends the original Cardano governance scheme that was based on a fixed number of governance keys.
It aims to provide a **first step** that is both valuable and, importantly, is also technically achievable
in the **near term** as part of the proposed Voltaire governance system.

It also seeks to act as a jumping-off point for continuing community input,
including on appropriate threshold settings and other on-chain settings.

Subsequent proposals may adapt and extend this proposal to meet emerging governance needs.

### Current governance mechanism design

The on-chain Cardano governance mechanism that was introduced in the Shelley ledger era is capable of:

1. modifying the values of the protocol parameters (including initiating "hard forks")
2. transferring Ada out of the reserves and the treasury (and also moving Ada between the reserves and the treasury)

In the current scheme, governance actions are initiated by special transactions that require `Quorum-Many` authorizations
from the governance keys (5 out of 7 on the Cardano mainnet)[^1].
Fields in the transaction body provide details of the proposed governance action:
either i) protocol parameter changes; or ii) initiating funds transfers.
Each transaction can trigger both kinds of governance actions, and a single action can have more than one effect (e.g. changing two or more protocol parameters).

- Protocol parameter updates use [transaction field nº6](https://github.com/input-output-hk/cardano-ledger/blob/8884d921c8c3c6e216a659fca46caf729282058b/eras/babbage/test-suite/cddl-files/babbage.cddl#L56) of the transaction body.
- Movements of the treasury and the reserves use [Move Instantaneous Rewards (abbrev. MIR) certificates](https://github.com/input-output-hk/cardano-ledger/blob/8884d921c8c3c6e216a659fca46caf729282058b/eras/babbage/test-suite/cddl-files/babbage.cddl#L180).

Properly authorized governance actions are applied on an epoch boundary (they are **enacted**).

#### Hard Forks

One of the protocol parameters is sufficiently significant to merit special attention:
changing the major protocol version enables Cardano to enact controlled hard forks.
This type of protocol parameter update therefore has a special status, since stake pools
must upgrade their nodes so they can support the new protocol version once the hard fork is enacted.

### Shortcomings of the Shelley governance design

The Shelley governance design was intended to provide a simple, transitional approach to governance.
This proposal aims to address a number of shortcomings with that design
that are apparent as we move into Voltaire.

1. The Shelley governance design gives no room for active on-chain participation of Ada holders.
While changes to the protocol are usually the results of discussions with selected community actors,
the process is currently driven mainly by the founding entities.
Ensuring that everyone can voice their concern is cumbersome, and can be perceived as arbitrary at times.

2. Movements from the treasury constitute a critical and sensitive topic.
However, they can be hard to track.  It is important to have more transparency
and more layers of control over these movements.

3. While they need to be treated specially by SPOs, hard forks are not differentiated from other protocol parameter changes.

4. Finally, while there is currently a somewhat common vision for _Cardano_ that is shared by its founding entities and also by many community members,
there is no clearly defined document where these guiding principles are recorded.
It makes sense to leverage the Cardano blockchain to record the shared Cardano ethos in an immutable fashion, as a formal Cardano Constitution.

### Out of scope

The following topics are considered to be out of the scope of this CIP.

#### The contents of the constitution

This CIP focuses only on on-chain mechanisms.  The provisions of the initial constitution are extremely important, as are any processes that
will allow it to be amended.  These merit their own separate and focused discussion.

#### The membership of the constitutional committee

This is an off-chain issue.

#### Legal issues

Any potential legal enforcement of either the Cardano protocol or the Cardano Constitution are completely out of scope for this CIP.


#### Off chain standards for governance actions

The Cardano community must think deeply about the correct standards and processes for handling the creation of the governance actions that are specified in this CIP.
In particular, the role of Project Catalyst in creating treasury withdrawal actions is completely outside the scope of this CIP.


#### Ada holdings and delegation

How any private companies, public or private institutions,  individuals etc. choose to hold or delegate their Ada, including delegation to stake pools or DReps, is outside the scope of this CIP.

## Specification

- [The Cardano Constitution](#the-cardano-constitution)
- [The constitutional committee](#the-constitutional-committee)
- [State of no-confidence](#state-of-no-confidence)
- [Constitutional committee keys](#constitutional-committee-keys)
- [Replacing the constitutional committee](#replacing-the-constitutional-committee)
- [Size of the constitutional committee](#size-of-the-constitutional-committee)
- [Terms](#terms)
- [Guardrails Script](#guardrails-script)
- [Delegated representatives (DReps)](#delegated-representatives-dreps)
- [Pre-defined Voting Options](#pre-defined-voting-options)
- [Registered DReps](#registered-dreps)
- [New stake distribution for DReps](#new-stake-distribution-for-dreps)
- [Incentives for Ada holders to delegate voting stake](#incentives-for-ada-holders-to-delegate-voting-stake)
- [DRep incentives](#drep-incentives)
- [Governance actions](#governance-actions)
- [Ratification](#ratification)
- [Requirements](#requirements)
- [Restrictions](#restrictions)
- [Enactment](#enactment)
- [Lifecycle](#lifecycle)
- [Content](#content)
- [Protocol parameter groups](#protocol-parameter-groups)
- [Votes](#votes)
- [Governance state](#governance-state)
- [Changes to the stake snapshot](#changes-to-the-stake-snapshot)
- [Definitions relating to voting stake](#definitions-relating-to-voting-stake)

### The Cardano Constitution

The Cardano Constitution is a text document that defines Cardano's shared values and guiding principles.
At this stage, the Constitution is an informational document that unambiguously captures the core values of Cardano
and acts to ensure its long-term sustainability.
At a later stage, we can imagine the Constitution perhaps evolving into a smart-contract based set of rules that drives the entire governance framework.
For now, however, the Constitution will remain an off-chain document whose hash digest value will be recorded on-chain.
As discussed above, the Constitution is not yet defined and its content is out of scope for this CIP.

<!--------------------------- Constitutional committee ------------------------>

### The constitutional committee

We define a _constitutional committee_ which represents a set of individuals or entities
(each associated with a Ed25519 or native or Plutus script credential) that are collectively responsible for **ensuring that the Constitution is respected**.

Though it **cannot be enforced on-chain**, the constitutional committee is **only** supposed to vote
on the constitutionality of governance actions (which should thus ensure the long-term sustainability of the blockchain) and should be replaced
(via the **no confidence** action) if they overstep this boundary.
Said differently, there is a social contract between the constitutional committee and the actors of the network.
Although the constitutional committee could reject certain governance actions (by voting 'No' on them),
they should only do so when those governance actions are in conflict with the Constitution.

For example, if we consider the hypothetical Constitution rule "The Cardano network must always be able to produce new blocks",
then a governance action that would reduce the maximum block size to `0` would be, in effect,
unconstitutional and so might not be ratified by the constitutional committee.  The rule does
not, however, specify the smallest acceptable maximum block size, so the constitutional committee would need to determine this number
and vote accordingly.

#### State of no-confidence

The constitutional committee is considered to be in one of the following two states at all times:

1. a normal state (i.e. a state of confidence)
2. a state of no-confidence

In a _state of no-confidence_, the current committee is no longer able to participate in governance actions
and must be replaced before any governance actions can be ratified (see below).

#### Constitutional committee keys

The constitutional committee will use a hot and cold key setup, similar to the existing "genesis delegation certificate" mechanism.

#### Replacing the constitutional committee

The constitutional committee can be replaced via a specific governance action
("Update committee", described below) that requires the approval of both
the **SPOs** and the **DReps**.
The threshold for ratification might be different depending on if the governance is
in a normal state or a state of no confidence.

The new constitutional committee could, in principle, be identical to or partially overlap the outgoing committee as long as the action is properly ratified.
This might happen, for example, if the electorate has collective confidence in all or part of the committee and wishes to extend its term of office.


#### Size of the constitutional committee

Unlike the Shelley governance design, the size of the constitutional committee is not fixed and can be any nonnegative number.
It may be changed whenever a new committee is elected ("Update committee").
Likewise, the committee threshold (the fraction of committee `Yes` votes that are required to ratify governance actions) is not fixed and
can also be varied by the governance action.
This gives a great deal of flexibility to the composition of the committee.
In particular, it is possible to elect an empty committee if the community wishes to abolish the constitutional committee entirely. Note that this is different from a state of no-confidence and still constitutes a governance system capable of enacting proposals.

There will be a new protocol parameter for the minimal size of the committee,
itself a nonnegative number called `committeeMinSize`.

#### Terms

Each newly elected constitutional committee will have a term.
Per-member terms allow for a rotation scheme, such as a third of the committee
expiring every year.
Expired members can no longer vote.
Member can also willingly resign early, which will be marked on-chain as an expired member.

If the number of non-expired committee members falls below the minimal
size of the committee, the constitutional committee will be unable to
ratify governance actions. This means that only governance actions
that don't require votes from the constitutional committee can still
be ratified.

For example, a committee of size five with a threshold of 60% a minimum size
of three and two expired members can still
pass governance actions if two non-expired members vote `Yes`.
However, if one more member expires then the constitutional committee becomes
unable to ratify any more governance actions.

The maximum term is a governance protocol parameter, specified as a number of epochs.
During a state of no-confidence, no action can be ratified,
so the committee should plan for its own replacement if it wishes to avoid disruption.

#### Guardrails Script

While the constitution is an informal, off-chain document, there will
also be an optional script that can enforce some guidelines. This script
acts to supplement the constitutional committee by restricting some
proposal types. For example, if the community wishes to have some hard
rules for the treasury that cannot be violated, a script that enforces
these rules can be voted in as the guardrails script.

The guardrails script applies only to protocol parameter update and
treasury withdrawal proposals.

<!---------------------------           DReps          ------------------------>

### Delegated representatives (DReps)

> **Warning**
> CIP-1694 DReps **should not be conflated** with Project Catalyst DReps.

#### Pre-defined Voting Options

In order to participate in governance, a stake credential must be delegated to a DRep.
Ada holders will generally delegate their voting rights to a registered DRep
that will vote on their behalf. In addition, two pre-defined voting options are available:

- `Abstain`

  If an Ada holder delegates to `Abstain`, then their stake is actively marked
  as not participating in governance.

  The effect of delegating to `Abstain` on chain is that the delegated stake *will not* be considered to be
  a part of the active voting stake.  However, the stake *will* be considered to be registered for the
  purpose of the incentives that are described in [Incentives for Ada holders to delegate voting stake](#incentives-for-ada-holders-to-delegate-voting-stake).

- `No Confidence`

  If an Ada holder delegates to `No Confidence`, then their stake is counted
  as a `Yes` vote on every `No Confidence` action and a `No` vote on every other action.
  The delegated stake *will* be considered part of the active voting stake.
  It also serves as a directly auditable measure of the confidence of Ada holders in the constitutional
  committee.


> **Note**
> The pre-defined voting options do not cast votes inside of transactions, their behavior is accounted for at the protocol level.
> The `Abstain` option may be chosen for a variety of reasons, including the desire to not
> participate in the governance system.

> **Note**
> Any Ada holder may register themselves as a DRep and delegate to themselves if they wish to actively participate in
> voting.

> **Note**
> Any wallet serving as the Registered Reward Wallet for a Stake Pool can be delegated to one of these Pre-defined Voting Options and doing so will serve as the default voting option selected by the SPO for all Governance Action votes, excepting Hard Fork Governance Actions. Due to the need for robust consensus around Hard Fork initiations, these votes must be met as a percentage of the stake held by all stake pools.

#### Registered DReps

In Voltaire, existing stake credentials will be
able to delegate their stake to DReps for voting purposes,
in addition to the current delegation to stake pools for block production.
DRep delegation will mimic the existing stake delegation mechanisms (via on-chain certificates).
Similarly, DRep registration will mimic the existing stake registration mechanisms.
Additionally, registered DReps will need to vote regularly to still be considered active.
Specifically, if a DRep does not submit any votes for `drepActivity`-many epochs, the DRep is considered inactive,
where `drepActivity` is a new protocol parameter.
Inactive DReps do not count towards the active voting stake anymore, and can become active again for
`drepActivity`-many epochs by voting on any governance actions or submitting a DRep update certificate.
The reason for marking DReps as inactive is so that DReps who stop participating but still have
stake delegated to them do not eventually leave the system in a state where no governance
action can pass.

Registered DReps are identified by a credential that can be either:

- A verification key (Ed25519)
- A native or Plutus script

The blake2b-224 hash digest of a serialized DRep credential is called the _DRep ID_.

The following new types of certificates will be added for DReps:
DRep registration certificates, DRep retirement certificates, and
vote delegation certificates.

##### DRep registration certificates

DRep registration certificates include:

- a DRep ID
- a deposit
- an optional anchor

An **anchor** is a pair of:

- a URL to a JSON payload of metadata
- a hash of the contents of the metadata URL

The structure and format of this metadata is deliberately left open in this CIP.
The on-chain rules will not check either the URL or the hash.
Client applications should, however, perform the usual sanity checks when fetching content from the provided URL.


##### DRep retirement certificates

DRep retirement certificates include:

- a DRep ID

Note that a DRep is retired immediately upon the chain accepting a retirement certificate,
and the deposit is returned as part of the transaction that submits the retirement certificate
(the same way that stake credential registration deposits are returned).

##### Vote delegation certificates

Vote delegation certificates include:

- the DRep ID to which the stake should be delegated
- the stake credential for the delegator

> **Note**
>
> DRep delegation always maps a stake credential to a DRep credential.
> This means that a DRep cannot delegate voting stake to another DRep.

##### Certificate authorization schemes

The authorization scheme (i.e. which signatures are required for registration, retirement or delegation) mimics the existing stake delegation certificate authorization scheme.

<!-- TODO: Provide CBOR specification in the annexe for those new certificates. -->


#### New stake distribution for DReps

In addition to the existing per-stake-credential distribution and the
per-stake-pool distribution, the ledger will now also determine the per-DRep stake
distribution. This distribution will determine how much stake each vote from a DRep
is backed by.

> **Warning**
>
> **Unlike** the distribution that is used for block production, we will always use the most
> current version of the per-DRep stake distribution as given on the epoch boundary.
>
> This means that **for any topic which individual voters care deeply about,
> they have time to delegate to themselves as a DRep and vote directly**.
> However, it means that there may be a difference between the stake that is used for block
> production and the stake that is used for voting in any given epoch.


#### Incentives for Ada holders to delegate voting stake

There will be a short [bootstrapping phase](#bootstrapping-phase) during which rewards will be earned
for stake delegation etc. and may be withdrawn at any time.
After this phase, although rewards will continue to be earned for block delegation etc., reward accounts will be
- *blocked from withdrawing any rewards** unless their associated stake credential is also delegated to a DRep or pre-defined voting option.
This helps to ensure high participation, and so, legitimacy.

> **Note**
>
> Even though rewards cannot be withdrawn, they are not lost.  As soon as a stake credential is delegated
> (including to a pre-defined voting option), the rewards can be withdrawn.

#### DRep incentives

DReps arguably need to be compensated for their work. Research on incentive models is still ongoing,
and we do not wish to hold up implementation of this CIP while this is resolved.

Our interim proposal is therefore to escrow Lovelace from the existing Cardano treasury until this
extremely important decision can be agreed on by the community, through the on-chain governance
mechanism that is being constructed.

Alternatively, DReps could pay themselves through instances of the "Treasury withdrawal" governance action.
Such an action would be auditable on-chain, and should reflect an off-chain agreement between DReps and delegators.

<!---------------------------           DReps          ------------------------>
<!---------------------------    Governance Actions    ------------------------>

### Governance actions

We define seven different types of **governance actions**.
A governance action is an on-chain event that is triggered by a transaction and has a deadline after which it cannot be enacted.

- An action is said to be **ratified** when it gathers enough votes in its favor (through the rules and parameters that are detailed below).
- An action that fails to be ratified before its deadline is said to have **expired**.
- An action that has been ratified is said to be **enacted** once it has been activated on the network.


| Action                                                        | Description                                                                                                              |
|:--------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------|
| 1. Motion of no-confidence                                    | A motion to create a _state of no-confidence_ in the current constitutional committee                                    |
| 2. Update committee and/or threshold and/or terms             | Changes to the members of the constitutional committee and/or to its signature threshold and/or terms                    |
| 3. New Constitution or Guardrails Script                      | A modification to the Constitution or Guardrails Script, recorded as on-chain hashes                                       |
| 4. Hard-Fork[^2] Initiation                                   | Triggers a non-backwards compatible upgrade of the network; requires a prior software upgrade                            |
| 5. Protocol Parameter Changes                                 | Any change to **one or more** updatable protocol parameters, excluding changes to major protocol versions ("hard forks") |
| 6. Treasury Withdrawals                                       | Withdrawals from the treasury                                                                                            |
| 7. Info                                                       | An action that has no effect on-chain, other than an on-chain record                                                     |

- *Any Ada holder** can submit a governance action to the chain.
They must provide a deposit of `govActionDeposit` Lovelace, which will be returned when the action is finalized
(whether it is **ratified** or has **expired**).
The deposit amount will be added to the _deposit pot_, similar to stake key deposits.
It will also be counted towards the stake of the reward address it will be paid back to, to not reduce the submitter's voting power to vote on their own (and competing) actions.

If a guardrails script is present, the transaction must include that
script in the witness set either directly, or via reference inputs,
and any other requirements that the guardrails script makes must be
satisfied.

Note that a motion of no-confidence is an extreme measure that enables Ada holders to revoke the power
that has been granted to the current constitutional committee.

> **Note**
> A **single** governance action might contain **multiple** protocol parameter updates. Many parameters are inter-connected and might require moving in lockstep.

#### Ratification

Governance actions are **ratified** through on-chain voting actions.
Different kinds of governance actions have different ratification requirements but always involve **two of the three** governance bodies,
with the exception of a hard-fork initiation and security-relevant protocol parameters, which requires ratification by all governance bodies.
Depending on the type of governance action, an action will thus be ratified when a combination of the following occurs:

- the constitutional committee approves of the action (the number of members who vote `Yes` meets the threshold of the constitutional committee)
- the DReps approve of the action (the stake controlled by the DReps who vote `Yes` meets a certain threshold of the total active voting stake)
- the SPOs approve of the action (the stake controlled by the SPOs who vote `Yes` meets a certain threshold of the total active voting stake, excepting Hard Fork Governance Actions)

> **Warning**
> As explained above, different stake distributions apply to DReps and SPOs.

A successful motion of no-confidence, update of the constitutional committee,
a constitutional change, or a hard-fork, delays
ratification of all other governance actions until the first epoch after their enactment. This gives
an updated constitutional committee enough time to vote on current proposals, re-evaluate existing proposals
with respect to a new constitution, and ensures that the in principle arbitrary semantic changes caused
by enacting a hard-fork do not have unintended consequences in combination with other actions.

##### Requirements

The following table details the ratification requirements for each governance action scenario. The columns represent:

- **Governance action type**<br/>
  The type of governance action. Note that the protocol parameter updates are grouped into four categories.

- **Constitutional committee (abbrev. CC)**<br/>
  A value of ✓ indicates that the constitutional committee must approve this action.<br/>
  A value of - means that constitutional committee votes do not apply.

- **DReps**<br/>
  The DRep vote threshold that must be met as a percentage of *active voting stake*.

- **SPOs**<br/>
  The SPO vote threshold which must be met as a certain threshold of the total active voting stake, excepting Hard Fork Governance Actions. Due to the need for robust consensus around Hard Fork initiations, these votes must be met as a percentage of the stake held by all stake pools. <br/>
  A value of - means that SPO votes do not apply.

| Governance action type                                               | CC | DReps    | SPOs     |
|:---------------------------------------------------------------------|:---|:---------|:---------|
| 1. Motion of no-confidence                                           | \- | $P_1$    | $Q_1$    |
| 2<sub>a</sub>. Update committee/threshold (_normal state_)           | \- | $P_{2a}$ | $Q_{2a}$ |
| 2<sub>b</sub>. Update committee/threshold (_state of no-confidence_) | \- | $P_{2b}$ | $Q_{2b}$ |
| 3. New Constitution or Guardrails Script                             | ✓  | $P_3$    | \-       |
| 4. Hard-fork initiation                                              | ✓  | $P_4$    | $Q_4$    |
| 5<sub>a</sub>. Protocol parameter changes, network group             | ✓  | $P_{5a}$ | \-       |
| 5<sub>b</sub>. Protocol parameter changes, economic group            | ✓  | $P_{5b}$ | \-       |
| 5<sub>c</sub>. Protocol parameter changes, technical group           | ✓  | $P_{5c}$ | \-       |
| 5<sub>d</sub>. Protocol parameter changes, governance group          | ✓  | $P_{5d}$ | \-       |
| 6. Treasury withdrawal                                               | ✓  | $P_6$    | \-       |
| 7. Info                                                              | ✓  | $100$    | $100$    |

Each of these thresholds is a governance parameter. There is one
additional threshold, `Q5`, related to security relevant protocol
parameters, which is explained below.
The initial thresholds should be chosen by the Cardano community as a whole.
All thresholds for the Info action are set to 100% since setting it any lower
would result in not being able to poll above the threshold.

Some parameters are relevant to security properties of the system. Any
proposal attempting to change such a parameter requires an additional
vote of the SPOs, with the threshold `Q5`.

The security relevant protocol parameters are:
- `maxBlockBodySize`
- `maxTxSize`
- `maxBlockHeaderSize`
- `maxValueSize`
- `maxBlockExecutionUnits`
- `txFeePerByte`
- `txFeeFixed`
- `utxoCostPerByte`
- `govActionDeposit`
- `minFeeRefScriptCostPerByte`

> **Note**
> It may make sense for some or all thresholds to be adaptive with respect to the Lovelace that is actively registered to vote.
> For example, a threshold could vary between 51% for a high level of registration and 75% for a low level registration.
> Moreover, the treasury threshold could also be adaptive, depending on the total Lovelace that is being withdrawn,
> or different thresholds could be set for different levels of withdrawal.

> **Note**
> To achieve legitimacy, the minimum acceptable threshold should be no less than 50% of the delegated stake.


##### Restrictions

Apart from _Treasury withdrawals_ and _Infos_, we include a mechanism for ensuring that governance
actions of the same type do not accidentally clash with each other in an unexpected way.

Each governance action must include the governance action ID for the most recently enacted action of its given type.
This means that two actions of the same type can be enacted at the same time,
but they must be *deliberately* designed to do so.


#### Enactment

Actions that have been ratified in the current epoch are prioritized as follows for enactment:

1. Motion of no-confidence
2. Update committee/threshold
3. New Constitution or Guardrails Script
4. Hard Fork initiation
5. Protocol parameter changes
6. Treasury withdrawals
7. Info

> **Note** _Info_ actions cannot be ratified or enacted, since they do not have any effect on the protocol.

##### Order of enactment

Governance actions are enacted in order of acceptance to the chain.
This resolves conflicts where, e.g. there are two competing parameter changes.

#### Lifecycle

Governance actions are checked for ratification only on an epoch boundary.
Once ratified, actions are staged for enactment.

All submitted governance actions will therefore either:

1. be **ratified**, then **enacted**
2. or **expire** after a number of epochs

In all of those cases, deposits are returned immediately.

All governance actions are enacted on the epoch boundary after their ratification.

#### Content

Every governance action will include the following:

- a deposit amount (recorded since the amount of the deposit is an updatable protocol parameter)
- a reward address to receive the deposit when it is repaid
- an anchor for any metadata that is needed to justify the action
- a hash digest value to prevent collisions with competing actions of the same type (as described earlier)

<!-- TODO: Provide a CBOR specification in the annexe for these new on-chain entities -->

In addition, each action will include some elements that are specific to its type:

| Governance action type                             | Additional data                                                                                                                                                                                 |
|:---------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 1. Motion of no-confidence                         | None                                                                                                                                                                                            |
| 2. Update committee/threshold                      | The set of verification key hash digests (members to be removed), a map of verification key hash digests to epoch numbers (new members and their term limit), and a fraction (new threshold)    |
| 3. New Constitution or Guardrails Script           | An anchor to the Constitution and an optional script hash of the Guardrails Script                                                                                                              |
| 4. Hard-fork initiation                            | The new (greater) major protocol version                                                                                                                                                        |
| 5. Protocol parameters changes                     | The changed parameters                                                                                                                                                                          |
| 6. Treasury withdrawal                             | A map from stake credentials to a positive number of Lovelace                                                                                                                                   |
| 7. Info                                            | None                                                                                                                                                                                            |

> **Note**
> The new major protocol version must be precisely one greater than the current protocol version.
> Any two consecutive epochs will therefore either have the same major protocol version, or the
> later one will have a major protocol version that is one greater.

> **Note**
> There can be no duplicate committee members - each pair of credentials in a committee must be unique.

Each governance action that is accepted on the chain will be assigned a unique identifier (a.k.a. the **governance action ID**),
consisting of the transaction hash that created it and the index within the transaction body that points to it.

#### Protocol Parameter groups

We have grouped the protocol parameter changes by type,
allowing different thresholds to be set for each group.

We are not, however, restricting each protocol parameter governance action to be contained within one group.
In case where a governance action carries updates for multiple parameters from different groups,
the maximum threshold of all the groups involved will apply to any given such governance action.

The _network_,  _economic_ and _technical_ parameter groups collect existing protocol parameters that were introduced during the Shelley, Alonzo and Babbage eras.
In addition, we introduce a new _governance_ group that is specific to the new governance parameters that will be introduced by CIP-1694.

The **network group** consists of:
- maximum block body size (`maxBlockBodySize`)
- maximum transaction size (`maxTxSize`)
- maximum block header size (`maxBlockHeaderSize`)
- maximum size of a serialized asset value (`maxValueSize`)
- maximum script execution units in a single transaction (`maxTxExecutionUnits`)
- maximum script execution units in a single block (`maxBlockExecutionUnits`)
- maximum number of collateral inputs (`maxCollateralInputs`)

The **economic group** consists of:
- minimum fee coefficient (`txFeePerByte`)
- minimum fee constant (`txFeeFixed`)
- delegation key Lovelace deposit (`stakeAddressDeposit`)
- pool registration Lovelace deposit (`stakePoolDeposit`)
- monetary expansion (`monetaryExpansion`)
- treasury expansion (`treasuryCut`)
- minimum fixed rewards cut for pools (`minPoolCost`)
- minimum Lovelace deposit per byte of serialized UTxO (`utxoCostPerByte`)
- prices of Plutus execution units (`executionUnitPrices`)

The **technical group** consists of:
- pool pledge influence (`poolPledgeInfluence`)
- pool retirement maximum epoch (`poolRetireMaxEpoch`)
- desired number of pools (`stakePoolTargetNum`)
- Plutus execution cost models (`costModels`)
- proportion of collateral needed for scripts (`collateralPercentage`)

The **governance group** consists of all the new protocol parameters that are introduced in this CIP:
- governance voting thresholds ($P_1$, $P_{2a}$, $P_{2b}$, $P_3$, $P_4$, $P_{5a}$, $P_{5b}$, $P_{5c}$, $P_{5d}$, $P_6$, $Q_1$, $Q_{2a}$, $Q_{2b}$, $Q_4$, $Q_5$)
- governance action maximum lifetime in epochs (`govActionLifetime`)
- governance action deposit (`govActionDeposit`)
- DRep deposit amount (`dRepDeposit`)
- DRep activity period in epochs (`dRepActivity`)
- minimal constitutional committee size (`committeeMinSize`)
- maximum term length (in epochs) for the constitutional committee members (`committeeMaxTermLength`)

<!-- TODO:
- Decide on the initial values for the new governance parameters

- Decide on coherence conditions on the voting thresholds.
    For example, the threshold for a motion of no-confidence should arguably be higher than that of a minor treasury withdrawal.
- ->

<!---------------------------    Governance Actions    ------------------------>

<!---------------------------          Votes           ------------------------>

### Votes

Each vote transaction consists of the following:

- a governance action ID
- a role - constitutional committee member, DRep, or SPO
- a governance credential witness for the role
- an optional anchor (as defined above) for information that is relevant to the vote
- a 'Yes'/'No'/'Abstain' vote

For SPOs and DReps, the number of votes that are cast (whether 'Yes', 'No' or 'Abstain') is proportional to the Lovelace that is delegated to them at the point the
action is checked for ratification.  For constitututional committee members, each current committee member has one vote.

> **Warning** 'Abstain' votes are not included in the "active voting stake".
>
> Note that an explicit vote to abstain differs from abstaining from voting.
> Unregistered stake that did not vote behaves like an 'Abstain' vote,
> while registered stake that did not vote behaves like a 'No' vote.
> To avoid confusion, we will only use the word 'Abstain' from this point onward to mean an on-chain vote to abstain.

The governance credential witness will trigger the appropriate verifications in the ledger according to the existing `UTxOW` ledger rule
(i.e. a signature check for verification keys, and a validator execution with a specific vote redeemer and new Plutus script purpose for scripts).

Votes can be cast multiple times for each governance action by a single credential.
Correctly submitted votes override any older votes for the same credential and role.
That is, the voter may change their position on any action if they choose.
As soon as a governance action is ratified, voting ends and transactions containing further votes are invalid.

#### Governance state

When a governance action is successfully submitted to the chain, its progress will be tracked by the ledger state.
In particular, the following will be tracked:

- the governance action ID
- the epoch that the action expires
- the deposit amount
- the rewards address that will receive the deposit when it is returned
- the total 'Yes'/'No'/'Abstain' votes of the constitutional committee for this action
- the total 'Yes'/'No'/'Abstain' votes of the DReps for this action
- the total 'Yes'/'No'/'Abstain' votes of the SPOs  for this action


#### Changes to the stake snapshot

Since the stake snapshot changes at each epoch boundary, a new tally must be calculated when each unratified governance action
is checked for ratification.  This means that an action could be enacted even though the DRep or SPO votes have not changed
(since the vote delegation could have changed).

#### Definitions relating to voting stake

We define a number of new terms related to voting stake:

- Lovelace contained in a transaction output is considered **active for voting** (that is, it forms the "active voting stake"):
- It contains a registered stake credential.
- The registered stake credential has delegated its voting rights to a DRep.
- Relative to some percentage `P`, a DRep (SPO) **vote threshold has been met** if the sum of the relative stake that has been delegated to the DReps (SPOs)
  that vote `Yes` to a governance action
  is at least `P`.

## Rationale

- [Role of the constitutional committee](#role-of-the-constitutional-committee)
- [Intentional omission of identity verification](#intentional-omission-of-identity-verification)
- [Reducing the power of entities with large amounts of Ada](#reducing-the-power-of-entities-with-large-amounts-of-ada)
- [Piggybacking on stake pool stake distribution](#piggybacking-on-stake-pool-stake-distribution)
- [Separation of hard-fork initiation from standard protocol parameter changes](#separation-of-hard-fork-initiation-from-standard-protocol-parameter-changes)
- [The purpose of the DReps](#the-purpose-of-the-dreps)
- [Ratification requirements table](#ratification-requirements-table)
- [Motion of no-confidence](#motion-of-no-confidence)
- [Update committee/threshold (state of no-confidence)](#update-committeethreshold-state-of-no-confidence)
- [The versatility of the info governance action](#the-versatility-of-the-info-governance-action)
- [Hard-fork initiation](#hard-fork-initiation)
- [New metadata structures](#new-metadata-structures)
- [Controlling the number of active governance actions](#controlling-the-number-of-active-governance-actions)
- [No AVST](#no-avst)

### Role of the constitutional committee

At first sight, the constitutional committee may appear to be a special committee that has been granted extra power over DReps.
However, because DReps can replace the constitutional committee at any time and DRep votes are also required to ratify every governance action,
the constitutional committee has no more (and may, in fact, have less) power than the DReps.
Given this, what role does the committee play, and why is it not superfluous?
The answer is that the committee solves the bootstrapping problem of the new governance framework.
Indeed, as soon as we pull the trigger and enable this framework to become active on-chain, then without a constitutional committee,
there would rapidly need to be sufficient DReps, so that the system did not rely solely on SPO votes.
We cannot yet predict how active the community will be in registering as DReps, nor how reactive other Ada holders will be regarding delegation of votes.

Thus, the constitutional committee comes into play to make sure that the system can transition from
its current state into fully decentralized governance in due course.
Furthermore, in the long run, the committee can play a mentoring and advisory role in the governance
decisions by being a set of elected representatives who are put under the spotlight for their judgment and guidance in governance decisions.
Above all, the committee is required at all times to adhere to the Constitution and to ratify proposals in accordance with the provisions of the Constitution.

### Intentional Omission of Identity Verification

Note that this CIP does not mention any kind of identity validation or verification for the members of the constitutional committee or the DReps.

This is intentional.

We hope that the community will strongly consider only voting for and delegating to those DReps who provide something like a DID to identify themselves.
However, enforcing identity verification is very difficult without some centralized oracle, which we consider to be a step in the wrong direction.

### Reducing the power of entities with large amounts of Ada

Various mechanisms, such as quadratic voting, have been proposed to guard against entities with a large amount of influence.
In a system based on "1 Lovelace, 1 vote", however, it is trivially easy to split stake into small amounts and undo the protections.
Without an on-chain identity verification system we cannot adopt any such measures.

### Piggybacking on stake pool stake distribution

The Cardano protocol is based on a Proof-of-Stake consensus mechanism, so using a stake-based governance approach is sensible.
However, there are many ways that could be used to define how to record the stake distribution between participants.
As a reminder, network addresses can currently contain two sets of credentials: one to identify who can unlock funds at an address
(a.k.a. payment credentials) and one that can be delegated to a stake pool (a.k.a. delegation credentials).

Rather than defining a third set of credentials, we instead propose to re-use the existing delegation credentials,
using a new on-chain certificate to determine the governance stake distribution. This implies that the set of DReps can (and likely will) differ from the set of SPOs,
so creating balance. On the flip side, it means that the governance stake distribution suffers from the same shortcomings as that for block production:
for example, wallet software providers must support multi-delegation schemes and must facilitate the partitioning of stake into sub-accounts should an Ada holder desire to delegate to multiple DReps,
or an Ada holder must manually split their holding if their wallet does not support this.

However, this choice also limits future implementation effort for wallet providers and minimizes the effort that is needed for end-users to participate in the governance protocol.
The latter is a sufficiently significant concern to justify the decision. By piggybacking on the existing structure,
the system remains familiar to users and reasonably easy to set up. This maximizes both the chance of success of, and the rate of participation in, the governance framework.

### Separation of Hard Fork Initiation from Standard Protocol Parameter Changes

In contrast to other protocol parameter updates, hard forks (or, more correctly, changes to the protocol's major version number) require much more attention.
Indeed, while other protocol parameter changes can be performed without significant software changes,
a hard fork assumes that a super-majority of the network has upgraded the Cardano node to support the new set of features that are introduced by the upgrade.
This means that the timing of a hard fork event must be communicated well ahead of time to all Cardano users, and requires coordination between stake pool operators, wallet providers, DApp developers, and the node release team.

Hence, this proposal, unlike the Shelley scheme, promotes hard fork initiations as a standalone governance action, distinct from protocol parameter updates.

### The purpose of the DReps

Nothing in this proposal limits SPOs from becoming DReps.
Why do we have DReps at all?
The answer is that SPOs are chosen purely for block production and not all SPOs will want to become DReps.
Voters can choose to delegate their vote to DReps without needing to consider whether they are
also a good block producer, and SPOs can choose to represent Ada holders or not.

### Ratification Requirements Table

The requirements in the [ratification requirement table](#requirements) are explained here.
Most of the governance actions have the same kind of requirements:
the constitutional committee and the DReps must reach a sufficient number of
'Yes' votes.
This includes these actions:
- Update committee/threshold (normal state)
- New Constitution
- Protocol parameter changes
- Treasury withdrawal

### Motion of no-confidence

A motion of no-confidence represents a lack of confidence by the Cardano community in the
current constitutional committee, and hence the constitutional committee should not
be included in this type of governance action.
In this situation, the SPOs and the DReps are left to represent the will of the community.

### Update committee/threshold (state of no-confidence)

Similar to the motion of no-confidence, electing a constitutional committee
depends on both the SPOs and the DReps to represent the will of the community.

### The versatility of the info governance action

While not binding on chain, the Info governance action could be useful in an number of
situations.  These include:

- ratifying a CIP
- deciding on the genesis file for a new ledger era
- recording initial feedback for future governance actions

### Hard-Fork initiation

Regardless of any governance mechanism, SPO participation is needed for any hard fork since they must upgrade their node software.
For this reason, we make their cooperation explicit in the hard fork initiation governance action,
by always requiring their vote.
The constitutional committee also votes, signaling the constitutionality of a hard fork.
The DReps also vote, to represent the will of every stake holder.

### New Metadata structures

The governance actions, the votes and the certificates and the Constitution use new metadata fields,
in the form of URLs and integrity hashes
(mirroring the metadata structure for stake pool registration).
The metadata is used to provide context.
Governance actions need to explain why the action is needed,
what experts were consulted, etc.
Since transaction size constraints should not limit this explanatory data,
we use URLs instead.

This does, however, introduce new problems.
If a URL does not resolve, what should be the expectation for voting on that action?
Should we expect everyone to vote 'No'?
Is this an attack vector against the governance system?
In such a scenario, the hash pre-image could be communicated in other ways, but we should be
prepared for the situation.
Should there be a summary of the justification on chain?

#### Alternative: Use of transaction metadata

Instead of specific dedicated fields in the transaction format, we could instead use the existing transaction metadata field.

Governance-related metadata can be clearly identified by registering a CIP-10 metadata label.
Within that, the structure of the metadata can be determined by this CIP (exact format TBD), using an index to map the vote or governance action ID to the corresponding metadata URL and hash.

This avoids the need to add additional fields to the transaction body, at the risk of making it easier for submitters to ignore.
However, since the required metadata can be empty (or can point to a non-resolving URL),
it is already easy for submitters to not provide metadata, and so it is unclear whether this makes the situation worse.

Note that transaction metadata is never stored in the ledger state, so it would be up to clients
to pair the metadata with the actions and votes in this alternative, and would not be available
as a ledger state query.

### Controlling the number of active governance actions

Since governance actions are available for anyone to submit, we need some mechanism to prevent those
individuals responsible for voting from becoming overwhelmed with a flood of proposals.
A large deposit is one such mechanism, but this comes at the unfortunate cost of being a barrier
for some people to submit an action.
Note, however, that crowd-sourcing with a Plutus script is always an option to gather the deposit.

We could, alternatively, accept the possibility of a large number of actions active at any given
time, and instead depend on off-chain socialization to guide voters' attention to those that merit it.
In this scenario, the constitutional committee might choose to only consider proposals which have
already garnered enough votes from the DReps.

### No AVST

An earlier draft of this CIP included the notion of an "active voting stake threshold", or AVST.
The purpose of AVST was to ensure the legitimacy of each vote, removing the possibility that, for example,
9 out of 10 Lovelace could decide the fate of millions of entities on Cardano.
There are really two concerns here, which are worth separating.

The first concern is that of bootstrapping the system, i.e. reaching the initial moment when
sufficient stake is registered to vote.
The second concern is that the system could lose participation over time.
One problem with the AVST is that it gives an incentive for SPOs to desire a low voting registration
(since their votes then hold more weight).
This is absolutely not a slight on the existing SPOs, but an issue with bad incentives.

We have chosen, therefore, to solve the two concerns differently.
We solve the bootstrapping problem as described in the section on bootstrapping.
We solve the long-term participation problem by not allowing reward withdrawals
(after the bootstrap phase) unless the stake is delegated to a DRep
(including the two special cases, namely 'Abstain' and 'No confidence').

### Changelog

#### Changes post Longmont workshop (March 2023)

- Thank the workshop attendees.
- We have added Constitutional Committee terms.
- Two new "pre-defined" voting options: abstain and no confidence.
- New "Info" governance action.
- Use the most recent DRep stake distribution for ratification.
  This means that if ever your DRep votes how you do not like,
  you can immediately make yourself a DRep and vote how you want.
- Escrow some ADA from the current treasury for potential future DRep
  incentives.
- Remove the tiered treasury actions in favor of something adaptive
  (so the "yes" threshold would depend on:
    1) how much ada,
    2) how high the registered voting stake, and maybe
    3) how much ada is released every epoch
- Split the protocol parameter updates into four groups:
  network, economic, technical, and governmental.
- Most governance actions can be enacted (upon ratification)
  right away. All but: protocol parameters and hard forks.
- Remove "one action per type per epoch" restriction in favor of
  tracking the last action ID of each type, and including this in
  the action.
- No AVST.
- Bootstrap phase: Until X% of ADA is registered to vote or Y epochs
  have elapsed, only parameter changes and hard forks can happen.
  PP changes just need CC threshold, HFs need CC and SPOs.
  After the bootstrap phase, we put in place the incentive to keep low
  DReps, but this mechanism **automatically** relaxes.
- New plutus script purpose for DReps.
- Multiple treasury withdrawals in one epoch.
- A section on the recursive problem of "how do we ratify this CIP".
- Changes to the local state-query protocol.
- New ideas, time permitting:
- Weigh SPO stake vote by pledge somehow.
- DReps can specify which other DRep gets their delegators
    in the event that they retire.
- Reduced government action deposit if one member of the CC signs off
    on it (which presumably means it has gone through some process).
- Include hash of (future) genesis configuration within HF proposal.

#### Changes post Edinburgh workshop (July 2023)

- Add guardrails script, which can control what treasury withdrawals and
  protocol parameter changes are allowed.
- Remove dropping of governance actions. The only effect this has is
  that in case a no confidence action passes, actions stay
  around. However, only new committee proposals that have been
  designed to build on top of that no confidence action can be
  enacted. If a new committee gets elected while some of those actions
  haven't expired, those actions can be ratified but the new committee
  has to approve them.
- All governance actions are enacted one epoch after they are ratified.
- Move post-bootstrapping restrictions into 'Other Ideas'.
- Add a section on different deposit amounts to 'Other Ideas'.
- Add a section for a minimum AVS to 'Other Ideas'.
- Rename some protocol parameters.
- Rename `TALLY` to `GOV`.
- Turn the Constitution into an anchor.
- Rework which anchors are required and which are optional.
- Clean up various inconsistencies and leftovers from older versions.

#### Security-relevant changes and other fixes (January 2024)

- Guard security-relevant changes behind SPO votes.
- The system does not enter a state of no confidence with insufficient
  active CC members, the CC just becomes unable to act.
- Clarify that CC members can use any kind of credential.

#### May 2024

- Update the section on the bootstrap period.
- Mention missing `Q_5` parameter.
- Various small fixes/consistency changes.

## Path to Active

### Acceptance Criteria

- [x] A new ledger era is enabled on the Cardano mainnet, which implements the above specification.
- Bootstrapping phase governance via Chang #1 hardfork
- Full governance via Plomin hardfork

### Implementation Plan

The features in this CIP require a hard fork.

This document describes an ambitious change to Cardano governance.
We propose to implement the changes via two hard forks: the first
one containing all new features but some being disabled for a bootstrap period
and the second one enabling all features.

In the following sections, we give more details about the various implementation work items that have already been identified.
In addition, the final section exposes a few open questions which will need to be finalized.
We hope that those questions can be addressed through community workshops and discussions.

#### Ratification of this proposal

The ratification of this proposal is something of a circular problem: we need some form of governance framework in order to agree on what the final governance framework should be.
As has been stated many times, CIPs are not authoritative, nor are they a governance mechanism.
Rather, they describe technical solutions that have been deemed sound (from a technical standpoint) by community of experts.

CIP-1694 arguably goes beyond the usual scope of the CIP process and there is a strong desire to ratify this CIP through _some process_.
However, that process is yet to be defined and it remains an open question.
The final ratification process is likely to be a blend of various ideas, such as:

- [x] Gather opinions from community-held workshops, akin to the Colorado workshop of February-March 2023.
- [ ] Exercise voting actions on a public testnet, with sufficient participation.
- [ ] Poll the established SPOs.
- [ ] Leverage Project Catalyst to gather inputs from the existing voting community (albeit small in terms of active stake).

#### Changes to the transaction body

- [x] New elements will be added to the transaction body, and existing update and MIR capabilities will be removed. In particular,

  The governance actions and votes will comprise two new transaction body fields.

- [x] Three new kinds of certificates will be added in addition to the existing ones:

- DRep registration
- DRep de-registration
- Vote delegation

  And similarly, the current MIR and genesis certificates will be removed.

- [x] A new `Voting` purpose will be added to Plutus script contexts.
  This will provide, in particular, the vote to on-chain scripts.

> **Warning** As usual, we will provide a CDDL specification for each of those changes.

#### Changes to the existing ledger rules

- The `PPUP` transition rule will be rewritten and moved out of the `UTxO` rule and into the `LEDGER` rule as a new `GOV` rule.

  It will process and record the governance actions and votes.

- The `NEWEPOCH` transition rule will be modified.
- The `MIR` sub-rule will be removed.
- A new `RATIFY` rule will be introduced to stage governance actions for enactment.

  It will ratify governance actions, and stage them for enactment in the current or next epoch, as appropriate.

- A new `ENACTMENT` rule will be called immediately after the `EPOCH` rule. This rule will enact governance actions that have previously been ratified.
- The `EPOCH` rule will no longer call the `NEWPP` sub-rule or compute whether the quorum is met on the PPUP state.

#### Changes to the local state-query protocol

The on-chain governance workload is large, but the off-chain workload for tools and applications will arguably be even larger.
To build an effective governance ecosystem, the ledger will have to provide interfaces to various governance elements.

While votes and DReps (de)registrations are directly visible in blocks and will, therefore, be accessible via the existing local-chain-sync protocols; we will need to upgrade the local-state-query protocol to provide extra insights on information which are harder to infer from blocks (i.e. those that require maintaining a ledger state). New state queries should cover (at least):

- Governance actions currently staged for enactment
- Governance actions under ratification, with the total and percentage of yes stake, no stake and abstain stake
- The current constitutional committee, and constitution hash digest

#### Bootstrapping Phase

We will need to be careful how we bootstrap this fledgling government.  All the parties
that are involved will need ample time to register themselves and to become familiar with the process.

Special provisions will apply in the initial bootstrap phase.
Firstly, during the bootstrap phase, a vote from the constitutional committee
is sufficient to change the protocol parameters.
Secondly, during the bootstrap phase, a vote from the constitutional committee,
together with a sufficient SPO vote, is sufficient to initiate a hard fork.
Thirdly, info actions will be available.
No other actions other than those mentioned in this paragraph are possible during the bootstrap phase.

The bootstrap phase ends when the Constitutional Committee and SPOs
ratify a subsequent hard fork, enabling the remaining governance
actions and DRep participation.
This is likely to be a number of months after the Chang hard fork.
Although all features will be technically available at this point, additional
requirements for using each feature may be specified in the constitution.

Moreover, there will be an interim Constitutional committee with a set term,
also specified in the next ledger era configuration file.
The rotational schedule of the first non-interim committee could be included in the constitution itself.
Note, however, that since the constitutional committee never votes on new committees,
it cannot actually enforce the rotation.

#### Other Ideas / Open Questions

##### Pledge-weighted SPO voting

The SPO vote could additionally be weighted by each SPO's pledge.
This would provide a mechanism for allowing those with literal stake in the game to have a stronger vote.
The weighting should be carefully chosen.

##### Automatic re-delegation of DReps

A DRep could optionally list another DRep credential in their registration certificate.
Upon retirement, all of the DRep's delegations would be automatically transferred to the
given DRep credential.  If that DRep had already retired, the delegation would be transfer
to the 'Abstain' voting option.

##### No DRep registration

Since the DRep registration does not perform any necessary functions,
the certificates for (de-)registering DReps could be removed. This
makes the democracy more liquid since it removes some bureaucracy and
also removes the need for the DRep deposit, at the cost of moving the anchor that is part of the
DRep registration certificate into the transaction metadata.

##### Reduced deposits for some government actions

The deposit that is attached to governance actions exists to prevent a flood of non-serious governance
actions, each of which would require time and attention from the Cardano community.
We could reduce this deposit for proposals which go through some agreed upon off-chain process.
This would be marked on-chain by the endorsement of at least one constitutional committee member.
The downside of this idea is that it gives more power to the constitutional committee.

##### Different deposit amounts for different governance actions

Multiple workshops for this CIP have proposed to introduce a different
deposit amount for each type of governance action. It was not clear
whether a majority was in favor of this idea, but this may be
considered if it becomes clear that it is necessary.

##### Minimum active voting stake

As a further guarantee to ensure governance actions cannot be proposed
right before a hard fork, be voted on by one DRep with a large amount
of stake and be enacted immediately, there could be an additional
requirement that a certain fixed absolute amount of stake needs to
cast a 'Yes' vote on the action to be enacted.

This does not seem necessary in the current design, since the stake of
all registered DReps behaves like a 'No' vote until they have actually
cast a vote. This means that for this scenario to occur, the malicious
actor needs at least to be in control of the fraction of DRep stake
corresponding to the relevant threshold, at which point this might as
well be considered a legitimate action.

##### Include hash of (future) genesis configuration within hard-fork proposal

Some hard-forks require new genesis configurations.
This has been the case for the Shelley and Alonzo hard forks (but not Allegra, Mary, Vasil or Valentine), may be the case in the future.
At the moment, this proposal doesn't state anything about such a genesis configuration:
it is implicitly assumed to be an off-chain agreement.
We could however, enforce that (the hash of) a specific genesis configuration is also captured within a hard-fork governance action.

##### Adaptive thresholds

As discussed above, it may make sense for some or all thresholds to be adaptive with respect to the Lovelace that is actively registered to vote,
so that the system provides greater legitimacy when there is only a low level of active voting stake.
The bootstrapping mechanism that is proposed above may subsume this, however, by ensuring that the governance system is activated
only when a minimum level of stake has been delegated to DReps.


##### Renaming DReps / state of no-confidence?

It has been stated several times that "DReps" as presented here, might be confused with Project Catalst DReps.
Similarly, some people have expressed confusion between the state of no-confidence, the motion of no-confidence and the no-confidence voting option.

We could imagine finding better terms for these concepts.

##### Rate-limiting treasury movements

Nothing prevents money being taken out of the treasury other than the proposed votes and voting thresholds. Given that the Cardano treasury is a quite fundamental component of its monetary policy, we could imagine enforcing (at the protocol level) the maximum amount that can removed from the treasury over any period of time.

##### Final safety measure, post bootstrapping

Many people have stated that they believe that the actual voting turnout will not be so large
as to be a strain on the throughput of the system.
We also believe that this is likely to be the case, but when the bootstrap phase ends we might
put one final, temporary safety	measure in place (this will also allow us to justify a low DRep deposit amount).

For values of $X$ and $Y$ that are still to be determined,
as soon as the bootstrap phase has ended,
when we calculate the DReps stake distribution for the next epoch boundary,
we will consider _only_ those DReps that are _either_ in the top $X$-many DReps ranked by stake amount,
or those DReps that have at least $Y$ Lovelace.
Every epoch, the value of $X$ will _increase_ and the value of $Y$ will decrease,
so that eventually $X$ will be effectively infinite and $Y$ will be zero.
Note that this is only an incentive, and nothing actually stops any DRep from casting their
vote (though it will not be counted if it does not meet the requirements).

If the community decides at some point that there is indeed a problem with congestion,
then a hard fork could be enacted that limits the number of DReps in a more restrictive way.

Reasonable numbers for the initial value of $X$ are probably 5,000-10,000.
Reasonable numbers for the initial value of $Y$ are probably the total number of Lovelace
divided by the initial value of $X$.

The mechanism should be set to relax at a rate where the restriction is completely eliminated after
a period of six months to one year.

## Acknowledgements

<details>
  <summary><strong>First draft</strong></summary>

Many people have commented on and contributed to the first draft of this document, which was published in November 2022.
We would especially like to thank the following people for providing their wisdom and insights:

- Jack Briggs
- Tim Harrison
- Philip Lazos
- Michael Madoff
- Evangelos Markakis
- Joel Telpner
- Thomas Upfield

We would also like to thank those who have commented via Github and other channels.
</details>

<details>
  <summary><strong>2023 Colorado Workshop (28/02 → 01/03)</strong></summary>

In addition, we would like to thank all the attendees of the workshop that was held in Longmont, Colorado on February 28th and March 1st 2023 for their valuable contributions
to this CIP, and for their active championing of Cardano's vision for minimal viable governance.  These include:

- Adam Rusch, ADAO & Summon
- Addie Girouard
- Andrew Westberg
- Darlington Wleh, LidoNation
- Eystein Hansen
- James Dunseith, Gimbalabs
- Juana Attieh
- Kenric Nelson
- Lloyd Duhon, DripDropz
- Marcus Jay Allen
- Marek Mahut, 5 Binaries
- Markus Gufler
- Matthew Capps
- Mercy, Wada
- Michael Dogali
- Michael Madoff
- Patrick Tobler, NMKR
- Philip Lazos
- π Lanningham, SundaeSwap
- Rick McCracken
- Romain Pellerin
- Sergio Sanchez Ferreros
- Tim Harrison
- Tsz Wai Wu
</details>

<details>
  <summary><strong>2023 Mexico City, Mexico Workshop (20/05)</strong></summary>

In addition, we would like to thank all the attendees of the workshop that was held in Mexico City, Mexico on May 20th 2023 for their valuable contributions
to this CIP, and for their active championing of Cardano's vision for minimal viable governance.  These include:

- Donovan Riaño
- Cristian Jair Rojas
- Victor Hernández
- Ramón Aceves
- Sergio Andrés Cortés
- Isaías Alejandro Galván
- Abigail Guzmán
- Jorge Fernando Murguía
- Luis Guillermo Santana

</details>

<details>
  <summary><strong>2023 Buenos Aires, Argentina Workshop (20/05)</strong></summary>

In addition, we would like to thank all the attendees of the workshop that was held in Buenos Aires, Argentina on May 20th 2023 for their valuable contributions
to this CIP, and for their active championing of Cardano's vision for minimal viable governance.  These include:

- Lucas Macchiavelli
- Alejando Pestchanker
- Juan Manuel Castro Pippo
- Federico Weill
- Jose Otegui
- Mercedes Ruggeri
- Mauro Andreoli
- Elias Aires
- Jorge Nasanovsky
- Ulises Barreiro
- Martin Ochoa
- Facundo Lopez
- Vanina Estrugo
- Luca Pestchanker
</details>

<details>
  <summary><strong>2023 Johannesburg, South Africa Workshop (25/05)</strong></summary>

In addition, we would like to thank all the attendees of the workshop that was held in Johannesburg, South Africa on May 25th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance.  These include:

- Celiwe Ngwenya
- Bernard Sibanda
- Dumo Mbobo
- Shaolyn Dzwedere
- Kunoshe Muchemwa
- Siphiwe Mbobo
- Lucas Sibindi
- DayTapoya
- Mdu Ngwenya
- Lucky Khumalo
- Skhangele Malinga
- Joyce Ncube
- Costa Katenhe
- Bramwell Kasanga
- Precious Abimbola
- Ethel Q Tshuma
- Panashe Sibanda
- Radebe Tefo
- Kaelo Lentsoe
- Richmond Oppong
- Israel Ncube
- Sikhangele Malinga
- Nana Safo
- Ndaba Delsie
- Collen Tshepang
- Dzvedere Shaolyn
- Thandazile Sibanda
- Ncube Joyce
- Lucas Sibindi
- Pinky Ferro
- Ishmael Ntuta
- Khumalo Lucky
- Fhulufelo
- Thwasile Ngwenya
- Kunashe Muchemwa
- Dube Bekezela
- Tinyiko Baloi
- Dada Nomathemba
</details>


<details>
  <summary><strong>2023 Bogota, Colombia Workshop (27/05)</strong></summary>

In addition, we would like to thank all the attendees of the workshop that was held in Bogota, Colombia on May 27th 2023 for their valuable contributions
to this CIP, and for their active championing of Cardano's vision for minimal viable governance.  These include:

- Alvaro Moncada
- Jaime Andres Posada Castro
- Jose Miguel De Gamboa
- Nicolas Gomez
- Luis Restrepo (Moxie)
- Juanita Jaramillo R.
- Daniel Vanegas
- Ernesto Rafael Pabon Moreno
- Carlos Eduardo Escobar
- Manuel Fernando Briceño
- Sebastian Pabon
</details>

<details>
  <summary><strong>2023 Caracas, Venezuela Workshop (27/05)</strong></summary>

In addition, we would like to thank all the attendees of the workshop that was held in Caracas, Venezuela on May 27th 2023 for their valuable contributions
to this CIP, and for their active championing of Cardano's vision for minimal viable governance.  These include:

- Jean Carlo Aguilar
- Wilmer Varón
- José Erasmo Colmenares
- David Jaén
- Félix Dávila
- Yaneth Duarte
- Nando Vitti
- Wilmer Rojas
- Andreina García
- Carmen Galban
- Osmarlina Agüero
- Ender Linares
- Carlos A. Palacios R
- Dewar Rodríguez
- Lennys Blanco
- Francys García
- Davidson Arenas
</details>

<details>
  <summary><strong>2023 Manizales, Colombia Workshop (27/05)</strong></summary>

In addition, we would like to thank all the attendees of the workshop that was held in Manizales, Colombia on May 27th 2023 for their valuable contributions
to this CIP, and for their active championing of Cardano's vision for minimal viable governance.  These include:

- Yaris Cruz
- Yaneth Duarte
- Ciro Gelvez
- Kevin Chacon
- Juan Sierra
- Caue Chianca
- Sonia Malagon
- Facundo Ramirez
- Hope R.
</details>

<details>
  <summary><strong>2023 Addis Ababa, Ethiopia Workshop (27/05 & 28/5)</strong></summary>

In addition, we would like to thank all the attendees of the workshop that was held in Addis Ababa, Ethiopia on May 27th and 28th 2023 for their valuable contributions
to this CIP, and for their active championing of Cardano's vision for minimal viable governance.  These include:

- Kaleb Dori
- Eyassu Birru
- Matthew Thornton
- Tamir Kifle
- Kirubel Tabu
- Bisrat Miherete
- Emmanuel Khatchadourian
- Tinsae Teka
- Yoseph Ephrem
- Yonas Eshetu
- Hanna Kaleab
- Tinsae Teka
- Robee Meseret
- Matias Tekeste
- Eyasu Birhanu
- yonatan berihun
- Nasrallah Hassan
- Andinet Assefa
- Tewodros Sintayehu
- KIDUS MENGISTEAB
- Djibril Konate
- Nahom Mekonnen
- Eyasu Birhanu
- Eyob Aschenaki
- Tinsae Demissie
- Yeabsira Tsegaye
- Tihitna Miroche
- Mearaf Tadewos
- Yab Mitiku
- Habtamu Asefa
- Dawit Mengistu
- Nebiyu Barsula
- Nebiyu Sultan
- Nathan Samson
</details>

<details>
  <summary><strong>2023 Kyoto and Fukuoka, Japan Workshop (27/05 & 10/06 )</strong></summary>

In addition, we would like to thank all the attendees of the workshop that was held in Kyoto and Fukuoka, Japan on May 27th and June 10th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance.  These include:

- Arimura
- Hidemi
- Nagamaru(SASApool)
- shiodome47(SODMpool)
- Wakuda(AID1pool)
- Yuta(Yuki Oishi)
- Andrew
- BANCpool
- Miyatake
- Muen
- Riekousagi
- SMAN8(SA8pool)
- Tatsuya
- カッシー
- 松
- ポンタ
- リサ
- Mako
- Ririco
- ながまる
- Baku
- マリア
- たりふん
- JUNO
- Kinoko
- Chikara
- ET
- Akira555
- Kent
- Ppp
- Shiodome47
- Sam
- ポール
- Concon
- Sogame
- ハンド
- Demi
- Nonnon
- banC
- SMAN8(SA8pool)
- りんむ
- Kensin
- りえこうさぎ
- アダマンタイト
- の/ゆすけ
- MUEN
- いちごだいふく
- Ranket
- A.yy
- N S
- Kazuya
- Daikon
</details>

<details>
  <summary><strong>2023 Monterey, California Workshop (28/05)</strong></summary>

In addition, we would like to thank all the attendees of the workshop that was held in Monterey, California on May 28th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance.  These include:

- Shane Powser
- Rodrigo Gomez
- Adam K. Dean
- John C. Valdez
- Kyle Solomon
- Erick "Mag" Magnana
- Bryant Austin
- John Huthmaker
- Ayori Selassie
- Josh Noriega
- Matthias Sieber
</details>

<details>
  <summary><strong>2023 Tlaxcala, Mexico Workshop (01/06)</strong></summary>

In addition, we would like to thank all the attendees of the workshop that was held in Tlaxcala, Mexico on June 1st 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance.  These include:

- Victor Hernández
- Cristian Jair Rojas
- Miriam Mejia
- Josmar Cabañas
- Lizbet Delgado
- José Alberto Sánchez
- Fátima Valeria Zamora
- Julio César Montiel
- Jesús Pérez
- José Adrián López
- Lizbeth Calderón
- Zayra Molina
- Nayelhi Pérez
- Josué Armas
- Diego Talavera
- Darían Gutiérrez
</details>

<details>
  <summary><strong>2023 LATAM Virtual Workshop (03/06)</strong></summary>

In addition, we would like to thank all the attendees of the workshop that was held in LATAM Virtual on June 3rd 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance.  These include:

- Juan Sierra
- @CaueChianca
- Ernesto Rafael
- Pabon Moreno
- Sonia Malagon
- Facundo Ramírez
- Mercedes Ruggeri
- Hope R.
- Yaris Cruz
- Yaneth Duarte
- Ciro Gélvez
- Kevin Chacon
- Juanita Jaramillo
- Sebastian Pabon
</details>

<details>
  <summary><strong>2023 Worcester, Massachusetts Workshop (08/06)</strong></summary>

In addition, we would like to thank all the attendees of the workshop that was held in Worcester, Massachusetts on June 8th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance.  These include:

- CardanoSharp
- Kenric Nelson
- Matthias Sieber
- Roberto Mayen
- Ian Burzynski
- omdesign
- Chris Gianelloni
</details>

<details>
  <summary><strong>2023 Chicago, Illinois Workshop (10/06)</strong></summary>

In addition, we would like to thank all the attendees of the workshop that was held in Chicago, Illinois on June 10th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance.  These include:

- Adam Rusch
- Jose Martinez
- Michael McNulty
- Vanessa Villanueva Collao
- Maaz Jedh
</details>

<details>
  <summary><strong>2023 Virtual Workshop (12/06)</strong></summary>

In addition, we would like to thank all the attendees of the workshop that was held virtually on June 12th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance.  These include:

- Rojo Kaboti
- Tommy Frey
- Tevo Saks
- Slate
- UBIO OBU
</details>

<details>
  <summary><strong>2023 Toronto, Canada Workshop (15/06)</strong></summary>

In addition, we would like to thank all the attendees of the workshop that was held in Toronto, Canada on June 15th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance.  These include:

- John MacPherson
- Lawrence Ley
</details>

<details>
  <summary><strong>2023 Philadelphia, Pennsylvania Workshop (17/06)</strong></summary>

In addition, we would like to thank all the attendees of the workshop that was held in Philadelphia, Pennsylvania on June 17th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance.  These include:

- NOODZ
- Jarhead
- Jenny Brito
- Shepard
- BONE Pool
- type_biggie
- FLAWWD
- A.I. Scholars
- Eddie
- Joker
- Lex
- Jerome
- Joey
- SwayZ
- Cara Mia
- PHILLY 1694
</details>

<details>
  <summary><strong>2023 Santiago de Chile Workshop (17/06)</strong></summary>

In addition, we would like to thank all the attendees of the workshop that was held in Santiago de Chile on June 17th 2023 for their valuable contributions
to this CIP, and for their active championing of Cardano's vision for minimal viable governance.  These include:

- Rodrigo Oyarsun
- Sebastián Aravena
- Musashi Fujio
- Geo Gavo
- Lucía Escobar
- Juan Cruz Franco
- Natalia Rosa
- Cristian M. García
- Alejandro Montalvo
</details>

<details>
  <summary><strong>2023 Virtual Workshop (17/06)</strong></summary>

In addition, we would like to thank all the attendees of the workshop that was held virtually on June 17th 2023 for their valuable contributions
to this CIP, and for their active championing of Cardano's vision for minimal viable governance.  These include:

- Juana Attieh
- Nadim Karam
- Amir Azem
- Rami Hanania
- LALUL Stake Pool
- HAWAK Stake Pool
</details>

<details>
  <summary><strong>2023 Taipai, Taiwan Workshop (18/06)</strong></summary>

In addition, we would like to thank all the attendees of the workshop that was held in Taipai, Taiwan on June 18th 2023 for their valuable contributions
to this CIP, and for their active championing of Cardano's vision for minimal viable governance.  These include:

- Michael Rogero
- Ted Chen
- Mic
- Jeremy Firster
- Eric Tsai
- Dylan Chiang
- JohnsonCai
- DavidCHIEN
- Zach Gu
- Jimmy WANG
- JackTsai
- Katherine Hung
- Will Huang
- Kwicil
</details>

<details>
  <summary><strong>2023 Midgard Vikingcenter Horten, Norway Workshop (19/06)</strong></summary>

In addition, we would like to thank all the attendees of the workshop that was held in Midgard Vikingcenter Horten, Norway on June 19th 2023 for their valuable contributions
to this CIP, and for their active championing of Cardano's vision for minimal viable governance.  These include:

- Daniel D. Johnsen
- Thomas Lindseth
- Eystein Hansen
- Gudbrand Tokerud
- Lally McClay
- $trym
- Arne Rasmussen
- Lise WesselTVVIN
- Bjarne
- Jostein Aanderaa
- Ken-Erik Ølmheim
- DimSum
</details>

<details>
  <summary><strong>2023 Virtual Workshop (19/06)</strong></summary>

In addition, we would like to thank all the attendees of the workshop that was held virtually on June 19th 2023 for their valuable contributions
to this CIP, and for their active championing of Cardano's vision for minimal viable governance.  These include:

- Nicolas Cerny
- Nils Peuser
- Riley Kilgore
- Alejandro Almanza
- Jenny Brito
- John C. Valdez
- Rhys
- Thyme
- Adam Rusch
- Devryn
</details>

<details>
  <summary><strong>2023 New York City, New York Workshop (20/06)</strong></summary>

In addition, we would like to thank all the attendees of the workshop that was held in New York City, New York on June 20th 2023 for their valuable contributions
to this CIP, and for their active championing of Cardano's vision for minimal viable governance.  These include:

- John Shearing
- Geoff Shearing
- Daniela Balaniuc
- SDuffy
- Garry Golden
- Newman
- Emmanuel Batse
- Ebae
- Mojira
</details>

<details>
  <summary><strong>2023 La Cumbre, Argentina Workshop (23/06)</strong></summary>

In addition, we would like to thank all the attendees of the workshop that was held in La Cumbre, Argentina on June 23rd 2023 for their valuable contributions
to this CIP, and for their active championing of Cardano's vision for minimal viable governance.  These include:

- Ulises Barreiro
- Daniel F. Rodriguez
- Dominique Gromez
- Leandro Chialvo
- Claudia Vogel
- Guillermo Lucero
- Funes, Brian Carrasco
- Melisa Carrasco
- Carlos Carrasco
</details>

<details>
  <summary><strong>2023 Minneapolis, Minnesota Workshop (23/06)</strong></summary>

In addition, we would like to thank all the attendees of the workshop that was held in Minneapolis, Minnesota on June 23rd 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance.  These include:

- Stephanie King
- Darlington Wleh
</details>

<details>
  <summary><strong>2023 La Plata, Argentina Workshop (23/06)</strong></summary>

In addition, we would like to thank all the attendees of the workshop that was held in La Plata, Argentina on June 23rd 2023 for their valuable contributions
to this CIP, and for their active championing of Cardano's vision for minimal viable governance.  These include:

- Mauro Andreoli
- Rodolfo Miranda
- Agustin Francella
- Federico Sting
- Elias Aires
- Lucas Macchiavelli
- Pablo Hernán Mazzitelli
</details>

<details>
  <summary><strong>2023 Puerto Madryn, Argentina Workshop (23/06)</strong></summary>

In addition, we would like to thank all the attendees of the workshop that was held in Puerto Madryn, Argentina on June 23rd 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance.  These include:

- Andres Torres Borda
- Federico Ledesma Calatayud
- Maximiliano Torres
- Federico Prado
- Domingo Torres
- Floriana Pérez Barria
- Martin Real
- Florencia García
- Roberto Neme
</details>

<details>
  <summary><strong>2023 Accra, Ghana Workshop (24/06)</strong></summary>

In addition, we would like to thank all the attendees of the workshop that was held in Accra, Ghana on June 24th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance.  These include:

- Wada
- Laurentine
- Christopher A.
- Nathaniel D.
- Edufua
- Michael
- Augusta
- Jeremiah
- Boaz
- Mohammed
- Richmond O.
- Ezekiel
- Megan
- Josue
- Michel T.
- Bineta
- Afia O.
- Mercy
- Enoch
- Kofi
- Awura
- Emelia
- Richmond S.
- Solomon
- Phillip
- Faakor
- Manfo
- Josh
- Daniel
- Mermose
</details>

<details>
  <summary><strong>2023 Virtual Workshop (24/06)</strong></summary>

In addition, we would like to thank all the attendees of the workshop that was held virtually on June 24th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance.  These include:

- Jonas Riise
- Thomas Lindseth
- André "Eilert" Eilertsen
- Eystein Hansen
</details>

<details>
  <summary><strong>2023 Seoul, South Korea Workshop (24/06)</strong></summary>

In addition, we would like to thank all the attendees of the workshop that was held in Seoul, South Korea on June 24th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance.  These include:

- Oscar Hong (JUNGI HONG)
- SPO_COOL (Kevin Kordano)
- SPO_KTOP (KT OH)
- WANG JAE LEE
- JAE HYUN AN
- INYOUNG MOON (Penny)
- HOJIN JEON
- SEUNG KYU BAEK
- SA SEONG MAENG
- JUNG MYEONG HAN
- BRIAN KIM
- JUNG HOON KIM
- SEUNG WOOK JUNG (Peter)
- HYUNG WOO PARK
- EUN JAE CHOI
- NA GYEONG KIM
- JADEN CHOI
</details>

<details>
  <summary><strong>2023 Abu Dhabi, UAE Workshop (25/06)</strong></summary>

In addition, we would like to thank all the attendees of the workshop that was held in Abu Dhabi, UAE on June 25th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance.  These include:

- Amir Azem
- Ian Arden
- Madina Abdibayeva
- BTBF (Yu Kagaya)
- محمد الظاهري
- Tegegne Tefera
- Rami Hanania
- Tania Debs
- Khalil Jad
- Mohamed Jamal
- Ruslan Yakubov
- OUSHEK Mohamed eisa
- Shehryar
- Wael Ben Younes
- Santosh Ray
- Juana Attieh
- Nadim Karam
- DubaistakePool
- HAWAK Pool
- LALKUL Stake Pools
</details>

<details>
  <summary><strong>2023 Williamsburg, New York Workshop (25/06)</strong></summary>

In addition, we would like to thank all the attendees of the workshop that was held in Williamsburg, New York on June 25th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance.  These include:

- Pi
- Joseph
- Skyler
- Forrest
- Gabriel
- Newman
</details>

<details>
  <summary><strong>2023 Lagos, Nigeria Workshop (28/06)</strong></summary>

In addition, we would like to thank all the attendees of the workshop that was held in Lagos, Nigeria on June 28th 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance.  These include:

- Jonah Benson
- Augusta
- Ubio Obu
- Olumide Hrosuosegbe
- Veralyn Chinenye
- Ona Ohimer
- William Ese
- Ruth Usoro
- William P
- Esther Simi
- Daniel Effiom
- Akinkurai Toluwalase
</details>

<details>
  <summary><strong>2023 Sao Paulo, Brazil Workshop (01/07)</strong></summary>

In addition, we would like to thank all the attendees of the workshop that was held in Sao Paulo, Brazil on July 1st 2023 for their valuable contributions to this CIP, and for their active championing of Cardano's vision for minimal viable governance.  These include:

- Otávio Lima
- Rodrigo Pacini
- Maria Carmo
- Cauê Chianca
- Daniela Alves
- Jose Lins Dias
- Felipe Barcelos
- Rosana Melo
- Johnny Oliveira
- Lucas Ravacci
- Cristofer Ramos
- Weslei Menck
- Leandro Tsutsumi
- Izaias Pessoa
- Gabriel Melo
- Yuri Nabeshima
- Alexandre Fernandes
- Vinicius Ferreiro
- Lucas Fernandes
- Alessandro Benicio
- Mario Cielho
- Lory Fernandes Lima
- Larissa Nogueira
- Latam Cardano Community
</details>

<details>
  <summary><strong>2023 Brazil Virtual Workshop (04/07)</strong></summary>

In addition, we would like to thank all the attendees of the workshop that was held in Brazil on July 4th 2023 for their valuable contributions
to this CIP, and for their active championing of Cardano's vision for minimal viable governance.  These include:

- Lincon Vidal
- Thiago da Silva Nunes
- Rodrigo Pacini
- Livia Corcino de Albuquerque
- Cauê Chianca
- Otávio Lima
</details>

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode)

[^1]: A formal description of the current rules for governance actions is given in the [Shelley ledger specification](https://github.com/input-output-hk/cardano-ledger/releases/latest/download/shelley-ledger.pdf).

- For protocol parameter changes (including hard forks), the `PPUP` transition rule (Figure 13) describes how protocol parameter updates are processed, and the `NEWPP` transition rule (Figure 43) describes how changes to protocol parameters are enacted.

- For funds transfers, the `DELEG` transition rule (Figure 24) describes how MIR certificates are processed, and the `MIR` transition rule (Figure 55) describes how treasury and reserve movements are enacted.

    > **Note**
    > The capabilities of the `MIR` transition rule were expanded in the [Alonzo ledger specification](https://github.com/input-output-hk/cardano-ledger/releases/latest/download/alonzo-ledger.pdf)


[^2]: There are many varying definitions of the term "hard fork" in the blockchain industry. Hard forks typically refer to non-backwards compatible updates of a network. In Cardano, we formalize the definition slightly more by calling any upgrade that would lead to _more blocks_ being validated a "hard fork" and force nodes to comply with the new protocol version, effectively obsoleting nodes that are unable to handle the upgrade.

[^3]: This is the definition used in "active stake" for stake delegation to stake pools, see Section 17.1, Total stake calculation, of the [Shelley ledger specification](https://github.com/input-output-hk/cardano-ledger/releases/latest/download/shelley-ledger.pdf).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-1852/README.md
---

- --
CIP: 1852
Title: HD (Hierarchy for Deterministic) Wallets for Cardano
Status: Active
Category: Wallets
Authors:
- Sebastien Guillemot <sebastien@emurgo.io>
- Matthias Benkort <matthias.benkort@cardanofoundation.org>
Implementors: N/A
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/33
- https://forum.cardano.org/t/cip1852-hd-wallets-for-cardano/41740
Created: 2019-10-28
License: CC-BY-4.0
- --

## Abstract

Cardano extends the [BIP44](https://github.com/bitcoin/bips/blob/master/bip-0044.mediawiki) by adding new chains used for different purposes. This document outlines how key derivation is done and acts as a registry for different chains used by Cardano wallets.

## Motivation: why is this CIP necessary?

For Cardano, we use a new purpose field `1852'` instead of `44'` like in BIP44. There are three main reasons for this:

1) During the Byron-era, `44'` was used. Since Byron wallets use a different algorithm for generating addresses from public keys, using a different purpose type allows software to easily know which address generation algorithm given just the derivation path (ex: given `m / 44' / 1815' / 0' / 0 / 0`, wallet software would know to handle this as a Byron-era wallet and not a Shelley-era wallet).
2) Using a new purpose helps bring attention to the fact Cardano is using `BIP32-Ed25519` and not standard `BIP32`.
3) Using a new purpose allows us to extend this registry to include more Cardano-specific functionality in the future

`1852` was chosen as it is the year of death of Ada Lovelace (following the fact that the `coin_type` value for Cardano is `1815` for her year of birth)

## Specification

Using `1852'` as the purpose field, we defined the following derivation path:

```
m / purpose' / coin_type' / account' / role / index
```

Example: `m / 1852' / 1815' / 0' / 0 / 0`

Here, `role` can be the following

| Name                              | Value | Description
| ---                               | ----- | ------------
| External chain                    | `0`   | Same as defined in [BIP44](https://github.com/bitcoin/bips/blob/master/bip-0044.mediawiki)
| Internal chain                    | `1`   | Same as defined in [BIP44](https://github.com/bitcoin/bips/blob/master/bip-0044.mediawiki)
| Staking Key                       | `2`   | See [CIP-0011](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0011/README.md)
| DRep Key                          | `3`   | See [CIP-0105](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0105/README.md)
| Constitutional Committee Cold Key | `4`   | See [CIP-0105](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0105/README.md)
| Constitutional Committee Hot Key  | `5`   | See [CIP-0105](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0105/README.md)

Wallets **MUST** implement this new scheme using the master node derivation algorithm from Icarus with sequential addressing (see [CIP3](https://cips.cardano.org/cips/cip3) for more information)

## Rationale: how does this CIP achieve its goals?

### Derivation style

Cardano does not use [BIP32](https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki) but actually uses [BIP32-Ed25519](https://input-output-hk.github.io/adrestia/static/Ed25519_BIP.pdf). The `-Ed25519` suffix is often dropped in practice (ex: we say the Byron release of Cardano supports [BIP44](https://github.com/bitcoin/bips/blob/master/bip-0044.mediawiki) but in reality this is BIP44-Ed25519).

The Byron implementation of Cardano uses `purpose = 44'` (note: this was already a slight abuse of notation because Cardano implements BIP44-Ed25519 and not standard BIP44).

There are two (incompatible) implementations of BIP32-Ed25519 in Cardano:

1) HD Random (notably used initially in Daedalus)
2) HD Sequential (notably used initially in Icarus)

The difference is explained in more detail in [CIP-0003](../CIP-0003).

### Future extensions

As a general pattern, new wallet schemes should use a different purpose if they intend to piggy-back on the same structure but for a different use-case (see for instance [CIP-1854](../CIP-1854)).

The `role` can however be extending with new roles so long as they have no overlapping semantic with existing roles. If they do, then they likely fall into the first category of extension and would better be done via a new purpose.

## Path to Active

### Acceptance Criteria

- [x] Standardisation of this derivation path among all wallets as of the Shelley ledger era.

### Implementation Plan

- [x] Common agreement on the above Motivation, Rationale and Specification during the planning of Cardano's Shelley release.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-1853/README.md
---

- --
CIP: 1853
Title: HD (Hierarchy for Deterministic) Stake Pool Cold Keys for Cardano
Status: Proposed
Category: Wallets
Authors:
- Rafael Korbas <rafael.korbas@vacuumlabs.com>
Implementors:
- Vacuum Labs <https://vacuumlabs.com/>
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/56
- https://forum.cardano.org/t/stake-pool-cold-keys-hd-derivation/43360
Created: 2020-12-14
License: CC-BY-4.0
- --

## Abstract

[CIP-1852] establishes how Shelley-era hierarchical deterministic (HD) wallets should derive their keys. This document is a follow-up of this CIP specifying how stake pool cold keys should be derived.

## Motivation: why is this CIP necessary?

(Hierarchical) deterministic derivation of stake pool cold keys enables their restorability from a seed and most importantly, their management on hardware wallet devices. This in turn mitigates man-in-the middle attacks to which pool operators would otherwise be vulnerable if they managed their stake pool cold keys on a device not specifically hardened against alteration of the data to be signed/serialized without operator's explicit consent.

## Specification

Using `1853'` as the purpose field, we define the following derivation path structure for stake pool cold keys:

```
m / purpose' / coin_type' / usecase' / cold_key_index'
```

Example: `m / 1853' / 1815' / 0' / 0'`

Here the `usecase` is currently fixed to `0'`.

Given that stake pool cold keys are cryptographically the same as wallet keys already covered in CIP-1852, the master node and subsequent child keys derivation **MUST** be implemented in the same way as specified for wallets in CIP-1852.

## Rationale: how does this CIP achieve its goals?

### Why introducing a new purpose?

Stake pools are not wallets and the core concept of "accounts" is not applicable to them, nor are they supposed to be related to a user's wallet in any meaningful way. Therefore treating stake pool cold keys as another "chain" within CIP-1852 specification would rather be a deviation from CIP-1852 than its logical extension. Hence we establish a separate purpose and path structure for stake pool cold keys, having their specifics and differences from standard "wallet" keys in mind.

### Why keeping `coin_type` in the path?

`coin_type` is kept in order to remain consistent with the "parent" CIP-1852 and also to leave space for the possibility that some Cardano hard-fork/clone in the future would reuse this specification to derive their own stake pool cold keys.

### `usecase` field

Similarly as we have the `chain` path component in CIP-1852 paths for different types of wallet keys, it's plausible that in the future, there could be multiple varieties of stake pools. One such example of a possible future extension of this CIP could be pools managed by a group of operators instead of a single operator, for which a separate set of stake pool cold keys, driven by this parameter, could make sense both from logical and security perspective.

### Hardened derivation at `cold_key_index`

Each stake pool is supposed to be managed separately so there is currently no incentive to connect them via a parent public key.

### Hardened derivation at `usecase`

We chose hardened derivation at the usecase index as there is no incentive to mix the stake pool cold keys with other potential usecases and if there was such incentive, it would most likely be more appropriate to create a separate usecase/purpose for that.

## Path to Active

### Acceptance Criteria

- [ ] Standardisation of this derivation path among three wallets as of the Shelley ledger era.
- Ledger App Cardano <https://github.com/LedgerHQ/app-cardano>

### Implementation Plan

- [x] Common agreement on the above Motivation, Rationale and Specification during the planning of Cardano's Shelley release.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

[CIP-1852]: https://github.com/cardano-foundation/CIPs/blob/master/CIP-1852

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-1854/README.md
---

- --
CIP: 1854
Title: Multi-signatures HD Wallets
Status: Active
Category: Wallets
Authors:
- Matthias Benkort <matthias.benkort@cardanofoundation.org>
- Pawel Jakubas <pawel.jakubas@cardanofoundation.org>
Implementors:
- Round Table wallet
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/69
Created: 2021-02-23
License: CC-BY-4.0
- --

## Abstract

This document describes how to realize multi-parties transactions in Cardano. It is defined as an alternative to [CIP-1852] for Cardano HD wallets. This specification does not cover the transport and sharing of partially signed transactions which is / will be covered in another document.

## Motivation: why is this CIP necessary?

### Overview

Term     | Definition
- --      | ---
HD       | Hierarchical Deterministic, refers to wallets as described in [BIP-0032].
Multisig | Shorthand for Multi-party signature scheme.

Multisig wallets are Cardano wallets capable of providing and tracking keys involved in multi-parties transactions. Such transactions are used to move funds from addresses where (typically) more than one user must sign transactions for its funds to be spent. A simple analogue is a joint bank account where two account holders must approve withdrawals from the account. Cardano has native support for multi-signature schemes: funds at a multi-signature script address are owned by a monetary script, which specifies one or more combinations of cryptographic signatures which must be present to unlock funds at the address (see the [Formal Ledger Spec, Figure 4: Multi-signature via Native Scripts][ledger-spec.pdf]). In a similar fashion, Cardano multisig scripts can also be used to capture stake rights on a particular address, shared between different parties.

In the Shelley era, Cardano offers six (partially overlapping) primitives for constructing monetary scripts which are summarized as the following grammar:

```abnf
SCRIPT
  = SIGNATURE KEY-HASH
  | ALL-OF 1*SCRIPT
  | ANY-OF 1*SCRIPT
  | N-OF UINT 1*SCRIPT
  | AFTER SLOT-NO
  | BEFORE SLOT-NO

KEY-HASH = 28OCTET

SLOT-NO = UINT
```

Scripts are thereby recursive structures which can contain scripts themselves. For example, a joint account between two parties which specifies that any of the two members can spend from it would be defined in pseudo-code as:

```abnf
JOINT-ACCOUNT := ANY-OF [SIGNATURE key-hash#1, SIGNATURE key-hash#2]
```

In order to spend from such script, one would have to provide a witness showing ownership of the key associated with either of `key-hash#1` or `key-hash#2` (or possibly, both).

### Creation of a Multisig Script/Address

The creation of a multisig address will not be covered in this document. Such addresses are described in [Shelley design specification - section 3.2 - Addresses and Credentials][delegation_design.pdf] and are obtained by serializing and hashing a multisig script. This functionality is assumed to be available through existing tooling or piece of software in a variety of ways.

### Spending From a Multisig Address

In order to spend from a multisig address, one must provide a special witness in the spending transaction called a "multisig witness". Such witness must be the exact script used as an input for creating the hash part of the multisig address. Then, any additional required verification key signatures specified by the script must be provided as separate verification key witnesses (i.e. signatures of the hashed transaction body for each signing key).

This means that a wallet software has access to the full script to validate and also identify verification key hashes present in transactions, but only does so when funds are being spent from a multisig address! From the Allegra era and beyond, it is also possible to include script pre-image in transaction auxiliary data. Softwares may use this to communicate scripts ahead of time.

## Specification

### HD Derivation

We consider the following HD derivation paths similarly to [CIP-1852]:

```
m / purpose' / coin_type' / account_ix' / role / index
```

To associate multi-signature keys to a wallet, we reserve however `purpose=1854'` to distinguish multisig wallets from standard wallets. The coin type remains `coin_type=1815'` to identify Ada as registered in [SLIP-0044]. The account index (`account_ix`) may vary across the whole hardened domain. `role=0` is used to identify payment keys, whereas `role=2` identifies stake keys. `role=1` is left unused for multisig wallets. Finally, the last `index` may vary across the whole soft domain, but according to the following rules:

- Wallet must derive multisig key indexes sequentially, starting from 0 and up to 2^31-1
- Wallet must prevent the creation of new multisig keys before past keys are seen in an on-chain script.
- Wallet should yet always provide up-to 20 consecutive unused multisig keys.

We can summarize the various paths and their respective domain in the following table:

| `purpose` | `coin_type` | `account_ix`       | `role`     | `index`         |
| ---       | ---         | ---                | ---        | ---             |
| `1854'`   | `1815'`     | `[2^31 .. 2^32-1]` | `0` or `2` | `[0 .. 2^31-1]` |

#### Examples

- `m/1854’/1815’/0’/0/0`
- `m/1854’/1815’/0’/2/14`
- `m/1854’/1815’/0’/2/42`
- `m/1854’/1815’/0’/0/1337`

### User-facing encoding

Multi-signatures payment verification and signing keys (`role=0`) with chain code should be presented bech32-encoded using `addr_shared_xvk` and `addr_shared_xsk` prefixes respectively, as specified in [CIP-5]. When represented without chain-code, `addr_shared_vk` and `addr_shared_sk` should be used instead.

Similarly we use `stake_shared_xvk`, `stake_shared_xsk`, `stake_shared_vk` and `stake_shared_sk` for multi-signatures stake verification and signing keys (`role=2`).

#### Examples

```yaml=
- base16: |
    179be1ac9e63abb5fe4666df03007b07
    33a3b63cdd08cd3635b8d364e16aaf26
    49856f2ba513786afdbe5c7a07565c02
    9ba7a4290d20aa3c80ecc1835841ed78
  bech32:
    addr_shared_xvk1z7d7rty7vw4mtljxvm0sxqrmque68d3um5yv6d34hrfkfct24unynpt09wj3x7r2lkl9c7s82ewq9xa85s5s6g928jqwesvrtpq767qs66fnu
```

### Multisig Wallets

#### Templates

To define a multisig wallet, participants must provide:

- A payment script template.
- A delegation script template (possibly null).
- All the cosigners role=0 and role=2 (4th level) public keys involved in templates. Alternatively, cosigners may also share their account public key (3rd level) directly.

A script template is a script where key hashes are replaced by a cosigner tag which captures the relation between the cosigners.

> **NOTE 1**: How an application represents tags is an implementation detail. Only the instantiated script appears on the blockchain.
>
> **NOTE 2**: As a reminder, a Cardano address is made of two parts: a payment part and a delegation part. The latter is optional and so is the delegation script template. If none is provided then the address does not contain any delegation part. Wallet softwares should decide whether they want to allow or forbid this but this proposal does not take position.

There must be a strict one-to-one mapping between the tags used in the template and the cosigner account keys provided. When a new address is needed, the software must instantiate the script by using keys derived from the relevant account. To instantiate a script from a template, a wallet software must abide by the following rules:

1. When deriving new keys, the same indexes must be used for all account involved in the template.
2. If a tag appears in multiple place in the script, then the same index must also be used for all instances of that tag.

(1) and (2) implies that a given instance of a script is associated to one and only one derivation index.

> **NOTE 3**: Wallet software should not authorize users to update the script templates after an address has been used.

For example, considering the joint-account example from above, an example of payment template can simply be:

```abnf
JOINT-ACCOUNT-TEMPLATE := ANY-OF
    [ SIGNATURE cosigner#1
    , SIGNATURE cosigner#2
    ]
```

and an instantiation of that template for e.g. an index equals to 1 could be:

```abnf
JOINT-ACCOUNT := ANY-OF
    [ SIGNATURE addr_shared_vkh1rcw47lrnczf8d4gt7d8vp6z95l8effslyejszez7069z6fa469v
    , SIGNATURE addr_shared_vkh1f8vd0s83hr7y2r96gct785e4d0d77ep9jn5wqfa4d6r4kqzxd65
    ]
```

Keys used in the instantiated template all come from the same derivation index for all cosigners. This allows all wallets to easily find back what indexes other cosigners used to derive a particular key (the same as they use for a particular address).

### Sending Transactions

In case the wallet needs a change address internally, it must use the smallest unused indexes known, in ascending order. An index is considered unused if there's no transaction in the ledger with a script using its corresponding key hash. When constructing a transaction, a wallet should use UTxOs associated with script addresses only (hybrid transactions using non-shared UTxOs are forbidden).

Once constructed, a transaction must be signed with all required private keys from the initiator. The initiator must also include all instances of scripts necessary (typically one per input) and then either broadcast the transaction to its cosigners or submits it to the network if valid. The mean by which the transaction is broadcast is out of the scope of this specification.

Upon receiving a partially signed transaction, wallets must for each input:

1. Find the script associated with the input in the transaction witnesses
2. Verify that the script matches the wallet's template(s)
3. Identify the derivation index used to instantiate the script from its known keys
4. Derive the public keys of each other co-signers from that same derivation index and the cosigners parent keys
5. Verify that there's at least one signature from another co-signer
6. Verify all signatures provided by co-signers with their associated public keys

> **NOTE** The step (5) is crucial and implies that wallets initiating transactions have to sign them before broadcasting them as a proof of provenance. Otherwise it is possible for attackers to broadcast a "fake" transaction to all cosigners who may not pay attention to its details.


When valid, the wallet should prompt the user for signing. The transaction can be submitted by any of the cosigners who deems it valid (it could be that only a subset of the cosigners is required to sign). Several cosigners may submit the transaction concurrently without issue, the ledger will ensure that only one transaction eventually get through.

> **NOTE:** The management of change addresses is an implementation detail of the wallet so long as all change addresses abide by the rules described earlier in this section. A wallet may choose to generate a single change address per transaction, while another may chose to generate many.

### Foreign Script Discovery

Wallets should warn users when discovering known keys in scripts which non-matching templates. Such addresses should not be included as part of the wallet's UTxO and should be treated as anomalies in the wallet. As a matter of facts, everything described in this document is a mere convention between wallets. There's however nothing at the protocol level that enforces that this specification is followed. It is therefore very much possible for advanced users to use their multisig keys in scripts that are different from the template. So long as indexes used are discoverable by the wallet, then such scripts are also discoverable.

However, because such scripts / addresses would not have been created by the wallet, they are considered not being part of the it altogether and would not count towards the wallet's balance. Software should however as much as possible alert users about the existence of such anomalies.

### Example

For example, if Alice and Bob wants to share a wallet by requiring a signature from each other on every payment, they can define the following payment script template:

```
ALL-OF (SIGNATURE alice) (SIGNATURE bob)
```

After exchanging their corresponding public keys, both wallets should be initialized and present to Alice and Bob exactly `20` identical addresses, where each address is associated with exactly one derivation index. The first address will use one key from Alice's wallet at path `m/1854’/1815’/0’/0/0` and a key from Bob's at exactly the same path. The next address will use keys at path `m/1854’/1815’/0’/0/1` and so forth.

When Alice initiates a transaction from this wallet, she'll construct the transaction body, sign it with her corresponding private key, include an instance of the script as witness and broadcast the transaction to Bob via Telegram. Upon reception, Bob is able to verify that the transaction was indeed signed by Alice using her private key at index #0 (Bob has indeed Alice's parent public key in its possession and is therefore able to derive a child key at index #0 to verify the signature on the transaction). Bob proceeds with the payment by signing the transaction in return and submitting it.

## Rationale: how does this CIP achieve its goals?

- Multisig keys are scoped to accounts, which allows wallet's owners to separate their activity easily.

- We use a different purpose for mainly two reasons:

- It prevents mixing up standard wallets with shared wallets, which would be undesirable and become rapidly a nightmare for software to maintain. In particular, it also makes it possible to share an intermediary account key with co-signers without disclosing any information about non-shared keys in our wallet.

- It makes it easier to extend any of the 1852' or 1854' in a similar manner. The addition of a new role can be done in both scheme very consistently.

  Using a different purpose also fits well the use-case on hardware wallets who can still rely on a single root seed to manage many types of wallets.

- One or many keys can be created from a parent public key via soft-derivation. This allows participant to easily share their multisig keys to participate in a multisig script without having to fetch for their hardware device. The device is still required for signing.

### Related Work

Description                                  | Link
- --                                          | ---
BIP-0032 - HD Wallets                        | https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki
CIP-5 - Common Bech32 Prefixes               | https://github.com/cardano-foundation/CIPs/tree/master/CIP-0005
CIP-1852 - Cardano HD Wallets                | https://github.com/cardano-foundation/CIPs/tree/master/CIP-1852
A Formal Specification of the Cardano Ledger | https://github.com/input-output-hk/cardano-ledger/releases/latest/download/shelley-ledger.pdf

## Path to Active

### Acceptance Criteria

- [x] Document at least one case of a community adopted CIP-1852 compliant multisig wallet:
- [x] [Round Table wallet](https://round-table.vercel.app)

### Implementation Plan

- [x] Community developed reference implementation: [github:ADAOcommunity/round-table](https://github.com/ADAOcommunity/round-table)

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

[rfc8610]: https://tools.ietf.org/html/rfc8610
[rfc8152]: https://tools.ietf.org/html/rfc8152
[BIP-0032]: https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki
[CIP-5]: https://github.com/cardano-foundation/CIPs/tree/master/CIP-0005
[CIP-1852]: https://github.com/cardano-foundation/CIPs/blob/master/CIP-1852
[CIP-11]: https://github.com/cardano-foundation/CIPs/blob/master/CIP-0011
[ledger-spec.pdf]: https://github.com/input-output-hk/cardano-ledger/releases/latest/download/shelley-ledger.pdf
[SLIP-0044]: https://github.com/satoshilabs/slips/blob/master/slip-0044.md

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-1855/README.md
---

- --
CIP: 1855
Title: Forging policy keys for HD Wallets
Status: Proposed
Category: Wallets
Authors:
- Samuel Leathers <samuel.leathers@iohk.io>
- John Lotoski <john.lotoski@iohk.io>
- Michael Bishop <michael.bishop@iohk.io>
- David Arnold <david.arnold@iohk.io>
Implementors: []
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/96
- https://github.com/cardano-foundation/CIPs/pull/194
Created: 2021-06-02
License: CC-BY-4.0
- --

## Abstract

This document describes how to derive forging policy keys used for minting/burning tokens.

## Motivation: why is this CIP necessary?

Forging tokens is derived from a script policy. The script policy includes hashes of keys needed to forge new tokens and must be witnessed by these keys in such a way as the script stipulates.
This CIP defines the derivation path at wich parties are expected to derive such keys.

## Specification

Term     | Definition
- --      | ---
HD       | Hierarchical Deterministic, refers to wallets as described in [BIP-0032].

### HD Derivation

We consider the following HD derivation paths similarly to [CIP-1852]:

```
m / purpose' / coin_type' / policy_ix'
```


To associate policy keys to a wallet, we reserve however `purpose=1855'` for policy keys for forging tokens. The coin type remains `coin_type=1815'` to identify Ada as registered in [SLIP-0044]. We use a hardened index for each policy key as derivation is not needed.

We can summarize the various paths and their respective domain in the following table:

| `purpose` | `coin_type` | `policy_ix`         |
| ---       | ---         | ---                 |
| `1855'`   | `1815'`     | `[2^31 .. 2^32-1]` |

### CIP-0005 prefixes

To distinguish such keys & derived material in the human readable prefix of the bech32 representation, we introduce the following prefixes for insertion into CIP-0005:

#### Keys

| Prefix             | Meaning                                            | Contents                           |
| ---                | ---                                                | ---                                |
| `policy_sk`        | CIP-1855's policy private key                      | Ed25519 private key                |
| `policy_vk`        | CIP-1855's policy public key                       | Ed25519 public key                 |

#### Hashes

| Prefix             | Meaning                                            | Contents                                               |
| ---                | ---                                                | ---                                                    |
| `policy_vkh`       | CIP-1855's Policy verification key hash            | blake2b\_224 digest of a policy verification key       |

### Examples

- `m/1855’/1815’/0’`
- `m/1855’/1815’/1’`
- `m/1855’/1815’/2’`

## Rationale: how does this CIP achieve its goals?

- ERC20 Converter IOHK is developing needs to keep track of policy keys. Rather than having randomly generated policy keys, a policy key can be associated with a mnemonic which is easier to backup.
- A 3rd party may want to have multiple tokens tied to same mnemonic, so we allow an index to specify the token.
- Contrary to CIP 1852, we don't use the `role` and `index` levels of the derivation path, since index is expressed at the 3rd level and no roles for policy signing keys are currently anticipated.

- No prefixes are defined for extended keys, since currently this CIP does not define further derivations.

- We use a different purpose for mainly two reasons:

- It prevents mixing up standard wallets with policy keys used for forging.

- Using a different purpose also fits well the use-case on hardware wallets who can still rely on a single root seed to manage many types of wallets.

### Related Work

Description                                  | Link
- --                                          | ---
BIP-0032 - HD Wallets                        | https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki
CIP-5 - Common Bech32 Prefixes               | https://github.com/cardano-foundation/CIPs/tree/master/CIP-0005
CIP-1852 - Cardano HD Wallets                | https://github.com/cardano-foundation/CIPs/tree/master/CIP-1852
A Formal Specification of the Cardano Ledger | https://github.com/input-output-hk/cardano-ledger/releases/latest/download/shelley-ledger.pdf

## Path to Active

### Acceptance Criteria

- [ ] Standardisation of this derivation path among three wallets as of the Shelley ledger era.

### Implementation Plan

- [x] Common agreement on the above Motivation, Rationale and Specification during the planning of Cardano's Shelley release.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

[BIP-0032]: https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki
[CIP-0005]: https://github.com/cardano-foundation/CIPs/tree/master/CIP-0005
[CIP-1852]: https://github.com/cardano-foundation/CIPs/blob/master/CIP-1852
[ledger-spec.pdf]: https://github.com/input-output-hk/cardano-ledger/releases/latest/download/shelley-ledger.pdf
[SLIP-0044]: https://github.com/satoshilabs/slips/blob/master/slip-0044.md

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CIP-9999/README.md
---

- --
CIP: 9999
Title: Cardano Problem Statements
Category: Meta
Status: Active
Authors:
- Matthias Benkort <matthias.benkort@cardanofoundation.org>
- Michael Peyton Jones <michael.peyton-jones@iohk.io>
Implementors: N/A
Discussions:
- https://github.com/cardano-foundation/CIPs/pulls/366
Created: 2022-10-14
License: CC-BY-4.0
- --

## Abstract

A Cardano Problem Statement (CPS) is a formalized document for the Cardano ecosystem and the name of the process by which such documents are produced and listed. CPSs are meant to complement CIPs and live side-by-side in the CIP repository as first-class citizens.

> [!NOTE]
> Read this CIP's number as "CIP minus 1" (in [tens' complement](https://en.wikipedia.org/wiki/Method_of_complements#Decimal_example))

## Motivation: why is this CIP necessary?

A common friction point regarding complex CIPs is how their main problems are stated. For example, the _'Motivation'_ section in CIPs is sometimes not sufficient -- or simply underused -- to describe the various aspects of a problem, its scope, and its constraints in the necessary detail. This lack of clarity leads, in the end, to poorly defined issues and unfruitful debates amongst participants who understand problems differently.

The introduction of the **Cardano Problem Statements** (CPSs) addresses this gap by defining a formal template and structure around the description of problems. CPSs are meant to replace the more elaborate motivation of complex CIPs. However, they may also exist on their own as requests for proposals from ecosystem actors who've identified a problem but are yet to find any suitable solution.

Over time, CPSs may complement grant systems that want to target well-known problems of the ecosystem; they can, for example, serve as the foundation for RFP (Request For Proposals) documents. We hope they may also help make some discussions more fluid by capturing a problem and its various constraints well.

## Specification

### CPS

#### Structure

CPSs are, first and foremost, documents that capture a problem and a set of constraints and hypotheses. Documents are [Markdown][Markdown] files with a front matter _Preamble_ and pre-defined sections. CPS authors must abide by the general structure, though they are free to organize each section as they see fit.

The structure of a CPS file is summarized in the table below:

Name               | Description
- --                | ---
Preamble           | Headers containing metadata about the CPS ([see below](#header-preamble)).
Abstract           | A short (\~200 word) description of the target goals and the technical obstacles to those goals.
Problem            | A more detailed description of the problem and its context. This section should explain what motivates the writing of the CPS document.
Use cases          | A concrete set of examples written from a user's perspective, describing what and why they are trying to do. When they exist, this section should give a sense of the current alternatives and highlight why they are unsuitable.
Goals              | A list of goals and non-goals a project is pursuing, ranked by importance. These goals should help understand the design space for the solution and what the underlying project is ultimately trying to achieve. <br/><br/>Goals may also contain requirements for the project. For example, they may include anything from a deadline to a budget (in terms of complexity or time) to security concerns. <br/><br/>Finally, goals may also serve as evaluation metrics to assess how good a proposed solution is.
Open Questions     | A set of questions to which any proposed solution should find an answer. Questions should help guide solutions design by highlighting some foreseen vulnerabilities or design flaws. Solutions in the form of CIP should thereby include these questions as part of their _'Rationale'_ section and provide an argued answer to each.
_optional sections_| If necessary, these sections may also be included in any order:<br/>**References**<br/>**Appendices**<br/>**Acknowledgements**<br>Do not add material in an optional section if it pertains to one of the standard sections.
Copyright                                       | The CPS must be explicitly licensed under acceptable copyright terms (see [Licensing](#licensing)).

##### Header preamble

Each CIP must begin with a YAML key:value style header preamble (also known as 'front matter data'), preceded and followed by three hyphens (`---`).

Field                | Description
- --                  | ---
`CPS`                | CPS number (without leading 0), or "\?" before being assigned
`Title`              | A succinct and descriptive title
`Category`           | One registered or well-known category covering one area of the ecosystem.
`Status`             | Open \| Solved \| Inactive (..._reason_...)
`Authors`            | A list of authors' real names and email addresses (e.g. John Doe <john.doe@email.domain>)
`Proposed Solutions` | A list of CIPs addressing the problem, if any
`Discussions`        | A list of links where major technical discussions regarding this CPS happened. Links should include any discussion before submission, a link to the pull request that created the CPS, and any pull request that modifies it.
`Created`            | Date created on, in ISO 8601 (YYYY-MM-DD) format
`License`            | Abbreviation of an approved license(s)

For example:

```yaml
- --
CPS: 1
Title: The Blockchain Trilemma
Category: Consensus
Status: Open
Authors:
- Alice <alice@domain.org>
- Bob <bob@domain.org>
Proposed Solutions: []
Discussions:
- https://forum.cardano.org/t/solving-the-blockchain-trilemma/107720
- https://github.com/cardano-foundation/cips/pulls/9999
Created: 2009-02-14
- --
```

> **Note** A reference template is available in [.github/CPS-TEMPLATE.md][CPS-TEMPLATE.md]

##### Repository Organization

A CPS must be stored in a specific folder named after its number and in a file called `README.md`. Before a number is assigned, use `????` as a placeholder name (thus naming new folders as `CPS-????`). After a number has been assigned, rename the folder.

Additional supporting files (such as diagrams, binary specifications, dialect grammars, JSON schemas etc.) may be added to the CPS's folder under freely chosen names.

For example:

```
CPS-0001
├── README.md
└── requirements.toml
```

#### Statuses

From its creation onwards, a problem statement evolves around the following statuses.

Status       | Description
- --          | ---
Open         | Any problem statement that is fully formulated but for which there still exists no solution that meets its goals. Problems that are only partially solved shall remain _'Open'_ and list proposed solutions so far in their header's preamble.
Solved       | Problems for which a complete solution has been found[^1] and implemented. When solved via one or multiple CIPs, the solved status should indicate it as such: `Solved: by <CIP-XXXX>[,<CIP-YYYY>,...]`.
Inactive    | The statement is deemed obsolete or withdrawn for another reason. A short reason must be given between parentheses. For example: `Inactive (..._reason_...).

> [!NOTE]
> There is no "draft" status: a proposal which has not been merged (and hence exists in a PR) is a draft CPS. Draft CPSs should include the status they aim for on acceptance, typically but not always; this will be _'Open'_.

#### Categories

As defined in [CIP-0001][].

#### Licensing

CPSs are licensed in the public domain. More so, they must be licensed under one of the following licenses. Each new CPS must identify at least one acceptable license in its preamble. In addition, each license must be referenced by its respective abbreviation below in the _"Copyright"_ section.

| Purpose             | Recommended License                                                                    |
| ---                 | ---                                                                                    |
| For software / code | Apache-2.0 - [Apache License, version 2.0][Apache-2.0]                                 |
| For documentation   | CC-BY-4.0 - [Creative Commons Attribution 4.0 International Public License][CC-BY-4.0] |

> [!WARNING]
> All licenses not explicitly included in the above lists are not acceptable terms for a Cardano Problem Statement unless a later CIP extends this one to add them.

### The CPS Process

#### 1. Early stages (same as CIP-0001)

##### 1.a. Authors open pull requests with their problem statement [(as defined in CIP-0001)](https://cips.cardano.org/cips/cip1#authors-a-open-pull-requests)

##### 1.b. Authors seek feedback [(as defined in CIP-0001)](https://cips.cardano.org/cips/cip1#authors-seek-feedback)

#### 2. Editors' role (same as CIP-0001)

##### 2.a. Triage in bi-weekly meetings [(as defined in CIP-0001)](https://cips.cardano.org/cips/cip1##triage-in-bi--weekly-meetings)

##### 2.b. Reviews [(as defined in CIP-0001)](https://cips.cardano.org/cips/cip1#reviews)

#### 3. Merging CPSs in the repository

A statement must be well-formulated (i.e. unambiguous) and demonstrate an existing problem (for which use cases exist with no suitable alternatives). When related to a current project, the problem statement must also have been acknowledged by its respective project maintainers. In some cases, problem statements may be written after the facts and be merged directly as _'Solved'_ should they document in more depth what motivated an existing solution.

Problem statements deemed unclear, for which alternatives exist with no significant drawbacks or establish unrealistic goals, shall be rejected (i.e. pull request closed without merge) with justifications or withdrawn by their authors.

Similarly, problems that appear abandoned by their authors shall also be rejected until resurrected by their authors or another community member.

##### 4. Actors of the ecosystem design and work on possible solutions

Once merged, authors, project maintainers, or any other ecosystem actors may propose solutions addressing the problem in the form of CIP. They add their CIP to the _'Proposed Solutions'_ field in the CPS' _'Preamble'_ once a solution has been fully implemented and reaches the goals fixed in the original statement.

Of course, a solution may only partially address a problem. In this case, one can alter the problem statement to incorporate the partial solutions and reflect the remaining issue(s) to solve.

### Editors

As defined in [CIP-0001][].

## Rationale: how does this CIP achieve its goals?

### Goals

Goals make it easier to assess whether a solution solves a problem. Goals also give a direction for projects to follow and can help navigate the design space. The section is purposely flexible -- which we may want to make more rigid in the future if it is proven hard for authors to articulate their intents. Ideally, goals capture high-level requirements.

### Use Cases

Use cases are essential to understanding a problem and showing that a problem addresses a need. Without use cases, there is, in fact, no problem, and merely disliking a design doesn't make it problematic. A use case is also generally user-driven, which encourages the ecosystem to open a dialogue with users to build a system that is useful to others and not only well-designed for the mere satisfaction of engineers.

### Open questions

This section is meant to _save time_, especially for problem statement authors who will likely be the ones who end up reviewing proposed solutions. Open questions allow authors to state upfront elements they have already thought of and that any solution should consider in its design. Moreso, it is an opportunity to mention, for example, security considerations or common pitfalls that solutions should avoid.

## Path to Active

### Acceptance Criteria

- [x] Review this proposal with existing actors of the ecosystem
- [x] Formulate at least one problem statement following this process
- [CPS-0001: Metadata Discoverability & Trust](https://github.com/cardano-foundation/CIPs/pull/371)
- [CPS-0002: Pointer Addresses](https://github.com/cardano-foundation/CIPs/pull/374)

### Implementation Plan

- [x] Confirm after repeated cycles of CPS submissions, reviews, and merges that the CPS process is both effective and accessible to the community.

## Copyright

This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

[^1]: A problem may be only _partially solved_, in which case it remains in status _'Open'_. Authors are encouraged to amend the document to explain what part of the problem remains to be solved. Consequently, CPS that are _'Solved'_ are considered fully addressed.

[Apache-2.0]: http://www.apache.org/licenses/LICENSE-2.0
[CC-BY-4.0]: https://creativecommons.org/licenses/by/4.0/legalcode
[CIP-0001]: https://github.com/cardano-foundation/CIPs/tree/master/CIP-0001
[CPS-TEMPLATE.md]: https://github.com/cardano-foundation/CIPs/tree/master/.github/CPS-TEMPLATE.md
[CODE_OWNERS]: https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/about-code-owners
[CPS]: https://github.com/cardano-foundation/CIPs/tree/master/CIP-9999
[Discussions:editors]: https://github.com/cardano-foundation/CIPs/discussions/new?category=editors
[Markdown]: https://en.wikipedia.org/wiki/Markdown
[PullRequest]: https://github.com/cardano-foundation/CIPs/pulls
[RFC 822]: https://www.ietf.org/rfc/rfc822.txt
[Repository]: https://github.com/cardano-foundation/CIPs/pulls
[CoC]: https://github.com/cardano-foundation/CIPs/tree/master/CODE_OF_CONDUCT.md
[Discord]: https://discord.gg/Jy9YM69Ezf

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CPS-0003/README.md
---

- --
CPS: 3
Title: Smart Tokens
Status: Open
Category: Tokens
Authors:
- Istvan Szentandrasi <szist@zoeldev.com>
- Robert Phair <rphair@cosd.com>
Proposed Solutions:
- CIP-0113: https://github.com/cardano-foundation/CIPs/pull/444
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/382
- https://github.com/cardano-foundation/CIPs/pull/947
- https://github.com/cardano-foundation/CIPs/issues/969
Created: 2022-11-20
- --

## Abstract

The aim of this problem statement is to raise the issue around the ability to control a native token's lifecycle at more points in time.
Currently only forging of the tokens can be controlled, but not interactions with them. Extending the blockchain to support other interactions is non-trivial and might require many tradeoffs.

## Problem

The Mary era introduced native tokens, that could be minted/burned using native scripts. These native scripts for example could allow defining basic conditions to allow forging like required signatures or time validity range. The native tokens are identified with `policyId` and `assetName`. The `policyId` identifies the "publisher" or "publishers" while `assetName` can be an arbitrary.

With the Alonzo era, plutus scripts were introduced. They allowed extending the functionality of the native scripts with more generic logic scoped at the transaction level. The format and shape of native tokens didn't change - the `policyId` became the hash of the plutus scripts pushing the "publisher" role responsibility to smart contracts.

Skipping ahead, although Babbage didn't add any additional functionality directly, the addition of reference inputs indirectly opened additional use-cases to the the token's lifecycle. To be specific an example: imagine an decentralized bridge with a wrapped token that gets dynamically minted or burned. As a safeguard they would introduce a global UTxO (e.g. with a datum including signature by a multisig wallet or validity token) controlled by recognized members or governance voting mechanism that includes an `enable` flag. The enable flag could be checked by the wrapped token minting policy through a reference input. When a governance vote would pass, the UTxO datum could be changed and the flag could be set to freeze the token.

Even in the Babbage era, after the token is minted, there is no way to control the token after it leaves a smart-contract environment. Users can freely exchange tokens and transfer among each other. This allows cheaper, more efficient and more open ecosystem. In contrast with this, ERC-20 tokens are smart-contracts that fully control any interaction with a token.

Similar to how UTxO addresses can be optionally fully controlled by smart-contracts, there could be potential also to optionally control other lifecycle points of a token than just forging on Cardano.

## Use cases

There are multiple use-cases that would benefit from having the ability to validate token interactions.

From the **NFT space**, having royalties for tokens is currently a social contract where the publisher provides the royalty information while minting the tokens. The marketplaces can choose to honor this information and transfer part of the token sales towards the author. The problem is, this is not enforced and "black market" can thrive. As an author of an art NFT, I would like to benefit from any token transfers that involve smart contracts (a marketplace platform).

From the **dapp-space**, consider validity and authorization tokens. For example, liquidity pools the liquidity provider token minting can be authorized by a validity token. This token _should_ be fully controlled by the liquidity pool smart contracts. If it were to leak out of the liquidity pool smart contract, it would give the ability to anyone to mint LP tokens and drain liquidity pools of the actual funds (as seen in the Minswap security vulnerability). IF the authorization token was itself controlled by a smart contract, even after it left the liquidity pool, the vulnerability could be avoided.

Another use-case comes up around **bridged wrapped tokens**. Due to regulations, some publishers might require the ability to freeze or selectively allow specific flow directions for the token at times. These issues were raised also at the `interoperability working group`. Bridges bridging stables from Ethereum would like more control over the wrapped bridged tokens to be in-line with regulations and help with security.

When it comes to regulations, also **native stable coins** are another case, where the publisher might want to control the ability to freeze or require authorization with interactions of their tokens.

## Goals

The first main goal is to **achieve a consensus** if having more control over the tokens is actually something that would be in-line with the values and direction of Cardano. There are multiple tradeoffs from the usability perspective. Having validated lifecycle for tokens would have an impact on all dapps and all wallets.

The second main goal is **create a CIP** with the technical direction how smart tokens could function. Below is a potential direction and possible issues and roadblocks that would need to be addressed in the CIP.

## Open Questions

1. How to support wallets to start supporting validators?
1. How would wallets know how to interact with these tokens? - smart contract registry?
1. Is there a possibility to have a transition period, so users won't have their funds blocked until the wallets start supporting smart tokens?
1. Can this be achieved without a hard fork?
1. How to make validator scripts generic enough without impacting costs significantly?
1. Can we introduce smart tokens without significantly increasing Cardano's attack surface?

## Copyright

This CPS is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CPS-0005/README.md
---

- --
CPS: 5
Title: Plutus Script Usability
Status: Open
Category: Plutus
Authors:
- Michael Peyton Jones <michael.peyton-jones@iohk.io>
Proposed Solutions:
- CIP-0038 (tentative)
- CIP-0057 (tentative)
- CIP-0069 (tentative)
- CIP-0087 (tentative)
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/497
- https://github.com/input-output-hk/Developer-Experience-working-group
Created: 2023-04-04
License: CC-BY-4.0
- --

## Abstract

The EUTXO model in Cardano makes it possible to implement a wide variety of rich and complex applications on top of Cardano.
However, it does not necessarily make it easy, and there are a number of usability issues that make it difficult for both users and automated systems to work with Plutus scripts in particular.

## Problem

There are a constellation of related issues around using Plutus scripts in practice and sending money to them.
In this CPS we restrict ourselves to usability issue with the _base_ experience of using Plutus scripts, i.e. not issues with implementing complex applications on top of Cardano, but issues with the basic actions of creating and spending outputs locked with Plutus scripts.

### 1. Plutus scripts need datums

Sending money to Plutus script addresses is harder than to other addresses because of the need for a datum in addition to the address and the value.
Moreover, the datum is mandatory, and if it is not included, the output will be unspendable.

On the flip side, many Plutus scripts don’t actually need datums.
Since they are forced to have one, they have to use a trivial fake datum, which just makes things harder for them.

### 2. It is not possible to know from the address whether a datum is required

Plutus scripts require datums otherwise they are unspendable.
Native scripts do not require datums.
But both Plutus script addresses and native script addresses are represented by “script addresses”.
So it’s possible to tell that an address is not a public key address, but not which kind of script address it is.

That means it’s not possible to know whether or not you need to provide a datum when creating  an output.
Since the cost of not including the datum when you need to is high (the output becomes unspendable), this can lead to overly cautious UX from wallets.
Currently some wallets warn when sending money to a native script address without a datum, because they can’t know that it’s not a Plutus script address!

### 3. Users are not familiar with the EUTXO concepts

Users are typically aware of the concept of addresses, but are much less likely to be aware of the concept of datums.
Thus, anything that requires them to think or operate with them is likely to be confusing, counterintuitive, and error-prone (at least at first).

### 4. Difficulty of communicating datums

Application developers often need to tell users to send money to a Plutus script address in order to participate in the application in some way (e.g. paying some money into the contract).
In order to do this, the user will need to provide a datum.
But it is difficult to communicate this to the user, because the most common format for making payments is to just provide an address and the amount to transfer.
This doesn’t accommodate adding a datum.

Plus, since users are not generally familiar with EUTXO concepts, attempts to communicate what they have to do via less direct means are difficult.

There are some tools that can help smooth this over, like the Dapp Connector, but they are not used universally.

### 5. Lack of affordances for EUTXO concepts

Many systems for working with Cardano don’t account for the need for datums or don’t make it easy to provide them.
A key example is wallets, but more generally many systems that can take an address to send funds to needs to explicitly accommodate datums, otherwise it will not be possible to use Plutus script addresses with it (see the Catalyst use case for an example).
This is often not obvious to the designers of the system!

### 6. Interaction models for Plutus scripts are obscure

Interacting with Plutus scripts is hard for everyone: users, wallets, and applications.
In base Cardano:

- You can’t find out how to form the datum and redeemer objects correctly
- You can’t find out what kind of actions you can take with a script
- You can’t find out how to take particular actions that you want to take

All of this makes it hard for both humans and computer systems to know whether they are interacting with the script correctly, which increases the risk of error significantly.
It also makes generic discoverability impossible, which forces application developers to provide lots of custom logic for every application in order to present users and systems with a comprehensible interface.

## Use Cases

### Simple escrow

Alice wants to escrow money using an escrow script S.
S requires a datum that indicates where to send the money back if the escrow fails to complete by the timeout.
It is difficult for the Alice to know a) that the datum is required, b) what the format is, c) how to actually enter it and make the payment.

### Catalyst payouts

Catalyst proposals include where to send the funds if the proposal is successful… in the form of an address.
This doesn’t support datums, so Plutus script addresses cannot be used to receive Catalyst funds.
Thus one cannot, for example, have the funds go directly into a DAO or similar system.

### DAO contribution

Bob wants to pay some money into a DAO or similar system which is supposed to reward them for their contributions with some special tokens.
In order for this to be enforced on-chain, we need a Plutus script output with a datum that records who the contributor was, so that their reward can be routed to them.

This means that Bob needs to construct a Plutus script output with a datum at some point, and that Bob cares about the content of the datum and cannot fully trust another party to construct it for them (otherwise they might just route the reward to themselves).
So Bob needs to somehow ensure that the correct output is created, and to verify the content of the datum.

### Ada Handles

The Ada Handle system works as follows:

1. Try to resolve handle H
2. Look for a specific NFT T that is related to H
3. H resolves to the address of the output at which T is held

This resolves the handle to an address, but as we have seen this is not enough to know how to make a payment to that address if it is a Plutus script address (which you can’t know).
A naive system which assumes it can send money to the address directly may create unspendable outputs if the handle token is held at a Plutus script address.

### Native script payments

Charlie wants to make a payment to a native script address.
This doesn’t require a datum, so they can do it simply based on the address, but their wallet gives them a scary warning because it doesn’t know that the address isn’t a Plutus script address that requires a datum.

### Smart contract wallet

Eve has no idea about key security, but cares about custodianship and hence prefers to use a wallet which allows her to recover funds using a social recovery key sharing scheme.
This system uses a Plutus script to be the “owner” of the funds.
When Eve wants to receive funds, her friends should not need to know what kind of wallet she is using - she should be able to provide them with a simple way to pay that is not meaningfully harder or different to paying into a normal wallet.

## Goals

### Avoid the need for users to know about datums in simple cases

Ideally users should be able to mostly not know about datums unless they actually want to determine the content of the datum themselves.

### Avoid forcing datums on scripts that don’t need them

If a script doesn’t want or need the facility to have datum then it shouldn’t be forced to have one, even if that makes things more consistent for the ledger.

### Uniform handling of payments to non-script addresses and script addresses

There should be a straightforward path for a system that deals with payments to support both non-script and script addresses correctly without additional effort.

### Single string for payments to script addresses

There should be a way to provide a user or system with a single string that contains all the information needed to make payment, whether it is to a Plutus script address or otherwise.

### Reduce the risk of accidentally making unspendable outputs

Try to make it less likely that users will accidentally create unspendable outputs, at least in some parts of the problem space (e.g. scripts that don’t “actually” need a datum).

## Open Questions

### Do we need to modify Cardano itself?

Many of the listed problems are about users interacting with Cardano.
That suggests that it may be possible to mitigate the problems in the systems that users use for interacting with Cardano (wallets, applications, etc.).

More generally, it’s unclear to what degree we should be aiming for excellent UX in Layer 1 itself.
But if we can make simple changes in Layer 1 that make it much easier for supporting systems to provide good UX, that might well be worth it.

### How many solutions do we need?

This CPS lists a lot of problems.
It’s not clear whether we will be able to come up with “big” solutions that solve many of the problems together, or whether we will need many “small” solutions that solve specific problems.

### How does this relate to generic metadata problems?

There has long been a problem of how to establish metadata about on-chain entities in Cardano (no CPS so far, but see CIP-25, CIP-26, CIP-68, etc. for various attempted solutions).
Many of the above problems could be mitigated with a good metadata solution, and it’s unclear to what degree this just “is” the metadata problem again.

For example, simply knowing the script itself (i.e. the pre-image of the hash used in the script address) helps with problem 1, because then you can know that it’s a Plutus script.
But it still doesn’t tell you what the form of the datum should be (problem 6), but this could be conveyed with additional metadata.

## Copyright

This CPS is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CPS-0007/README.md
---

- --
CPS: 7
Title: Voltaire era Governance
Status: Open
Category: Ledger
Authors:
- Pi Lanningham <pi@sundaeswap.finance>
Proposed Solutions:
- CIP-1694
Discussions:
- https://discord.gg/hdqHwSgWvG
Created: 2023-03-14
License: CC-BY-4.0
- --

## **Abstract**

It has long been part of Cardano's vision for the "final" roadmap era to be one of Governance: allowing the community of ADA token holders to meaningfully own the decision making process by which the chain evolves.

To frame this discussion, it's important to outline a set of goals and baseline truths. Some of these will be neutral and uncontroversial, while others are in need of community discussion.

### **Acknowledgements**
<details>
<summary> Thank you to the following people who helped review this draft before it was published: </summary>

- Adam K. Dean
- Jared Corduan
- Vanessa Harris
- HeptaSean

</details>

## **Problem**

At the core of it, a system of governance boils down to how decisions are made and enforced.  Cardano currently has 3 core decisions that greatly impact the operation of the chain:

- When and how to change the ledger rules (a "Controlled Hard Fork")
- These changes are often drastic, with fundamental tradeoffs and extremely high risks
- When and how to update various protocol parameters
- These parameter changes, while limited in scope compared to a hard fork, can still have potentially existential risks on the chain
- When and to whom to disburse funds from the treasury
- Over the last several years, this treasury has accumulated nearly 1 billion ADA, currently worth hundreds of millions of dollars

The impact of these decisions are also often closely intertwined: development of the next hard fork may need funding from the treasury; parameters may need to be tweaked after a hard fork; etc.

Today, these decisions are primarily made by the founding entities (IOG, Cardano Foundation, and Emurgo). While these entities often consult the community when making these decisions, the final power lies in their hands, with very little that can be done beyond an uncontrolled hard fork by the stake pool operators.

For a variety of reasons, including moral, philosophical, financial, and possibly regulatory, these founding entities want to dilute their own power substantially, and share the burden of governance of the chain with the community that has arisen around it.

IOG in particular has continued to provide skill and resources for years longer than initially intended, and is significantly over-budget and over-extended. IOG has said that they may be unable or unwilling to support development of the chain indefinitely, and so an expedient and iterative approach to governance is likely to have higher positive impact on our ability to maintain velocity as an ecosystem.

However, diluting that power to the community comes with substantial risks and challenges, not the least of which is ensuring that the will of the community is captured *accurately*. For example, one barrier to that accuracy is the possibility of "sybil attacks", wherein the anonymity of the blockchain allows someone to cast their vote many times.

## **Goals**

Synthesizing from the above, and with input from the broader Cardano community, the following goals, constraints, considerations, or ground truths for any governance system arise:

1. A system of on-chain rules should allow decisions to be made and enacted regarding (at a minimum) the three types of changes listed above.
2. Every (or nearly every) ADA holder should be able to meaningfully participate in this decision making process.
3. This system should not materially undermine the security of the chain.
4. This system should not undermine its ability to serve its financial function, such as overburdening the chain, damaging the monetary policy, etc.
5. This system should be implementable within a reasonable time frame and budget.
6. This system should be recursively updatable; if improvements or changes to the system of governance are needed, they should be voted on and enacted via this system.
7. This system is highly dependent on the off chain user experience and community tooling, since very few users are reading raw CBOR dumps from the chain.
8. This system should be cognizant of the fact that different decisions can have different impact and risk thresholds.
9. This system cannot avoid the fact that stake pool operators of the network, collectively, always maintain control over a "pocket nuke": under sufficient discontent and misalignment with the will of governance, they can modify the code and initiate an uncontrolled hard fork to produce a separate chain aligned with their values.

Additionally, here is a more opinionated (and potentially controversial) list of potential considerations for a CIP, presented purely to foster discussion:

1. There is currently no known decentralized "proof of personhood" system that is well specified and easily implementable, and without such a system, "one lovelace one vote" systems are the most effective defense against sybil attacks.
2. Given the current near total control over these decisions that these founding powers have, any dilution of power that doesn't compromise the security of the chain or risk a deadlock is a worthwhile step to take.
3. The system loses legitimacy if participation is extremely low, or participation is severely uninformed; If a significant portion of the Cardano community has not yet had a reasonable opportunity to participate, or the majority of participation is indinstinguishable from randomness, then any decisions made during this time are illegitimate.
4. Not every ADA holder is interested in *directly* participating in every decision. Often an ADA holders voice can still be heard through delegation to another ADA holder.
5. If the system contains such a notion of delegation, the ability to withdraw, redirect, or reclaim that voice at any time is an important tool in fighting corruption.
6. While it may be useful to draw inspiration from existing real-world systems, it should also be understood that blockchain governance will have an entirely different set of constraints.  For example, the ADA community does not share one culture or geographic locality. Additionally, the decisions being made are not the same kinds of decisions that a world government needs to make.
7. The uncontrolled hard fork described above is dangerous (from a safety standpoint) and disruptive (from an economic standpoint); it will likely seriously undermine the cohesion of the Cardano community. For this reason, it should be considered a tool of absolute last resort.

### Open Questions

Based on the goals above, an incomplete set of questions arises that any proposed solution likely ought to answer:

1. What are the exact categories of actors considered by the proposed system, and what actions are available to them under what conditions? What are the explicit justifications for the additional complexity added by distinguishing each category as a separate category?
2. How is the weight of each vote determined in the proposed system, and how does it defend against fraud and manipulation?
3. How does the proposed system decide that it has "reached quorum", and can legitimately enforce the decision that has been reached?
4. What vectors for denial of service (at both the protocol level and social level), and how does the proposed system mitigate these?
5. What is the proposed path to adoption of the proposed system?
6. What "transitionary" periods does the solution include, and what are the justifications for them? Or, conversely, what is the justification for *not* having a transitionary period?
7. What is an estimated timeline and budget for implementation of the proposed system?
8. Who is going to implement the proposed solution, and how will it be paid for?

### Philosophical Questions

Below are a set of philosophical questions applicable across a broad range of solutions that are being discussed by the community:

1. Are there any "fundamental rights" of ecosystem participants that should be protected by the governance system?
2. Must any and all voting related to governance matters occur directly on the layer 1 ledger?
3. What role, if any, do the founding entities play after governance is adopted?
4. What kinds of community standards are needed outside of the scope of a specific solution at the ledger layer?
5. What kinds of incentives (or disinsentives) should we use to align behavior with our goals? Should we try to compensate actors for the time and effort they put in, or does that create corrupt and misaligned incentives of their own?
6. Is the ability to delegate overall more helpful or harmful? Does the risk of "popularity" based delegation (as opposed to trust / expertise based delegation) dilute the votes of others more than it increases inclusivity?

## Copyright

This CPS is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CPS-0008/README.md
---

- --
CPS: 8
Title: Domain Name Resolution
Status: Open
Category: Tools
Authors:
- Hinson Wong <hinson@cns.space>
Proposed Solution: []
Discussions:
- https://github.com/cns-space/CIPs
- https://github.com/cardano-foundation/CIPs/pull/605
Created: 2023-10-14
License: CC-BY-4.0
- --

## Abstract

As different domain projects emerge, the same prefix or suffix may be employed by different projects, leading to potential naming conflicts. One of the key features of blockchain domain service is to resolve domain information. Conflicting names would lead to the integration ambiguity in resolving. To address this, a community-aligned mechanism should be in place to enable users to select their preferred project when resolving names. This ensures a seamless user experience.

The CPS is written up accordingly to open for discussion on potential approaches to the above issue.

## Problem

As the ecosystem emerges, more and more domain service projects entering Cardano. Currently noticeably there are 3 domain projects:

1. `ADA Handle`
2. `adadomains`
3. `Cardano Name Service (CNS)`

Both `adadomains` and `CNS` has chosen `.ada` as domain suffix. When proceeding with resolving integration, one common issue faced is the ambiguity in resolving approach, which could lead to user confusion at the minimal impact. On some specific information resolving, it might even cause serious loss of fund when sending tokens with monetary value to unexpected recipients.

## Use cases

Let's illustrate the potential issues with a hypothetical example in wallet address resolving area.

1. Assuming `Yoroi` wallet is going to integrate with `CNS` and `NuFi` wallet is going to integrate `adadomains`. (And both `CNS` and `adadomains` opt for `.ada` as the suffix)
2. Both protocol has their separate domain NFT called `hinson.ada` and there are separate holder of `hinson.ada` for 2 domain projects.
3. When the `hinson.ada` `CNS` holder requesting fund from others, there would be potential loss of fund when the user sending fund to `hinson.ada` through `NuFi` since the fund would be sent to `adadomains` domain holder instead.

This is not a desirable outcome as funds are mistakenly sending to the incorrect recipient without users noticing it.

## Goals

The goal of this `CPS` is to invite community discussion on how to best approach this potential problem when the ecosystem evolves and having more and more domain projects entering the space.

### Pro-decentralization Solution

The solution should embrace the decentralization property of a blockchain where it welcomes multiple domain name services exist in the market without user experience competition at the infrastructure level. The solution should not be made customized for any specific domain service providers, ideally shifting domain service projects' focus to enhancing features rather than competing with each other by oligarchic alliance.

### Clear User Experience Flow

It ensures a user-friendly experience and allows users to make informed decisions in case of naming conflicts. It accommodates multiple projects with the same suffix while avoiding disputes and confusion, fostering the integration process of wallet and domain service.

## Open Questions

### On Domain Service Providers

1. Should there be any standard in domain service provider side to store user information?
2. Should there be standardization in domain metadata to assist with consistent integration?
3. Apart from address resolving, should there be any standardized way to resolve domain information?
4. Is there any threshold or minimum requirement on domain projects in order to be considered as applicable to apply the potential solution?
5. Following `question 4 on integration partners`, would it be the domain service providers' responsibility to ensure the integration is in compliance with the potential solution?

### On Integration Partners (e.g. Wallets)

1. Should the potential solution just provide a high level guideline on integration user experience flow? Or a detailed guideline would be preferred?
2. How to inform the community on whether which projects participated in the solution `CIP` so to alleviate users' concern when proceeding with integration partners? For example user could be confirmed that their fund would not be sent to unexpected recipients when using certain wallets participating in the `CIP`.
3. Should there be any kind of community verification to include integration partners on to the participation list?
4. If the potential solution `CIP` only serves as a soft guideline, how could we make it useful to community when projects integrating with domain services do bother to spend time and effort into this alignment?

## Copyright

This CPS is licensed under [CC-BY-4.0].

[CC-BY-4.0]: https://creativecommons.org/licenses/by/4.0/legalcode

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CPS-0009/README.md
---

- --
CPS: 9
Title: Coin Selection Including Native Tokens
Status: Open
Category: Wallets
Authors:
- Hinson Wong <wongkahinhinson@gmail.com>
- Tsz Wai Wu <wu.tszwai@outlook.com>
Proposed Solutions: []
Discussions:
- https://github.com/cardano-foundation/CIPs/issues/232
- https://github.com/cardano-foundation/CIPs/pull/611
Created: 2023-10-20
- --

## Abstract

The introduction of native tokens in Cardano has introduced unique challenges related to coin selection and transaction efficiency. This Cardano Problem Statement (CPS) addresses the need for a specialized coin selection approach to optimize transactions involving native tokens while minimizing transaction size and complexity.

## Problem

The integration of native tokens in Cardano transactions has brought both opportunities and challenges. One of the significant challenges is associated with coin selection when dealing with native tokens.

### 1. Need for Efficient Coin Selection

Cardano transactions involving native tokens require attaching a minUTxO value (lovelace) to the transaction. In scenarios where multiple tokens are associated with a single UTxO, selecting such UTxOs as inputs can lead to inefficient transactions. This inefficiency arises from the increased transaction size due to token information, potentially impacting the decentralized applications and network performance.

### 2. Streamlining the Selection Process

Streamlining the selection process is crucial. Enhancements should be made to the selection algorithms to ensure that, even in complex scenarios, the coin selection process remains efficient and doesn't compromise the user experience.

### 3. Compatibility with Serialization Libraries

The largest off-chain serialization library, `cardano-serialization-lib`, still follows a modified version of the [CIP-2 standard](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0002/README.md), which was established in the pre-native token era. There's a need to ensure that the proposed coin selection approach remains compatible with existing serialization libraries, making it accessible to a wider range of developers and applications.

While CIP-2 certainly works well in an environment where native assets such as tokens and NFTs don't exist, it has been expanded upon differently by each serialization library, with their own custom solutions to select for tokens. It would be useful to once again have some standard for coin selection that is trusted by the community.

### 4. Min UTxO Value for change with tokens

CIP-2 doesn't consider `minUTxOValue` in the change output with tokens, if a pure ADA change output cannot be created that reaches the `minUTxOValue`, it is possible to simply increase the fees paid for the transaction by that small amount to balance the transaction. This, however, doesn't work when tokens are involved, any tokens in the transaction inputs MUST be included in the change output.

## Use Cases

### Native Token Transactions

Users and applications frequently engage in native token transactions, making efficient coin selection essential to minimize transaction costs and network congestion.

### DApps and DeFi

Decentralized applications and DeFi projects require efficient coin selection to maintain the performance and cost-effectiveness of their transactions.

### Example Implementations

Some real world use cases of coin selection algorithms taking Native Tokens into account are listed below:
1. [UTxO utils of Cardano in WASM](https://www.npmjs.com/package/cardano-utxo-wasm)
2. [Cardano Wallet](https://github.com/cardano-foundation/cardano-wallet/tree/master/lib/coin-selection)
3. [cardano-multiplatform-lib](https://github.com/dcSpark/cardano-multiplatform-lib)
4. [RoundTable](https://github.com/ADAOcommunity/round-table)

### Network Scalability

Efficient coin selection contributes to network scalability by reducing the size and complexity of transactions, ensuring smooth and rapid processing.

## Goals

### Specialized Coin Selection Approach

The primary goal of this CPS is to establish a specialized coin selection approach for transactions involving native tokens in Cardano. This approach should prioritize necessary tokens, improve transaction efficiency, and minimize the impact of native token inclusion on transaction size.

### Consider change and fees

In Cardano, there is somewhat of a cyclic dependency between UTxO selection, change output and fees. It would be extremely helpful if some consensus was reached on how to handle this part of transaction building. Since the most naïve approach essentially requires rebuilding the transaction several times, there is potential to significantly reduce latency in DApps with a more efficient approach.

### Useful change outputs

CIP-2 also selects inputs in order to generate "useful" change outputs. This will be a significantly more complex problem when native tokens are considered, but will significantly improve the collective efficiency of transaction generation in wallets.

### Streamlined Transactions

By optimizing coin selection, we aim to streamline Cardano transactions, reducing their size and complexity while preserving the integrity of native token operations.

### Cross-Compatibility

The proposed approach should remain compatible with existing off-chain serialization libraries and protocols, ensuring accessibility to a wide range of developers and projects.

### Improved Network Performance

Efficient coin selection contributes to overall network performance, making Cardano more scalable and reliable for users, DApps, and DeFi.

## Open Questions

### Implementation

1. How can we effectively implement and promote the specialized coin selection approach?
2. What changes, enhancements, or protocols need to be adopted within the Cardano ecosystem to achieve this?
3. Is there any community collective intelligence could gather for this area? Particularly, would engineers from Emurgo (who maintain `cardano-serialization-lib`) and developers of `cardano-multiplatform-lib` have any form of insight to push forth community progress?

### Developer Education

1. Are there any changes on application code itself with improvements on coin selection algorithms?
2. If so, how can developers be educated and informed about any new coin selection approach to ensure its successful adoption and integration into their projects?

### Community Consensus

1. How can we gather and build consensus within the Cardano community regarding any proposed coin selection approach?
2. Do we need any support from academia with formal proof to impose the standard?
3. What methods can be employed to ensure widespread acceptance and adoption? Do we need endorsement from any of IOHK, CF or Emurgo?
4. If academic formal research is not needed for this consensus, how can we set the bar for an acceptable algorithm? Would there be a core committee making the decision?

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CPS-0010/README.md
---

- --
CPS: 10
Title: Wallet Connectors
Status: Open
Category: Wallets
Authors:
- Ryan Williams <ryan.williams@intersectmbo.org>
- Vladimir Kalnitsky <vladimir@mlabs.city>
Proposed Solutions: []
Discussions:
- https://github.com/cardano-foundation/cips/pull/619
Created: 2023-07-28
License: CC-BY-4.0
- --

## Abstract
Wallets the foundational element of Web3, being the primary interface between users and blockchains.
Wallet connectors allow users to connect their wallet to client stacks (i.e. dApps), facilitating a wide range of specialized user experiences.

Wallet connector standards largely consist of two parts: the connection standard and the API.
The connection standard defines how the wallet and dApp initiate communication, for example, using an injected Javascript object.
The API defines what communications look like between the dApp and wallet post connection.

This problem statement is concerned with the issues surrounding Cardano's current and future wallet connectors.
These interfaces are difficult to define and historically have been even harder to iterate upon.
We wish to provide a comprehensive catalogue of the current offerings and their drawbacks to be able to make suggestions on future standards.

For the many contributors to this proposal, see [Acknowledgements](#acknowledgements).

## Problem
The motivation for this document is to outline the current state of Cardano's wallet connectors, discuss their flaws, and identify key concerns for future connector authors to be aware of.
We hope that by discussing the issues, we will inspire the next generation of Cardano wallet connectors so that the ecosystem can grow beyond its first connector iterations.

Ineffective connectors can cause a range of issues for wallets, dApp developers, and thus users.
Due to the nature of these connections, there can often be unforeseen impacts from small design decisions.

For users, connectors should offer secure and reliable compatibility with a wide range of dApps.
For dApps, connectors should be reliable and provide stable and rich APIs.
For wallets, connectors should be secure, optionally extendable, and with APIs which do not expect wallets to go beyond standard expectations.

### Core Concerns
Here we outline what we see as the necessary concerns which connector authors should consider.

#### 1. Security
Security should remain of paramount concern within Web3.
Connection standards must strive to, above all, not compromise the security of wallets or dApps.
Connection standards should be aware of the potential impacts of standard security vulnerabilities, such as man-in-the-middle attacks.

The security of the API should remain eminent.
An example of this is that no secret information should ever be allowed to leave the wallet.

#### 2. Range of supported connection
Connection standards should support a wide range of wallets and client platforms.
This means we shouldn't assume a software environment (e.g. JavaScript in the browser) and define the APIs using schema languages widely used in language-agnostic contexts.
With the base-level connections only offering an indication of supported APIs bidirectionally.

#### 3. API Expressiveness
APIs should allow for an expressive range of information to be exchanged.
Information should be able to be passed in both directions.

#### 4. Versioning
Connection standards and APIs should be explicitly versioned to clearly allow upgrades.
Furthermore, ideally, APIs should allow for optional extendability to facilitate specialization of connection.

#### 5. Adherence to Expected Roles
APIs should be written to have a clear scope, understanding the potential strains placed on wallet providers.

### Context

#### Cardano's Wallet Connector*s*
[CIP-30](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0030/README.md) is *the* wallet connector for Cardano.
This standard has facilitated the emergence of dApp development on Cardano by defining both a connection standard and an API.
It is, to date, the only wallet connector to see wide adoption in the ecosystem.
Using an injected Javascript object for communication between wallets and web-based dApps.
Since its' authoring, CIP-30 has seen continued iteration with many tweaks to its API.

##### CIP-30 Alternatives
Post CIP-30's acceptance there have only been two other competing standards, in CIP-45 | Decentralized WebRTC dApp-Wallet Communication ([#395](https://github.com/cardano-foundation/CIPs/pull/395)) and CIP-90 | Extendable dApp-Wallet Web Bridge ([#462](https://github.com/cardano-foundation/CIPs/pull/462/)).
With neither of these standards gaining significant adoption.

With CIP-45 offering an alternative connection standard based on initiating a WebRTC connection via WebTorrent tracker, taking a similar to [Wallet Connect](https://walletconnect.com/).
This was motivated by a want to diversify the connection standards in Cardano.
Unlike CIP-30 this proposal does not define any standards for the API shared after connection time, instead it just reuses the [CIP-30 Full API](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0030/README.md#full-api).

CIP-90 was created in reaction to the last major changes to CIP-30, within [#446](https://github.com/cardano-foundation/CIPs/pull/446).
This proposal aims to reuse the CIP-30 connection standard, whilst making the CIP-30 API optional, allowing for other CIPs to specify further APIs.
This contrasted the ideas presented within [#446](https://github.com/cardano-foundation/CIPs/pull/446) which stipulated that all connections should include the CIP-30 Full API.

Although not a direct competitor, [CIP-13 | Cardano URI Scheme](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0013/README.md) could be seen as an alternative standard, fitting some wallet-client niches.
This URI scheme allows a sort-of one way connection, where information can be passed to a wallet to trigger a function.

#### Historical Issues
CIP-30 is likely the most iterated upon CIP, post merger into the CIPs repository.
These changes have been attempting to remedy issues with the initial proposal.

##### Versioning
Despite the CIP-30 API including a [versioning mechanism](https://github.com/cardano-foundation/CIPs/blob/master/CIP-0030/README.md#cardanowalletnameapiversion-string) it was not utilized.
Alterations to the API were accepted without incrementing the version number.

These changes resulted in diverging implementations and thus broken compatibility.
At connection time there would be no way to tell which version of CIP-30 the wallet had implemented.
Often resulting in errors during dApp-wallet communication, degrading usability.

A result of this has been that dApps whitelist only those wallets which they have tested against.
This completely undermines the benefits of having such a standard.
Furthermore this whitelisting has encouraged those smaller wallets to emulate the whitelisted wallets, by injecting themselves under the name of the whitelisted wallets.
This sort of arms race, is completely unnecessary and results in future complexities for both sides.

##### Unclear Responsibilities
CIP-30 does not provide rationale to what should be the concern of the wallet and clients.

The knock-on effect of this is uncertainty regarding the direction future CIP-30 development should follow.

##### Limited Scope
By it's nature CIP-30 is a base level connector designed to get the first dApps on Cardano moving.
It does, of course, have a finite scope which cannot support future upgrades or specialized connections.

##### Language Dependent
CIP-30's connection standard and API is defined using Typescript.
Whilst this is convenient for Javascript-based clients this is not convenient for other stacks.

Tying the wallet connector to a single language is naturally limiting and will cause friction for potential adopters.

##### Limited to Web-based Stacks
The CIP-30 connection standard is based upon injecting code into shared web windows.
This is convenient for browser extension wallets and web-based client stacks, but is awkward for all other implementors.

One consequence of this is that mobile wallets are thus required to reimplement web browsers in their applications, which is wasted effort.
A similar novel solution would be required for the current full node wallets to be able to utilize CIP-30 compatible dApps.
This is not great as non-web-based wallets (and their users) are unable to utilize the benefits of Cardano's web3.

##### Undefined Behavior
The overall descriptions of CIP-30 behavior are brief, leading to many potential cases of undefined behavior.

For example; should the results of `.getUtxos()` include UTxOs the wallet wishes to reserve/not spend? or include UTxOs the wallet knows are in the process of being spent? how should these reserved/pending UTxOs factor into the result of `.getBalance()`?
Such behavior is unclear and thus different wallets will implement behavior differently, leading to inconsistent experiences for dApps and users.

##### Combined Standard
CIP-30 defined both a connection standard and an API which must be used on every connection.
This is limiting as it forces all connections using the CIP-30 connect to always use the same endpoints.
This adds unneeded complexity, making iterating on connection or API more difficult.

##### No event listener
The CIP-30 API and connector is purely based on synchronous and asynchronous calls made by the client dApp.
This prevents useful advantages of event-based design, such as dApps subscribing to state update events emitted by wallets.

#### CIP-30 Iteration Improvements
CIP-30 has seen some efforts to address its flaws.

##### CIP-30 Extensions
The CIP-30 extendibility mechanism was introduced as a novel versioning scheme within [#446](https://github.com/cardano-foundation/CIPs/pull/446) and was further refined in [#577](https://github.com/cardano-foundation/CIPs/pull/577).
The motivation was to provide a safe way to optionally modify the CIP-30 API without breaking existing implementations.
Furthermore these changes were introduced to stop any further modifications to the CIP-30 API, negating the need for versioning.
This has allowed for the creation of further CIPs defining API extensions, to upgrade, specialize and enhance CIP-30 connections.
With extensions also going part way to be able to address undefined behavior.

At time of writing there are four proposed extensions, with only one being merged into the CIPs repository.

##### CIP-30 Alteration Freeze
As part of the extension mechanism be added within [#446](https://github.com/cardano-foundation/CIPs/pull/446), a less formal shift in attitude was adopted by the CIP editors.
In that no more alterations to CIP-30 would be allowed, instead all changes must be done through the extension mechanism.
This prevents anymore downstream issues caused by lack of versioning.

##### This Problem Statement
The hopefully final remediation to CIP-30 is this document.
We want to discuss CIP-30's drawbacks, so they can be understood moving one step closer to solving them.

## Use cases

### NFT Marketplace
Alice wishes to buy a NFT from an smart contract based NFT marketplace for an agreed price because she wishes to support the artist.
Alice wants to be able to use a familiar website interface where she can browse rendered NFTs then select and buy on the same site.

Without the dApp and her wallet communicating it is very difficult to construct the needed transaction to buy the NFT Alice wants.

### Simple Wallet Login
Bob wants to use his wallet to login to a website.
The website requires that Bob presents both; his public credentials and also prove his ownership of them.

It is difficult for Bob to produce and share proof of ownership for public credentials in a secure way without the dApp and wallet communicating.

### dApp Developer
Carol is a dApp developer, who wants many users to be able interact with her dApp via web interface.
She wants to be able to support a wide range of wallets and platforms to maximize potential user base.
Whilst she wants the API standards to be expressive so that her dApp is able to offer a range of functionality.

### Wallet provider
Dave has created a wallet, he wants to minimize the cost of running his wallet.

Without clear role of the wallet, and optional API extension scoping Dave's infrastructure may be asked to do many operations incurring cost.

## Goals
These goals we outline for future wallet connection and API standards are based upon solving our [Core Concerns](#core-concerns).

- *Wallet connectors should:**

### 1. Prioritize Security
Potential security concerns of wallet connector designs should be well understood and discussed in accompanying CIPs.
Authors should seek the collective knowledge of the CIP process to obtain wide range of perspectives.

The impact of any data leaving the wallet should be discussed, especially for the cases of data that is not observable via the chain, such as root public keys.

Although security tolerances are at an implementors discretion, the potential negative impact on the ecosystem should be taken into account by authors.

### 2. Provide Interoperable, Optional and Extensible APIs
Future API standards should be connection agnostic as well as stack standard agnostic.
Meaning API CIP-1234 should work if connection has been initiated via CIP-30 connection or CIP-45, or CIP-4321.

API standards should exist within an extensible framework, whereby specific connections can be permissioned and specialized.
APIs should be developed via the CIP process, whereby CIPs will each define their own set of endpoints.

Implementing the API support should be fully at the discretion of the wallet providers.
Although at connection time dApps should be free to request as many APIs as they wish, they should not rely on the implementation of optional APIs beyond APIs implemented in [No-data Wallets](#no-data-wallets).
Connections be based on "who am I? what do I support?"

### 3. Support Versioning
Versioning standards should be utilized by connection standard and API authors.
APIs should inherit the novel versioning utilized for CIP-30 extensions.
Connection standards should be free to employ any versioning scheme they wish.

### 4. Work Within the Role of Wallet
Connection and API authors should understand that there are different types of wallets, each with their own constraints.
Standards should be aware of this and discuss which wallets are applicable in their CIPs.

Whilst most software wallets generally offer users a range of features, at their *core* all wallets are concerned with the management of cryptographic operations.
The majority of wallets build on this by using a chain indexer, so information related to wallets' cryptographic credentials can be gathered and presented to users.

By only expecting wallets to support cryptographic operations we define a universal rule for all wallet connectors.
This means at a minimum all dApps can expect from wallets during connection is cryptographic operations.
Although, dApps should be able to request additional functionalities on connection if they wish.

- *Building on this we define further divisions:**

#### Types of Wallet
All possible wallet standards can be divided into three groups based on a single criterion: what data they provide to dApps via the API endpoint.

This division is important because each of the groups allows for different dApp architectures.

- *Note** The groups are nested: every wallet is a no-data wallet and every full-data wallet is also an own-data wallet if we look at the dApp architectures they enable.

##### No-data Wallets
No-data wallets do not provide data queries and are only concerned with cryptographic operations (e.g. hardware wallets).
All management of UTxOs is placed on dApps side.
No-data wallets can use multiple addresses, but they do not allow to query for available UTxOs and do not indicate which of the addresses are actually used on-chain.

These wallets may even rely on dApps to derive addresses, by sharing the root public key.

![diagram showing wallet and dApp architecture for no-data wallets](./no-data-wallet.drawio.png)

This design ensures that wallets can function without a query layer and thus without runtime infrastructure needed.

##### Own-data Wallets
Own-data wallets provide chain queries related to users' own data and wallet state (users' addresses and UTxOs).
CIP-30 falls into this category.

![diagram showing possible wallet and dApp architecture for own-data wallets](./own-data-wallet.drawio.png)

This architecture has two major issues:

- **Unclear separation of concerns** - while wallets provide certain functions for dApp developers, they have no means to enforce their use, and naturally some developers opt to using their own query layers when they see fit, with consequences that are hard to predict. For example, the wallet may not allow signing a transaction that consumes an UTxO it considers "locked".

- **Two sources of truth problem** - on Cardano, every running node has its own opinion on the set of currently unspent transaction outputs. Only [eventual consistency](https://en.wikipedia.org/wiki/Eventual_consistency) is guaranteed. Any dApp that interacts with an own-data wallet has to deal with the inconsistency between local `cardano-node`-based query layer and light wallet query layer, especially when dApp workflow involves sending multiple interactions with the wallet in quick succession. For example, a wallet may refuse to sign a transaction containing an UTxO it does not (yet) know about.

Even if we enforce clear separation of concerns in the standards, e.g. by stating that sending a transaction bypassing the wallet is not allowed, the problem of two sources of truth will remain.

##### Full-data Wallets
Full-data wallets allow to query blockchain data outside of user's scope (i.e. anything not covered by own-data wallets).

![diagram showing possible wallet and dApp architecture for full-data wallets](./full-data-wallet.drawio.png)

Depending on dApp needs, full-data wallets open a way to implement fully-functional dApps that use non-local blockchain data without the need for the developer to maintain dApp backend infrastructure.
As a result, more decentralized and censorship-resistant architectures become possible: a dApp can be distributed as a set of files, and deployed to IPFS or static hosting websites.

Full-data wallets enable "single source of truth" architecture: no need to work around data inconsistency between two query layers.

They are more expensive to operate due to higher query layer requirements, but the risks could be lowered by letting query layer providers and wallet developers be separate entities.
That would also allow the end users to choose query layer providers they trust more rather than being forced to trust wallet's infrastructure as well as code, and, optionally, use their own query layer deployments, thus alleviating the need to trust any external wallet backend while enjoying the usual interface.

### 5. Minimize Growing Pains
The pain caused by uprooting all CIP-30 implementations should not be underestimated.
Thus, future standards should work to minimize the potential pain, be it by support CIP-30 implementations in a legacy mode.

## Open Questions

### Can a universal connector be pursued?
Every wallet connector that simply adds functionality on top of another standard can be considered a valid implementation of the base standard.
If the set of standards is designed in a way that allows for extensions without conflicts, then a superset of all standards can be considered a universal connector. How to make it possible to clearly define a "universal connector" and whether it is useful at all remains to be seen.

### Can a universal API be pursued?
Platform differences are a concern: by using specifications that do not rely on assumptions about data transfer methods, software environment or programming language, it is possible to create a standard that is truly reusable across platforms.

There are many types of dApp architectures enabled by different wallet types, but there is no requirement to use the APIs provided by wallets, so dApps that use simpler standards should still remain functional when used with their extensions.

### How interoperable can standards be with those of other ecosystems?
Cardano is sufficiently different from most of the blockchains that aim to unify their wallet standards.

What options do we have for integration with Cardano sidechains, rollups, hardforks, private testnets, etc.?

### How can we effectively police API scope?
How can we prevent duplication of functionality and make sure different APIs do not overlap?

## Acknowledgements

<details>
  <summary><strong>First workshop - 2023-11-27</strong></summary>

  We would like to thank those that contributed to the first workshop hosted by Adam Dean and Ryan Williams ([see shared drive with resources](https://drive.google.com/drive/folders/1gYGeVJBLmDhCGEp1mTkCrsJYspd5hSoM?usp=drive_link)).
- Beatrice Anihiri
- Denis Kalinin
- Evgenii Lisitskii
- George APEX Pool
- George Flerovsky
- George Humphreys
- Hernán Rajchert
- Jack Rousian
- Joshua Marchand
- Ken Fritschy
- Ken-Erik Ølmheim
- Leo H
- Marcel Baumberg
- Martynas Kazlaukas
- Michael Chappell
- Mircea Hasegan
- Nicolas Ayotte
- Rhys Bartels-Waller
- Robert Phair
- Rodolpho Ribeiro
- Steven Johnson
- Teodora Sevastru Lunn
- Thomas Lindseth
- Thomas Upfield
- Vladimir Kalnitsky

</details>

<details>
  <summary><strong>Second workshop - 2023-12-08</strong></summary>

  We would like to thank those that contributed to the second workshop hosted by Adam Dean and Ryan Williams ([see shared drive with resources](https://drive.google.com/drive/folders/1gYGeVJBLmDhCGEp1mTkCrsJYspd5hSoM?usp=drive_link)).
- Alex Dochioiu
- George APEX Pool
- Mark Byers
- Leo H

</details>

<details>
  <summary><strong>Third workshop - 2024-01-06</strong></summary>

  We would like to thank those that contributed to the third workshop hosted by Ryan Williams ([see shared drive with resources](https://drive.google.com/drive/folders/1gYGeVJBLmDhCGEp1mTkCrsJYspd5hSoM?usp=drive_link)).
- Brent
- Ishita Verma
- Jonathan Kelly
- Leo H
- Nick Ulrich
- NOODZ
- Vladimir Kalnitsky

</details>

## Copyright

This CPS is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CPS-0011/README.md
---

- --
CPS: 11
Title: Universal JSON Encoding for Domain Types
Status: Open
Category: Tools
Authors:
- Vladimir Kalnitsky <vladimir@mlabs.city>
Proposed Solutions: []
Discussions:
- https://github.com/cardano-foundation/cips/pulls/742
Created: 2024-01-10
License: CC-BY-4.0
- --

## Abstract

dApps require the use of communication protocols that facilitate data transfer between:

- query layer providers and dApps
- wallets and dApps
- wallets and query layer providers
- dApp frontends and backends
- different dApps
- Cardano blockchain indexers and their API consumers

Usually these protocols include a specification of Cardano domain types, i.e. ledger block and all of its sub-components.

## Problem

Cardano domain types have canonical CDDL definitions (for every era), but when it comes to use in web apps, where JSON is the universally accepted format, there is no definite standard.

As a result, software solutions are incompatible with each other, and dApp developers are forced to write code for conversions that could in principle be unnecessary, because the semantics of different JSON layouts are often the same. In particular, this problem is very real when offchain libraries need to provide support for different query layer providers (examples: [Lucid](https://lucid.spacebudz.io/docs/getting-started/choose-provider/), [Mesh.js](https://meshjs.dev/providers), [cardano-transaction-lib](https://github.com/Plutonomicon/cardano-transaction-lib/blob/develop/doc/runtime.md), [Atlas](https://haddock.atlas-app.io/GeniusYield-Providers.html)).

The [initiative](https://github.com/cardano-foundation/CIPs/pull/625) to standardize query layers on Cardano is currently blocked due to absence of a standardized JSON data schema. However, such a schema would be useful in contexts other than query layers, which is the reason why this CPS is separate.

## Use cases

- dApp developers want to have a definite encoding of JSON data, so that they don't need to specify the format themselves
- dApp developers want to be sure they will be able to reuse data coming from different sources (third parties) without format changes
- dApp developers want to provide external APIs that must be easily consumable
- Query layer developers want to make sure their APIs will be easily consumable
- Multiple service/product owners, who utilize different competing JSON encoding conventions want to converge to a single convention

## Goals

The main goal of a CIP that corresponds to this CPS is to construct a JSON schema that explicitly defines all domain types for the current era. This CIP should evolve in sync with the Cardano Ledger CDDL specification, so that when a new era spec comes out, a new JSON schema file is provided with the needed modifications.

Optional goals:

- Providing support for eras older than the current one. The goal of the spec is to be used across different software solutions that run on Cardano mainnet. Compatibility with past ledger versions should be provided only as long as there is a real need.

Non-goals:

- Maintain roundtrip property for conversion between JSON and CBOR: this is impossible, because CBOR uses varying binary encodings for arrays and maps.
- Provide support for different eras in a single schema. Specifications should be kept in separate files.
- Maintain correctness of signatures for transactions encoded as JSON: this is impossible in the general case, because signature validity depends on binary layout determined by CBOR encoding. Conventions may apply to preserve signatures, but they are out of scope of the standard, because they apply to CBOR encoding, not JSON.
- Making the encoding as compact as possible (reducing encoded data size)

## Open Questions

- What's the best approach to specifying address formats in json-schema format?
- Should the schema be concerned with different representations of addresses (bech32 vs. hex)?
- How can we programmatically or manually test the constructed schema file?
- How to encode long integer arithmetic? Some JSON encoding implementations simply refuse to handle long integers, e.g. [CSL](https://github.com/Emurgo/cardano-serialization-lib/blob/4a35ef11fd5c4931626c03025fe6f67743a6bdf9/rust/src/plutus.rs#L1370).
- [RFC8949](https://www.rfc-editor.org/rfc/rfc8949.html#name-converting-data-between-cbo) and [RFC8610](https://datatracker.ietf.org/doc/html/rfc8610#appendix-E) contain recommendations for developers who want to maintain interoperability with JSON. Can we apply these in our context?

## Copyright

This CPS is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CPS-0012/README.md
---

- --
CPS: 12
Title: Query Layer Standardization
Status: Open
Category: Tools
Authors:
- Vladimir Kalnitsky <vladimir@mlabs.city>
- Ryan Williams <ryan.williams@intersectmbo.org>
Proposed Solutions: []
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/625
Created: 2023-11-27
License: CC-BY-4.0
- --

## Abstract

Query layer services abstract away the difficulties of indexing blockchains, offering builders API interfaces to access data.

Cardano's query layers lack standardization.
This leads to suboptimal tooling, dApp and wallet architecture.

## Problem

Cardano nodes offer limited options for querying chain data.
For builders, running chain indexing infrastructure can alleviate the data availability issues of the Cardano node.
But this brings with it large overheads in running the nodes, chain followers, data storage and data access.

Query layer providers offer builders the opportunity to abstract away the infrastructure complexities of Cardano data availability, usually to some cost.
These providers are extremely useful and in turn are used as a foundational element for hundreds of tools.

Cardano has a range of query layer providers, each with their own interfaces and business motivations.
This can present issues for builders who wish to build with multiple providers.

### Query Layers and Tooling

Lack of a standardized query layer results in multiple different implementations of roughly the same set of functionality:

- [Blockfrost](https://blockfrost.io/)
- [Koios](https://www.koios.rest/)
- [Maestro](https://www.gomaestro.org)

As a result, there is a need to support multiple incompatible APIs in downstream tools, examples of which are:

- [Mesh.js](https://meshjs.dev/providers)
- [Lucid](https://lucid.spacebudz.io/)
- [cardano-transaction-library](https://github.com/Plutonomicon/cardano-transaction-lib/blob/develop/doc/runtime.md)
- [cardano-js-sdk](https://github.com/input-output-hk/cardano-js-sdk/tree/master/packages/core/src/Provider)

Query layer providers are not identical, which means that the *promise* of abstracting away from a particular query layer provider completely, that an offchain library may want to give to its users, will either be left *unfulfilled* (i.e. some features will work with some providers, but not others) or the scope of the downstream API will have to be *reduced* to the very minimum that is covered by every supported query layer.

### Query Layers and Wallets

This CPS initiative originated in the discussion about [Extensive Wallet Standard CIP](https://github.com/cardano-foundation/CIPs/pull/620) on the CIP Discord server ([invite](https://discord.gg/P59aNVN8zu))
in the [`#general`](https://discord.com/channels/971785110770831360/992011119872970762/1176567729017327737) channel, continuing in a dedicated [`#query-layer-standard`](https://discord.com/channels/971785110770831360/1178763938389823598) channel.

Every light wallet has its own backend infrastructure: functioning of the browser extension relies on the availability of the data sources. However, none of the wallets currently provide a way to override their query layer endpoint URLs.

In the past, users have encountered problems with ability to submit transactions due to limited mempool capacity, which resulted in some wallets providing a way to configure custom [tx-submit-api](https://github.com/blinklabs-io/tx-submit-api) endpoints - but this only covers transaction submission.

Tight coupling between wallets and query layers results in unnecessary economic burden imposed on wallets, and forces them to make opinionated choices (of query layer provider) that could otherwise be delegated to the end user, similarly to how it is done in Metamask with RPC provider selection.

The economic burden, in turn, results in discouraging of innovation in the wallet ecosystem, because there is a very strong pressure to keep wallet standards minimal coming from wallet providers.

Another downside of tight coupling between wallets and query layers is that, unlike Ethereum wallets, Cardano wallets can't be used to interact with dApps deployed to custom (private) testnets. Only public testnets are supported, and as a result in order to simply test a dApp end-to-end, it must be deployed to a public testnet. This is a privacy concern for dApp developers, because making on-chain contracts available publicly (although in a compiled form) before they are officially open-sourced can be seen as data breach.

### Centralization and Ecosystem Risks

The inability to interchange query layer providers results in vendor lock-in. Query layer providers are disincentivized to opensource their infrastructure setup or provide a competitive service. Dependency on a fixed set of entities to run the infrastructure makes it easier for adversaries to attack dApps by taking over the infrastructure.

Having accepted standards drastically reduces the complexity and cost for new providers to develop.
This creates a query layer marketplace where providers can be more easily compared by customers.

### Barriers to adoption

Such a standard could likely face resistance to adoption.

The potential risk for commercial offerings adopting this standard is the loss of the ability to differentiate themselves from competitors via data shape.

Differentiating themselves via data shape is useful in two ways for query layer providers.
Firstly, data providers each use their own unique infrastructure, so the costs of the same query are not uniform between providers.
By adjusting query shape, providers can adjust queries to suit their architecture, ensuring speed and reducing cost.

Secondly, is the potential monetary advantages.
Without a standard commercial providers are able to shape their data in a way to attract customers.
A provider's business model may be to charge per request, which incentivises them to keep the data small to increase the number of queries by customers.
A standardized set of queries would reduce the ability for providers to do this.

#### Mitigation

To mitigate the impact of varying provider architectures, authors of solutions should seek to involve query layer providers in the development of a standard.
This will allow providers to voice concerns over potentially expensive or awkward queries.

To address loss of avenue for differentiation, via data shape standard authors should seek to ensure there are other ways in which providers can differentiate.
These could be built into the standard such as optional batching, allowing providers to offer tiers of batching support for their endpoints.

## Use cases

### Multi-Provider Wallets

Wallets may wish to allow users to bring their own data layer provider (on public or custom networks).
By having a standard interface wallets would be able to support this, which would reduce the wallet's operating costs.
For the users this gives them flexibility, redundancy and the option to run their wallet on their own infrastructure.

### DApps Avoiding Lock-In

DApp developers don't want to tie their infrastructure to one query layer provider.
By being able to switch providers without the need for significant engineering, they can avoid providers which do not offer fair pricing.

### New Infrastructure Providers

New Cardano infrastructure providers want to be able join pools of interchangeable query layer suppliers.
Without standardization new providers must invest significant time to develop their own backends (including APIs).

## Goals

1. Create an extensive query layer API specification that is not tied to any particular implementation

2. Describe how can it be used in different contexts: HTTP API, JavaScript interfaces, browser-based wallets, standalone wallets, dApp backends.

3. A query layer standard which meets the needs of query layer providers, wallet developers and dApp developers.

## Open Questions

### How can we encourage query layer providers to adopt the solution?

Such standardization could have drawbacks for existing query layer providers.

The primary argument against such an initiative is the potential loss of business advantage from query layer providers.
As standardized providers would loose the ability to differentiate themselves via data shape and language.

For providers which charge per request this can negatively impact their ability to control their value proposition.
Such providers are currently incentivized to make the data returned via queries contain the least possible useful information.
Standards would mean providers would not be able to control the amount of data that is returned upon a query.
This would mean existing providers would have to adjust their business and value proposition to customers.

### How can we encourage wallet developers to adopt the solution?

For wallet providers, a standardized query layer would offer long term benefits which outweigh any upfront engineering costs.
With users being able to bring their own data providers, wallets will incur less costs to the use of their providers.

### How can we encourage dApp developers to adopt the solution?

DApp developers would benefit from a more open and competitive query layer market.
Developers will be able to choose the provider which best fits the needs of their dApp.

### How can we address multiple Cardano ledger eras?

How a standard distinguishes between data from different Cardano ledger eras is something that needs to be addressed.
Should providers be expected to support a superset of all Cardano eras?

## Acknowledgements

<details>
  <summary><strong>Workshop 1 - 2024-01-25</strong></summary>

  We would like to thank those that contributed to the first Query Layer Standardization workshop hosted by The Wallets Working Group ([see shared drive with resources](https://drive.google.com/drive/folders/1baSYHfWJdUh5dwRkHjY7qnaufjuO8sP2?usp=sharing)).

  Hosts:

- Ryan Williams
- Adam Dean

  Participants:

- Dmang
- George APEX Pool
- Leo H
- Marcin Szamotulski
- Markus Gufler
- Matt Davis
- Matthieu Pizenberg
- Michael Chappell
- NEXUS Crypto
- Nick Cook
- Rhys Bartels-Waller
- Ruslan Dudin
- Torbjørn Løvseth Finnøy

</details>

## Copyright

This CPS is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CPS-0013/README.md
---

- --
CPS: 13
Title: Better builtin data structures in Plutus
Status: Proposed
Category: Plutus
Authors:
- Michael Peyton Jones <michael.peyton-jones@iohk.io>
- Philip DiSarro <philip.disarro@iohk.io>
- Pi Lanningham <pi@sundaeswap.finance>
Discussions:
Created: 2023-12-14
License: CC-BY-4.0
- --

## Abstract

Plutus Core lacks builtin data structures with good asymptotic performance for some use cases.

## Problem

Plutus Core has a few builtin data structures, but these are mostly used to make a minimally adequate representation of the `Data` type.
It does not have builtin data structures optimized for performance.

Users can implement their own data structures (since Plutus Core is an expressive programming language), but in practice this has not happened much.
In particular, we will focus on two examples here:

1. Arrays with constant-time lookup
2. Maps with logarithmic-time lookup (also Sets, but we can treat them as a special case of Maps)

Both of these are difficult to implement in Plutus Core:

1. Arrays are (we believe) impossible without some kind of primitive with constant-time lookup
2. Maps are possible but are typically moderately complex data structures which require a lot of code, and this has not been done in practice

## Use cases

### Arrays

#### Order matching

A common pattern in DEXs is to have a list of inputs/outputs to match up in a datum.
In some cases the order is highly significant, e.g. earlier orders should be processed first, and the outcome of processing an earlier order may affect later ones.

For example, we might have:
```
inputIdxs :: BuiltinList Integer
outputIdxs :: BuiltinList Integer
```
We then want to go through these lists, looking up the corresponding inputs and outputs and check some property (e.g. that the value is directly transferred from one to the other).

This requires a quadratic amount of work, which puts a low ceiling on how many orders can be processed at once.
Empirically, many are capped at about 30, whereas if they were limited only by the amount of space in the transaction for inputs and outputs the limit would be hundreds.

If we had arrays with constant time indexing, we could make this linear instead.
Note that unless we also implemented the "Data fields" suggestion below we would still need to do a linear amount of work to create arrays for the transaction inputs and outputs from the lists in the script context.

#### Data fields

The `Data` type has a `Constr` alternative which is used for encoding datatype constructors.
This is used for encoding the script context, and is used by languages such as Aiken extensively for representing user-defined datatypes also.

The fields of the constructor are encoded in a list; hence to access a particular field the compiled code needs to do a linear amount of work.
If the arguments to a `Constr` were an array, we could access the fields in constant time.

Similarly, the `List` and `Map` constructors of `Data` could use arrays.

### Maps

#### Operations on `Value`

The `Value` type is a nested map: it is a map from bytestrings (representing policy IDs) to maps from bytestrings (representing token names) to integers (representing quantities).
Since map operations are currently linear, this means that even simple operations like checking whether one value is less than another can have quadratic cost.

This would be much better if map operations were logarithmic cost.

#### Indexing by party

Many applications have a known set of participants identified by some bytestring, typically a public key.
It is therefore natural to store per-party state in a map indexed by the party identifier.

Since map operations currently have much worse complexity than a good map data structure (often linear/quadratic instead of logarithmic/linear), this is needlessly expensive and imposes a limit on the number of parties.

## Goals

1. Reduce the cost of operations on `Value` by a factor of 2-10
2. Reduce the cost of a matching algorithm such that we can handle hundreds of matches for the same cost it currently takes to do 30.

## Open questions

- Can we implement a set/map data structure in Plutus Core code that has acceptable performance and doesn’t require too much size overhead?
- Do we need generic maps or is a map-from-bytestring sufficient? What about map-from-integer?
- Generic maps are harder since we typically need to know how to order the key type
- Is an array type useful even if it is immutable?
- We are unlikely to be able to offer mutable arrays
- Are builtin data structures useful enough even if they can only contain builtin types?
- This would mean that complex data structures would have to be stored inside arrays as `Data`, rather than using Scott encoding or sum-of-products representation
- Can we feasibly change the structure of the builtin `Data` type so that `Constr` arguments are in an array?
- We would need to retain both versions for backwards compatibility

## References

- https://x.com/Quantumplation/status/1733298551571038338?s=20

## Copyright

This CPS is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CPS-0014/README.md
---

- --
CPS: 14
Title: Register of CBOR Tags for Cardano Data structures
Status: Open
Category: Tools
Authors:
- Steven Johnson <steven.johnson@iohk.io>
Proposed Solutions:
- https://github.com/cardano-foundation/CIPs/tree/master/CIP-0114
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/751
- https://github.com/cardano-foundation/CIPs/pull/752
Created: 2024-01-24
License: CC-BY-4.0
- --

## Abstract

Cardano uses CBOR for a lot of data structures, both on and off chain.
CBOR defines a number of tags which identify the types of encoded data, and IANA maintain a registry of these tags.
Cardano data structures could benefit from consistent tagging of CBOR fields.

### **Acknowledgements**

Thank you to the following people who helped review this draft before it was published:

- Ryan Williams

## Problem

[CBOR] defines the ledger contents, transactions, and commonly the metadata attached to transactions.
For example [CIP-0036] metadata used for registering voters in Project Catalyst.
[CBOR] defines a set of [CBOR Tags] which are used to define the type of data being encoded.
While [CBOR] defines a basic set of [CBOR Tags] it also defines a [CBOR Tags Registry] to enable further tags to be defined.
[IANA][IANA - Homepage] maintains a registry of CBOR Tags at [IANA - CBOR Tags Registry][IANA].
[BCR-2020-006] defines CBOR tags related to some blockchain data structures.
Most (all?) of these are registered with IANA.

Without Tags, [CBOR] data types can become ambiguous and rely on comments in a specification to help resolve the contents.
[CBOR] tags unequivocally disambiguate the encoding of data.
They also help with forward compatibility and extensibility of data structures.

However, with the exception of [#6.258] there are no Cardano specific registered tags.
Also, Tag use in CIPs is currently haphazard and inconsistent.
For example, [CIP-0042] uses tag **#6.102**, which [IANA] list as "Unassigned".
This Tag is able to be assigned by IANA at any time.
It's unofficial use could break, if a decoder tries to interpret the data according to it's official [IANA] assignment.

Defining a CBOR tag for a Cardano data structure will also define a canonical representation for the data type so tagged.
This will help with interoperability and consistency of specification.
CIPs can simply refer to the Tag, and the CIP in which it is defined, rather than define how that data will be represented.

## Use cases

In Project Catalyst CIP-36 a Voting key is defined as:

```cddl
$cip36_vote_pub_key /= bytes .size 32
```

This is a public [ED25519-BIP32] key.
If is used to validate that a Vote has been signed correctly by the registered voter.

However, there are a couple of issues:

1. How the Key is encoded is NOT defined.
1. It is 32 bytes, but is it Big-endian or Little-endian encoded?
2. The kind of key is defined only in the words of the specification.

If there was a canonical specification for tagging and encoding [ED25519-BIP32] keys.
And say that tag was `7777`, then this could be defined simply as:

```cddl
$cip36_vote_pub_key /= #6.7777(bytes .size 32)
```

Without any extra information, it can be now determined the Key type that is valid, and the encoding.
This would lead to greater clarity and simpler specifications.

It would also allow simple and unambiguous extension.
Say Project Catalyst desired to also the use of Signing Public Keys as defined in [BCR-2023-011].
All it needs to do is define that `#6.40022` is a valid public key type.
The decoder would be able to unambiguously identify the key type.
The basic metadata encoding would not need to change, just the list of valid public key types.
A decoder based on an older version of the specification would be able to detect that the new key type was not supported.
This would provide forward compatibility.

## Goals

1. Definitive sources of truth for CBOR encoding of common Cardano data structures.
2. Uniform use of CBOR tags to unambiguously identify common Cardano data structures inside CBOR.
3. An easily accessible reference to all CBOR tags defined by CIPs, to help eliminate their re-definition.
4. Guidelines on the creation of new CBOR tags as data structures emerge, or existing data structures are formalized.
5. Clarity on who is responsible to have new Tags registered with IANA.

It is NOT a goal of this CPS to cause all pre-existing Cardano data structures to be standardized with CBOR tags.
Individual projects should evaluate if the existing tags are sufficient or if they require new ones for their purposes.
it would then be the responsibility of the project to raise a CIP to standardize the CBOR tag for the pre-existing data structure.

It is also not a goal of this CPS to define how general data structures are tagged.
The data structure should be either defined by a CIP or integral to Cardano.
An example is `ULID` this has no current CBOR Tag, but it is not defined by a CIP nor integral to Cardano.
Even though a CIPs metadata may use one, its formalization should be done outside the CIP process.

## Open Questions

- Should we define a canonical list of tags, the CIP they are defined in, and if they are IANA registered or not?
- Who should register the Tag with IANA?
- Would this become a necessary part of the "path to Active" of any CIP which defines a CBOR tag?
- How should duplicate structures be handled (same data encoded differently with a different tag)?
- Should defining tags be their own CIP, or can they be part of a larger CIP?
- Should we define recommendations for the use of Tags in Metadata CIPs?

## References

- [CIP-0036 - Catalyst Registration Transaction Metadata Format (Updated)][CIP-0036]
- [CIP-0042 - New Plutus Builtin serialiseData][CIP-0042]
- [RFC8949 - Concise Binary Object Representation (CBOR)][CBOR]
- [BCR-2020-006 - Registry of Uniform Resource (UR) Types][BCR-2020-006]
- [BCR-2023-011 - UR Type Definitions for Public Key Cryptography][BCR-2023-011]
- [IANA CBOR Tag Registry][IANA]

## Copyright

This CPS is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

[CIP-0036]: https://github.com/cardano-foundation/CIPs/tree/master/CIP-0036
[CIP-0042]: https://github.com/cardano-foundation/CIPs/blob/master/CIP-0042/README.md
[CBOR]: https://datatracker.ietf.org/doc/html/rfc8949
[CBOR Tags]: https://datatracker.ietf.org/doc/html/rfc8949#name-tagging-of-items
[CBOR Tags Registry]: https://datatracker.ietf.org/doc/html/rfc8949#ianatags
[IANA - Homepage]: https://www.iana.org/
[IANA]: https://www.iana.org/assignments/cbor-tags/cbor-tags.xhtml
[BCR-2020-006]: https://github.com/BlockchainCommons/Research/blob/master/papers/bcr-2020-006-urtypes.md
[ED25519-BIP32]: https://github.com/input-output-hk/adrestia/raw/bdf00e4e7791d610d273d227be877bc6dd0dbcfb/user-guide/static/Ed25519_BIP.pdf
[BCR-2023-011]: https://github.com/BlockchainCommons/Research/blob/master/papers/bcr-2023-011-public-key-crypto.md

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CPS-0016/README.md
---

- --
CPS: 16
Title: Cardano URIs
Status: Open
Category: Tools
Authors:
- Adam Dean <adam@crypto2099.io>
Proposed Solutions:
- CIP-0013: https://github.com/cardano-foundation/CIPs/tree/master/CIP-0013
- CIP-0099: https://github.com/cardano-foundation/CIPs/tree/master/CIP-0099
- CIP-0107: https://github.com/cardano-foundation/CIPs/tree/master/CIP-0107
- CIP-0134: https://github.com/cardano-foundation/CIPs/tree/master/CIP-0134
Discussions:
- https://github.com/cardano-foundation/CIPs/issues/836
- https://github.com/cardano-foundation/CIPs/pull/559
- https://github.com/cardano-foundation/CIPs/pull/635
- https://github.com/cardano-foundation/CIPs/pull/841
- https://github.com/cardano-foundation/CIPs/pull/888
Created: 2024-06-15
License: CC-BY-4.0
- --

## Abstract

Cardano URI schemes ([CIP-13]) have been defined since early 2021. However, some
of the proposed standards have languished and struggled for adoption event as
the amount of wallet developers in the ecosystem has skyrocketed. This CPS aims
to create a centralized point of reference for those interested in creating new
standards or publishing integration solutions.

## Problem

A keen pain point in the utilization of blockchain is the archaic and convoluted
processes necessary to interact with the blockchain or cite historical ledger
data. Well-defined URI schemes can provide a plethora of easy access and
interaction methods, particularly to mobile users, since they can leverage
deep-linking and QR technologies to pass data between applications.

## Use cases

The Cardano URI Scheme(s) that exist already present several fantastic use cases
for URI schemes. We welcome additional or expanded definitions to be added to
this section for further discussion, development, and adoption by the community.

### Easy On-Chain Interactions

#### Payments

1. The original specification of [CIP-13][CIP-13-payment] defined only the
   base `web+cardano:` **scheme** and an **authority**-less path consisting of a
   payment address and an optional `amount` query parameter to specify via QR
   code or deep-link an address and amount of Lovelace to send.
2. In 2024 [a new CIP was proposed](https://github.com/cardano-foundation/CIPs/pull/843) defining an explicit `//pay`
- *authority** as well as path and query parameters to support _Native Assets_
   and transaction _Metadata_.

#### Staking/Delegation

1. The first extension to [CIP-13][CIP-13-staking] added the `//stake`
- *authority** and some query parameter options to specify a Cardano Stake
   Pool. The goal of these URIs would be to make it easy for a user to switch
   their wallet's stake delegation to the pool in question. This could be
   performed via easy deep-link integration on stake pool website(s) and social
   media, and also through real-world advertising such as banners, flyers, and
   business cards at marketing and networking events.

#### Onboarding/Airdrops

1. In 2023 [CIP-99] was introduced, adding a `//claim` **authority** and
   defining a URI + wallet interaction protocol that would allow a user with
   only access to a URI to "claim" an airdrop of _Lovelace_ and/or _Native
   Assets_ assuming that both the wallet and the project server were correctly
   configured to follow the protocol.

### Easy Historical Reference

#### Blocks and Transactions

1. In 2023 [CIP-107] was proposed, this CIP introduced the `//block` and
   `//transaction` **authorities** along with relevant **path** and **query**
   parameters. This novel use of URI structures allows for easy reference to
   specific events and points in time of the blockchain ledger's history. This
   proposal makes referencing a point in the chain's history relatively trivial
   which can be an important step in data availability layers for external on-
   and off-chain solutions.

## Goals

The goals of this CPS are as follows:

- [X] Provide an easy-to-reference repository for all `web+cardano:` URI
  solutions to simplify integrations.
- [X] Define a list of known `web+cardano:` URI authorities to prevent overlap
  and collision by developers creating new solutions.
- [X] Provide a platform to centralize new discussions about emerging standards
  that may arise in the future.

## Open Questions

1. What can we do as a community to encourage more applications and software
   providers to support `web+cardano:` URIs?
2. What software or best practices exist to aid developers in deploying support
   for `web+cardano:` URIs?
2. What new authorities or protocols could be built to leverage these URIs?
3. What does the process look like to register a new `web+cardano:` URI
   authority or protocol?

### Proposal Process

Recent best practices in this repository suggest that the preferred method is
for new standards that expand functionality should be defined as new, discrete
CIPs unless the existing CIP provides for the capability of versioning and
iterative improvement.

By creating separate, discrete CIPs for new standards this allows for a clean
history of discussion and community consensus building around new standards as
well as measurable progress towards adoption.

### Registered URI Authorities

Per [URI Syntax] a URI consists of the following structure:

```abnf
URI = scheme ":" ["//" authority] path ["?" query] ["#" fragment]
```

Given that the shared scheme of `web+cardano:` is known and accepted and the
original version of [CIP-13] declared an authority-less, path-only structure:
- *All future `web+cardano:` URI extensions MUST register a new, unique authority
or SHOULD define a new version of an existing **authority** that has explicitly
allowed for versioning in its definition.

The following are the currently registered URI **authorities** mentioned in
CIPs:

- `null`: (Blank/no Authority) registered in [CIP-13][CIP-13-payment]
- `//stake`: registered in [CIP-13][CIP-13-staking]
- `//claim`: supports versioning, registered in [CIP-99]
- `//transaction`: supports versioning (?), registered in [CIP-107]
- `//block`: supports versioning (?), registered in [CIP-107]
- `//addr/`: registered in [CIP-134]

## Copyright

This CPS is licensed under [CC-BY-4.0].

[CIP-13]:https://github.com/cardano-foundation/CIPs/tree/master/CIP-0013

[CIP-13-payment]:https://github.com/cardano-foundation/CIPs/tree/master/CIP-0013#for-payment-uris

[CIP-13-staking]:https://github.com/cardano-foundation/CIPs/tree/master/CIP-0013#for-stake-pool-uris

[CIP-99]:https://github.com/cardano-foundation/CIPs/tree/master/CIP-0099

[CIP-107]:https://github.com/cardano-foundation/CIPs/tree/master/CIP-0107

[CIP-134]:https://github.com/cardano-foundation/CIPs/tree/master/CIP-0134

[CC-BY-4.0]:https://creativecommons.org/licenses/by/4.0/legalcode

[URI Syntax]:https://en.wikipedia.org/wiki/Uniform_Resource_Identifier#Syntax

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CPS-0017/README.md
---

- --
CPS: 17
Title: Settlement Speed
Category: Consensus
Status: Open
Authors:
- Arnaud Bailly <arnaud.bailly@iohk.io>
- Brian W. Bush <brian.bush@iohk.io>
- Hans Lahe <hans.lahe@iohk.io>
Proposed Solutions:
	["CIP-0140 : Ouroboros Peras - Faster Settlement"
	,"CIP-0161 : Ouroboros Phalanx - Breaking Grinding Incentives"
	]
Implementors: N/A
Discussions:
Created: 2024-09-30
License: Apache-2.0
- --

## Abstract

Existing and emerging Cardano use cases would greatly benefit from faster "settlement" or "finality" of transactions. Moreover, some current and pending use cases are hampered or blocked by not having transaction settlement within minutes of including of a transaction on a block. Examples are partner chains, bridges, exchanges (centralized and decentralized), Dapps, and ordinary high-value transactions. Although rollbacks of transactions on the Cardano `mainnet` are uncommon because of the careful design of the Ouroboros protocol and the robust implementation of the Cardano node's memory pool and block diffusion, extraordinary adversarial conditions can in principle produce forks that cause impactful rollbacks with some probability: stake-based and CPU-based attacks might seek to create long-lived adversarial forks, for example. The Cardano `mainnet` currently has a 5% active slot coefficient and a security parameter of 2160 blocks (approximately 12 hours). Recent research indicates that for faster settlement (precisely defined in terms of stakeholder use cases) the Ouroboros protocol logic could be modified and the active slot coefficient could be optimized.


## Problem

Fast settlement is a critical part of Cardano scaling, as described in [*Scaling blockchain protocols: a research-based approach*](https://www.youtube.com/watch?v=Czmg9WmSCcI). Under Ouroboros Praos, settlement occurs probabilistically on the Cardano blockchain, where the probability that a block will be rolled back from the preferred chain decreases exponentially as the chain grows beyond the block and where that rate of decrease is slower when adversarial activity is stronger. Some use cases require high assurance that a block (and the transactions within it) will not be rolled back, necessitating a potentially lengthy wait before a transaction is considered "settled" or "finalize". Some major centralized exchanges, for example, require fifteen confirmations (i.e., blocks) before a transaction is considered settled: this amounts to waiting ten minutes before a consumer has their transacted funds or tokens available for subsequent use. This situation is not unique to Cardano: centralized exchanges generally require at least five minutes wait for most of the common blockchains. Partner chains, Dapps, or bridges may have stringent requirements for fast and highly certain settlement.

There are several definitions of "settlement" or "finality", and precision is important when discussing these. Two noteworthy scenarios can be defined precisely. Both scenarios are expressed in terms of the probability a transaction or block that's been observed _locally_ to be part of the chain will be rolled-back in the future. Remember that rollbacks are the natural consequence of the eventually consistent nature of blockchains.

- *Ex ante* settlement probability: "What is the probability that a transaction that I just submitted will ever be rolled back?"
- *Ex post facto* settlement probability: "Given that I submitted my transaction $x$ seconds ago and it has not yet been rolled back, what is the probability that it will ever be rolled back?"

Even better would be to have an on-chain indication that transactions up to a particular point have been settled with high probability.

If one is unwilling or unable to re-submit a rolled-back transaction, then the *ex ante* probability might be of most interest. This matches use cases where there is no opportunities for the parties involved in a transaction to resubmit it: for example, one party might have purchased physical goods and left the vendor's premises, leaving no chance to resubmit a rolled-back transaction.

Other use cases are better suited for *ex post facto* settlement: for example, a partner chain, bridge, or decentralized exchange can monitor the ledger for a fixed time and see that the transaction either is not or is rolled back, knowing that there is only a vanishingly small chance of that status ever changing once they have watched the chain for the fixed amount of time, giving them an opportunity to re-submit the transaction if it was rolled back. Both opportunity and back-end infrastructure distinguish these use cases. Protocol research has historically focused on optimizing *ex post facto* certainty after predefined waiting times.

Settlement failures (i.e., roll backs, "slot battles", and "height battles") can occur naturally as short forks occur due to multiple slot leaders being elected in the same slot or due to network delays. Under typical uncongested conditions such forks often do not create settlement failures because a user's transaction tends to the diffuse throughout the global memory pool and is included in a block on each of the honest, naturally occurring forks. Careful observation might reveal that the transaction was rolled back from one fork but was adopted on the newly preferred fork. Cardano tooling should routinely deal with such situations even though they are rarely apparent to or affect end users. Most wallets do not display information about such short forks and rollbacks to users. Short forks occur hourly at local Cardano nodes, but globally visible forks are much less common, and anecdotal evidence indicates that rollbacks of more than three blocks never occur on a large scale on Cardano `mainnet`.  (There may be one exception to this, however, where a longer rollback is said to have occurred.) As with settlement, a precise definition of near-global rollbacks is required to measure and discuss this phenomenon in detail.

Casual perusal of information about Cardano on the internet reveals a wide variety of conflicting explanations and assertions regarding settlement and finality. The terms "settlement" and "finality" are not clearly defined in the context of Cardano and statements about them range from saying that only one block/confirmation is required to consider a transaction settled to 2160 blocks or 12 hours is required for settlement. The Ouroboros security parameter *k* also seems poorly described.

Many centralized exchanges have published the number of confirmations (blocks) required for them to consider a transaction settled. Taking Kraken as an example,[^1] we see in the following table the significant delay required for transactions to be treated as final. Cardano is not among the cluster of fast-settling blockchains. (Note that the following table contains internal contradictions between the "confirmation" column and the "time" column, a situation which further highlights imprecise understanding of settlement.)

| Blockchain | Confirmations Required | Approximate time (minutes) |
| ---------- | ---------------------: | -------------------------: |
| Algorand   |                     10 |                          1 |
| Aptos      |                     50 |                          5 |
| Avalance   |                     20 |                          1 |
| Bitcoin    |                      3 |                         30 |
| Cardano    |                     15 |                         10 |
| Dogecoin   |                     40 |                         40 |
| Ethereum   |                     70 |                         14 |
| Polkadot   |                    n/a |                          5 |
| Ripple     |                    n/a |                          0 |
| Solana     |                    n/a |                          0 |
| Tezos      |                      6 |                          3 |
[^1]: Data extracted from [https://support.kraken.com/hc/en-us/articles/203325283-Cryptocurrency-deposit-processing-times](https://support.kraken.com/hc/en-us/articles/203325283-Cryptocurrency-deposit-processing-times) on 7 August 2024.

Cardano Dapps and DEXes vary quite a bit regarding how many confirmations they require before they consider a transaction settled. As few as one confirmation is required for some Dapps, but other require twelve hours.

When Cardano becomes used by large institutions or nation states, attacks on settlement may become more attractive and lucrative. Hence, it is not safe to assume that stake-based and grinding attacks will not occur in the future.

## Use cases

Fast settlement primarily benefits use cases where a party needs certainty, after a fixed amount of time, about the settlement status of a transaction. The generic use case follows.

1. The party submits a transaction to the local memory pool.
2. A block producer includes the transaction in a newly forged block.
3. After a fixed time passes or a specific indication is observed, the party considers their transaction settled.
4. Contrarily, the party might observe that their transaction was rolled back from the preferred chain, so they may choose to resubmit it to the memory pool.

Specific use cases involving time-constrained, high-value transactions conform to this generic pattern. When the value at risk is low, a one-in-a-million chance of a rollback might not be as concerning as it would be for a large transaction. Examples follow.

- Centralized exchanges, where fast settlement improves the user convenience and experience
- Partner chains and bridges, where certainty about synchronization between two chains is essential
- Dapps where fixed-horizon certainty is needed to orchestrate transactions
- Ordinary transactions where a brief wait is acceptable but a roll-back is not

For example, the partner-chain use case might leverage faster settlement as follows. The desired target for cross-chain transfers is on the order of minutes.

1. Funds or tokens need to be transferred from the partner chain to the Cardano chain.
2. A smart-contract transaction escrows the funds/tokens on the partner chain.
3. Simultaneously, a mirror of that smart-contract transaction is submitted on Cardano.
4. After a short amount of time, the Cardano transaction has been incorporated into a newly-formed block.
5. Wait for settlement or non-settlement, either by waiting for a specific number of blocks to cover the transaction or by observing some other aspect of the chain.
1. If the transaction settled, complete the escrow contract on the partner chain.
2. If the transaction was rolled back, resubmit it.

Two prominent use cases are the Cardano-Midnight ZK Bridge and the Cardano-Bitcoin BOS Grail Bridge. These bridges and partner chains likely will require fast settlement on Cardano if they intend to keep chains in sync and rapidly move value to/from Cardano.


## Goals

The overall goals are to precisely define settlement metrics and to propose protocol or parameter changes that significantly speed settlement without negatively impacting performance or security.

1. Develop precise settlement/finality metrics relevant for stakeholders.
1. Some metrics should be defined in terms of the whole life cycle of a transaction, from its submission to the memory pool to its becoming settled in a block.
2. Block- versus slot-based metrics have importance for different use cases.
3. Embody these metrics in an open-source library or toolkit for estimating settlement times or measuring finality of Cardano transactions.
4. Create metrics that are practical for wallets to implement and to present clearly to users.
2. Shorten settlement time, as defined by stakeholder-relevant metrics, in the face of even moderate adversarial activity.
1. Stake-based adversaries.
2. Attacks utilizing adversarial resources (CPU or network).
3. Natural disruption of infrastructure or networks (e.g., data-center or internet outages).
4. Fall back to Praos-like security in the face of strongly adversarial conditions.

It addition to the goals above, it is advisable to avoid the following potentially costly changes.

1. Avoid making major changes to the existing Ouroboros protocol parameters or logic.
2. Do not weaken Ouroboros security or substantially enlarge its attack surface.
3. Minimize changes that increase the resource usages of Cardano nodes or the cost of operating them.

Three approaches are under active consideration or development to address the settlement and finality problem. The first two mitigate stake-based and grinding attacks, respectively.

- *Voting approaches* strengthen the weight of the preferred chain, making it extremely difficult to roll back blocks on the prefix of the chain that was made weightier by a quorum (consensus) of stake-based votes. [Ouroboros Peras](https://github.com/cardano-foundation/CIPs/pull/872) is an example of this. This approach does not mitigate CPU-resource attacks.
- *Anti-grinding approaches* make prohibitively expensive any attacks that rely on CPU resources to weaken cryptographic guarantees of the pseudo-randomness of the slot-leadership schedule. This approach does not mitigate stake-based attacks, but grinding requires some stake in order to manipulate the slot-leadership schedule.
- *The protocol-parameter approach* simply lowers the active slot coefficient (currently set to 5% of the slots on `mainnet`) to a somewhat lower value, so that the block production rate and settlement are faster. This approach can only moderately shorten settlement times and does not mitigate against the aforementioned attacks.


## Open questions

- Is finality on the order of a couple of minutes feasible on Cardano? What is the theoretically fastest finality possible for a Praos-like consensus algorithm?
- What definitions of settlement and finality are most relevant to Cardano stakeholder use cases? Is a single definition common to all use cases?
- How best can empirical data on Cardano `mainnet` settlement and finality be collected and communicated?
- Can a single approach to faster settlement work in the face of both stake-based and resource-based adversaries?
- Would faster settlement negatively impact or be undercut by other planned Cardano node updates?
- What are the trade-offs between settlement speed, throughput, performance, and security?
- To what extent would tiered pricing for transactions improve settlement times for high-value transactions.
- Is "rollback insurance" a viable alternative to shortening settlement times?


## Copyright

This CPS is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CPS-0018/README.md
---

- --
CPS: 18
Title: Greater Transaction Throughput
Category: Consensus
Status: Open
Authors:
- Arnaud Bailly <arnaud.bailly@iohk.io>
- Brian W. Bush <brian.bush@iohk.io>
- Hans Lahe <hans.lahe@iohk.io>
Proposed Solutions: []
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/925/
Created: 2024-10-11
License: Apache-2.0
- --



## Abstract

The Cardano mainnet occasionally experiences congestion where there are too many transactions in the memory pool to be included in the next block or in the next few blocks. Sometimes the block utilization peaks above 90% for an extended period of time. This not only impacts general user experience but it can also severely impact use cases such as airdrops, oracles, partner chains, DEXes, and Dapps. Emerging use cases and application deployments promise to accelerate the need for high throughput on Cardano. Applied research on several fronts is needed to propose and provide evidence for techniques that increase throughput measured in terms of transactions, transaction size, and script execution units. Such work should be based on a clear understanding of stakeholder requirements.

## Problem

The Cardano mainnet occasionally encounters periods of congestion, where the number of pending transactions in the memory pool exceeds the network's capacity to include them in the upcoming block or even the next several blocks. During these times, block utilization can consistently peak above 90%, sometimes for an extended duration. This high level of congestion not only degrades the general user experience by causing delays in transaction processing but also poses significant challenges for specific use cases. For instance, activities such as airdrops, which require efficient processing of large numbers of transactions, can be significantly hampered. Similarly, oracles that depend on timely data updates might face disruptions, while partner chains could experience slower cross-chain interactions. The impact is also felt by decentralized exchanges (DEXes) that need fast transaction confirmations to maintain liquidity and decentralized applications (DApps) whose performance and user interactions are affected.

Moreover, the ongoing evolution of the Cardano ecosystem is expected to amplify these demands. New and emerging use cases, along with an increasing number of application deployments, are likely to accelerate the need for higher throughput and improved scalability on the network. As the ecosystem expands to support more diverse and sophisticated use cases, such as real-time financial applications, gaming, or supply chain solutions, the pressure on Cardano's infrastructure to handle larger transaction volumes efficiently will continue to grow. Addressing these scaling challenges will be essential to ensure a seamless experience for users and to maintain Cardano’s competitive position in the rapidly evolving blockchain space.

Cardano's current throughput (measured both in data rate and available script execution units) is usually (but not universally) adequate for the current demand. There is also some protocol-parameter opportunity to increase the block sizes and script execution limits to meet emerging future demands for increased network capacity. There are however fundamental limits to how far the block size and the script execution budget can be pushed, while maintaining system security.

In Ouroboros Praos, maintaining the security of the system requires that blocks be distributed reliably across the network within a specified time frame, known as $\Delta$, which is set at five seconds on the Cardano mainnet. The process of relaying blocks is inherently sequential: blocks are transmitted from one block producer node to the next through a series of intermediary relay nodes. The time required for this process depends on the number of network hops between consecutive block producers and the network latency associated with each hop, considering that these hops often span the entire globe. Since this entire operation must consistently be completed within a five-second window, it imposes strict limitations on the maximum block size and the amount of time available for validating transactions and scripts.

To significantly scale beyond these limitations, fundamental changes to the overall blockchain algorithm are necessary. The potential for scaling is substantial, as the network and computational resources of most nodes are largely underutilized, remaining almost idle for much of the time. By adopting a different algorithm, these resources could be leveraged more effectively to increase the total bandwidth of the blockchain. Such improvements could enable the system to handle a higher volume of transactions while maintaining security and efficiency, addressing current limitations and unlocking new levels of scalability for the Cardano network. As the blockchain continues to evolve, optimizing the utilization of network and computational resources will be crucial to supporting future growth and expanding the capabilities of the platform.

Additionally, certain applications demand predictability or specific quality-of-service guarantees to function optimally. These applications might not necessarily need high levels of sustained throughput, but they are particularly sensitive to fluctuations in how quickly a transaction can be processed and included in a block after entering the memory pool. In such cases, even small delays or variances in the time it takes for a transaction to move from the memory pool into a confirmed block can significantly impact the performance, reliability, and user experience associated with these applications.

For example, in financial services, delays in processing transactions could disrupt trading activities, arbitrage opportunities, or other time-sensitive financial operations where precise timing is critical. Similarly, gaming applications or real-time auctions require transactions to be confirmed quickly to maintain a seamless user experience or to uphold the integrity of the bidding process. Predictable block times are also important for supply chain applications, where time-sensitive tracking and updates must be performed in real-time to reflect changes in inventory or shipments.

Quality-of-service guarantees can also be crucial for smart contracts that rely on external data feeds (oracles). These contracts might need a high degree of predictability in transaction processing to ensure that data updates happen within specific timeframes, thereby maintaining the accuracy of the contract's execution. The lack of consistency in transaction inclusion times could lead to issues such as missed deadlines, inconsistent states, or degraded performance for automated processes.

Thus, while the need for raw throughput is one aspect of blockchain performance, the ability to ensure a predictable and stable processing time for transactions is equally important for many applications. Addressing this challenge involves optimizing the underlying network protocol, enhancing transaction prioritization mechanisms, or implementing features that can deliver the necessary guarantees for latency-sensitive use cases. As more sophisticated applications continue to emerge on blockchain platforms, meeting these requirements will be essential to ensuring that the technology can support a diverse range of real-world use cases effectively.

#### Urgency

While recent improvements like reference scripts have provided some relief, throughput limitations persist and will become increasingly critical as on-chain activity grows. This is evidenced by events like the launches of SundaeSwap and Snek.fun, where application-level queue times reached days and order processing was severely limited due to block capacity constraints. This hinders Cardano's ability to compete and attract wider adoption.

While theoretical maximum TPS provides an indication of potential capacity, real-time TPS offers a more accurate reflection of current network capabilities. Looking at other ecosystem, [the data](https://chainspect.app/dashboard) reveals that the maximum recorded TPS for Solana was 7229 TPS, for Base 93 TPS and for Arbitrum 944 TPS. These figures are relevant for attracting new projects and investments to an ecosystem. Throughput, finality times and gas fees are among the primary technical attributes that projects look for when choosing which web3 network to pick.

| Blockchain | Max Recorded TPS |
|---|---|
| Cardano | 12 |
| Solana | 7229 |
| Algorand | 5716 |
| Hedera | 3302 |
| BNB | 1731 |
| Arbitrum | 944 |
| Base | 293 |
| Polygon | 429 |
| NEAR | 342 |
| Ethereum | 62 |

Source for table data: https://chainspect.app/dashboard

To achieve a sustainable and thriving ecosystem, Cardano needs to handle significantly higher transaction volumes at current or lower fee levels. This necessitates a proactive approach to scaling that goes beyond addressing sporadic spikes in activity and focuses on supporting the growth required for long-term network profitability without relying on inflation. This aligns with the vision of scaling Cardano to support nation-state level usage by 2030.

### Evidence

Historical data indicates that an appreciable fraction of the blocks on the Cardano mainnet have been nearly full and that periods of high utilization can last for minutes. In such situations it is likely that additional transactions queue up in the nodes' memory pool, awaiting inclusion in a future block. Needless to say, block congestion correlates directly with transaction throughput. With the current average block-production rate of one per twenty seconds, that queuing can translate into unacceptably long waits for a transaction to be included in a block and receive its first confirmation. Even without the additional demand anticipated when new projects come online in the future, there sometimes are periods where user experience is degraded by limited throughput.

The plots below illustrate the significant frequency of blocks that are nearly full. (The maximum block size since Epoch 335 has been 90,112 bytes.) The six-minute average block size also indicates the presence of full blocks, but the block-size limit does not appear significant in six-hour average. On the epoch-average level there are periodic peaks in block size. Note that when interpreting these diagrams, it is important to consider that transactions will not exactly fill a block. Also, some blocks hit their limit on Plutus execution cost and memory before well below the maximum-size limit: they are, nevertheless, fully utilized.

|                                                            |                                                            |
| ---------------------------------------------------------- | ---------------------------------------------------------- |
| ![Distribution of block sizes](images/block-size-1blk.svg) | ![Distribution of block sizes](images/block-size-6min.svg) |
| ![Distribution of block sizes](images/block-size-6hr.svg)  | ![Distribution of block sizes](images/block-size.svg)      |

Of particular interest is the following plot that shows the distribution of the length of runs of consecutive blocks that are all larger than 80 kB. Occasionally, there are stretches of more than ten blocks being almost full, and in one case there was a series of 194 almost-full blocks. These long periods of nearly full blocks may be correlated with long waits between the time a user submits a transaction to the memory pool and the time it appears in a block.

![Runs of nearly-full blocks](images/block-run.svg)

The following plots illustrate the situation for memory and execution units relative to the maximum allowed for a block. This confirms the rule of thumb that Plutus memory is a tighter constraint than Plutus steps. A not-insignificant number of blocks are close to or at the Plutus execution limits.

|                                                               |                                                              |
| ------------------------------------------------------------- | ------------------------------------------------------------ |
| ![Distribution of block sizes](images/block-mem.svg)          | ![Distribution of block sizes](images/block-steps.svg)       |
| ![Distribution of block sizes](images/block-memory-epoch.svg) | ![Distribution of block sizes](images/block-steps-epoch.svg) |

More concerning are the situations shown below where consecutive blocks are near or at the Plutus execution budget. In one case twenty-five consecutive blocks were near that limit.

![Runs of nearly-full blocks](images/block-ex-run.svg)

## Use cases

Even with the existing rate of transactions on the Cardano mainnet, there are periods where throughput-limits delay the inclusion of transactions in blocks and hamper settlement. Growing and emerging use cases will exacerbate the situation.

- Time-sensitive applications like DEXes and Dapps require prompt inclusion of their transactions on the blockchain, and any delay also translates to a delay in settlement. See also [CPS-17 Faster Settlement](https://github.com/cardano-foundation/CIPs/pull/922).
- Newly released high-profile Cardano applications tend to create congestion as many users experiment and transact with the new capabilities shortly after they become available. Greater transaction throughput will improve the initial experience of new users of those applications, and some of those new users may be new to Cardano. *First impressions are important.*
- Partner chains, bridges, and oracles rely on quality of service guarantees that support a regular and predictable rhythm of their transactions being included in blocks. Delays in such transactions' inclusion in blocks can cascade to Dapps that interact with such services. Delays on oracles result in stale data being provided to Dapps or in Dapps having to wait for the updated oracle state to be posted. Delays on partner chains or bridges result in bottlenecks in the transfer of funds or information between chains.
- Transaction "scoopers" and "batchifiers" work most efficiently when high throughput is possible.
- Air drops are well known to have caused spikes in network load and block utilization.
- Any of the above use cases that also involve executing Plutus scripts add an additional requirement of execution-unit throughput in addition to transaction-size throughput. Applications that do complex validation encounter this extra dimension of resource usage.

The advent of the Cardano-Midnight ZK bridge and the prospect of a Cardano-Bitcoin BOS Grail bridge promise to significantly increase the transaction load on the Cardano mainnet.

## Goals

1. Develop precise requirements for transaction and script-execution throughput for Cardano mainnet, categorized by use case and metrics for quality of service.
2. Propose safe increases in the maximum block size and Plutus execution limits for blocks.
3. Increase transaction throughput in terms of number, size, and execution units and provide evidence that the proposed techniques meet stakeholder requirements.
4. Investigate and semi-quantitatively compare throughput techniques such as input endorsers, zero-knowledge technologies, transaction prioritization, offloading work (Mithril, partner chains, etc.), and protocol-parameter changes.
5. Propose methods for guaranteeing specific levels of throughput, including priority tiers and reservations.

In addition to the goals above, it is advisable to avoid the following:

1. Avoid approaches with long development timelines or high opportunity costs.
2. Do not weaken Ouroboros security or substantially enlarge its attack surface.
3. Minimize changes that increase the resource usages of Cardano nodes or the cost of operating them.
4. Guard against protocol alterations that adversely impact other scaling metrics such as settlement time.

## Open questions

- How much larger can existing Ouroboros Praos blocks be made without affecting Cardano mainnet safety or performance?
- How much can the block-production rate (the active-slot coefficient) be increased without affecting Cardano mainnet safety or performance?
- What fraction of theoretical global network bandwidth can techniques like input endorsers efficiently utilize?
- Are zero-knowledge techniques a viable option for increasing transaction throughput?
- How much will implementing greater transaction throughput impact the hardware requirements for and the cost of operating a Cardano stakepool?
- Will changes to the memory pool be necessary to support transaction throughput?
- Will increasing throughput adversely affect other performance metrics such as settlement time?
- Will higher throughput open Cardano to a broader spectrum of denial-of-service and other attacks?
- To what extent is the Plutus execution budget for blocks a more limiting constraint than the size budget for blocks? What statistics support this? What types of applications hit this constraint, and how often?
- Can high-throughput solutions simplify the operation of transactions scoopers and batchifiers?
- Does [Ouroboros Leios](https://iohk.io/en/research/library/papers/high-throughput-blockchain-consensus-under-realistic-network-assumptions/) satisfy stakeholder requirements for greater throughput? Would simpler solutions be adequate in the short term?
- How much can pay-for-priority schemes alleviation throughput concerns for high-value applications that are particularly sensitive to it?

## Copyright

This CIP is licensed under [Apache-2.0](http://www.apache.org/licenses/LICENSE-2.0).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CPS-0020/README.md
---

- --
CPS: 20
Title: Governance Stakeholder Incentivization
Status: Open
Category: Tools
Authors:
- Seomon (Cardano After Dark) <Seomonregister@gmail.com>
- Martin Marinov <martin.marinov@cardano.marketing>
- Sebastián Pereira Gutiérrez <sebpereira33@gmail.com>
Proposed Solutions:
- 
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/997
Created: 2025-02-13
License: CC-BY-4.0
- --

## Abstract
The Cardano governance framework depends on the active participation of Delegated Representatives (DReps), the Constitutional Committee (CC), and Ada holders, alongside the already incentivized Stake Pool Operators (SPOs), to maintain a highly effective, representative and democratic governance structure. Presently, the system's heavy reliance on the altruistic behavior of governance participants, which poses a significant risk, potentially leading to a concentration of voting power among a small, select group. This situation undermines broader community participation and threatens democratic principles. The primary goal should be to establish a balanced but robust incentivization framework that encourages consistent and informed participation from a diverse range of stakeholders with different levels of governance responsibilities, commitments and impact. However, achieving this objective faces several technical obstacles and game theoretic challenges, including designing fair reward mechanisms, ensuring the sustainability of incentives, and preventing the misuse of incentivization structures. Addressing these challenges is crucial to fostering a more balanced and representative governance model for the Cardano community.

## Problem
The Cardano governance framework relies on Delegated Representatives (DReps), Constitutional Committee (CC), Stake Pool Operators (SPOs), and ADA holders to make informed decisions, exercise veto power, and select both SPOs and DReps that reflect the broader community's interests. However, the current system lacks a structured mechanism to incentivize DReps, CC members, and ADA holders delegating to DReps effectively. Without adequate rewards, DReps and CC members are primarily driven by altruism or self-interest, which is neither sustainable nor scalable in the long term. ADA holders also lack any reason to deeply connect with DReps. This reliance on voluntary participation risks a significant imbalance in governance, where only a small, motivated subset of the community holds disproportionate influence.

The absence of an incentivization framework discourages widespread participation and may lead to voter apathy, reducing the overall quality and representativeness of decisions. This problem is further exacerbated by the potential for centralization, as a few highly active stakeholders can dominate governance processes, marginalizing the voices of the broader community.

The writing of this CPS document is motivated by the urgent need to address these issues and create a comprehensive incentivization system. Such a system aims to ensure that DReps, CC members, and ADA holders are fairly compensated for their efforts, thereby promoting active, informed participation and maintaining a decentralized governance structure that truly reflects the diverse interests of the Cardano community.

Any incentives program for the above mentioned stakeholders should take into account the current SPO rewards. Rewards and incentives have to be considered along the yearly inflows of ada to the treasury. All governance stakeholders must be considered as part of a whole which is the Cardano governance model.



## Use Cases
### **1. Dedicated DReps Struggle to Stay Engaged**
Bob is a long-time Cardano supporter who wants to actively participate in governance. He has been following governance actions and has taken the time to research and write about important proposals. His insights gain traction, and several small ADA holders delegate their stake to him, recognizing his contributions.

However, despite his dedication, Bob finds it increasingly difficult to justify the time required to analyze governance actions, engage with the community, and vote responsibly. He works a full-time job and has personal responsibilities, leaving him with little bandwidth to commit to governance. While some Stake Pool Operators (SPOs) and project owners have been able to transition into full-time roles within the ecosystem, Bob has no way of sustaining himself as a DRep. Over time, he is forced to step back and delegate his voting power to another DRep, reducing the diversity of governance representation.

A well-designed incentivization framework would allow individuals like Bob to remain engaged by compensating them for their time and effort. This would lead to a more representative governance model and prevent valuable contributors from exiting the system due to financial constraints.

![DRep registrations and deregistrations over time from epoch 510](https://github.com/Ryun1/metadata/blob/main/drep-reg-and-drep-over-time.png?raw=true)

### **2. Constitutional Committee members (CC members)**
The CC is the body with the smallest pool of participants and we expect this to remain the case even after it opens up to more members. As such, CC members will have a bigger workload per individual when compared to other governance bodies.

CC members run the risk of being overwhelmed by the quantity of proposals they’ll need to consider. Compensation for CC members allows them to dedicate more time to the process and also consider it as part of their professional activities. Members of the CC should be incentivesed by their on-chain votes on all governance actions where their role plays a part. This on-chain information can be the basis to check on their activity and performace.

### **3. Ada Holders are incentivized but not for governance involvement**
Ada holders play a crucial role in the Cardano ecosystem. While they are incentivized to stake to a DRep, there is no clear motivation for them to stay actively engaged in their DRep’s voting behavior or governance participation. This creates a disconnect between delegation and governance outcomes, weakening the representativeness of governance decisions.

Many Ada holders delegate their stake precisely because they lack the time or expertise to follow governance actions closely. However, the system still requires their periodic engagement, to assess whether their chosen DRep continues to align with their values. Without an incentive to review or reconsider their delegation, Ada holders risk passively contributing to governance stagnation, where ineffective or misaligned DReps retain power simply due to voter inertia.

For governance to be truly decentralized and representative, Ada holders must have a reason to stay informed—even at a minimal level—to ensure their delegation remains an active choice rather than a passive default.


### **4. Stakepool Operators are already incentivized**
While Stake Pool Operators play a dual role in the Cardano ecosystem now, their governance participation is not directly rewarded.

SPOs who are also DReps face overlapping responsibilities, meaning they have the opportunity to do almost the same amount of work for double the impact in some cases.

Since there is little additional incentive for them, to engage deeply in governance beyond their required votes this could lead to a passive voting behaviour, where SPOs vote without fully engaging in the governance discourse. Another concern is the Concentration of governance power, if large SPOs who act as DReps accumulate excessive influence over governance decisions as some already have.

### **5. Centralization of Governance Due to a Lack of Incentives and Guardrails**
In the absence of financial incentives, only a small group of highly motivated individuals participate in governance. Over time, governance power becomes concentrated among a few active representatives who have the time and resources to engage, while many potential DReps remain on the sidelines.

This concentration of power increases the risk of governance capture, where a small number of stakeholders control the outcome of key decisions and their only financial incentive is voting to serve their own interests. Additionally, inactive DReps or SPOs who continue to hold delegation without participating, or voters who lost their keys but still stake to DReps, contribute to governance stagnation, reducing the legitimacy of voting outcomes.

A structured incentive system would mitigate these risks by encouraging broader participation and preventing governance power from consolidating among a select few. Implementing periodic delegation resets for inactive DReps would further ensure that voting power remains in the hands of engaged representatives.

### **6. Uninformed Voting Reduces Governance Effectiveness**
DReps & SPOs are expected to vote in the best interest of the Cardano community, but without the right incentives, there is little motivation to conduct thorough research on governance actions. Some DReps/SPOs may vote based on limited information, personal biases, the current SPO incentives or simply follow the decisions of other prominent representatives without critical evaluation.

This lack of diligence increases the likelihood of errors in leadership and policy, including the approval of poorly structured proposals or the rejection of initiatives that would benefit the ecosystem. Without a mechanism to reward informed voting, governance risks becoming a process of rubber-stamping rather than a meaningful evaluation of proposals.

By introducing performance-based rewards that factor in voting participation, rationalization, and retrospective scoring of governance actions, the system can encourage DReps to engage more thoughtfully in decision-making. This would improve the overall quality of governance and ensure that voting outcomes align with the long-term interests of the Cardano ecosystem.

### 7. Disincentives
The prior use cases have focused thus far on frameworks that use positive incentives to increase participation, support, and broaden the pool of stakeholders. There is a case to be made to also introduce disincentives to the governance process where mechanisms punish and discourage bad actors. For this to have an effect, the positive behavior of each stakeholder needs to be identified and made clear for all participants. Any punitive actions at the level of the protocol have to be designed with a clear set of positive ones. These must be identified via a community-led effort that ensures all participants can have input in the process.

Any introduction of protocol-level disincentives has to be taken with proper data and community consensus. If these mechanisms are not adequately discussed and tested, the network can introduce punitive actions for behaviours that may be within the democratic model of Cardano. For this reason, this document cannot recommend any such mechanism; the aim is only to leave the possibility open for those in the community who want to research this area of governance.

## Goals
### **Primary Objective**
Establish a structured and sustainable incentivization framework that motivates and rewards governance stakeholders to participate actively and make well-informed governance decisions, therefore upholding democratic, sustainable, ethical as well as decentralized principles of the Cardano ecosystem.

### **Key Goals**
#### Compensate stakeholders for their meaningful work
Governance stakeholders invest significant time and effort into understanding, evaluating, and voting on governance actions. Without financial incentives, this responsibility falls predominantly on individuals who either have the financial freedom to participate voluntarily, are driven purely by altruism, or have self-serving motives. A fair and transparent reward mechanism will enable DReps to commit to governance activities without financial strain, ensuring the role remains accessible to all willing participants and serves the greater ecosystem.

#### Increase stakeholder participation
The current system lacks sufficient incentives, leading to a low engagement rate among potential governance participants. This results in a concentration of power among a small number of highly active participants, which increases the risk of centralization and reduces diversity in governance decision-making. By implementing a rewards system, more individuals will be encouraged to participate, leading to a more decentralized and representative governance model. Inclusivity should be considered as a democratic core value to increase stakeholder buy-in and participation.

#### Improve the quality of stakeholder actions
The effectiveness of governance decisions depends on the depth of understanding and thoroughness of analysis conducted by governance stakeholders. A well-designed incentivization mechanism will encourage stakeholders to dedicate time to research governance proposals, critically evaluate their impact, and act in a manner that aligns with the broader interests of the Cardano community. Furthermore, implementing performance-based rewards, such as factoring in participation rates and the quality of voting rationales as well as scoring of governance actions in retrospect will discourage uninformed voting and promote responsible decision-making.

#### Ensure fair and transparent reward distribution
The incentivization model must be structured to fairly distribute rewards based on clear and measurable parameters. Rewards should not be arbitrary or biased toward wealthier or more influential participants but should instead reflect the quality and impact of a DRep’s contributions. The framework must be transparent, allowing the Cardano Community to understand how rewards are calculated and distributed.

#### Encourage long-term engagement and stability
Governance in Cardano is an evolving process that requires consistency and long-term commitment. An effective incentivization structure should not only attract new participants but also encourage existing DReps to remain engaged over extended periods. This will ensure continuity in governance practices, minimize abrupt power shifts, and provide a stable decision-making environment for the ecosystem.

#### Prevent gaming and misuse of incentives
Any reward system carries the risk of exploitation, where participants attempt to maximize rewards through minimal effort, collusion, or other manipulative tactics. The incentive framework must be robust and include safeguards against such behaviors, such as requiring active participation, considering qualitative assessments of governance contributions, and periodically reviewing and adjusting parameters to close potential loopholes.

#### Promote global and cultural diversification
Cardano is a global ecosystem, and its governance should reflect the diverse perspectives and experiences of its community members. A key goal is to encourage participation from a wide range of geographic regions, cultural backgrounds, and socioeconomic contexts. Achieving this will help governance decisions better align with the needs and values of the global Cardano community, rather than being dominated by any single region or demographic.

#### Maximize governance effectiveness
Traditional governance mechanisms often suffer from inefficiencies, slow decision-making, and resource-heavy processes that do not scale well. The incentivization framework for DReps must be designed to optimize governance effectiveness while minimizing unnecessary complexity and resource consumption.


## Open Questions
- Is change needed at the Ledger/protocol level or tooling/wallet level?
- What are the current incentives for stakeholders?
- Is the current architecture avoiding or aiding centralization?
- What’s the proper reward amount?
- How much should the rewards be based on ADA delegation, the DRep’s participation or other factors?
- What’s the impact of stakeholder rewards on Cardano’s treasury?

## **References**
- [Reward Schemes and Committee Sizes in Proof of Stake Governance](https://arxiv.org/abs/2406.10525)
- [Should DReps receive compensation? Poll]()
## Copyright
This CIP is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CPS-0021/README.md
---

- --
CPS: 21
Title: Ouroboros Randomness Manipulation
Category: Consensus
Status: Open
Authors:
- Nicolas Henin <nicolas.henin@iohk.io>
- Raphael Toledo <raphael.toledo@iohk.io>
Proposed Solutions:
  ["CIP-0161 : Ouroboros Phalanx - Breaking Grinding Incentives"]
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/1009
Created: 2025-10-03
License: Apache-2.0
- --

## Abstract

The **Ouroboros protocol** ⚙️, underpinning Cardano’s consensus mechanism, relies on a modular design where the **Randomness Generation Sub-Protocol** is pivotal in addressing the *Coin-Flipping Problem*—the generation of **fair**, **unbiased**, and **unpredictable** randomness in a distributed system.

This CPS formally documents this challenge, aiming to coordinate community-driven efforts through **Cardano Improvement Proposals (CIPs)** 📝 to mitigate or resolve it within Ouroboros. Randomness is foundational to sub-protocols like **leader election**, where vulnerabilities could compromise Cardano’s **security** 🔒, **fairness** ⚖️, and **decentralization** 🌐.

As the consensus protocol is a core component of any blockchain, and the **Randomness Sub-Protocol** is central to Ouroboros, this problem represents a *core of the core* issue for Cardano, with profound impacts on its integrity.

However, this issue is not exclusive to Cardano; it is a pervasive concern across **Proof-of-Stake (PoS)** blockchains, including Ethereum, Solana, Algorand, and Tezos, where randomness underpins validator selection and consensus integrity.

This CPS examines Ouroboros’ approach, and poses critical questions about **vulnerability**, **detection**, and **deterrence**. Rather than prescribing solutions, it invites the Cardano community to assess and address this shared challenge, fostering research and innovation to enhance Ouroboros’ resilience within the broader PoS landscape 🌍.

## Problem

This **Cardano Problem Statement (CPS)** 📜 builds upon the foundational work of the [**Cardano Problem Definition (CPD): Ouroboros Randomness Generation Sub-Protocol – The Coin-Flipping Problem**](./CPD/README.md) 📑, which provides an in-depth technical analysis of the issue at hand. While the **CPD** retains the full depth of technical material—including detailed modeling, cost definitions, and adversarial analysis—the **CPS** is structured for formal submission within the **CIP process** ✍️, prioritizing accessibility and alignment with community-driven solution development. As such, the technical details are preserved in the **CPD** as an appendix, while this **CPS** offers a concise, high-level summary of the problem, connecting it to broader concerns within Cardano and the blockchain community.


### Summary of Findings

This **[**CPD**](./CPD/README.md)** examines the *Randomness Generation Sub-Protocol* within the *Ouroboros Praos* ⚙️, highlighting its vulnerabilities and their implications for *Cardano’s* **security** 🔒. Key insights include:

- **Randomness Vulnerability**: *Ouroboros Praos* employs **VRFs** for randomness generation, but this approach is susceptible to *grinding attacks*, where adversaries manipulate outcomes to influence **leader election**, threatening Cardano’s **fairness** ⚖️ and **integrity**.
- **Attack Likelihood**: Attacks become significantly more feasible when an adversary controls **over 20% of the total stake** (approximately **4.36 billion ADA**, as of March 2025), while smaller stakes (e.g., **5%**) make such attempts highly unlikely over extended periods.
- **Economic Barrier**: Gaining enough stake to execute an attack requires a **substantial investment** 💰—billions of USD for a **20% share**—posing a financial risk, as a successful attack could devalue the asset and undermine network trust.
- **Computational Feasibility**: The feasibility of attacks varies widely based on the computational resources an adversary can deploy, becoming progressively more accessible as stake accumulates:
- Small-scale attacks, costing as little as ~**$56**, are easily achievable with minimal resources, such as a standard computer, making them a low-barrier threat that even individual actors could attempt.
- Large-scale attacks, costing up to ~**$3.1 billion**, require extensive computational infrastructure, such as large data centers with millions of CPUs, placing them in a range from feasible for well-funded entities (e.g., corporations or nation-states) to nearly impractical for most adversaries due to the immense resource demands.
- The intensity of these attacks scales with stake: the more stake an adversary holds, the greater their influence over **leader election**, amplifying their ability to manipulate randomness. In a simplistic view, this can be likened to manipulating a $256$-bits nonce—a value $\rho$ ranging from $0$ to $256$— where higher stake progressively grants more control, potentially allowing full manipulation of the nonce at the upper limit.
- The wide cost disparity reflects how the complexity of the attack—such as the scope of the targeted time window and the depth of evaluation—drastically increases resource needs, acting as a natural deterrent for more ambitious manipulations.

To illustrate the **Computational Feasibility**, the graph below (sourced from the **CPD**, Section [**3. The Cost of Grinding: Adversarial Effort and Feasibility**](./CPD/README.md#3-the-cost-of-grinding-adversarial-effort-and-feasibility)) maps attack feasibility across four scenarios—**Ant Glance**, **Ant Patrol**, **Owl Stare**, and **Owl Survey**—based on the nonce value $\rho$ (0 to 256 bits). Each scenario reflects different attack complexities, with feasibility shifting as computational and economic demands grow:

<div align="center">
<img src="./CPD/image/grinding_depth_scenarios_cost_with_feasibility_layers_gradient.png" alt="Grinding Depth Scenarios with Feasibility Thresholds"/>
</div>

The table below delineates the **$\rho$ values** at which each scenario transitions across feasibility categories, illustrating the computational and economic thresholds:

| **Feasibility Category**                  | **🔵 Ant Glance**   | **🟠 Ant Patrol**   | **🟢 Owl Stare**   | **🔴 Owl Survey**   |
|--------------------------------------------|---------------------|---------------------|--------------------|--------------------|
| **🟢 🌱 Trivial for Any Adversary**        | $0 \to 53.6$        | $0 \to 32.9$        | $0 \to 31.6$       | $0 \to 31.1$       |
| **🟡 💰 Feasible with Standard Resources** | $53.6 \to 60$     | $32.9 \to 39.5$     | $31.6 \to 38.3$    | $31.1 \to 37.8$    |
| **🟠 🏭 Large-Scale Infrastructure Required** | $60 \to 69.7$  | $39.5 \to 49.5$     | $38.2 \to 48.2$    | $37.8 \to 47.7$    |
| **🔴 🚫 Borderline Infeasible**            | $69.7 \to 79.4$     | $49.5 \to 59.5$     | $48.2 \to 58.2$    | $47.7 \to 57.7$    |
| **🔴 🚫 Infeasible**                      | $79.4 \to 256$      | $59.5 \to 256$      | $58.2 \to 256$     | $57.7 \to 256$     |


- *Context**: The scenarios represent increasing attack sophistication (e.g., *Ant Glance* is a quick, low-effort attack; *Owl Survey* is a comprehensive, resource-intensive one). As $\rho$ increases, so does the difficulty, shifting feasibility from trivial (e.g., a lone actor with a laptop) to infeasible (e.g., requiring nation-state-level resources).

These findings underscore critical risks to Cardano’s consensus mechanism, urging the **Cardano community** 🌐 to address them through potential strategies like improved detection, stake pool diversity, or protocol enhancements. For detailed technical analysis, stakeholders are encouraged to consult the **CPD**.


## Use Cases

The *Coin-Flipping Problem* 🎲 within the **Ouroboros protocol** ⚙️ represents a **core of the core issue** for Cardano. The consensus mechanism is a fundamental component of any blockchain, and the *Randomness Generation Sub-Protocol* is integral to Ouroboros’ operation. Consequently, the consequences of randomness manipulation are **diverse and far-reaching** 🌍, impacting Cardano’s **security** 🔒, **fairness** ⚖️, and **decentralization** 🌐 in numerous ways. Given this breadth, providing an exhaustive list of use cases is challenging. However, a significant impact is evident: randomness manipulation enables an adversary to **group the intervals** during which they are selected as a **slot leader**, concentrating their influence over block production.

This ability to cluster slot leadership **amplifies classical blockchain attacks** 💥, exacerbating their impact. For example:

- In a **Private Forking Attack** 🍴, an adversary withholds blocks to create a secret fork, later revealing it to outpace the honest chain. Clustering slot leadership enhances their ability to extend this fork, increasing the attack’s success rate.
- Similarly, this manipulation intensifies other attacks, including:
- **Selfish Mining** 💰: Withholding blocks to gain disproportionate rewards.
- **Double-Spending Attacks** 💸: Reversing transactions for financial gain.
- **Censorship Attacks** 🚫: Excluding specific transactions to suppress competition.

As detailed in the [**CPD**](./CPD/README.md#213-potential-outcomes-of-grinding-attacks), adversaries could also exploit randomness to enable additional attack vectors, such as:

- **Economic Exploitation** : Prioritizing transactions for higher fees.
- **Minority Stake Exploitation** 📊: Amplifying a small stake’s influence.
- **Settlement Delays** ⏳: Undermining trust in confirmation times.

These scenarios **underscore the critical need** ❗ to address randomness manipulation to safeguard Cardano’s integrity.

## Goals

The goal is to **mitigate or completely eliminate grinding attacks** on the protocol by introducing **targeted protocol enhancements** to address this issue. Two approaches are actively being explored to address the **Randomness Manipulation Problem**:

- **Complete Elimination of Grinding Attacks** – Ongoing research aims to make the protocol fully resistant to such attacks. One notable example is *[Efficient Random Beacons with Adaptive Security for Ungrindable Blockchains](https://eprint.iacr.org/2021/1698.pdf).*
- **Partial Mitigation by Increasing Attack Complexity** – While full protection may not yet be feasible, making such attacks **computationally and economically prohibitive** can significantly reduce their viability. This approach is the basis of **Ouroboros Phalanx** (Coming soon as a CIP)].

However, while **fully protecting the protocol from Randomness Manipulation attacks** may not yet be feasible, it is crucial to advance in the following areas:

- **Risk Quantification** : Assessing the **profitability and feasibility of attacks**, along with **refining risk assessment models**, will provide deeper insights into vulnerabilities and their potential impact on the protocol's security and stability.

- **Transparency on Manipulations** : **Enhancing detection mechanisms**, such as **self-mixing analysis** and **forking manipulation detection**, can help identify potential exploits and assess ongoing threats in real time.

- **Game Theory & Economic Disincentives** –
- *Promoting stake operator diversity** and **strengthening decentralization incentives** will reduce the economic viability of manipulation, fostering a more **resilient and distributed** stake pool ecosystem.

We strongly encourage the community to actively engage in addressing this challenge by contributing research, proposing solutions, and participating in discussions. Collaborative efforts will be crucial in refining detection mechanisms, strengthening protocol resilience, and ensuring the long-term security and fairness of Ouroboros.



## Open Questions

The *Coin-Flipping Problem* 🎲 within the **Ouroboros protocol** ⚙️ poses critical uncertainties that challenge Cardano’s **security** 🔒, **fairness** ⚖️, and **decentralization** 🌐. These open questions, rooted in the [**CPD**](./CPD/README.md) 📑 analysis, call for exploration by the **Cardano community** 🌍 to strengthen the protocol’s resilience:

- **Is randomness manipulation currently occurring, and how detectable is it?** 🕵️‍♂️
  Are grinding attacks already subtly affecting Cardano undetected? What tools, metrics, or on-chain signals could reveal adversarial manipulation in real-time, given the protocol’s design?

- **How will Peras influence grinding attack capabilities?** 🔄
  With Peras enhancing Ouroboros’ settlement times, does it increase, decrease, or shift the opportunities for adversaries to exploit randomness through grinding attacks?

- **Are current economic deterrents effectively discouraging randomness manipulation?** 💰
  Does the high cost of acquiring significant stake (e.g., ~4.36 billion ADA for 20%) adequately dissuade adversaries, or do potential rewards (e.g., staking profits, governance power) offset this barrier? Can we do better ?

- **How does preparing for the worst-case grinding attack scenario affect the security parameter $k$?** 🛡️
  If we adjust Ouroboros to handle extreme grinding attacks, what impact would this have on the **security parameter $k$**, which governs chain stability and finality ? What benefits could we gain by reducing $k$?

- **Who benefits most from executing a grinding attack?** 🎯
  Which actors—malicious stakeholders, competing blockchains, or economic opportunists—stand to gain the most from manipulating randomness, and how might their motives shape attack strategies?

- **Can grinding attacks be fully eliminated from Ouroboros?** 🚫
  Is it technically feasible to design a randomness mechanism that completely removes the possibility of grinding attacks, or are trade-offs (e.g., scalability, complexity) inevitable?

- **Can we increase the cost of grinding attacks to deter adversaries?** 💸
  What protocol modifications—such as higher computational demands or stake penalties—could make grinding attacks prohibitively expensive, and how would they balance with Cardano’s usability?

## Copyright

This CIP is licensed under [Apache-2.0](http://www.apache.org/licenses/LICENSE-2.0).



---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CPS-0022/README.md
---

- --
CPS: 22
Title: Lost Stake
Category: Ledger
Status: Open
Authors:
- Ryan Wiley <rian222@gmail.com>
Proposed Solutions: []
Discussions:
- https://github.com/cardano-foundation/CIPs/pull/1060
Created: 2025-07-22
License: CC-BY-4.0
- --

## Abstract

In everyday Cardano discussions the umbrella term “Sticky Stake” is used for any stake that stubbornly remains delegated, regardless of whether its owner is still around. This Cardano Problem Statement (CPS) zooms in on the most critical slice of that phenomenon that we dubbed “Lost Stake”: Ada that (a) remains delegated to a stake pool or dRep, yet (b) can never again be moved because the controlling private keys are irretrievably lost (e.g., seed-phrase loss, death of the sole key holder, catastrophic wallet failure).

This CPS formalises the Lost Stake problem and quantifies its systemic impact: dilution of the rewards pot available to active participants, distortion of pool-selection incentives, and ossification of governance power. Some amount of ADA may already be draining from the rewards pot into permanently unreachable wallets each epoch. Left unchecked, compounding Lost Stake will siphon a significant amount of ADA in rewards and an ever-growing share of voting weight into wallets that nobody controls, making future remediation far costlier and more contentious.


## Problem

Lost Stake continues to earn and compound staking rewards and carries voting weight despite being permanently inaccessible.  Even though the funds are gone for good, the ledger continues to treat them as live stake. They keep:

• earning a proportional share of every epoch’s rewards,
• compounding themselves through those rewards, and
• exerting voting weight whenever their chosen dRep participates.

“Lost delegation” may be a more technically correct phrase since it is the delegation certificate that survives, but we will use the more familiar term "Lost Stake" to stay consistent with community vocabulary around Sticky Stake.

- *Figure 1** (below) visualises these relationships with overlapping circles:

<img src="fig1.jpg" alt="Figure 1: Circles depict the Total Rewards Pot and its diminishing share with some rewards also flowing to Sticky Stake and Lost Stake addresses every epoch." width="50%" />

> **Figure 1:** Circles depict the Total Rewards Pot and its diminishing share with some rewards also flowing to Sticky Stake and Lost Stake addresses every epoch.

Cardano already distributes a significant amount of ADA every epoch in staking rewards to addresses that are permanently inaccessible. This occurs when ADA is lost, such as when a holder loses their seed phrase or passes away without sharing their keys, rendering the funds permanently unreachable. In most other cryptocurrencies, lost coins simply exit circulation. For example, it is estimated that around 20% of all Bitcoin supply is lost forever [[investopedia.com](https://www.investopedia.com/news/20-all-btc-lost-unrecoverable-study-shows/)], with more granular analyses by Ledger Academy and Chainalysis both converging on roughly 4% of all Bitcoin being lost each year [[ledger.com](https://www.ledger.com/academy/topics/economics-and-regulation/how-many-bitcoin-are-lost-ledger); [chainalysis.com](https://www.chainalysis.com/blog/money-supply/)].

Cardano’s design, however, allows lost ADA to remain economically “active” if it was delegated prior to loss. Once delegated, a stake key remains registered and tied to a stake pool (for block production) and potentially to a dRep (for voting) until it is actively changed or deregistered. A user who loses access cannot undelegate or spend those funds, meaning the ADA continues to stay delegated indefinitely.

The lost-stake problem is the accumulation of this unreachable-yet-delegated ADA within the Cardano ecosystem. Such lost stake still contributes to stake-pool sizes and earns staking rewards every epoch, even though the rewards accumulate in an address that nobody controls. For as long as the chosen stake pool produces staking rewards, the lost ADA compounds. Rewards paid to these addresses increase their delegated stake, which in turn earns more rewards, and so on. Similarly, if the lost ADA was delegated to a governance representative (dRep) for Voltaire-era on-chain voting, that voting power remains with the dRep permanently (or until the dRep retires or is marked inactive). The original owner is no longer present to adjust their delegation in response to changing conditions. This creates a class of delegation that cannot be reallocated or corrected.

### Detrimental effects

Lost stake and lost ADA have several detrimental effects on the Cardano network:

#### Perpetual reward dilution
Each epoch, a portion of the total ADA rewards is distributed to all staked ADA, whether active or lost. Rewards sent to addresses with lost ADA are effectively removed from circulation forever, resulting in active delegators and stake-pool operators (SPOs) receiving a smaller share than they would if that lost stake did not exist. In effect, active participants are subsidizing the lost stake. Over time, the compounding of these rewards to lost ADA can significantly dilute the reward pool available to real users and operators.

#### Reward increases worsen the problem
Any attempt to increase staking rewards—such as raising the reward rate, boosting incentives for SPOs or delegators, providing additional yield from Partner Chains, or otherwise enlarging the total rewards pot—will also proportionally increase the amount of rewards paid to lost ADA. As a result, well-intentioned efforts to improve returns for active participants can actually make the lost stake problem worse, since a fixed percentage of all new rewards will continue to be siphoned off to permanently unreachable addresses.

#### Skewed stake-pool incentives
Lost ADA that remains delegated contributes to a stake pool’s apparent stake and saturation level. Pools with large amounts of lost stake may continue to produce blocks and earn rewards from that stake without any risk of it ever being withdrawn. This can distort competitive incentives. For example, a pool might appear reliably saturated or have high loyalty even if some of its delegation is simply abandoned funds. In extreme cases, if a pool amasses substantial lost ADA, it could remain highly ranked or saturated based on stake that no active delegator can respond to (e.g., they cannot move that stake if the pool underperforms). This reduces the effectiveness of normal market dynamics in the staking ecosystem and can harm network security.

#### Governance participation anomalies
In Cardano’s governance model (e.g., under [CIP-1694](https://cips.cardano.org/cips/cip1694/)), voting power is tied to stake. Lost ADA that was delegated to a dRep continues to bolster that dRep’s voting power indefinitely. This means decisions may be swayed by stake with no active owner, potentially undermining the representativeness of votes. The governance framework acknowledges this risk—for instance, CIP-1694 introduces an inactivity mechanism so that dReps who stop voting are marked inactive. However, if lost ADA remains delegated to an **ACTIVE** dRep, it will keep influencing outcomes with no way for the original holder (or anyone) to ever retract that delegation.

#### Long-term economic inefficiencies
As the proportion of lost stake grows, Cardano’s monetary and incentive system will face sustainability issues. Eventually, block rewards will rely more on transaction fees (as treasury reserves deplete). If a significant fraction of stake is lost ADA, then a matching fraction of all transaction fees (and any remaining rewards) gets continually paid to unreachable addresses. This reduces fee efficiency and causes the network to effectively waste a chunk of fees on lost stakeholders, making less available to reward the operators and holders who actually secure and use the system. In a scenario where, say, 30% of all stake is lost stake decades from now, that 30% of fees and rewards is perpetually locked up, potentially requiring higher fees or other adjustments to adequately incentivize active validators.

It is important to formally document the lost-stake problem now, even before it becomes visibly acute, because the Cardano community needs a clear understanding of the issue’s scope and implications.

### Why the Protocol Behaves This Way

Cardano’s ledger does not distinguish between active and inactive stake. All ADA is treated equally under the consensus rules. This design choice (common to many PoS systems) avoids complexity and respects the principle that tokens are the bearer’s property indefinitely. However, the unintended consequence is that there is no built-in mechanism to recognize or mitigate lost keys. From a protocol perspective, lost ADA is indistinguishable from a perfectly content long-term holder. Any potential solution must therefore carefully balance improving incentives with respecting property rights and avoiding false positives (e.g., not seizing or disabling legitimately held ADA).

### Current Mitigations and Their Limits

As noted earlier, governance proposals like [CIP-1694](https://cips.cardano.org/cips/cip1694/) include measures to limit the impact of inactive delegated stake on voting outcomes. Additionally because DRep delegation is a new feature, all previously lost stake is unable to engage. These measures (such as marking inactive dReps) help prevent governance paralysis, but do not address the underlying issue of lost ADA still existing and, in some cases, continuing to accumulate rewards. However, when a stake pool with lost stake retires or shuts down, the lost ADA delegated to it is actually much less of a problem. That ADA effectively becomes undelegated and removed from circulation, meaning it no longer receives staking rewards or participates in governance. While the system currently has no direct way to reclaim or reassign lost ADA, the most persistent issues arise when lost stake remains delegated to active pools or dReps. Indirect mitigations only address symptoms (like governance quorum) rather than the root cause.

### Why Ignoring the Problem Is Risky

Some might argue that lost coins simply increase the value of the remaining ones (through scarcity) or that the effect is negligible for now. However the effect is not static. It grows over time and can reach levels that materially impact network operation. Unlike in Bitcoin (where lost coins arguably don’t harm network security or functionality [[investopedia.com](https://www.investopedia.com/tech/how-much-bitcoin-has-been-lost/)]), in Cardano most lost coins still participate in consensus. Therefore, ignoring lost stake and lost ADA means accepting a slow-growing skew in the system that could ultimately undermine user trust and network performance. Early recognition allows for carefully researched, minimally disruptive solutions before the problem becomes too large and contentious to fix.


## Use Cases

The motivation for addressing lost stake and lost ADA is grounded in preserving fairness, efficiency, and the long-term health of Cardano’s proof-of-stake and governance mechanisms. At present, the problem may seem minor or largely theoretical, as lost ADA is not immediately visible on a small scale. But the impact compounds over time, and proactive understanding is crucial.

Even a modest annual loss rate combined with ongoing rewards can, in theory, lead to exponential increases in the amount of ADA effectively trapped as lost stake. Over decades, lost ADA (plus the rewards it continually accrues) could constitute an ever-growing share of the total circulating supply.  As the proportion of rewards and transaction fees paid to lost ADA grows, the situation would become increasingly unacceptable to active users. At some point, most users would likely abandon the ecosystem rather than continue subsidizing unreachable addresses, making such runaway growth of lost stake unsustainable. Without a mechansism to prevent continous compounding of lost stake it is expected that reward dilution will become more severe, stake pools and governance will become heavily influenced by non-recoverable funds, and the active Cardano community would be supporting an increasing “dead weight” in the ecosystem until a breaking point is reached.


## Goals

- **Reward Fairness:** Cardano’s reward mechanism is zero-sum—if a portion goes to inaccessible wallets, everyone else simply gets less. Active delegators and SPOs should not have their rewards continuously diminished by wallets that no one can ever use. Over long periods, this will erode the attractiveness of staking for newcomers (who would see lower returns because part of the yield is effectively burned by lost stake and lost ADA).

- **Governance Legitimacy:** For on-chain governance to be legitimate and effective, voting power should reflect real, engaged stakeholders. If a growing percentage of voting power is tied up in lost ADA (delegated to dReps or otherwise), it calls into question how representative the outcomes are. In the worst case, crucial governance actions might face quorum issues or skewed results due to a bloc of inactive stake that cannot be mobilized or removed. The community will become disenfranchised if “votes” are attributed to lost ADA swing decisions.

- **Decentralization and Dynamism:** A healthy PoS ecosystem relies on the ability of stakeholders to move, re-delegate, or withdraw their stake in response to performance and incentives. Lost stake undermines this dynamism. It introduces static pools of stake that remain in place regardless of performance, potentially propping up some pools or dReps indefinitely. This will slow down the natural reallocation of stake that helps decentralization (e.g., shifting away from an underperforming or oversaturated pool) because some portion of stake simply cannot move. In extreme scenarios, network adaptability and resilience will likely suffer.

- **Economic Sustainability:** In the long term, as block-reward inflation tapers off, Cardano’s security will hinge on transaction fees and community participation. If a significant chunk of ADA is effectively out of economic circulation (yet still “consuming” rewards/fees), it means the active economy has to carry that burden. The security budget (total incentives for validators) would be partially drained to non-participants. This inefficiency will necessitate higher fees or protocol changes to compensate, which is undesirable for growth. In short, allowing lost stake and lost ADA to grow unchecked may undermine the sustainability of the network’s incentive model.


## Open Questions
- How can the protocol reliably identify truly lost credentials?
- Could an inactivity period (epochs/years) be acceptable before stake is considered “lost”?
- Which economic / social mechanisms can prevent reward dilution without violating property rights?
- Would the introduction of some solution to lost stake violate some of prior promises from the protocol?

## References

- **Cardano Improvement Proposal 1694 (CIP-1694):** On-Chain Decentralized Governance
  [cips.cardano.org](https://cips.cardano.org/cips/cip1694/)

- **Ledger Academy – “How Many Bitcoin Are Lost?”**
  [ledger.com](https://www.ledger.com/academy/topics/economics-and-regulation/how-many-bitcoin-are-lost-ledger)

- **Chainalysis – “Money Supply: What Does It Mean for Crypto?”**
  [chainalysis.com](https://www.chainalysis.com/blog/money-supply/)

- **Wall Street Journal Analysis of Lost Bitcoin:**
  [[investopedia.com](https://www.investopedia.com/tech/how-much-bitcoin-has-been-lost/)]

## Acknowledgements

<details>
  <summary><strong>Community SPO Incentives Working Group</strong></summary>

This CPS could not have been created without the support, assistance, and input of all participants in the community-led SPO Incentives Working Group.

- Stef M [RABIT]
- Rich Manderino [ECP]
- Wayne Cataldo [OTG]
- Homer [AAA]
- Chad [BBHMM]
- Mark H [UPSTR]
- Carlos Lopez de Lara [Input|Output]
- Pedro Lucas
- Seomon
- OYSTR Pool

</details>

## Copyright

This CPS is licensed under [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode).

---
Source: https://raw.githubusercontent.com/cardano-foundation/CIPs/refs/heads/master/CODE_OF_CONDUCT.md
---

# Contributor Covenant Code of Conduct

## Our Pledge

In the interest of fostering an open and welcoming environment, we as
contributors and maintainers pledge to making participation in our project and
our community a harassment-free experience for everyone, regardless of age, body
size, disability, ethnicity, sex characteristics, gender identity and expression,
level of experience, education, socio-economic status, nationality, personal
appearance, race, religion, or sexual identity and orientation.

## Our Standards

Examples of behavior that contributes to creating a positive environment
include:

- Using welcoming and inclusive language
- Being respectful of differing viewpoints and experiences
- Gracefully accepting constructive criticism
- Focusing on what is best for the community
- Showing empathy towards other community members

Examples of unacceptable behavior by participants include:

- The use of sexualized language or imagery and unwelcome sexual attention or
 advances
- Trolling, insulting/derogatory comments, and personal or political attacks
- Public or private harassment
- Publishing others' private information, such as a physical or electronic
 address, without explicit permission
- Other conduct which could reasonably be considered inappropriate in a
 professional setting

## Our Responsibilities

Project maintainers are responsible for clarifying the standards of acceptable
behavior and are expected to take appropriate and fair corrective action in
response to any instances of unacceptable behavior.

Project maintainers have the right and responsibility to remove, edit, or
reject comments, commits, code, wiki edits, issues, and other contributions
that are not aligned to this Code of Conduct, or to ban temporarily or
permanently any contributor for other behaviors that they deem inappropriate,
threatening, offensive, or harmful.

## Scope

This Code of Conduct applies both within project spaces and in public spaces
when an individual is representing the project or its community. Examples of
representing a project or community include using an official project e-mail
address, posting via an official social media account, or acting as an appointed
representative at an online or offline event. Representation of a project may be
further defined and clarified by project maintainers.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported by contacting the project team at cip@cardanofoundation.org. All
complaints will be reviewed and investigated and will result in a response that
is deemed necessary and appropriate to the circumstances. The project team is
obligated to maintain confidentiality with regard to the reporter of an incident.
Further details of specific enforcement policies may be posted separately.

Project maintainers who do not follow or enforce the Code of Conduct in good
faith may face temporary or permanent repercussions as determined by other
members of the project's leadership.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,
available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html

[homepage]: https://www.contributor-covenant.org

For answers to common questions about this code of conduct, see
https://www.contributor-covenant.org/faq
